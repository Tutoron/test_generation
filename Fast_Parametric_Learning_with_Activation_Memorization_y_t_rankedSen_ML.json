{"doc_name": "Fast_Parametric_Learning_with_Activation_Memorization", "symbol_expr": "y_{t}", "ranked_sentence": [" Here,  $\\mathbb{I}\\{y_{t}\\}$  is the one-hot target vector,  $V$  denotes the vocabulary of classes, and  $c_{t}$  is defined to be a counter of class occurrences during training \u2014 which is used to anneal  $\\lambda_{t}$  as described in ( 4 )", "$$\\theta[y_{t}]$$", " This is interpolated with the previous layer\u2019s hidden activation  $h_{t}$  for the active class  $y_{t}$ ,", "Motivated from these memory systems, we explore a very simple optimization procedure where the network accumulates activations  $h_{t}$  directly into the softmax layer weights  $\\theta[y_{t}]$  when a class  $y_{t}$  has been seen a small number of times, and uses gradient descent otherwise", "$$\\mathbb{I}\\{y_{t}\\}$$", "$$\\displaystyle\\hat{\\theta}_{t+0.5}[i]\\leftarrow\\begin{cases}\\theta_{t}[i]-%\n\\alpha\\,(p_{i}-1)\\,h_{t}&i=y_{t}\\\\\n\\theta_{t}[i]-\\alpha\\,p_{i}\\,h_{t}&i\\neq y_{t}\\end{cases}$$", " Here the vector  $\\hat{\\theta}_{t+05}$  denotes the parameters  $\\theta_{t}[y_{t}]$  of the final layer softmax corresponding to the active class  $y_{t}$  after one step of gradient descent", "where  $\\mathcal{N}_{k}(h_{t})$  refers to the  $k$  nearest parameters to the activation  $h_{t}$  that do not correspond to  $y_{t}$ , the class label", "$$\\theta_{t}[y_{t}]$$", "$$\\displaystyle\\theta_{t+1}[i]\\leftarrow\\begin{cases}\\lambda_{t}\\,h_{t}+(1-%\n\\lambda_{t})\\,\\hat{\\theta}_{t+0.5}[i]&i=y_{t}\\\\\n\\hat{\\theta}_{t+0.5}[i]&i\\neq y_{t}\\;,\\end{cases}$$", " When  $\\lambda_{t}=1$  this corresponds to the rule  $\\theta_{t+1}\\leftarrow h_{t}\\cdot\\mathbb{I}\\{y_{t}\\}$  where  $\\mathbb{I}\\{y_{t}\\}\\in[0,1]^{m}$  is a one-hot target vector", " Accumulating or smoothing network activations into the weights actually corresponds to the well-known Hebbian learning update rule  $W[i,j]\\leftarrow\\frac{1}{n}\\sum_{t=1}^{n}x_{t}^{i}x_{t}^{j}$   \\citep hebb1949organization in the special case of classification on the output layer, where  $W,x_{t}^{i},x_{t}^{j}$  correspond to  $\\theta,h_{t},y_{t}$  respectively", " The value  $\\theta[y_{t}]$  would indeed be pulled towards a large inner product with  $h_{t}$ , however neighbouring parameters  $\\theta[i];\\;i\\neq y_{t}$  would be pushed towards a large negative inner product with  $h_{t}$  and this could lead to catastrophic forgetting of previously consolidated classes", "$$\\theta_{t+1}\\leftarrow h_{t}\\cdot\\mathbb{I}\\{y_{t}\\}$$", "$$x_{j}=\\mathbb{I}\\{y_{t}\\}$$", "$$\\mathbb{I}\\{y_{t}\\}\\in[0,1]^{m}$$", "$$y_{t}$$", "$$\\theta,h_{t},y_{t}$$", "$$\\lambda_{t}=1/c[y_{t}]$$", "$$y_{t}$$", "$$\\theta[y_{t}]$$", "$\\displaystyle\\hat{\\theta}_{t+05}[i]\\leftarrow\\begin{cases}\\theta_{t}[i]-%\n\\alpha\\,(p_{i}-1)\\,h_{t}&i=y_{t}\\\\\n\\theta_{t}[i]-\\alpha\\,p_{i}\\,h_{t}&i\\neq y_{t}\\end{cases}$", " a constant  $\\lambda$  or pure annealing  $\\lambda_{t}=1/c[y_{t}]$ ", "$$y_{t}$$", "$\\displaystyle\\theta_{t+1}[i]\\leftarrow\\begin{cases}\\lambda_{t}\\,h_{t}+(1-%\n\\lambda_{t})\\,\\hat{\\theta}_{t+05}[i]&i=y_{t}\\\\\n\\hat{\\theta}_{t+05}[i]&i\\neq y_{t}\\;,\\end{cases}$", "$$\\theta[i];\\;i\\neq y_{t}$$", " In this case Hebbian update rule,  $W_{ij}\\leftarrow x_{i}x_{j}$  for  $x_{i}=h_{t}$  the hidden output and  $x_{j}=\\mathbb{I}\\{y_{t}\\}$  the target", "$\\lambda_{t}=\\max(1\\,/\\,\\mathbf{c}[y_{t}],\\;\\gamma)\\,\\cdot\\,\\mathbb{I}\\{\\mathbf%\n{c}[y_{t}]<T\\}$", "$$\\lambda_{t}=\\max(1\\,/\\,\\mathbf{c}[y_{t}],\\;\\gamma)\\,\\cdot\\,\\mathbb{I}\\{\\mathbf%\n{c}[y_{t}]<T\\}$$", "$$y_{t}$$"]}