{"doc_name": "Input_Convex_Neural_Networks", "symbol_expr": "\\mathcal{Y}", "sentences": [{"type": "text", "expr": "\nGiven (typically high-dimensional) structured input and output spaces  $\\mathcal{X}\\times\\mathcal{Y}$ , we can build a network over  $(x,y)$  pairs\nthat encodes the energy function for this pair, following typical energy-based\nlearning formalisms  \\citep lecun2006tutorial", "word_idx": 4048, "sentence_idx": 43, "label": "definition"}, {"type": "text", "expr": " Prediction involves finding the\n $y\\in\\mathcal{Y}$  that\nminimizes the energy for a given  $x$ , which is exactly\nthe argmin problem in ( 1 )", "word_idx": 4321, "sentence_idx": 44, "label": "usecase"}, {"type": "text", "expr": "\nIn our setting, assuming that  $\\mathcal{Y}$  is a convex space (a common\nassumption in structured prediction), this optimization\nproblem is convex", "word_idx": 4463, "sentence_idx": 45, "label": "usecase"}, {"type": "math", "expr": "$$\\mathcal{X}\\times\\mathcal{Y}$$", "word_idx": 4903, "sentence_idx": 47, "label": "none"}, {"type": "math", "expr": "$$y\\in\\mathcal{Y}$$", "word_idx": 4942, "sentence_idx": 50, "label": "none"}, {"type": "math", "expr": "$$\\mathcal{Y}$$", "word_idx": 4957, "sentence_idx": 51, "label": "unknown"}, {"type": "text", "expr": "Similar to structured prediction\nbut slightly more generic, if we are given some space  $\\mathcal{Y}$ \nwe can learn a network  $f(y;\\theta)$  (removing the additional  $x$ \ninputs, though these can be added as well) that, given an example with\nsome subset  $\\mathcal{I}$  missing, imputes the likely values of these variables\nby solving the optimization problem as above  $\\hat{y}_{\\mathcal{I}}=\\argmin_{y_{\\mathcal{I}}}f(y_{\\mathcal{I}},y_{\\bar{%\n\\mathcal{I}}};\\theta)$ \nThis could be used e", "word_idx": 4989, "sentence_idx": 54, "label": "usecase"}, {"type": "math", "expr": "$$\\mathcal{Y}$$", "word_idx": 5588, "sentence_idx": 56, "label": "unknown"}, {"type": "text", "expr": " We can recover a feedforward network by\nnoting that a traditional feedforward network  $\\hat{f}(x;\\theta)$  where  $f:\\mathcal{X}\\rightarrow\\mathcal{Y}$ ,\ncan be viewed as a network with an inner\nproduct  $f(x;\\theta)^{T}y$  in its last layer\n(see e", "word_idx": 20883, "sentence_idx": 195, "label": "usecase"}, {"type": "math", "expr": "$$f:\\mathcal{X}\\rightarrow\\mathcal{Y}$$", "word_idx": 21407, "sentence_idx": 200, "label": "none"}, {"type": "text", "expr": "$\\minimize_{y\\in\\mathcal{Y}}f(x,y;\\theta)$", "word_idx": 21787, "sentence_idx": 209, "label": "none"}, {"type": "math", "expr": "$$\\minimize_{y\\in\\mathcal{Y}}f(x,y;\\theta)$$", "word_idx": 21829, "sentence_idx": 210, "label": "none"}, {"type": "text", "expr": "$\\hat{y}\\leftarrow\\mathcal{P}_{\\mathcal{Y}}\\left(\\hat{y}-\\alpha\\nabla_{y}f(x,%\n\\hat{y};\\theta)\\right)$", "word_idx": 23681, "sentence_idx": 229, "label": "none"}, {"type": "math", "expr": "$$\\hat{y}\\leftarrow\\mathcal{P}_{\\mathcal{Y}}\\left(\\hat{y}-\\alpha\\nabla_{y}f(x,%\n\\hat{y};\\theta)\\right)$$", "word_idx": 23783, "sentence_idx": 230, "label": "none"}, {"type": "text", "expr": " This algorithm\nspecifically applies to the (common) case where  $\\mathcal{Y}$ \nis bounded, which we assume to be  $\\mathcal{Y}=[0,1]^{n}$ \n(other upper or lower bounds can be attained through scaling)", "word_idx": 25322, "sentence_idx": 238, "label": "definition"}, {"type": "text", "expr": "\nThe method is also easily extensible to the setting where elements\nof  $\\mathcal{Y}$  belong to a higher-dimensional probability simplex as well", "word_idx": 25523, "sentence_idx": 239, "label": "none"}, {"type": "math", "expr": "$$\\mathcal{Y}$$", "word_idx": 25695, "sentence_idx": 242, "label": "unknown"}, {"type": "math", "expr": "$$\\mathcal{Y}=[0,1]^{n}$$", "word_idx": 25706, "sentence_idx": 243, "label": "usecase"}, {"type": "math", "expr": "$$\\mathcal{Y}$$", "word_idx": 25727, "sentence_idx": 244, "label": "unknown"}, {"type": "text", "expr": " The function acts as a barrier because,\nalthough it does not approach infinity as it reaches the barrier of the feasible\nset, its gradient  does  approach infinity as it reaches the barrier, and\nthus the optimal solution will always lie in the interior of the unit hypercube\n $\\mathcal{Y}$ ", "word_idx": 26325, "sentence_idx": 251, "label": "usecase"}, {"type": "math", "expr": "$$\\mathcal{Y}$$", "word_idx": 26702, "sentence_idx": 254, "label": "unknown"}, {"type": "text", "expr": "$y^{k+1}:=\\argmin_{y\\in\\mathcal{Y}}\\max_{1\\leq i\\leq k}\\{f(x,y^{i};\\theta)+%\n\\nabla_{y}f(x,y^{i};\\theta)^{T}(y-y^{i})\\}$", "word_idx": 76924, "sentence_idx": 1045, "label": "none"}, {"type": "math", "expr": "$$y^{k+1}:=\\argmin_{y\\in\\mathcal{Y}}\\max_{1\\leq i\\leq k}\\{f(x,y^{i};\\theta)+%\n\\nabla_{y}f(x,y^{i};\\theta)^{T}(y-y^{i})\\}.$$", "word_idx": 77044, "sentence_idx": 1046, "label": "none"}, {"type": "text", "expr": "$y^{k+1},t^{k+1}:=\\argmin_{y\\in\\mathcal{Y},t}\\;\\;\\{t\\mid Gy+h\\leq t1\\}$", "word_idx": 77268, "sentence_idx": 1048, "label": "none"}, {"type": "math", "expr": "$$y^{k+1},t^{k+1}:=\\argmin_{y\\in\\mathcal{Y},t}\\;\\;\\{t\\mid Gy+h\\leq t1\\}$$", "word_idx": 77339, "sentence_idx": 1049, "label": "none"}, {"type": "text", "expr": "$y^{k+1}$ ,  $t^{k+1}$   $\\leftarrow$ $\\argmin_{y\\in\\mathcal{Y},t}\\;\\;\\{t\\mid G_{1:k}y+h_{1:k}\\leq t1\\}$", "word_idx": 78605, "sentence_idx": 1085, "label": "none"}, {"type": "math", "expr": "$$\\argmin_{y\\in\\mathcal{Y},t}\\;\\;\\{t\\mid G_{1:k}y+h_{1:k}\\leq t1\\}$$", "word_idx": 78733, "sentence_idx": 1089, "label": "none"}, {"type": "text", "expr": "$\\begin{split}\\displaystyle\\minimize_{\\theta,\\xi\\geq 0}&\\displaystyle\\frac{%\n\\lambda}{2}\\|\\theta\\|_{2}^{2}+\\sum_{i=1}^{m}\\xi_{i}\\\\\n\\displaystyle\\subjectto&\\displaystyle\\tilde{f}(x_{i},y_{i};\\theta)\\leq\\min_{y%\n\\in\\mathcal{Y}}\\left(\\tilde{f}(x_{i},y;\\theta)-\\Delta(y_{i},y)\\right)-\\xi_{i}%\n\\end{split}$", "word_idx": 84535, "sentence_idx": 1265, "label": "none"}, {"type": "math", "expr": "$$\\begin{split}\\displaystyle\\minimize_{\\theta,\\xi\\geq 0}&\\displaystyle\\frac{%\n\\lambda}{2}\\|\\theta\\|_{2}^{2}+\\sum_{i=1}^{m}\\xi_{i}\\\\\n\\displaystyle\\subjectto&\\displaystyle\\tilde{f}(x_{i},y_{i};\\theta)\\leq\\min_{y%\n\\in\\mathcal{Y}}\\left(\\tilde{f}(x_{i},y;\\theta)-\\Delta(y_{i},y)\\right)-\\xi_{i}%\n\\end{split}$$", "word_idx": 84836, "sentence_idx": 1266, "label": "none"}, {"type": "text", "expr": "$y^{\\star}=\\argmin_{y\\in\\mathcal{Y}}f(x_{i},y;\\theta)-\\Delta(y_{i},y)$", "word_idx": 85945, "sentence_idx": 1278, "label": "none"}, {"type": "math", "expr": "$$y^{\\star}=\\argmin_{y\\in\\mathcal{Y}}f(x_{i},y;\\theta)-\\Delta(y_{i},y)$$", "word_idx": 86015, "sentence_idx": 1279, "label": "none"}]}