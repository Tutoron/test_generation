{"sparse_tensor_generation": [{"type": "text", "expr": "Fredrik Kjolstad", "word_idx": 0, "sentence_idx": 0, "label": "unlabeled"}, {"type": "text", "expr": "Fredrik Kjolstad", "word_idx": 16, "sentence_idx": 1, "label": "unlabeled"}, {"type": "text", "expr": "fred@csail", "word_idx": 32, "sentence_idx": 2, "label": "unlabeled"}, {"type": "text", "expr": "fred@csail", "word_idx": 42, "sentence_idx": 3, "label": "unlabeled"}, {"type": "text", "expr": "Shoaib Kamil", "word_idx": 52, "sentence_idx": 4, "label": "unlabeled"}, {"type": "text", "expr": "Shoaib Kamil", "word_idx": 64, "sentence_idx": 5, "label": "unlabeled"}, {"type": "text", "expr": "Adobe Research, USA", "word_idx": 76, "sentence_idx": 6, "label": "unlabeled"}, {"type": "text", "expr": "Adobe Research, USA", "word_idx": 95, "sentence_idx": 7, "label": "unlabeled"}, {"type": "text", "expr": "kamil@adobe", "word_idx": 114, "sentence_idx": 8, "label": "unlabeled"}, {"type": "text", "expr": "kamil@adobe", "word_idx": 125, "sentence_idx": 9, "label": "unlabeled"}, {"type": "text", "expr": "Saman Amarasinghe", "word_idx": 136, "sentence_idx": 10, "label": "unlabeled"}, {"type": "text", "expr": "Saman Amarasinghe", "word_idx": 153, "sentence_idx": 11, "label": "unlabeled"}, {"type": "text", "expr": "saman@csail", "word_idx": 170, "sentence_idx": 12, "label": "unlabeled"}, {"type": "text", "expr": "saman@csail", "word_idx": 181, "sentence_idx": 13, "label": "unlabeled"}, {"type": "text", "expr": "Abstract Recent advances in compiler theory describe how to compile sparse tensor\nalgebra", "word_idx": 192, "sentence_idx": 14, "label": "unlabeled"}, {"type": "text", "expr": "\nPrior work, however, does not describe how to generate efficient code that\ntakes advantage of temporary workspaces", "word_idx": 281, "sentence_idx": 15, "label": "unlabeled"}, {"type": "text", "expr": " These are often used to\nhand-optimize important kernels such as sparse matrix multiplication and the\nmatricized tensor times Khatri-Rao product", "word_idx": 396, "sentence_idx": 16, "label": "unlabeled"}, {"type": "text", "expr": "\nWithout this capability, compilers and code generators cannot automatically\ngenerate efficient kernels for many important tensor algebra expressions", "word_idx": 540, "sentence_idx": 17, "label": "unlabeled"}, {"type": "text", "expr": "\nWe describe a compiler optimization called operator splitting that breaks up\ntensor sub-computations by introducing workspaces", "word_idx": 689, "sentence_idx": 18, "label": "unlabeled"}, {"type": "text", "expr": "\nOur case studies demonstrate that operator splitting is surprisingly general,\nand our results show that it increases the performance of important generated\ntensor kernels to match hand-optimized code", "word_idx": 816, "sentence_idx": 19, "label": "unlabeled"}, {"type": "text", "expr": "Abstract", "word_idx": 1016, "sentence_idx": 20, "label": "unlabeled"}, {"type": "text", "expr": "Recent advances in compiler theory describe how to compile sparse tensor\nalgebra", "word_idx": 1024, "sentence_idx": 21, "label": "unlabeled"}, {"type": "text", "expr": "\nPrior work, however, does not describe how to generate efficient code that\ntakes advantage of temporary workspaces", "word_idx": 1104, "sentence_idx": 22, "label": "unlabeled"}, {"type": "text", "expr": " These are often used to\nhand-optimize important kernels such as sparse matrix multiplication and the\nmatricized tensor times Khatri-Rao product", "word_idx": 1219, "sentence_idx": 23, "label": "unlabeled"}, {"type": "text", "expr": "\nWithout this capability, compilers and code generators cannot automatically\ngenerate efficient kernels for many important tensor algebra expressions", "word_idx": 1363, "sentence_idx": 24, "label": "unlabeled"}, {"type": "text", "expr": "\nWe describe a compiler optimization called operator splitting that breaks up\ntensor sub-computations by introducing workspaces", "word_idx": 1512, "sentence_idx": 25, "label": "unlabeled"}, {"type": "text", "expr": "\nOur case studies demonstrate that operator splitting is surprisingly general,\nand our results show that it increases the performance of important generated\ntensor kernels to match hand-optimized code", "word_idx": 1639, "sentence_idx": 26, "label": "unlabeled"}, {"type": "text", "expr": "sparse tensors, tensor algebra, linear algebra, optimization, operator split, workspaces, performance", "word_idx": 1839, "sentence_idx": 27, "label": "unlabeled"}, {"type": "text", "expr": "\\settopmatter", "word_idx": 1940, "sentence_idx": 28, "label": "unlabeled"}, {"type": "text", "expr": "printfolios=true,printccs=false,printacmref=false \\startPage 1 \\setcopyright none \\definecolor todocolorrgb0", "word_idx": 1953, "sentence_idx": 29, "label": "unlabeled"}, {"type": "text", "expr": "8,0,0 \\definecolor keywordcolorrgb0", "word_idx": 2061, "sentence_idx": 30, "label": "unlabeled"}, {"type": "text", "expr": "5,0,0", "word_idx": 2096, "sentence_idx": 31, "label": "unlabeled"}, {"type": "text", "expr": "5 \\definecolor textgraygray0", "word_idx": 2101, "sentence_idx": 32, "label": "unlabeled"}, {"type": "text", "expr": "\\startPage", "word_idx": 2129, "sentence_idx": 33, "label": "unlabeled"}, {"type": "text", "expr": "\\setcopyright", "word_idx": 2139, "sentence_idx": 34, "label": "unlabeled"}, {"type": "text", "expr": "\\definecolor", "word_idx": 2152, "sentence_idx": 35, "label": "unlabeled"}, {"type": "text", "expr": "\\definecolor", "word_idx": 2164, "sentence_idx": 36, "label": "unlabeled"}, {"type": "text", "expr": "\\definecolor", "word_idx": 2176, "sentence_idx": 37, "label": "unlabeled"}, {"type": "text", "expr": "Automatic Generation of Sparse Tensor Kernels with Workspaces]Automatic Generation of Sparse Tensor Kernels\n with Workspaces", "word_idx": 2188, "sentence_idx": 38, "label": "unlabeled"}, {"type": "text", "expr": "1  Introduction", "word_idx": 2312, "sentence_idx": 39, "label": "unlabeled"}, {"type": "text", "expr": "Tensor algebra is an important tool for computing on multi-mode data in domains\nsuch as machine learning\u00a0 , data\nanalytics\u00a0 , engineering\u00a0 ,\nand science\u00a0 ", "word_idx": 2327, "sentence_idx": 40, "label": "unlabeled"}, {"type": "text", "expr": " Tensors generalize matrices to\nany number of dimensions and can model both linear and multilinear\nrelationships", "word_idx": 2481, "sentence_idx": 41, "label": "unlabeled"}, {"type": "text", "expr": " Interesting tensors are often sparse, which means most\ncomponents are zero", "word_idx": 2593, "sentence_idx": 42, "label": "unlabeled"}, {"type": "text", "expr": " Compressing these tensors is often necessary", "word_idx": 2668, "sentence_idx": 43, "label": "unlabeled"}, {"type": "text", "expr": " For\nexample, a tensor encoding Amazon reviews\u00a0  used to predict user\nresponse to a new product\u00a0  contains 15 quintillion\ncomponents where only 1", "word_idx": 2713, "sentence_idx": 44, "label": "unlabeled"}, {"type": "text", "expr": "7 billion are nonzeros", "word_idx": 2858, "sentence_idx": 45, "label": "unlabeled"}, {"type": "text", "expr": "Abadi et\u00a0al", "word_idx": 2880, "sentence_idx": 46, "label": "unlabeled"}, {"type": "text", "expr": " (2016)", "word_idx": 2891, "sentence_idx": 47, "label": "unlabeled"}, {"type": "text", "expr": "Bader\net\u00a0al", "word_idx": 2898, "sentence_idx": 48, "label": "unlabeled"}, {"type": "text", "expr": " (2008)", "word_idx": 2909, "sentence_idx": 49, "label": "unlabeled"}, {"type": "text", "expr": "Anandkumar et\u00a0al", "word_idx": 2916, "sentence_idx": 50, "label": "unlabeled"}, {"type": "text", "expr": " (2014)", "word_idx": 2932, "sentence_idx": 51, "label": "unlabeled"}, {"type": "text", "expr": "Kolecki (2002)", "word_idx": 2939, "sentence_idx": 52, "label": "unlabeled"}, {"type": "text", "expr": "Einstein (1916)", "word_idx": 2953, "sentence_idx": 53, "label": "unlabeled"}, {"type": "text", "expr": "Feynman\net\u00a0al", "word_idx": 2968, "sentence_idx": 54, "label": "unlabeled"}, {"type": "text", "expr": " (1963)", "word_idx": 2981, "sentence_idx": 55, "label": "unlabeled"}, {"type": "text", "expr": "Smith et\u00a0al", "word_idx": 2988, "sentence_idx": 56, "label": "unlabeled"}, {"type": "text", "expr": " (2017b)", "word_idx": 2999, "sentence_idx": 57, "label": "unlabeled"}, {"type": "text", "expr": "McAuley and\nLeskovec (2013)", "word_idx": 3007, "sentence_idx": 58, "label": "unlabeled"}, {"type": "text", "expr": "Leskovec and\nKrevl (2014)", "word_idx": 3034, "sentence_idx": 59, "label": "unlabeled"}, {"type": "text", "expr": "We recently proposed a compiler theory that shows us how to automatically\ngenerate kernels for tensor algebra expressions with both dense and sparse\ntensors\u00a0 ", "word_idx": 3059, "sentence_idx": 60, "label": "unlabeled"}, {"type": "text", "expr": " We implemented the theory in a library and showed\nthat the performance of many generated kernels is competitive with the\nperformance of hand-optimized kernels in existing libraries", "word_idx": 3217, "sentence_idx": 61, "label": "unlabeled"}, {"type": "text", "expr": "Kjolstad et\u00a0al", "word_idx": 3398, "sentence_idx": 62, "label": "unlabeled"}, {"type": "text", "expr": " (2017b)", "word_idx": 3412, "sentence_idx": 63, "label": "unlabeled"}, {"type": "text", "expr": "Our previous approach, however, did not generate kernels that take advantage of\ndense temporary workspace arrays to accumulate temporary values", "word_idx": 3420, "sentence_idx": 64, "label": "unlabeled"}, {"type": "text", "expr": " Such arrays\nare a common optimization technique in sparse linear algebra algorithms, such\nas Gustavson\u2019s sparse matrix multiplication (SpMM)\u00a0 , and was applied to the matricized tensor times\nKhatri-Rao product (MTTKRP) by\n Smith et\u00a0al", "word_idx": 3563, "sentence_idx": 65, "label": "unlabeled"}, {"type": "text", "expr": " Without workspaces,\nkernels underperform due to expensive insertions into sparse tensors, branches\nfrom code to merge sparse tensors, and redundant loop-invariant work", "word_idx": 3798, "sentence_idx": 66, "label": "unlabeled"}, {"type": "text", "expr": "Gustavson (1978)", "word_idx": 3966, "sentence_idx": 67, "label": "unlabeled"}, {"type": "text", "expr": "Pissanetzky (1984)", "word_idx": 3982, "sentence_idx": 68, "label": "unlabeled"}, {"type": "text", "expr": "Gilbert\net\u00a0al", "word_idx": 4000, "sentence_idx": 69, "label": "unlabeled"}, {"type": "text", "expr": " (1992)", "word_idx": 4013, "sentence_idx": 70, "label": "unlabeled"}, {"type": "text", "expr": "Smith et\u00a0al", "word_idx": 4020, "sentence_idx": 71, "label": "unlabeled"}, {"type": "text", "expr": "2017a", "word_idx": 4031, "sentence_idx": 72, "label": "unlabeled"}, {"type": "text", "expr": "This paper introduces a novel compiler optimization called operator splitting\non our iteration graph tensor algebra intermediate\nrepresentation\u00a0 ", "word_idx": 4036, "sentence_idx": 73, "label": "unlabeled"}, {"type": "text", "expr": " An operator split breaks up a tensor\nsub-expression with respect to an index variable and introduces a dense\nworkspace to store intermediate results", "word_idx": 4181, "sentence_idx": 74, "label": "unlabeled"}, {"type": "text", "expr": " Operator splitting reveals that\nprevious proposed algorithms that use workspaces for different purposes are the\nresults of the same transformation", "word_idx": 4330, "sentence_idx": 75, "label": "unlabeled"}, {"type": "text", "expr": " Our case studies show that operator\nsplitting is a general optimization that applies to many important tensor\nexpressions, including sparse matrix multiplication with the linear combination\nalgorithm, sparse matrix addition, and the matricized tensor times Khatri-Rao\nproduct", "word_idx": 4477, "sentence_idx": 76, "label": "unlabeled"}, {"type": "text", "expr": "2017b", "word_idx": 4753, "sentence_idx": 77, "label": "unlabeled"}, {"type": "text", "expr": "The key contributions are:", "word_idx": 4758, "sentence_idx": 78, "label": "unlabeled"}, {"type": "text", "expr": "The key contributions are:", "word_idx": 4784, "sentence_idx": 79, "label": "unlabeled"}, {"type": "text", "expr": "Formulation", "word_idx": 4810, "sentence_idx": 80, "label": "unlabeled"}, {"type": "text", "expr": "We identify the lack of workspaces in the tensor\ncompilation literature", "word_idx": 4821, "sentence_idx": 81, "label": "unlabeled"}, {"type": "text", "expr": "We identify the lack of workspaces in the tensor\ncompilation literature", "word_idx": 4892, "sentence_idx": 82, "label": "unlabeled"}, {"type": "text", "expr": "Operator Split", "word_idx": 4963, "sentence_idx": 83, "label": "unlabeled"}, {"type": "text", "expr": "We introduce a compiler optimization that removes\nexpensive inserts into sparse results, eliminates merge code, and hoists\nloop invariant code", "word_idx": 4977, "sentence_idx": 84, "label": "unlabeled"}, {"type": "text", "expr": "We introduce a compiler optimization that removes\nexpensive inserts into sparse results, eliminates merge code, and hoists\nloop invariant code", "word_idx": 5119, "sentence_idx": 85, "label": "unlabeled"}, {"type": "text", "expr": "Applicability", "word_idx": 5261, "sentence_idx": 86, "label": "unlabeled"}, {"type": "text", "expr": "We define preconditions for applying operator splits and\ndiscuss their trade-offs", "word_idx": 5274, "sentence_idx": 87, "label": "unlabeled"}, {"type": "text", "expr": "We define preconditions for applying operator splits and\ndiscuss their trade-offs", "word_idx": 5355, "sentence_idx": 88, "label": "unlabeled"}, {"type": "text", "expr": "Case Studies", "word_idx": 5436, "sentence_idx": 89, "label": "unlabeled"}, {"type": "text", "expr": "We show that operator splits recreate several important\nalgorithms with workspaces from the literature and generalizes to important\nnew kernels", "word_idx": 5448, "sentence_idx": 90, "label": "unlabeled"}, {"type": "text", "expr": "We show that operator splits recreate several important\nalgorithms with workspaces from the literature and generalizes to important\nnew kernels", "word_idx": 5591, "sentence_idx": 91, "label": "unlabeled"}, {"type": "text", "expr": "Finally, we evaluate operator splitting by showing that the performance of the\nresulting code is competitive with hand-optimized implementations with\nworkspaces in the MKL, Eigen, and SPLATT high-performance\nlibraries\u00a0 ", "word_idx": 5734, "sentence_idx": 92, "label": "unlabeled"}, {"type": "text", "expr": "2  Motivating Example", "word_idx": 5953, "sentence_idx": 93, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 5974, "sentence_idx": 94, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 5985, "sentence_idx": 95, "label": "unlabeled"}, {"type": "text", "expr": "Sparse CSR index of  $B$", "word_idx": 5996, "sentence_idx": 96, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 6020, "sentence_idx": 97, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 6031, "sentence_idx": 98, "label": "unlabeled"}, {"type": "text", "expr": "Dense  $m\\times o$  matrix  $B$", "word_idx": 6042, "sentence_idx": 99, "label": "unlabeled"}, {"type": "math", "expr": "$$m\\times o$$", "word_idx": 6073, "sentence_idx": 100, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 6082, "sentence_idx": 101, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 6093, "sentence_idx": 102, "label": "unlabeled"}, {"type": "text", "expr": "$A_{ij}=\\sum_{k}B_{ik}C_{kj}$  (sparse  $B$ ,  $C$ )", "word_idx": 6104, "sentence_idx": 103, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ij}=\\sum_{k}B_{ik}C_{kj}$$", "word_idx": 6156, "sentence_idx": 104, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 6183, "sentence_idx": 105, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 6194, "sentence_idx": 106, "label": "unlabeled"}, {"type": "text", "expr": "$A_{ij}=\\sum_{k}B_{ik}C_{kj}$  (sparse  $A$ ,  $B$ ,  $C$ )", "word_idx": 6205, "sentence_idx": 107, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ij}=\\sum_{k}B_{ik}C_{kj}$$", "word_idx": 6264, "sentence_idx": 108, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:  \n(a) The CSR sparse matrix index structure of a matrix  $B$ , (b) the matrix\n $B$ , and (c-d) two sparse matrix multiplication kernels written in C", "word_idx": 6291, "sentence_idx": 109, "label": "unlabeled"}, {"type": "text", "expr": " The\nfirst kernel (c) operators on matrices stored in the CSR format and its\nresult is dense", "word_idx": 6449, "sentence_idx": 110, "label": "unlabeled"}, {"type": "text", "expr": " The second kernel (d) also operates on CSR matrices, but\nits result is also a CSR matrix", "word_idx": 6541, "sentence_idx": 111, "label": "unlabeled"}, {"type": "text", "expr": " The second kernel uses a workspace", "word_idx": 6630, "sentence_idx": 112, "label": "unlabeled"}, {"type": "text", "expr": " The\ncode to zero  $A$  is omitted and result indices have been pre-assembled", "word_idx": 6665, "sentence_idx": 113, "label": "unlabeled"}, {"type": "text", "expr": "\nSection\u00a0 6  discusses the code that assembles result\nindices", "word_idx": 6742, "sentence_idx": 114, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:", "word_idx": 6803, "sentence_idx": 115, "label": "unlabeled"}, {"type": "text", "expr": "We will use sparse matrix multiplication to introduce sparse tensor data\nstructures, sparse kernels, and the need for workspaces", "word_idx": 6812, "sentence_idx": 116, "label": "unlabeled"}, {"type": "text", "expr": " The ideas, however,\ngeneralize to higher-order tensor kernels", "word_idx": 6940, "sentence_idx": 117, "label": "unlabeled"}, {"type": "text", "expr": " Matrix multiplication in linear\nalgebra notation is  $A=BC$  and in tensor index notation it is", "word_idx": 7002, "sentence_idx": 118, "label": "unlabeled"}, {"type": "math", "expr": "$$A=BC$$", "word_idx": 7098, "sentence_idx": 119, "label": "unlabeled"}, {"type": "text", "expr": "$A_{ij}=\\sum_{k}B_{ik}C_{kj}$", "word_idx": 7102, "sentence_idx": 120, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ij}=\\sum_{k}B_{ik}C_{kj}.$$", "word_idx": 7131, "sentence_idx": 121, "label": "unlabeled"}, {"type": "text", "expr": "The code in a matrix multiplication kernel depends on the storage formats of\nthe operands and the result", "word_idx": 7159, "sentence_idx": 122, "label": "unlabeled"}, {"type": "text", "expr": " Many matrix storage formats have been proposed in\nthe literature", "word_idx": 7263, "sentence_idx": 123, "label": "unlabeled"}, {"type": "text", "expr": " They can be classified as dense formats that store every\nmatrix component and sparse formats that store only the components that are\nnonzero", "word_idx": 7328, "sentence_idx": 124, "label": "unlabeled"}, {"type": "text", "expr": " Figure\u00a0 1  shows two linear combination of rows matrix\nmultiplication kernels", "word_idx": 7469, "sentence_idx": 125, "label": "unlabeled"}, {"type": "text", "expr": " We will study this algorithm, instead of the inner\nproduct algorithm, because its sparse variant has better asymptotic\ncomplexity\u00a0  and because the inputs are conveniently all\nrow major", "word_idx": 7547, "sentence_idx": 126, "label": "unlabeled"}, {"type": "text", "expr": "Sparse kernels are more involved than dense kernels because they iterate over\nsparse data structures", "word_idx": 7733, "sentence_idx": 127, "label": "unlabeled"}, {"type": "text", "expr": " Figure\u00a0 1  shows a sparse matrix\nmultiplication kernel where the result matrix is stored dense row-major and the\noperand matrices are stored with the compressed sparse row format\n(CSR)\u00a0 ", "word_idx": 7833, "sentence_idx": 128, "label": "unlabeled"}, {"type": "text", "expr": "The CSR format and its column-major CSC sibling are ubiquitous in sparse linear\nalgebra libraries\u00a0  due to their generality and\nperformance", "word_idx": 8020, "sentence_idx": 129, "label": "unlabeled"}, {"type": "text", "expr": " In the CSR format, each matrix row is compressed (only nonzero\ncomponents are stored)", "word_idx": 8159, "sentence_idx": 130, "label": "unlabeled"}, {"type": "text", "expr": " This requires two index arrays to describe the matrix\ncoordinates and positions of the nonzeros", "word_idx": 8245, "sentence_idx": 131, "label": "unlabeled"}, {"type": "text", "expr": " Figure\u00a0 1  shows the CSR\ndata structure of matrix  $B$  in Figure\u00a0 1 ", "word_idx": 8341, "sentence_idx": 132, "label": "unlabeled"}, {"type": "text", "expr": " It consists of the\nindex arrays  B_pos  and  B_idx  and a value array  B ", "word_idx": 8411, "sentence_idx": 133, "label": "unlabeled"}, {"type": "text", "expr": " The\narray  B_idx  contains the column coordinate of each nonzero value in the\ncorresponding position in  B ", "word_idx": 8485, "sentence_idx": 134, "label": "unlabeled"}, {"type": "text", "expr": " The array  B_pos  stores the position\nof the first column coordinate of each row in  B_idx , as well as a\nsentinel with the number of nonzeros ( nnz ) in the matrix", "word_idx": 8593, "sentence_idx": 135, "label": "unlabeled"}, {"type": "text", "expr": " Thus,\ncontiguous values in  B_pos  store the beginning and end\n[inclusive-exclusive) of a row in the arrays  B_idx  and  B ", "word_idx": 8758, "sentence_idx": 136, "label": "unlabeled"}, {"type": "text", "expr": " For\nexample, the column coordinates of the third row are stored in  B_idx  at\npositions  $[$ ,  $)$ ", "word_idx": 8882, "sentence_idx": 137, "label": "unlabeled"}, {"type": "text", "expr": " Moreover, some\nlibraries require the entries within each row to be sorted in order of\nascending coordinate value, which results in faster performance for some\nalgorithms", "word_idx": 8983, "sentence_idx": 138, "label": "unlabeled"}, {"type": "text", "expr": "B_pos", "word_idx": 9153, "sentence_idx": 139, "label": "unlabeled"}, {"type": "text", "expr": "B_idx", "word_idx": 9158, "sentence_idx": 140, "label": "unlabeled"}, {"type": "text", "expr": "B_idx", "word_idx": 9163, "sentence_idx": 141, "label": "unlabeled"}, {"type": "text", "expr": "B_pos", "word_idx": 9168, "sentence_idx": 142, "label": "unlabeled"}, {"type": "text", "expr": "B_idx", "word_idx": 9173, "sentence_idx": 143, "label": "unlabeled"}, {"type": "text", "expr": "B_pos", "word_idx": 9178, "sentence_idx": 144, "label": "unlabeled"}, {"type": "text", "expr": "B_idx", "word_idx": 9183, "sentence_idx": 145, "label": "unlabeled"}, {"type": "text", "expr": "B_idx", "word_idx": 9188, "sentence_idx": 146, "label": "unlabeled"}, {"type": "text", "expr": "B_pos [2]", "word_idx": 9193, "sentence_idx": 147, "label": "unlabeled"}, {"type": "text", "expr": "B_pos", "word_idx": 9202, "sentence_idx": 148, "label": "unlabeled"}, {"type": "text", "expr": "B_pos [3]", "word_idx": 9207, "sentence_idx": 149, "label": "unlabeled"}, {"type": "text", "expr": "B_pos", "word_idx": 9216, "sentence_idx": 150, "label": "unlabeled"}, {"type": "text", "expr": "Because matrix multiplication contains the sub-expression  $B_{ik}$ , the kernel\nin\u00a0Figure\u00a0 1  iterates over the matrix  $B$  with the loops over  $i$ \n(line\u00a01) and  $k$  (lines\u00a02\u20133)", "word_idx": 9221, "sentence_idx": 151, "label": "unlabeled"}, {"type": "text", "expr": " The loop over  $i$  is dense because the CSR\nformat stores every row", "word_idx": 9403, "sentence_idx": 152, "label": "unlabeled"}, {"type": "text", "expr": " The loop over  $k$ , however, is sparse because each\nrow is compressed", "word_idx": 9472, "sentence_idx": 153, "label": "unlabeled"}, {"type": "text", "expr": " To iterate over the column coordinates of the  $i$ th row,\nthe  $k$  loop iterates over  $[$ ,  $)$  in\n B_idx ", "word_idx": 9543, "sentence_idx": 154, "label": "unlabeled"}, {"type": "text", "expr": " We have highlighted  $B$ \u2019s index arrays in the code in\nFigure\u00a0 1 ", "word_idx": 9655, "sentence_idx": 155, "label": "unlabeled"}, {"type": "math", "expr": "$$B_{ik}$$", "word_idx": 9722, "sentence_idx": 156, "label": "unlabeled"}, {"type": "text", "expr": "B_pos", "word_idx": 9728, "sentence_idx": 157, "label": "unlabeled"}, {"type": "text", "expr": "B_pos", "word_idx": 9733, "sentence_idx": 158, "label": "unlabeled"}, {"type": "text", "expr": "[ i ]", "word_idx": 9738, "sentence_idx": 159, "label": "unlabeled"}, {"type": "text", "expr": "B_pos", "word_idx": 9743, "sentence_idx": 160, "label": "unlabeled"}, {"type": "text", "expr": "B_pos", "word_idx": 9748, "sentence_idx": 161, "label": "unlabeled"}, {"type": "text", "expr": "[ i +1]", "word_idx": 9753, "sentence_idx": 162, "label": "unlabeled"}, {"type": "text", "expr": "B_idx", "word_idx": 9760, "sentence_idx": 163, "label": "unlabeled"}, {"type": "text", "expr": "The kernel is further complicated when the result matrix  $A$  is sparse, because\nthe assignment to  $A$  (line\u00a06) is nested inside the reduction loop  $k$ ", "word_idx": 9765, "sentence_idx": 164, "label": "unlabeled"}, {"type": "text", "expr": " This\ncauses the inner loop  $j$  to iterate over and insert into each row of  $A$ \nseveral times", "word_idx": 9921, "sentence_idx": 165, "label": "unlabeled"}, {"type": "text", "expr": " Sparse data structures, however, do not support fast random\ninserts (only appends)", "word_idx": 10018, "sentence_idx": 166, "label": "unlabeled"}, {"type": "text", "expr": " Inserting into the middle of a CSR matrix costs\n $\\Theta{}(\\textrm{nnz})$  because the new value must be inserted into the middle\nof an array", "word_idx": 10101, "sentence_idx": 167, "label": "unlabeled"}, {"type": "text", "expr": " To get the  $\\Theta{}(1)$  insertion cost of dense formats, the\nkernel in\u00a0Figure\u00a0 1  introduces a dense workspace", "word_idx": 10243, "sentence_idx": 168, "label": "unlabeled"}, {"type": "text", "expr": " The\nworkspace and accompanying loop transformations is the purpose of the operator\nsplit optimization", "word_idx": 10357, "sentence_idx": 169, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Theta{}(\\textrm{nnz})$$", "word_idx": 10459, "sentence_idx": 170, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Theta{}(1)$$", "word_idx": 10481, "sentence_idx": 171, "label": "unlabeled"}, {"type": "text", "expr": "A workspace is a temporary tensor that is typically dense with fast insertion\nand random access", "word_idx": 10492, "sentence_idx": 172, "label": "unlabeled"}, {"type": "text", "expr": " Because values can be scattered efficiently into a dense\nworkspace, the loop nest  $k,j$  (lines\u00a02\u20138) in\u00a0Figure\u00a0 1  looks\nsimilar to the kernel in\u00a0Figure\u00a0 1 ", "word_idx": 10587, "sentence_idx": 173, "label": "unlabeled"}, {"type": "text", "expr": " Instead of assigning values\nto the result matrix\u00a0 $A$ , however, it assigns them to a dense workspace vector", "word_idx": 10745, "sentence_idx": 174, "label": "unlabeled"}, {"type": "text", "expr": "\nWhen a row of the result is fully computed in the workspace, it is appended to\n $A$  in a second loop over  $j$  (lines\u00a010\u201314)", "word_idx": 10854, "sentence_idx": 175, "label": "unlabeled"}, {"type": "text", "expr": " This loop iterates over the row\nin  $A$ \u2019s sparse index structure, which assumes  $A$ \u2019s CSR index has been\npre-assembled", "word_idx": 10981, "sentence_idx": 176, "label": "unlabeled"}, {"type": "text", "expr": " Pre-assembling index structures increases performance when\nassembly can be moved out of inner loops, which is common in material\nsimulations\u00a0 ", "word_idx": 11103, "sentence_idx": 177, "label": "unlabeled"}, {"type": "text", "expr": " We will describe the code to assemble result\nindices, which keeps track of the nonzero coordinates in the workspace,\nin\u00a0Section\u00a0 6 ", "word_idx": 11246, "sentence_idx": 178, "label": "unlabeled"}, {"type": "math", "expr": "$$k,j$$", "word_idx": 11378, "sentence_idx": 179, "label": "unlabeled"}, {"type": "text", "expr": "3  Tensor Algebra Compilation Background", "word_idx": 11381, "sentence_idx": 180, "label": "unlabeled"}, {"type": "text", "expr": "In this section we will review the iteration graph intermediate representation\nfor tensor algebra compilation\u00a0  that is the target for\noperator splits", "word_idx": 11421, "sentence_idx": 181, "label": "unlabeled"}, {"type": "text", "expr": " Iteration graphs are constructed from tensor index notation\nand describe the constraints imposed by sparse tensors on the iteration space\nof the index variables", "word_idx": 11571, "sentence_idx": 182, "label": "unlabeled"}, {"type": "text", "expr": " Sparse tensors provide an opportunity and a challenge", "word_idx": 11732, "sentence_idx": 183, "label": "unlabeled"}, {"type": "text", "expr": "\nThey store only nonzeros and loops therefore avoid iterating over zeros, but\nthey also enforce a particular iteration order because they encode tensor\ncoordinates hierarchically", "word_idx": 11786, "sentence_idx": 184, "label": "unlabeled"}, {"type": "text", "expr": "2017b", "word_idx": 11964, "sentence_idx": 185, "label": "unlabeled"}, {"type": "text", "expr": "An iteration graph is constructed from tensor index notation, such as\n $A_{ij}=\\sum_{lk}B_{ilk}C_{lj}D_{kj}$  or\n $a_{i}=\\sum_{j}B_{ij}c_{j}+d_{i}$ ", "word_idx": 11969, "sentence_idx": 186, "label": "unlabeled"}, {"type": "text", "expr": " In index notation, index variables range\nover the dimensions they index and computations happen at each point in the\niteration space", "word_idx": 12117, "sentence_idx": 187, "label": "unlabeled"}, {"type": "text", "expr": " Index variables are nodes in iteration graphs and each\noperand access, such as  $B_{ij}$ , becomes a path through index variables\ninvolved", "word_idx": 12250, "sentence_idx": 188, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ij}=\\sum_{lk}B_{ilk}C_{lj}D_{kj}$$", "word_idx": 12389, "sentence_idx": 189, "label": "unlabeled"}, {"type": "math", "expr": "$$a_{i}=\\sum_{j}B_{ij}c_{j}+d_{i}$$", "word_idx": 12424, "sentence_idx": 190, "label": "unlabeled"}, {"type": "math", "expr": "$$B_{ij}$$", "word_idx": 12455, "sentence_idx": 191, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 12461, "sentence_idx": 192, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 12472, "sentence_idx": 193, "label": "unlabeled"}, {"type": "text", "expr": "$A_{ij}=\\sum_{k}B_{ik}C_{kj}$", "word_idx": 12483, "sentence_idx": 194, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ij}=\\sum_{k}B_{ik}C_{kj}$$", "word_idx": 12512, "sentence_idx": 195, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 12539, "sentence_idx": 196, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 12550, "sentence_idx": 197, "label": "unlabeled"}, {"type": "text", "expr": "$A_{ijk}=B_{ijk}+C_{ijk}$", "word_idx": 12561, "sentence_idx": 198, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ijk}=B_{ijk}+C_{ijk}$$", "word_idx": 12586, "sentence_idx": 199, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 12609, "sentence_idx": 200, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 12620, "sentence_idx": 201, "label": "unlabeled"}, {"type": "text", "expr": "$A_{ij}=\\sum_{k}B_{ijk}c_{k}$", "word_idx": 12631, "sentence_idx": 202, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ij}=\\sum_{k}B_{ijk}c_{k}$$", "word_idx": 12660, "sentence_idx": 203, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 12687, "sentence_idx": 204, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 12698, "sentence_idx": 205, "label": "unlabeled"}, {"type": "text", "expr": "$A_{ij}=\\sum_{k}B_{ij}C_{ik}D_{kj}$", "word_idx": 12709, "sentence_idx": 206, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ij}=\\sum_{k}B_{ij}C_{ik}D_{kj}$$", "word_idx": 12744, "sentence_idx": 207, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 12777, "sentence_idx": 208, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 12788, "sentence_idx": 209, "label": "unlabeled"}, {"type": "text", "expr": "$A_{ij}=\\sum_{lk}B_{ikl}C_{lj}D_{kj}$", "word_idx": 12799, "sentence_idx": 210, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ij}=\\sum_{lk}B_{ikl}C_{lj}D_{kj}$$", "word_idx": 12836, "sentence_idx": 211, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:  \nFive iteration graphs:\n( \\subref fig:iteration-graphs-spmm)\u00a0matrix multiplication,\n( \\subref fig:iteration-graphs-tadd)\u00a0tensor addition,\n( \\subref fig:iteration-graphs-ttv) tensor-vector multiplication,\n( \\subref fig:iteration-graphs-sddmm) sampled dense-dense matrix multiplication (SDDMM), and\n( \\subref fig:iteration-graphs-mttkrp) matricized tensor times Khatri-Rao\nproduct (MTTKRP)", "word_idx": 12871, "sentence_idx": 212, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:", "word_idx": 13269, "sentence_idx": 213, "label": "unlabeled"}, {"type": "text", "expr": "\\subref", "word_idx": 13278, "sentence_idx": 214, "label": "unlabeled"}, {"type": "text", "expr": "\\subref", "word_idx": 13285, "sentence_idx": 215, "label": "unlabeled"}, {"type": "text", "expr": "\\subref", "word_idx": 13292, "sentence_idx": 216, "label": "unlabeled"}, {"type": "text", "expr": "\\subref", "word_idx": 13299, "sentence_idx": 217, "label": "unlabeled"}, {"type": "text", "expr": "\\subref", "word_idx": 13306, "sentence_idx": 218, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a0 2  shows several iteration graphs, including matrix\nmultiplication, sampled dense-dense matrix multiplication from machine\nlearning\u00a0 , and the matricized tensor times Khatri-Rao product used\nto factorize tensors\u00a0 ", "word_idx": 13313, "sentence_idx": 219, "label": "unlabeled"}, {"type": "text", "expr": " The index notation for tensor-vector\nmultiplication is", "word_idx": 13534, "sentence_idx": 220, "label": "unlabeled"}, {"type": "text", "expr": "$A_{ij}=\\sum_{k}B_{ijk}c_{k}$", "word_idx": 13589, "sentence_idx": 221, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ij}=\\sum_{k}B_{ijk}c_{k}.$$", "word_idx": 13618, "sentence_idx": 222, "label": "unlabeled"}, {"type": "text", "expr": "The corresponding\niteration graph in Figure\u00a0 2  has a node for each index\nvariable  $i$ ,  $j$ , and  $k$  and a a path for each of the three tensor accesses\n $B_{ijk}$  (blue),  $c_{k}$  (purple), and  $A_{ij}$  (stippled green)", "word_idx": 13646, "sentence_idx": 223, "label": "unlabeled"}, {"type": "text", "expr": " We draw\nstippled paths for results", "word_idx": 13875, "sentence_idx": 224, "label": "unlabeled"}, {"type": "text", "expr": " Figure\u00a0 3  shows code generated from this\niteration graph when  $B$  and  $c$  are sparse", "word_idx": 13910, "sentence_idx": 225, "label": "unlabeled"}, {"type": "text", "expr": " Each index variable node becomes\na loop that iterates over the sparse tensor indices belonging to the incoming\nedges", "word_idx": 14000, "sentence_idx": 226, "label": "unlabeled"}, {"type": "math", "expr": "$$B_{ijk}$$", "word_idx": 14117, "sentence_idx": 227, "label": "unlabeled"}, {"type": "math", "expr": "$$c_{k}$$", "word_idx": 14124, "sentence_idx": 228, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ij}$$", "word_idx": 14129, "sentence_idx": 229, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:  \nCode generated from the iteration graph in\u00a0Figure\u00a0 2 \nwhen  $A$  is dense while  $B$  and  $c$  are sparse", "word_idx": 14135, "sentence_idx": 230, "label": "unlabeled"}, {"type": "text", "expr": " Each index variable\nbecomes a loop that iterates over the sparse tensor indices of its incoming\npaths", "word_idx": 14253, "sentence_idx": 231, "label": "unlabeled"}, {"type": "text", "expr": " The  $k$  loop iterates over the intersection of the last dimension\nof  $B$  and  $c$ ", "word_idx": 14355, "sentence_idx": 232, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:", "word_idx": 14442, "sentence_idx": 233, "label": "unlabeled"}, {"type": "text", "expr": "Two or more input paths meet at the same index variable when it was used to\nindex into two or more tensors", "word_idx": 14451, "sentence_idx": 234, "label": "unlabeled"}, {"type": "text", "expr": " The iteration space of the tensor dimensions\nthe variable indexes must be merged in the generated code", "word_idx": 14557, "sentence_idx": 235, "label": "unlabeled"}, {"type": "text", "expr": " The index variables\nare annotated with operators that tell the code generator what kind of merge\ncode to generate", "word_idx": 14660, "sentence_idx": 236, "label": "unlabeled"}, {"type": "text", "expr": " If the tensors are multiplied, then the generated code\niterates over the intersection of the indexed tensor dimensions\n(Figure\u00a0 5 )", "word_idx": 14774, "sentence_idx": 237, "label": "unlabeled"}, {"type": "text", "expr": " If they are added, then it iterates over their\nunion (Figure\u00a0 6 )", "word_idx": 14906, "sentence_idx": 238, "label": "unlabeled"}, {"type": "text", "expr": " If more than two tensors are indexed by the\nsame index variable, then code is generated to iterate over a mix of\nintersections and unions of tensor dimensions", "word_idx": 14972, "sentence_idx": 239, "label": "unlabeled"}, {"type": "text", "expr": "In prior work\u00a0  we described an algorithm to generate code\nfrom iteration graphs, including a mechanism called merge lattices to generate\ncode to co-iterate over tensor dimensions", "word_idx": 15131, "sentence_idx": 240, "label": "unlabeled"}, {"type": "text", "expr": " Understanding our workspace\noptimization does not require understanding the details of the code generation\nalgorithm or merge lattices", "word_idx": 15310, "sentence_idx": 241, "label": "unlabeled"}, {"type": "text", "expr": " We should note, however, that the performance of\ncode that merges sparse tensors may suffer from many conditionals", "word_idx": 15445, "sentence_idx": 242, "label": "unlabeled"}, {"type": "text", "expr": " Code to\nco-iterate over a combination of a single sparse and one or more dense tensors,\non the other hand, does not require conditionals\n(e", "word_idx": 15560, "sentence_idx": 243, "label": "unlabeled"}, {"type": "text", "expr": ",\u00a0Figure\u00a0 5  and Figure\u00a0 6 )", "word_idx": 15700, "sentence_idx": 244, "label": "unlabeled"}, {"type": "text", "expr": " One of the\nbenefits of introducing a workspace is to improve performance by turning\nsparse-sparse iteration into sparse-dense iteration", "word_idx": 15728, "sentence_idx": 245, "label": "unlabeled"}, {"type": "text", "expr": "2017b", "word_idx": 15864, "sentence_idx": 246, "label": "unlabeled"}, {"type": "text", "expr": "4  Operator Split", "word_idx": 15869, "sentence_idx": 247, "label": "unlabeled"}, {"type": "text", "expr": "Operator splitting is an optimization that applies to a binary operator at an\nindex variable in an index notation expression", "word_idx": 15886, "sentence_idx": 248, "label": "unlabeled"}, {"type": "text", "expr": " It splits the index variable\nin two and peels off the operator\u2019s left sub-expression and stores it to a\ntemporary dense workspace", "word_idx": 16010, "sentence_idx": 249, "label": "unlabeled"}, {"type": "text", "expr": " The original expression is then rewritten to\nreplace the left sub-expression with the workspace", "word_idx": 16140, "sentence_idx": 250, "label": "unlabeled"}, {"type": "text", "expr": " Operator splits have two\nadvantages:", "word_idx": 16236, "sentence_idx": 251, "label": "unlabeled"}, {"type": "text", "expr": "Operator splitting is an optimization that applies to a binary operator at an\nindex variable in an index notation expression", "word_idx": 16273, "sentence_idx": 252, "label": "unlabeled"}, {"type": "text", "expr": " It splits the index variable\nin two and peels off the operator\u2019s left sub-expression and stores it to a\ntemporary dense workspace", "word_idx": 16397, "sentence_idx": 253, "label": "unlabeled"}, {"type": "text", "expr": " The original expression is then rewritten to\nreplace the left sub-expression with the workspace", "word_idx": 16527, "sentence_idx": 254, "label": "unlabeled"}, {"type": "text", "expr": " Operator splits have two\nadvantages:", "word_idx": 16623, "sentence_idx": 255, "label": "unlabeled"}, {"type": "text", "expr": "Simplifies merges", "word_idx": 16660, "sentence_idx": 256, "label": "unlabeled"}, {"type": "text", "expr": "Splitting a binary operator that has sparse\noperands in both sub-expressions replaces a sparse-sparse merge with a\npotentially cheaper sparse-dense merge", "word_idx": 16677, "sentence_idx": 257, "label": "unlabeled"}, {"type": "text", "expr": " By removing sparse-sparse merges\nwe eliminate conditionals and loops and may also remove expensive random\ninserts into sparse data structures", "word_idx": 16830, "sentence_idx": 258, "label": "unlabeled"}, {"type": "text", "expr": "Splitting a binary operator that has sparse\noperands in both sub-expressions replaces a sparse-sparse merge with a\npotentially cheaper sparse-dense merge", "word_idx": 16972, "sentence_idx": 259, "label": "unlabeled"}, {"type": "text", "expr": " By removing sparse-sparse merges\nwe eliminate conditionals and loops and may also remove expensive random\ninserts into sparse data structures", "word_idx": 17125, "sentence_idx": 260, "label": "unlabeled"}, {"type": "text", "expr": "Hoists loop invariant computations", "word_idx": 17267, "sentence_idx": 261, "label": "unlabeled"}, {"type": "text", "expr": "Splitting an operator may result\nin the loop for one of the new index variables being emitted in a higher\nloop nest", "word_idx": 17301, "sentence_idx": 262, "label": "unlabeled"}, {"type": "text", "expr": " That is, it is revealed as loop-invariant\nand hoisted out of inner loops, which removes redundant computation", "word_idx": 17416, "sentence_idx": 263, "label": "unlabeled"}, {"type": "text", "expr": "Splitting an operator may result\nin the loop for one of the new index variables being emitted in a higher\nloop nest", "word_idx": 17526, "sentence_idx": 264, "label": "unlabeled"}, {"type": "text", "expr": " That is, it is revealed as loop-invariant\nand hoisted out of inner loops, which removes redundant computation", "word_idx": 17641, "sentence_idx": 265, "label": "unlabeled"}, {"type": "text", "expr": "Many important kernels benefit from operator splits, including sparse matrix\nmultiplication, matrix addition, and the matricized tensor times Khatri-Rao\nproduct", "word_idx": 17751, "sentence_idx": 266, "label": "unlabeled"}, {"type": "text", "expr": " In this section we describe the optimization with simple examples,\nbut we will explore its application to sophisticated real-world kernels\nin\u00a0Section\u00a0 5 ", "word_idx": 17911, "sentence_idx": 267, "label": "unlabeled"}, {"type": "text", "expr": "Splitting an operator in an iteration graph causes the associated index\nvariable  $i$  to be divided in two: one for the sub-expression left of the\noperator ( $i_{\\textrm{left}}$ ) and one for the whole expression\n( $i_{\\textrm{right}}$ )", "word_idx": 18065, "sentence_idx": 268, "label": "unlabeled"}, {"type": "text", "expr": " The incoming arrows are divided between the two new\nindex variables", "word_idx": 18303, "sentence_idx": 269, "label": "unlabeled"}, {"type": "text", "expr": " An operator split also introduces a dense workspace that is\nthe result of the first index variable and an operand of the second", "word_idx": 18371, "sentence_idx": 270, "label": "unlabeled"}, {"type": "text", "expr": " In\neffect, the operator\u2019s left sub-expression is stored to the workspace, and the\nworkspace takes the place of the left sub-expression at the second index\nvariable", "word_idx": 18499, "sentence_idx": 271, "label": "unlabeled"}, {"type": "text", "expr": " For example, splitting the multiplication operator in a\ncomponent-wise multiplication  $a_{i}=b_{i}c_{i}$  results in", "word_idx": 18663, "sentence_idx": 272, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{\\textrm{left}}$$", "word_idx": 18781, "sentence_idx": 273, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{\\textrm{right}}$$", "word_idx": 18798, "sentence_idx": 274, "label": "unlabeled"}, {"type": "math", "expr": "$$a_{i}=b_{i}c_{i}$$", "word_idx": 18816, "sentence_idx": 275, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle w_{i_{\\textrm{left}}}$", "word_idx": 18832, "sentence_idx": 276, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle w_{i_{\\textrm{left}}}$$", "word_idx": 18869, "sentence_idx": 277, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=b_{i_{\\textrm{left}}}$", "word_idx": 18904, "sentence_idx": 278, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=b_{i_{\\textrm{left}}}$$", "word_idx": 18941, "sentence_idx": 279, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle a_{i_{\\textrm{right}}}$", "word_idx": 18976, "sentence_idx": 280, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle a_{i_{\\textrm{right}}}$$", "word_idx": 19014, "sentence_idx": 281, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=w_{i_{\\textrm{right}}}c_{i_{\\textrm{right}}}$", "word_idx": 19050, "sentence_idx": 282, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=w_{i_{\\textrm{right}}}c_{i_{\\textrm{right}}}.$$", "word_idx": 19110, "sentence_idx": 283, "label": "unlabeled"}, {"type": "text", "expr": "The rationale for this transformation is that  w  may be a dense tensor\nand that the resulting multiplication therefore needs fewer conditional checks,\nas demonstrated in\u00a0Figure\u00a0 5 ", "word_idx": 19169, "sentence_idx": 284, "label": "unlabeled"}, {"type": "text", "expr": " Furthermore, when the\ntransformation is applied inside a loop nest then it can lead to loop invariant\ncode motion as shown in\u00a0Section\u00a0 5", "word_idx": 19350, "sentence_idx": 285, "label": "unlabeled"}, {"type": "text", "expr": " Finally, when applied to an\nassignment it facilitates scattering values into a sparse result", "word_idx": 19487, "sentence_idx": 286, "label": "unlabeled"}, {"type": "text", "expr": " Operator\nsplits can be applied to more than one operator in sequence and each resulting\nindex variable becomes a loop in the generated code", "word_idx": 19580, "sentence_idx": 287, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:  \nGeneric operator split", "word_idx": 19720, "sentence_idx": 288, "label": "unlabeled"}, {"type": "text", "expr": " The arrows to the left of the operator are\npeeled off to the left index variable, and a workspace is the result of\nthe left and an operand of the right index variable", "word_idx": 19754, "sentence_idx": 289, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:", "word_idx": 19921, "sentence_idx": 290, "label": "unlabeled"}, {"type": "text", "expr": "Operator Split Definition: \n\nLet  $\\textrm{expr}_{\\textrm{left}}\\oplus_{i}\\textrm{expr}_{\\textrm{right}}$  be the index expression at index variable\n $i$ , where  $\\oplus$  is any operator including assignment from left to right\n $\\stackrel{\\mathclap{\\tiny\\mbox{$\\rightarrow$}}}{=}$ ", "word_idx": 19930, "sentence_idx": 291, "label": "unlabeled"}, {"type": "text", "expr": " Moreover, let  $\\textrm{expr}_{\\textrm{left}}$  include all\noperands  $l_{1},\\ldots,l_{m}$  to the left of the operator and let\n $\\textrm{expr}_{\\textrm{right}}$  include all operands  $r_{1},\\ldots,r_{n}$  to the\nright", "word_idx": 20213, "sentence_idx": 292, "label": "unlabeled"}, {"type": "text", "expr": " An operator split of  $\\oplus_{i}$  creates two new index variable\n $i_{\\textrm{left}}$  and  $i_{\\textrm{right}}$ ", "word_idx": 20433, "sentence_idx": 293, "label": "unlabeled"}, {"type": "text", "expr": " The index variable\n $i_{\\textrm{left}}$  is given the expression\n $\\textrm{expr}_{\\textrm{left}}\\stackrel{\\mathclap{\\tiny\\mbox{$\\rightarrow$}}}{=%\n}w_{i_{\\textrm{left}}}$ , and\nthe expression of  $i_{\\textrm{right}}$  is rewritten to\n $w_{i_{\\textrm{right}}}\\oplus\\textrm{expr}_{\\textrm{right}}$ ", "word_idx": 20549, "sentence_idx": 294, "label": "unlabeled"}, {"type": "text", "expr": "Operator Split Definition:", "word_idx": 20846, "sentence_idx": 295, "label": "unlabeled"}, {"type": "math", "expr": "$$\\textrm{expr}_{\\textrm{left}}\\oplus_{i}\\textrm{expr}_{\\textrm{right}}$$", "word_idx": 20872, "sentence_idx": 296, "label": "unlabeled"}, {"type": "math", "expr": "$$\\oplus$$", "word_idx": 20941, "sentence_idx": 297, "label": "unlabeled"}, {"type": "math", "expr": "$$\\stackrel{\\mathclap{\\tiny\\mbox{$\\rightarrow$}}}{=}$$", "word_idx": 20947, "sentence_idx": 298, "label": "unlabeled"}, {"type": "math", "expr": "$$\\textrm{expr}_{\\textrm{left}}$$", "word_idx": 20997, "sentence_idx": 299, "label": "unlabeled"}, {"type": "math", "expr": "$$l_{1},\\ldots,l_{m}$$", "word_idx": 21026, "sentence_idx": 300, "label": "unlabeled"}, {"type": "math", "expr": "$$\\textrm{expr}_{\\textrm{right}}$$", "word_idx": 21044, "sentence_idx": 301, "label": "unlabeled"}, {"type": "math", "expr": "$$r_{1},\\ldots,r_{n}$$", "word_idx": 21074, "sentence_idx": 302, "label": "unlabeled"}, {"type": "math", "expr": "$$\\oplus_{i}$$", "word_idx": 21092, "sentence_idx": 303, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{\\textrm{left}}$$", "word_idx": 21102, "sentence_idx": 304, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{\\textrm{right}}$$", "word_idx": 21119, "sentence_idx": 305, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{\\textrm{left}}$$", "word_idx": 21137, "sentence_idx": 306, "label": "unlabeled"}, {"type": "math", "expr": "$$\\textrm{expr}_{\\textrm{left}}\\stackrel{\\mathclap{\\tiny\\mbox{$\\rightarrow$}}}{=%\n}w_{i_{\\textrm{left}}}$$", "word_idx": 21154, "sentence_idx": 307, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{\\textrm{right}}$$", "word_idx": 21256, "sentence_idx": 308, "label": "unlabeled"}, {"type": "math", "expr": "$$w_{i_{\\textrm{right}}}\\oplus\\textrm{expr}_{\\textrm{right}}$$", "word_idx": 21274, "sentence_idx": 309, "label": "unlabeled"}, {"type": "text", "expr": "An operator split is shown graphically in\u00a0Figure\u00a0 4 ", "word_idx": 21332, "sentence_idx": 310, "label": "unlabeled"}, {"type": "text", "expr": " The index\nvariables  $i_{\\textrm{left}}$  and  $i_{\\textrm{right}}$  divide the incoming\noperator arrows:  $i_{\\textrm{left}}$  has the arrows from the operator\u2019s left\nside and  $i_{\\textrm{right}}$  has the arrows from the operator\u2019s right side", "word_idx": 21384, "sentence_idx": 311, "label": "unlabeled"}, {"type": "text", "expr": "\nFurthermore,  $i_{\\textrm{right}}$  retains the result arrow  $a$ , while the\nresult of  $i_{\\textrm{left}}$  is stored to the workspace  $w$  that also replaces\nthe left expression in  $i_{\\textrm{right}}$ ", "word_idx": 21630, "sentence_idx": 312, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{\\textrm{left}}$$", "word_idx": 21838, "sentence_idx": 313, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{\\textrm{right}}$$", "word_idx": 21855, "sentence_idx": 314, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{\\textrm{left}}$$", "word_idx": 21873, "sentence_idx": 315, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{\\textrm{right}}$$", "word_idx": 21890, "sentence_idx": 316, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{\\textrm{right}}$$", "word_idx": 21908, "sentence_idx": 317, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{\\textrm{left}}$$", "word_idx": 21926, "sentence_idx": 318, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{\\textrm{right}}$$", "word_idx": 21943, "sentence_idx": 319, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a05:  \nSparse vector inner product  $a=\\sum_{i}b_{i}c_{i}$ ", "word_idx": 21961, "sentence_idx": 320, "label": "unlabeled"}, {"type": "text", "expr": " The top shows the\niteration graph and associated code that iterates over the intersection of\nthe sparse vectors", "word_idx": 22025, "sentence_idx": 321, "label": "unlabeled"}, {"type": "text", "expr": " The bottom shows the iteration graph and code after\nsplitting the multiplication", "word_idx": 22137, "sentence_idx": 322, "label": "unlabeled"}, {"type": "text", "expr": " The sparse-sparse merge code is replaced\nwith a copy to a dense workspace followed by a sparse-dense merge", "word_idx": 22218, "sentence_idx": 323, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a05:", "word_idx": 22325, "sentence_idx": 324, "label": "unlabeled"}, {"type": "math", "expr": "$$a=\\sum_{i}b_{i}c_{i}$$", "word_idx": 22334, "sentence_idx": 325, "label": "unlabeled"}, {"type": "text", "expr": "Consider two examples, vector product and addition, that show operator splits\nfor iteration graphs with a single variable", "word_idx": 22354, "sentence_idx": 326, "label": "unlabeled"}, {"type": "text", "expr": " In\u00a0Section\u00a0 5  we will\nexplore operator splits applied to deep iteration graphs that result from\nhigher-dimensional tensor expressions", "word_idx": 22475, "sentence_idx": 327, "label": "unlabeled"}, {"type": "text", "expr": " Figure\u00a0 5  shows the\niteration graphs for a vector inner product before and after splitting the\nmultiplication", "word_idx": 22610, "sentence_idx": 328, "label": "unlabeled"}, {"type": "text", "expr": " The generated code before the split iterates over the\nintersection of the coordinates in  $b$  and  $c$  that have nonzero component\nvalues", "word_idx": 22721, "sentence_idx": 329, "label": "unlabeled"}, {"type": "text", "expr": " Thus, the loop iterates while both  $c$  and  $b$  have values left and\nadds to  $a$  if both have a value at a coordinate", "word_idx": 22861, "sentence_idx": 330, "label": "unlabeled"}, {"type": "text", "expr": " After the split the\niteration graph has two index variables", "word_idx": 22984, "sentence_idx": 331, "label": "unlabeled"}, {"type": "text", "expr": " The first stores to a dense workspace\nand the second reads from it", "word_idx": 23044, "sentence_idx": 332, "label": "unlabeled"}, {"type": "text", "expr": " Multiplying a dense and sparse vector does not\nrequire merge code since you can iterate over the sparse vector and retrieve\nvalues from the dense vector", "word_idx": 23111, "sentence_idx": 333, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a06:  \nSparse vector addition  $a_{i}=b_{i}+c_{i}$ ", "word_idx": 23264, "sentence_idx": 334, "label": "unlabeled"}, {"type": "text", "expr": " The top shows the iteration\ngraph and generated code that iterates over the union of the sparse\nvectors", "word_idx": 23320, "sentence_idx": 335, "label": "unlabeled"}, {"type": "text", "expr": " The bottom shows the iteration graph and code after splitting the\naddition and assignment", "word_idx": 23424, "sentence_idx": 336, "label": "unlabeled"}, {"type": "text", "expr": " The sparse merge code is replaced with loops that\nadd each operand to the workspace and one that copies it to the result", "word_idx": 23514, "sentence_idx": 337, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a06:", "word_idx": 23635, "sentence_idx": 338, "label": "unlabeled"}, {"type": "math", "expr": "$$a_{i}=b_{i}+c_{i}$$", "word_idx": 23644, "sentence_idx": 339, "label": "unlabeled"}, {"type": "text", "expr": "The iteration graphs for a vector addition before and after splitting the\naddition and the assignment operators are shown in\u00a0Figure\u00a0 6 ", "word_idx": 23661, "sentence_idx": 340, "label": "unlabeled"}, {"type": "text", "expr": "\nCode generated from the iteration graph before splitting has three while loops\nthat together iterate over the union of the operands", "word_idx": 23796, "sentence_idx": 341, "label": "unlabeled"}, {"type": "text", "expr": " The first loop iterates\nwhile both  $b$  and  $c$  have coordinates left and computes results if either\nhave a nonzero at a coordinate; the remaining two loops add the rest of the\noperand with nonzeros left", "word_idx": 23928, "sentence_idx": 342, "label": "unlabeled"}, {"type": "text", "expr": " We split both the addition and the assignment to\nshow the effect of an assignment split", "word_idx": 24135, "sentence_idx": 343, "label": "unlabeled"}, {"type": "text", "expr": " The resulting code has two loops that\nstore each operand into a workspace and one that copies the nonzeros to  $a$ ", "word_idx": 24223, "sentence_idx": 344, "label": "unlabeled"}, {"type": "text", "expr": "\nThese loops have fewer conditionals and simpler loop bounds, at the cost of\nreduced temporal data locality as the reuse distance in  $w$  can be large", "word_idx": 24339, "sentence_idx": 345, "label": "unlabeled"}, {"type": "text", "expr": "\nWhich code performs better depends on the machine and on the particular\nsparsity of the inputs", "word_idx": 24490, "sentence_idx": 346, "label": "unlabeled"}, {"type": "text", "expr": "The final loop on lines 9\u201313 in\u00a0Figure\u00a0 6  after the split\niterates over the result  $a$ ", "word_idx": 24585, "sentence_idx": 347, "label": "unlabeled"}, {"type": "text", "expr": " This assumes that the index structure of  $a$  has\nbeen assembled prior to this code executing", "word_idx": 24674, "sentence_idx": 348, "label": "unlabeled"}, {"type": "text", "expr": " Our code generation can emit\nseparate assembly and compute kernels, and in pure compute kernels it assumes\nthat the result index structure has been pre-assembled", "word_idx": 24769, "sentence_idx": 349, "label": "unlabeled"}, {"type": "text", "expr": " We will discuss the\nassembly code that builds  $a$ \u2019s index structure in\u00a0Section\u00a0 6 ", "word_idx": 24931, "sentence_idx": 350, "label": "unlabeled"}, {"type": "text", "expr": "1  Precondition", "word_idx": 25016, "sentence_idx": 351, "label": "unlabeled"}, {"type": "text", "expr": "Operator splits do not always apply", "word_idx": 25031, "sentence_idx": 352, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, an operator can be split\nonly if its index variable does not have a reduction variable predecessor\nin the iteration graph that the operator does not distribute across", "word_idx": 25066, "sentence_idx": 353, "label": "unlabeled"}, {"type": "text", "expr": "Operator splits do not always apply", "word_idx": 25247, "sentence_idx": 354, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, an operator can be split\nonly if its index variable does not have a reduction variable predecessor\nin the iteration graph that the operator does not distribute across", "word_idx": 25282, "sentence_idx": 355, "label": "unlabeled"}, {"type": "text", "expr": "Reduction Precondition: \nLet  $j$  be an index variable with a predecessor reduction variable  $k$  with\nreduction operator  $\\oplus$ ", "word_idx": 25463, "sentence_idx": 356, "label": "unlabeled"}, {"type": "text", "expr": " Furthermore, let  $j$  and  $k$  have a shared\npredecessor  $i$  and let the incoming arrows on  $j$  from  $k$  and  $i$  be merged\nwith the operator  $\\otimes$ ", "word_idx": 25597, "sentence_idx": 357, "label": "unlabeled"}, {"type": "text", "expr": " Then  $j$  can be split on the operator  $\\otimes$ \nif and only if  $\\otimes$  distributes over  $\\oplus$  (i", "word_idx": 25760, "sentence_idx": 358, "label": "unlabeled"}, {"type": "text", "expr": ",  $b\\otimes(c\\oplus d)=(b\\otimes c)\\oplus(b\\otimes d)$ )", "word_idx": 25870, "sentence_idx": 359, "label": "unlabeled"}, {"type": "text", "expr": "Reduction Precondition:", "word_idx": 25927, "sentence_idx": 360, "label": "unlabeled"}, {"type": "math", "expr": "$$\\oplus$$", "word_idx": 25950, "sentence_idx": 361, "label": "unlabeled"}, {"type": "math", "expr": "$$\\otimes$$", "word_idx": 25956, "sentence_idx": 362, "label": "unlabeled"}, {"type": "math", "expr": "$$\\otimes$$", "word_idx": 25963, "sentence_idx": 363, "label": "unlabeled"}, {"type": "math", "expr": "$$\\otimes$$", "word_idx": 25970, "sentence_idx": 364, "label": "unlabeled"}, {"type": "math", "expr": "$$\\oplus$$", "word_idx": 25977, "sentence_idx": 365, "label": "unlabeled"}, {"type": "math", "expr": "$$b\\otimes(c\\oplus d)=(b\\otimes c)\\oplus(b\\otimes d)$$", "word_idx": 25983, "sentence_idx": 366, "label": "unlabeled"}, {"type": "text", "expr": "The reason for the precondition is that after splitting  $\\otimes$  at  $j$  the\nindex variable associated with the arrow from  $i$  will no longer be dominated\nby  $k$ ", "word_idx": 26033, "sentence_idx": 367, "label": "unlabeled"}, {"type": "text", "expr": " The loop resulting from this  $j$  will therefore be hoisted out of the\n $k$  reduction loop and will be added in only once", "word_idx": 26202, "sentence_idx": 368, "label": "unlabeled"}, {"type": "math", "expr": "$$\\otimes$$", "word_idx": 26326, "sentence_idx": 369, "label": "unlabeled"}, {"type": "text", "expr": "2  The Result Tensor as Workspace", "word_idx": 26333, "sentence_idx": 370, "label": "unlabeled"}, {"type": "text", "expr": "If the result is dense then it pays to use it to accumulate the results of both\nsub-expressions resulting from an operator, instead of introducing a workspace", "word_idx": 26366, "sentence_idx": 371, "label": "unlabeled"}, {"type": "text", "expr": "\nThus the result is used as a temporary workspace", "word_idx": 26524, "sentence_idx": 372, "label": "unlabeled"}, {"type": "text", "expr": " This is useful in sparse\nvector addition when the result is dense", "word_idx": 26573, "sentence_idx": 373, "label": "unlabeled"}, {"type": "text", "expr": " Since the result is dense, there is\nno need for a temporary workspace", "word_idx": 26639, "sentence_idx": 374, "label": "unlabeled"}, {"type": "text", "expr": " This optimization introduces another\nprecondition to ensure results are not overwritten", "word_idx": 26709, "sentence_idx": 375, "label": "unlabeled"}, {"type": "text", "expr": "If the result is dense then it pays to use it to accumulate the results of both\nsub-expressions resulting from an operator, instead of introducing a workspace", "word_idx": 26797, "sentence_idx": 376, "label": "unlabeled"}, {"type": "text", "expr": "\nThus the result is used as a temporary workspace", "word_idx": 26955, "sentence_idx": 377, "label": "unlabeled"}, {"type": "text", "expr": " This is useful in sparse\nvector addition when the result is dense", "word_idx": 27004, "sentence_idx": 378, "label": "unlabeled"}, {"type": "text", "expr": " Since the result is dense, there is\nno need for a temporary workspace", "word_idx": 27070, "sentence_idx": 379, "label": "unlabeled"}, {"type": "text", "expr": " This optimization introduces another\nprecondition to ensure results are not overwritten", "word_idx": 27140, "sentence_idx": 380, "label": "unlabeled"}, {"type": "text", "expr": "Reuse Precondition: \nLet  $j$  be an index variable that has a predecessor  $k$  through two paths that\nare merged with operator  $\\otimes_{j}$ ", "word_idx": 27228, "sentence_idx": 381, "label": "unlabeled"}, {"type": "text", "expr": " Furthermore, let  $j$  and  $k$  have a\nshared predecessor  $i$ , and let the arrow from  $i$  to  $k$  be a result arrow", "word_idx": 27372, "sentence_idx": 382, "label": "unlabeled"}, {"type": "text", "expr": "\nThen splitting  $\\otimes_{j}$  must introduce a new workspace", "word_idx": 27494, "sentence_idx": 383, "label": "unlabeled"}, {"type": "text", "expr": "Reuse Precondition:", "word_idx": 27556, "sentence_idx": 384, "label": "unlabeled"}, {"type": "math", "expr": "$$\\otimes_{j}$$", "word_idx": 27575, "sentence_idx": 385, "label": "unlabeled"}, {"type": "math", "expr": "$$\\otimes_{j}$$", "word_idx": 27586, "sentence_idx": 386, "label": "unlabeled"}, {"type": "text", "expr": "This precondition is, for example, not satisfied by the operator split to\nremove redundant computations in the MTTKRP kernel in\u00a0Section\u00a0 5", "word_idx": 27597, "sentence_idx": 387, "label": "unlabeled"}, {"type": "text", "expr": "\nTherefore, a temporary workspace is necessary to optimize that kernel", "word_idx": 27735, "sentence_idx": 388, "label": "unlabeled"}, {"type": "text", "expr": "3  Applying Operator Splits", "word_idx": 27805, "sentence_idx": 389, "label": "unlabeled"}, {"type": "text", "expr": "Operator splitting increases the performance of many important kernels because\nit removes inserts into sparse results, expensive merge code, and loop\ninvariant code", "word_idx": 27832, "sentence_idx": 390, "label": "unlabeled"}, {"type": "text", "expr": " It does, however, impose costs from constructing, maintaining,\nand using workspaces", "word_idx": 27996, "sentence_idx": 391, "label": "unlabeled"}, {"type": "text", "expr": " Constructing a workspace requires a  malloc \nfollowed by a  memset  to zero its values and it must be reinitialized\nbetween uses", "word_idx": 28080, "sentence_idx": 392, "label": "unlabeled"}, {"type": "text", "expr": " Furthermore, a workspace reduces temporal locality due to the\nincreased reuse distance from storing values to the workspace and later reading\nthem back to store to the result", "word_idx": 28209, "sentence_idx": 393, "label": "unlabeled"}, {"type": "text", "expr": "malloc", "word_idx": 28384, "sentence_idx": 394, "label": "unlabeled"}, {"type": "text", "expr": "memset", "word_idx": 28390, "sentence_idx": 395, "label": "unlabeled"}, {"type": "text", "expr": "A system design is more flexible if it separates mechanism (what to do) from\npolicy (how to do it)\u00a0 ", "word_idx": 28396, "sentence_idx": 396, "label": "unlabeled"}, {"type": "text", "expr": " Performance is a key design\ncriteria in a tensor algebra system", "word_idx": 28496, "sentence_idx": 397, "label": "unlabeled"}, {"type": "text", "expr": " A good system design should therefore\nseparate policy decisions of how to optimize generated code from the mechanisms\nthat carry out the optimization", "word_idx": 28560, "sentence_idx": 398, "label": "unlabeled"}, {"type": "text", "expr": " This paper provides the mechanism for\noperator splitting", "word_idx": 28710, "sentence_idx": 399, "label": "unlabeled"}, {"type": "text", "expr": "We imagine many fruitful policy approaches such as user-specified policy,\nheuristics, mathematical optimization, machine learning, and autotuning", "word_idx": 28767, "sentence_idx": 400, "label": "unlabeled"}, {"type": "text", "expr": " We\nleave the design of automated policy systems as future work", "word_idx": 28912, "sentence_idx": 401, "label": "unlabeled"}, {"type": "text", "expr": " To facilitate\npolicy research, we see operator splits as a key tensor algebra scheduling\nconstruct", "word_idx": 28975, "sentence_idx": 402, "label": "unlabeled"}, {"type": "text", "expr": " The Halide system\u00a0  shows that a scheduling language\nis effective at separating mechanism from policy", "word_idx": 29074, "sentence_idx": 403, "label": "unlabeled"}, {"type": "text", "expr": " Scheduling languages leave\nusers in control of performance, while freeing them from low level code\ntransformations", "word_idx": 29176, "sentence_idx": 404, "label": "unlabeled"}, {"type": "text", "expr": " The goal, of course, is a fully automated system where users\nare freed from performance decisions as well", "word_idx": 29291, "sentence_idx": 405, "label": "unlabeled"}, {"type": "text", "expr": " Such a system, however, also\nprofits from a well-design scheduling language because it it lets researchers\nexplore different policy approaches without re-implementing mechanisms", "word_idx": 29397, "sentence_idx": 406, "label": "unlabeled"}, {"type": "text", "expr": " For\ntensor algebra, a scheduling language should include formats, operator splits,\nand loop optimizations", "word_idx": 29575, "sentence_idx": 407, "label": "unlabeled"}, {"type": "text", "expr": "4  Dimensionality and Choice of Workspaces", "word_idx": 29681, "sentence_idx": 408, "label": "unlabeled"}, {"type": "text", "expr": "The examples in this paper use vector workspaces", "word_idx": 29723, "sentence_idx": 409, "label": "unlabeled"}, {"type": "text", "expr": " The operator split\noptimization, however, applies to kernels where higher-dimensional workspaces,\nsuch as a matrix or a tensor, are needed", "word_idx": 29771, "sentence_idx": 410, "label": "unlabeled"}, {"type": "text", "expr": " The dimensionality of a workspace is\ndetermined by counting the number of index variables above the second index\nvariable after the split, and the sizes of dimensions are determined by the\nranges of the index variables", "word_idx": 29910, "sentence_idx": 411, "label": "unlabeled"}, {"type": "text", "expr": "The examples in this paper use vector workspaces", "word_idx": 30129, "sentence_idx": 412, "label": "unlabeled"}, {"type": "text", "expr": " The operator split\noptimization, however, applies to kernels where higher-dimensional workspaces,\nsuch as a matrix or a tensor, are needed", "word_idx": 30177, "sentence_idx": 413, "label": "unlabeled"}, {"type": "text", "expr": " The dimensionality of a workspace is\ndetermined by counting the number of index variables above the second index\nvariable after the split, and the sizes of dimensions are determined by the\nranges of the index variables", "word_idx": 30316, "sentence_idx": 414, "label": "unlabeled"}, {"type": "text", "expr": "Furthermore, dense arrays are not the only choice for workspaces; a tensor of\nany format will do", "word_idx": 30535, "sentence_idx": 415, "label": "unlabeled"}, {"type": "text", "expr": " The format, however, affects the generated code and its\nperformance", "word_idx": 30631, "sentence_idx": 416, "label": "unlabeled"}, {"type": "text", "expr": " Operator splits are often used to remove expensive sparse-sparse\nmerge code, and dense workspaces are attractive because they result in cheaper\nsparse-dense merges", "word_idx": 30699, "sentence_idx": 417, "label": "unlabeled"}, {"type": "text", "expr": "\nAn alternative is another format with random access such as a hash map", "word_idx": 30863, "sentence_idx": 418, "label": "unlabeled"}, {"type": "text", "expr": " These\nresult in slower execution\u00a0 , but uses only memory\nproportional to the number of nonzeros", "word_idx": 30934, "sentence_idx": 419, "label": "unlabeled"}, {"type": "text", "expr": "5  Case Studies", "word_idx": 31030, "sentence_idx": 420, "label": "unlabeled"}, {"type": "text", "expr": "In this section we will study three important linear and tensor algebra\nexpressions that can be optimized with operator splits", "word_idx": 31045, "sentence_idx": 421, "label": "unlabeled"}, {"type": "text", "expr": " The resulting kernels\nare competitive with hand-optimized kernels from the\nliterature\u00a0 ", "word_idx": 31171, "sentence_idx": 422, "label": "unlabeled"}, {"type": "text", "expr": " The optimization, however,\ngeneralizes to an uncountable number of kernels that have not been implemented\nbefore", "word_idx": 31259, "sentence_idx": 423, "label": "unlabeled"}, {"type": "text", "expr": " We will show one example, MTTKRP with sparse matrices,\nin\u00a0Section\u00a0 5", "word_idx": 31372, "sentence_idx": 424, "label": "unlabeled"}, {"type": "text", "expr": "1  Matrix Multiplication", "word_idx": 31441, "sentence_idx": 425, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption Iteration graph before split", "word_idx": 31465, "sentence_idx": 426, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 31505, "sentence_idx": 427, "label": "unlabeled"}, {"type": "text", "expr": "Iteration graph before split", "word_idx": 31516, "sentence_idx": 428, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption Iteration graph after split", "word_idx": 31544, "sentence_idx": 429, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 31583, "sentence_idx": 430, "label": "unlabeled"}, {"type": "text", "expr": "Iteration graph after split", "word_idx": 31594, "sentence_idx": 431, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a07:  \nMatrix multiplication  $A_{ij}=\\sum_{k}B_{ik}C_{kj}$ , using the\nlinear combination of rows algorithm where all matrices are in the CSR\nformat", "word_idx": 31621, "sentence_idx": 432, "label": "unlabeled"}, {"type": "text", "expr": " Splitting the assignment operator yields\n Gustavson \u2019s algorithm\u00a0 , which we\nshowed in\u00a0Figure\u00a0 1 ", "word_idx": 31775, "sentence_idx": 433, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a07:", "word_idx": 31873, "sentence_idx": 434, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ij}=\\sum_{k}B_{ik}C_{kj}$$", "word_idx": 31882, "sentence_idx": 435, "label": "unlabeled"}, {"type": "text", "expr": "Gustavson", "word_idx": 31909, "sentence_idx": 436, "label": "unlabeled"}, {"type": "text", "expr": "The preferred algorithm for multiplying sparse matrices is to compute the\nlinear combinations of rows or columns\u00a0 ", "word_idx": 31918, "sentence_idx": 437, "label": "unlabeled"}, {"type": "text", "expr": " This\nalgorithm was introduced by  Gustavson \u00a0 ,\nwho showed that it is asymptotically superior to computing inner products when\nthe matrices are sparse", "word_idx": 32032, "sentence_idx": 438, "label": "unlabeled"}, {"type": "text", "expr": " Furthermore, both operands and the result are the\nsame format", "word_idx": 32183, "sentence_idx": 439, "label": "unlabeled"}, {"type": "text", "expr": " A sparse inner product algorithm inconveniently needs the first\noperand to be row major (CSR) and the second column major (CSC)", "word_idx": 32245, "sentence_idx": 440, "label": "unlabeled"}, {"type": "text", "expr": "Gustavson", "word_idx": 32373, "sentence_idx": 441, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a0 7  shows the iteration graph for a linear combination\nof rows algorithm, where the matrices are stored in the CSR format", "word_idx": 32382, "sentence_idx": 442, "label": "unlabeled"}, {"type": "text", "expr": " The\niteration graph has an issue at index variable  $j$ ", "word_idx": 32510, "sentence_idx": 443, "label": "unlabeled"}, {"type": "text", "expr": " Because the assignment to\n $A$  at  $j$  is dominated by the summation index variable  $k$ , the generated code\nmust repeatedly add new values into  $A$ ", "word_idx": 32567, "sentence_idx": 444, "label": "unlabeled"}, {"type": "text", "expr": " This is expensive when  $A$  is sparse\ndue to costly inserts into its sparse data structure", "word_idx": 32721, "sentence_idx": 445, "label": "unlabeled"}, {"type": "text", "expr": "In Figure\u00a0 7 , the assignment operator at  $j$  has been split to\nyield two new index variables  $j_{C}$  and  $j_{A}$ ", "word_idx": 32813, "sentence_idx": 446, "label": "unlabeled"}, {"type": "text", "expr": " The first index variable  $j_{C}$ \naccumulates values into a dense workspace  $w$ , while  $j_{A}$  copies the nonzero\nvalues from the workspace to  $A$ ", "word_idx": 32932, "sentence_idx": 447, "label": "unlabeled"}, {"type": "text", "expr": " Because the workspace is dense, the merge\nwith  $C$  at  $j_{C}$  is trivial: the kernel iterates over  $C$  and scatters values\ninto  $w$ ", "word_idx": 33086, "sentence_idx": 448, "label": "unlabeled"}, {"type": "text", "expr": " Furthermore, the second index variable  $j_{A}$  is not dominated by the\nsummation variable  $k$  and values are therefore appended to  $A$ ", "word_idx": 33226, "sentence_idx": 449, "label": "unlabeled"}, {"type": "math", "expr": "$$j_{C}$$", "word_idx": 33367, "sentence_idx": 450, "label": "unlabeled"}, {"type": "math", "expr": "$$j_{A}$$", "word_idx": 33372, "sentence_idx": 451, "label": "unlabeled"}, {"type": "math", "expr": "$$j_{C}$$", "word_idx": 33377, "sentence_idx": 452, "label": "unlabeled"}, {"type": "math", "expr": "$$j_{A}$$", "word_idx": 33382, "sentence_idx": 453, "label": "unlabeled"}, {"type": "math", "expr": "$$j_{C}$$", "word_idx": 33387, "sentence_idx": 454, "label": "unlabeled"}, {"type": "math", "expr": "$$j_{A}$$", "word_idx": 33392, "sentence_idx": 455, "label": "unlabeled"}, {"type": "text", "expr": "The code listing in Figure\u00a0 1  showed the code generated from a\nmatrix multiplication iteration graph where the assignment operator has been\nsplit", "word_idx": 33397, "sentence_idx": 456, "label": "unlabeled"}, {"type": "text", "expr": " Each index variable results in a loop, loops generated from index\nvariables connected by an arrow are nested, and loops generated from index\nvariables that share a direct predecessor are sequenced", "word_idx": 33543, "sentence_idx": 457, "label": "unlabeled"}, {"type": "text", "expr": " The loop of  $j_{A}$ \ncopies values from the workspace to  $A$ , so it can either iterate over the\nnonzeros of the workspace or the index structure of  $A$ ", "word_idx": 33740, "sentence_idx": 458, "label": "unlabeled"}, {"type": "text", "expr": " The loop on\nlines\u00a010\u201314 in the code listing iterates over the index structure of  $A$ ,\nmeaning it must be pre-assembled before this code is executed", "word_idx": 33897, "sentence_idx": 459, "label": "unlabeled"}, {"type": "text", "expr": " The alternative\nis to emit code that tracks the nonzeros inserted into the workspace, but this\nis more expensive", "word_idx": 34047, "sentence_idx": 460, "label": "unlabeled"}, {"type": "text", "expr": " It is often more efficient to separate the code that\nassembles  $A$ \u2019s index structure from the code that computes its\nvalues\u00a0 ", "word_idx": 34160, "sentence_idx": 461, "label": "unlabeled"}, {"type": "text", "expr": " For ease of exposition we choose to show pure\ncompute kernels in most code listings", "word_idx": 34288, "sentence_idx": 462, "label": "unlabeled"}, {"type": "text", "expr": " We will, however, discuss code\ngeneration for pure assembly and fused assembly-and-compute kernels in\nSection\u00a0 6 ", "word_idx": 34372, "sentence_idx": 463, "label": "unlabeled"}, {"type": "text", "expr": " These kernels cannot assume the results have been\npre-assembled and must maintain and iterate over a workspace index", "word_idx": 34486, "sentence_idx": 464, "label": "unlabeled"}, {"type": "math", "expr": "$$j_{A}$$", "word_idx": 34603, "sentence_idx": 465, "label": "unlabeled"}, {"type": "text", "expr": "2  Matrix Addition", "word_idx": 34608, "sentence_idx": 466, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption Iteration graph before split", "word_idx": 34626, "sentence_idx": 467, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 34666, "sentence_idx": 468, "label": "unlabeled"}, {"type": "text", "expr": "Iteration graph before split", "word_idx": 34677, "sentence_idx": 469, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption Iteration graph after split", "word_idx": 34705, "sentence_idx": 470, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 34744, "sentence_idx": 471, "label": "unlabeled"}, {"type": "text", "expr": "Iteration graph after split", "word_idx": 34755, "sentence_idx": 472, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a08:  \nSparse matrix addition  $A_{ij}=B_{ij}+C_{ij}$ ", "word_idx": 34782, "sentence_idx": 473, "label": "unlabeled"}, {"type": "text", "expr": " Splitting the addition\nand assignment operators removes expensive merge code at the cost of\nreduced temporal locality", "word_idx": 34841, "sentence_idx": 474, "label": "unlabeled"}, {"type": "text", "expr": " The resulting inner loop is similar to sparse\nvector addition after an operator split, which we showed\nin\u00a0Figure\u00a0 6 ", "word_idx": 34959, "sentence_idx": 475, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a08:", "word_idx": 35076, "sentence_idx": 476, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ij}=B_{ij}+C_{ij}$$", "word_idx": 35085, "sentence_idx": 477, "label": "unlabeled"}, {"type": "text", "expr": "Sparse matrix addition demonstrates operator splits for addition operators", "word_idx": 35105, "sentence_idx": 478, "label": "unlabeled"}, {"type": "text", "expr": "\nSparse additions result in involved code to iterate over the union of the\nnonzeros of the two operands, as a multi-way merge with three loops\n\u00a0 ", "word_idx": 35179, "sentence_idx": 479, "label": "unlabeled"}, {"type": "text", "expr": " Figure\u00a0 8  shows the iteration graph for\na sparse matrix addition", "word_idx": 35324, "sentence_idx": 480, "label": "unlabeled"}, {"type": "text", "expr": " When the matrices are stored in the CSR format,\nwhich is sparse in the second dimension, the compiler must emit code to merge\n $B$  and  $C$  at the  $j$  index variable", "word_idx": 35390, "sentence_idx": 481, "label": "unlabeled"}, {"type": "text", "expr": " Such merge code contains many if\nstatements that are expensive on modern processors", "word_idx": 35560, "sentence_idx": 482, "label": "unlabeled"}, {"type": "text", "expr": " Merge code also grows\nexponential with the number of addition, so if many matrices are added it is\nnecessary to either split the input expression or, better, to use an operator\nsplit on the inner index variable so that the outer loop can still be shared", "word_idx": 35644, "sentence_idx": 483, "label": "unlabeled"}, {"type": "text", "expr": "Splitting the addition and assignment operators at the  $j$  index variable\nintroduces a dense row workspace that  $B$  and  $C$  are in turn are added into,\nand that is then copied over to  $A$ ", "word_idx": 35898, "sentence_idx": 484, "label": "unlabeled"}, {"type": "text", "expr": " The resulting code, whose inner loop is\nsimilar to\u00a0Figure\u00a0 6 , has decreased temporal locality, as the\nworkspace reuse distance is can be large, but avoids expensive merges", "word_idx": 36093, "sentence_idx": 485, "label": "unlabeled"}, {"type": "text", "expr": " Whether\nthis results in an overall performance gain depends on the machine and the\nnumber of operands that are merged", "word_idx": 36266, "sentence_idx": 486, "label": "unlabeled"}, {"type": "text", "expr": " We show results for one machine\nin\u00a0Section\u00a0 7 ", "word_idx": 36384, "sentence_idx": 487, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption Iteration graph before split", "word_idx": 36431, "sentence_idx": 488, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 36471, "sentence_idx": 489, "label": "unlabeled"}, {"type": "text", "expr": "Iteration graph before split", "word_idx": 36482, "sentence_idx": 490, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 36510, "sentence_idx": 491, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 36521, "sentence_idx": 492, "label": "unlabeled"}, {"type": "text", "expr": "Iteration graph after  $*$ -split", "word_idx": 36532, "sentence_idx": 493, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 36565, "sentence_idx": 494, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 36576, "sentence_idx": 495, "label": "unlabeled"}, {"type": "text", "expr": "Code diff showing the effect of the  $*$ -split", "word_idx": 36587, "sentence_idx": 496, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 36634, "sentence_idx": 497, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 36645, "sentence_idx": 498, "label": "unlabeled"}, {"type": "text", "expr": "Iteration graph after  $*$  and  $=$  splits", "word_idx": 36656, "sentence_idx": 499, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 36700, "sentence_idx": 500, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 36711, "sentence_idx": 501, "label": "unlabeled"}, {"type": "text", "expr": "Code diff showing the effect of the  $=$ -split", "word_idx": 36722, "sentence_idx": 502, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a09:  \nIteration graphs for the matricized tensor times Khatri-Rao product\n(MTTKRP)  $A_{ij}=\\sum_{kl}B_{ikl}C_{lj}D_{kj}$ ", "word_idx": 36769, "sentence_idx": 503, "label": "unlabeled"}, {"type": "text", "expr": " Splitting the\nmultiplication at  $j$  hoists the multiplication with  $D$  out of the inner\nloop, which removes redundant work", "word_idx": 36897, "sentence_idx": 504, "label": "unlabeled"}, {"type": "text", "expr": " If the matrix  $A$  is sparse, then also\nsplitting the assignment introduces a random access workspace that removes\nthe need to insert into  $A$ ", "word_idx": 37024, "sentence_idx": 505, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a09:", "word_idx": 37170, "sentence_idx": 506, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ij}=\\sum_{kl}B_{ikl}C_{lj}D_{kj}$$", "word_idx": 37179, "sentence_idx": 507, "label": "unlabeled"}, {"type": "text", "expr": "3  Matricized Tensor Times Khatri-Rao Product", "word_idx": 37214, "sentence_idx": 508, "label": "unlabeled"}, {"type": "text", "expr": "The matricized tensor times Khatri-Rao product (MTTKRP) is a critical kernel in\nthe alternating least squares algorithms to compute the canonical polyadic\ndecomposition of tensors\u00a0 ", "word_idx": 37259, "sentence_idx": 509, "label": "unlabeled"}, {"type": "text", "expr": " It generalizes the singular\nvalue decomposition to higher-order tensors, and has applications in data\nanalytics\u00a0 , machine learning\u00a0 ,\nneuroscience\u00a0 , image classification and\ncompression\u00a0 , and other fields\u00a0 ", "word_idx": 37440, "sentence_idx": 510, "label": "unlabeled"}, {"type": "text", "expr": "The MTTKRP can be expressed with tensor index notation as  $A_{ij}=\\sum_{kl}B_{ikl}C_{lj}D_{kj}$ ", "word_idx": 37650, "sentence_idx": 511, "label": "unlabeled"}, {"type": "text", "expr": " That is, we multiply a three-dimensional\ntensor by two matrices in the  $l$  and  $k$  dimensions", "word_idx": 37747, "sentence_idx": 512, "label": "unlabeled"}, {"type": "text", "expr": " These simultaneous\nmultiplications require four nested loops", "word_idx": 37845, "sentence_idx": 513, "label": "unlabeled"}, {"type": "text", "expr": " Figure\u00a0 9  shows\nthe iteration graph before optimization, where the matrices are stored\nrow-major", "word_idx": 37906, "sentence_idx": 514, "label": "unlabeled"}, {"type": "text", "expr": " The iteration graph results in four nested loops", "word_idx": 38004, "sentence_idx": 515, "label": "unlabeled"}, {"type": "text", "expr": " The three\noutermost loops iterate over the sparse data structure of  $B$ , while the\ninnermost loop iterates over the range of the  $j$  index variable", "word_idx": 38053, "sentence_idx": 516, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ij}=\\sum_{kl}B_{ikl}C_{lj}D_{kj}$$", "word_idx": 38205, "sentence_idx": 517, "label": "unlabeled"}, {"type": "text", "expr": "After splitting the multiplication operator at  $j$  we get the iteration graph\nin Figure\u00a0 9 ", "word_idx": 38240, "sentence_idx": 518, "label": "unlabeled"}, {"type": "text", "expr": " The index variable  $j$  has been split in two", "word_idx": 38333, "sentence_idx": 519, "label": "unlabeled"}, {"type": "text", "expr": "\nThe second variable,  $j_{D}$ , is no longer dominated by  $l$ , which means it is\nevaluated higher up in the resulting loop nest", "word_idx": 38380, "sentence_idx": 520, "label": "unlabeled"}, {"type": "text", "expr": " Furthermore, if the matrices\n $C$  and  $D$  were sparse in the second dimension, the split also removes the\nneed to merge their sparse data structures", "word_idx": 38510, "sentence_idx": 521, "label": "unlabeled"}, {"type": "text", "expr": " The code listing in\nFigure\u00a0 9  shows a code diff of the effect of the operator\nsplit on the code when the matrices are dense", "word_idx": 38662, "sentence_idx": 522, "label": "unlabeled"}, {"type": "text", "expr": " The code specific to the\niteration graph before the  $*$ -split is colored red, and the code specific to\nthe iteration graph after the split is colored green", "word_idx": 38787, "sentence_idx": 523, "label": "unlabeled"}, {"type": "text", "expr": " Shared code is not\ncolored", "word_idx": 38945, "sentence_idx": 524, "label": "unlabeled"}, {"type": "text", "expr": " The  $*$ -split results in code where the loop over  $j$ , that\nmultiplies  $B$  with  $D$ , has been lifted out of the  $l$  loop, resulting in\nfewer total multiplication", "word_idx": 38972, "sentence_idx": 525, "label": "unlabeled"}, {"type": "text", "expr": " The drawback is that the workspace reduces\ntemporal locality, as the reuse distance between writing values to it and\nreading them back can be large", "word_idx": 39144, "sentence_idx": 526, "label": "unlabeled"}, {"type": "text", "expr": " Our evaluation in\u00a0Section\u00a0 7  shows\nthat this optimization can result in significant gains on large data sets", "word_idx": 39292, "sentence_idx": 527, "label": "unlabeled"}, {"type": "math", "expr": "$$j_{D}$$", "word_idx": 39402, "sentence_idx": 528, "label": "unlabeled"}, {"type": "text", "expr": "The MTTKRP kernel does two simultaneous matrix multiplications", "word_idx": 39407, "sentence_idx": 529, "label": "unlabeled"}, {"type": "text", "expr": " Like the\nsparse matrix multiplication kernel in\u00a0Section\u00a0 5", "word_idx": 39469, "sentence_idx": 530, "label": "unlabeled"}, {"type": "text", "expr": "1 , therefore, it needs\nto scatter values into the middle of the result matrix  $A$ ", "word_idx": 39528, "sentence_idx": 531, "label": "unlabeled"}, {"type": "text", "expr": " The reason is that\nthe  $j$  and  $j_{D}$  index variables are dominated by reduction variables", "word_idx": 39612, "sentence_idx": 532, "label": "unlabeled"}, {"type": "text", "expr": " If the\nmatrix  $A$  is sparse then inserts are expensive, and the code benefits from\nsplitting the assignment at  $j_{D}$ , as shown in\u00a0Figure\u00a0 9 ", "word_idx": 39708, "sentence_idx": 533, "label": "unlabeled"}, {"type": "text", "expr": "\nThe effect is that values are scattered into a dense workspace with random\naccess, and copied to the result after a full row of the result has been\ncomputed", "word_idx": 39855, "sentence_idx": 534, "label": "unlabeled"}, {"type": "text", "expr": " Figure\u00a0 9  shows a code diff of the effect of\nmaking the result matrix  $A$  sparse and splitting the assignment operator", "word_idx": 40012, "sentence_idx": 535, "label": "unlabeled"}, {"type": "text", "expr": "\nBoth the code from before the split (red) and the code after (green) assumes\nthe operand matrices  $C$  and  $D$  are sparse, as opposed to\nFigure\u00a0 9  where  $C$  and  $D$  were dense", "word_idx": 40134, "sentence_idx": 536, "label": "unlabeled"}, {"type": "text", "expr": " As in the sparse\nmatrix multiplication code, the code after assignment split scatters into a\ndense workspace and, when a full row has been computed, appends the workspace\nnonzeros to the result", "word_idx": 40318, "sentence_idx": 537, "label": "unlabeled"}, {"type": "math", "expr": "$$j_{D}$$", "word_idx": 40512, "sentence_idx": 538, "label": "unlabeled"}, {"type": "math", "expr": "$$j_{D}$$", "word_idx": 40517, "sentence_idx": 539, "label": "unlabeled"}, {"type": "text", "expr": "6  Workspace Assembly", "word_idx": 40522, "sentence_idx": 540, "label": "unlabeled"}, {"type": "text", "expr": "In code listings that compute sparse results, we have so far shown only kernels\nthat compute results without assembling sparse index structures\n(Figures\u00a0 1 ,  6 ,\nand\u00a0 9 )", "word_idx": 40543, "sentence_idx": 541, "label": "unlabeled"}, {"type": "text", "expr": " This let us focus on the loop structures\nwithout the added complexity of workspace assembly", "word_idx": 40714, "sentence_idx": 542, "label": "unlabeled"}, {"type": "text", "expr": " Moreover, it is common in\nnumerical code to separate the kernel that assembles index structures (often\ncalled symbolic computation) from the kernel that computes values (numeric\ncomputation)\u00a0 ", "word_idx": 40806, "sentence_idx": 543, "label": "unlabeled"}, {"type": "text", "expr": " The code generation algorithm\nfor iteration graphs can be used to emit either kernel or a kernel that\nsimultaneously assembles the result index structures and computes its\nvalues\u00a0 ", "word_idx": 40999, "sentence_idx": 544, "label": "unlabeled"}, {"type": "text", "expr": "2017b", "word_idx": 41180, "sentence_idx": 545, "label": "unlabeled"}, {"type": "text", "expr": "When generating assembly kernels from iteration graphs, a workspace consists of\ntwo arrays that together track its nonzero index structure", "word_idx": 41185, "sentence_idx": 546, "label": "unlabeled"}, {"type": "text", "expr": " The first array\n wlist  is a list of coordinates that have been inserted into the\nworkspace, and the second array ( w ) is a boolean array that guards\nagainst redundant inserts into the coordinate list", "word_idx": 41323, "sentence_idx": 547, "label": "unlabeled"}, {"type": "text", "expr": "wlist", "word_idx": 41525, "sentence_idx": 548, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a010:  \nA sparse matrix multiplication assembly kernel (the compute kernel is given\nin\u00a0Figure\u00a0 1 )", "word_idx": 41530, "sentence_idx": 549, "label": "unlabeled"}, {"type": "text", "expr": " The coordinates of row  $i$  are inserted\ninto  wlist  on line\u00a011 and copied to  A  on line 29", "word_idx": 41633, "sentence_idx": 550, "label": "unlabeled"}, {"type": "text", "expr": " The\narray  w  guards against redundant inserts", "word_idx": 41728, "sentence_idx": 551, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a010:", "word_idx": 41775, "sentence_idx": 552, "label": "unlabeled"}, {"type": "text", "expr": "wlist", "word_idx": 41785, "sentence_idx": 553, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a0 10  shows assembly code for sparse matrix\nmultiplication generated from the iteration graph in\u00a0Figure\u00a0 7 ", "word_idx": 41790, "sentence_idx": 554, "label": "unlabeled"}, {"type": "text", "expr": "\nIt is generated from the same iteration graph as the compute kernel\nin\u00a0Figure\u00a0 1 , so the loop structure is the same except for the\nloop to copy the workspace to  A  on line\u00a027", "word_idx": 41903, "sentence_idx": 555, "label": "unlabeled"}, {"type": "text", "expr": " In compute kernels, the\nindex structure of  A  must be pre-assembled, so the code generation\nalgorithm emits a loop to iterate over  A ", "word_idx": 42080, "sentence_idx": 556, "label": "unlabeled"}, {"type": "text", "expr": " In an assembly kernel,\nhowever, it emits code to iterate over the index structure of the workspace", "word_idx": 42216, "sentence_idx": 557, "label": "unlabeled"}, {"type": "text", "expr": "\nFurthermore, the assembly kernel inserts into the workspace index\n( wlist ), on lines\u00a010\u201313, instead of computing a result, and sorts the\nindex list on line\u00a018 so that the new row of  A  is ordered", "word_idx": 42315, "sentence_idx": 558, "label": "unlabeled"}, {"type": "text", "expr": " Note that\nthe sort is optional and only needed if the result must be ordered", "word_idx": 42513, "sentence_idx": 559, "label": "unlabeled"}, {"type": "text", "expr": " For\nexample, one of the MKL matrix multiplication kernels does not sort", "word_idx": 42590, "sentence_idx": 560, "label": "unlabeled"}, {"type": "text", "expr": " Finally,\nthe assembly kernel allocates memory on lines\u00a01\u20132, 21\u201324 (by repeated\ndoubling), and 35", "word_idx": 42662, "sentence_idx": 561, "label": "unlabeled"}, {"type": "text", "expr": "wlist", "word_idx": 42759, "sentence_idx": 562, "label": "unlabeled"}, {"type": "text", "expr": "7  Evaluation", "word_idx": 42764, "sentence_idx": 563, "label": "unlabeled"}, {"type": "text", "expr": "In this section, we evaluate the effectiveness of the workspace optimization by\ncomparing performance against hand-written state-of-the-art sparse libraries", "word_idx": 42777, "sentence_idx": 564, "label": "unlabeled"}, {"type": "text", "expr": "In this section, we evaluate the effectiveness of the workspace optimization by\ncomparing performance against hand-written state-of-the-art sparse libraries", "word_idx": 42933, "sentence_idx": 565, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:  \nTest matrices and tensors from the SuiteSparse Matrix\nCollection\u00a0  and the FROSTT Tensor\nCollection\u00a0 ", "word_idx": 43089, "sentence_idx": 566, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:", "word_idx": 43201, "sentence_idx": 567, "label": "unlabeled"}, {"type": "text", "expr": "2017b", "word_idx": 43209, "sentence_idx": 568, "label": "unlabeled"}, {"type": "text", "expr": "Tensor Domain NNZ Density", "word_idx": 43214, "sentence_idx": 569, "label": "unlabeled"}, {"type": "text", "expr": "Tensor", "word_idx": 43239, "sentence_idx": 570, "label": "unlabeled"}, {"type": "text", "expr": "Tensor", "word_idx": 43245, "sentence_idx": 571, "label": "unlabeled"}, {"type": "text", "expr": "Domain", "word_idx": 43251, "sentence_idx": 572, "label": "unlabeled"}, {"type": "text", "expr": "Domain", "word_idx": 43257, "sentence_idx": 573, "label": "unlabeled"}, {"type": "text", "expr": "Density", "word_idx": 43263, "sentence_idx": 574, "label": "unlabeled"}, {"type": "text", "expr": "Density", "word_idx": 43270, "sentence_idx": 575, "label": "unlabeled"}, {"type": "text", "expr": "bcsstk17 Structural 428,650", "word_idx": 43277, "sentence_idx": 576, "label": "unlabeled"}, {"type": "text", "expr": "bcsstk17", "word_idx": 43304, "sentence_idx": 577, "label": "unlabeled"}, {"type": "text", "expr": "Structural", "word_idx": 43312, "sentence_idx": 578, "label": "unlabeled"}, {"type": "text", "expr": "428,650", "word_idx": 43322, "sentence_idx": 579, "label": "unlabeled"}, {"type": "text", "expr": "$4\\times 10^{-3}$", "word_idx": 43329, "sentence_idx": 580, "label": "unlabeled"}, {"type": "math", "expr": "$$4\\times 10^{-3}$$", "word_idx": 43346, "sentence_idx": 581, "label": "unlabeled"}, {"type": "text", "expr": "pdb1HYS Protein data base 4,344,765", "word_idx": 43361, "sentence_idx": 582, "label": "unlabeled"}, {"type": "text", "expr": "pdb1HYS", "word_idx": 43396, "sentence_idx": 583, "label": "unlabeled"}, {"type": "text", "expr": "Protein data base", "word_idx": 43403, "sentence_idx": 584, "label": "unlabeled"}, {"type": "text", "expr": "4,344,765", "word_idx": 43420, "sentence_idx": 585, "label": "unlabeled"}, {"type": "text", "expr": "$3\\times 10^{-3}$", "word_idx": 43429, "sentence_idx": 586, "label": "unlabeled"}, {"type": "math", "expr": "$$3\\times 10^{-3}$$", "word_idx": 43446, "sentence_idx": 587, "label": "unlabeled"}, {"type": "text", "expr": "rma10 3D CFD 2,329,092", "word_idx": 43461, "sentence_idx": 588, "label": "unlabeled"}, {"type": "text", "expr": "rma10", "word_idx": 43483, "sentence_idx": 589, "label": "unlabeled"}, {"type": "text", "expr": "3D CFD", "word_idx": 43488, "sentence_idx": 590, "label": "unlabeled"}, {"type": "text", "expr": "2,329,092", "word_idx": 43494, "sentence_idx": 591, "label": "unlabeled"}, {"type": "text", "expr": "$1\\times 10^{-3}$", "word_idx": 43503, "sentence_idx": 592, "label": "unlabeled"}, {"type": "math", "expr": "$$1\\times 10^{-3}$$", "word_idx": 43520, "sentence_idx": 593, "label": "unlabeled"}, {"type": "text", "expr": "cant FEM/Cantilever 4,007,383", "word_idx": 43535, "sentence_idx": 594, "label": "unlabeled"}, {"type": "text", "expr": "FEM/Cantilever", "word_idx": 43564, "sentence_idx": 595, "label": "unlabeled"}, {"type": "text", "expr": "4,007,383", "word_idx": 43578, "sentence_idx": 596, "label": "unlabeled"}, {"type": "text", "expr": "$1\\times 10^{-3}$", "word_idx": 43587, "sentence_idx": 597, "label": "unlabeled"}, {"type": "math", "expr": "$$1\\times 10^{-3}$$", "word_idx": 43604, "sentence_idx": 598, "label": "unlabeled"}, {"type": "text", "expr": "consph FEM/Spheres 6,010,480", "word_idx": 43619, "sentence_idx": 599, "label": "unlabeled"}, {"type": "text", "expr": "consph", "word_idx": 43647, "sentence_idx": 600, "label": "unlabeled"}, {"type": "text", "expr": "FEM/Spheres", "word_idx": 43653, "sentence_idx": 601, "label": "unlabeled"}, {"type": "text", "expr": "6,010,480", "word_idx": 43664, "sentence_idx": 602, "label": "unlabeled"}, {"type": "text", "expr": "$9\\times 10^{-4}$", "word_idx": 43673, "sentence_idx": 603, "label": "unlabeled"}, {"type": "math", "expr": "$$9\\times 10^{-4}$$", "word_idx": 43690, "sentence_idx": 604, "label": "unlabeled"}, {"type": "text", "expr": "Facebook Social Media 737,934", "word_idx": 43705, "sentence_idx": 605, "label": "unlabeled"}, {"type": "text", "expr": "Facebook", "word_idx": 43734, "sentence_idx": 606, "label": "unlabeled"}, {"type": "text", "expr": "Social Media", "word_idx": 43742, "sentence_idx": 607, "label": "unlabeled"}, {"type": "text", "expr": "737,934", "word_idx": 43754, "sentence_idx": 608, "label": "unlabeled"}, {"type": "text", "expr": "$1\\times 10^{-7}$", "word_idx": 43761, "sentence_idx": 609, "label": "unlabeled"}, {"type": "math", "expr": "$$1\\times 10^{-7}$$", "word_idx": 43778, "sentence_idx": 610, "label": "unlabeled"}, {"type": "text", "expr": "NELL-2 Machine learning 76,879,419", "word_idx": 43793, "sentence_idx": 611, "label": "unlabeled"}, {"type": "text", "expr": "NELL-2", "word_idx": 43827, "sentence_idx": 612, "label": "unlabeled"}, {"type": "text", "expr": "Machine learning", "word_idx": 43833, "sentence_idx": 613, "label": "unlabeled"}, {"type": "text", "expr": "76,879,419", "word_idx": 43849, "sentence_idx": 614, "label": "unlabeled"}, {"type": "text", "expr": "$2\\times 10^{-5}$", "word_idx": 43859, "sentence_idx": 615, "label": "unlabeled"}, {"type": "math", "expr": "$$2\\times 10^{-5}$$", "word_idx": 43876, "sentence_idx": 616, "label": "unlabeled"}, {"type": "text", "expr": "NELL-1 Machine learning 143,599,552", "word_idx": 43891, "sentence_idx": 617, "label": "unlabeled"}, {"type": "text", "expr": "NELL-1", "word_idx": 43926, "sentence_idx": 618, "label": "unlabeled"}, {"type": "text", "expr": "Machine learning", "word_idx": 43932, "sentence_idx": 619, "label": "unlabeled"}, {"type": "text", "expr": "143,599,552", "word_idx": 43948, "sentence_idx": 620, "label": "unlabeled"}, {"type": "text", "expr": "$9\\times 10^{-13}$", "word_idx": 43959, "sentence_idx": 621, "label": "unlabeled"}, {"type": "math", "expr": "$$9\\times 10^{-13}$$", "word_idx": 43977, "sentence_idx": 622, "label": "unlabeled"}, {"type": "text", "expr": "1  Methodology", "word_idx": 43993, "sentence_idx": 623, "label": "unlabeled"}, {"type": "text", "expr": "All experiments are run on a dual-socket 12-core/24-thread 2", "word_idx": 44007, "sentence_idx": 624, "label": "unlabeled"}, {"type": "text", "expr": "5 GHz Intel Xeon\nE5-2680v3 machine with 30 MB of L3 cache per socket, running Ubuntu 14", "word_idx": 44067, "sentence_idx": 625, "label": "unlabeled"}, {"type": "text", "expr": "5\nLTS", "word_idx": 44154, "sentence_idx": 626, "label": "unlabeled"}, {"type": "text", "expr": " The machine contains 128\u00a0GB of memory and runs Linux kernel version\n3", "word_idx": 44159, "sentence_idx": 627, "label": "unlabeled"}, {"type": "text", "expr": "0 and GCC 5", "word_idx": 44229, "sentence_idx": 628, "label": "unlabeled"}, {"type": "text", "expr": " For all experiments, we ensure the machine is otherwise\nidle and report average cold cache performance, without counting the first run,\nwhich often incurs overheads due to dynamic loading and other first-run\noverheads", "word_idx": 44240, "sentence_idx": 629, "label": "unlabeled"}, {"type": "text", "expr": " Unless otherwise noted, all experiments are single-threaded", "word_idx": 44458, "sentence_idx": 630, "label": "unlabeled"}, {"type": "text", "expr": "All experiments are run on a dual-socket 12-core/24-thread 2", "word_idx": 44518, "sentence_idx": 631, "label": "unlabeled"}, {"type": "text", "expr": "5 GHz Intel Xeon\nE5-2680v3 machine with 30 MB of L3 cache per socket, running Ubuntu 14", "word_idx": 44578, "sentence_idx": 632, "label": "unlabeled"}, {"type": "text", "expr": "5\nLTS", "word_idx": 44665, "sentence_idx": 633, "label": "unlabeled"}, {"type": "text", "expr": " The machine contains 128\u00a0GB of memory and runs Linux kernel version\n3", "word_idx": 44670, "sentence_idx": 634, "label": "unlabeled"}, {"type": "text", "expr": "0 and GCC 5", "word_idx": 44740, "sentence_idx": 635, "label": "unlabeled"}, {"type": "text", "expr": " For all experiments, we ensure the machine is otherwise\nidle and report average cold cache performance, without counting the first run,\nwhich often incurs overheads due to dynamic loading and other first-run\noverheads", "word_idx": 44751, "sentence_idx": 636, "label": "unlabeled"}, {"type": "text", "expr": " Unless otherwise noted, all experiments are single-threaded", "word_idx": 44969, "sentence_idx": 637, "label": "unlabeled"}, {"type": "text", "expr": "We evaluate our approach by comparing performance on linear algebra kernels\nwith Eigen\u00a0  and Intel MKL\u00a0  2018", "word_idx": 45029, "sentence_idx": 638, "label": "unlabeled"}, {"type": "text", "expr": " For tensor algebra,\nwe compare against the Matlab Tensor Toolbox\u00a0  and against\nSPLATT\u00a0 , a high-performance C++ library for sparse tensor\nfactorization", "word_idx": 45138, "sentence_idx": 639, "label": "unlabeled"}, {"type": "text", "expr": " We obtained the real-world inputs for the experiments in\nSections\u00a0 7", "word_idx": 45290, "sentence_idx": 640, "label": "unlabeled"}, {"type": "text", "expr": "2  and\u00a0 7", "word_idx": 45359, "sentence_idx": 641, "label": "unlabeled"}, {"type": "text", "expr": "3  from the SuiteSparse Matrix\nCollection\u00a0  and the FROSTT Tensor Collection\u00a0 \nrespectively", "word_idx": 45368, "sentence_idx": 642, "label": "unlabeled"}, {"type": "text", "expr": " Details of the matrices and tensors used in the experiments are\nshown in Table\u00a0 1 ", "word_idx": 45459, "sentence_idx": 643, "label": "unlabeled"}, {"type": "text", "expr": " We constructed the synthetic sparse inputs\nusing the random matrix generator in  taco \u00a0 , which places\nnonzeros randomly to reach a target sparsity", "word_idx": 45542, "sentence_idx": 644, "label": "unlabeled"}, {"type": "text", "expr": " All sparse matrices are in\ncompressed sparse row (CSR) format", "word_idx": 45690, "sentence_idx": 645, "label": "unlabeled"}, {"type": "text", "expr": "2017b", "word_idx": 45752, "sentence_idx": 646, "label": "unlabeled"}, {"type": "text", "expr": "2017a", "word_idx": 45757, "sentence_idx": 647, "label": "unlabeled"}, {"type": "text", "expr": "2  Sparse Matrix-Matrix Multiplication", "word_idx": 45762, "sentence_idx": 648, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a011:   Sparse matrix multiplication results for\nthe matrices in Table\u00a0 1 ", "word_idx": 45800, "sentence_idx": 649, "label": "unlabeled"}, {"type": "text", "expr": " We show performance for both\nsorted and unsorted column entries; Eigen\u2019s algorithm sorts them while MKL\u2019s\n mkl_sparse_spmm  function leaves them unsorted", "word_idx": 45879, "sentence_idx": 650, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a011:", "word_idx": 46033, "sentence_idx": 651, "label": "unlabeled"}, {"type": "text", "expr": "mkl_sparse_spmm", "word_idx": 46043, "sentence_idx": 652, "label": "unlabeled"}, {"type": "text", "expr": "\\newcolumntype", "word_idx": 46058, "sentence_idx": 653, "label": "unlabeled"}, {"type": "text", "expr": "R[2]\u00bf \\adjustbox angle=#1,lap=0pt-(#2)l\u00a1", "word_idx": 46072, "sentence_idx": 654, "label": "unlabeled"}, {"type": "text", "expr": "\\adjustbox", "word_idx": 46112, "sentence_idx": 655, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:  \nBreakdown of time, in milliseconds (with 3 significant digits), to multiply\nthe test matrices in Table\u00a0 1  with a random operand of\ndensity 0", "word_idx": 46122, "sentence_idx": 656, "label": "unlabeled"}, {"type": "text", "expr": " Running time is given separately for the workspace assemble\nand compute kernels, as well as the variant that assembles and computes in one\nkernel (fused)", "word_idx": 46274, "sentence_idx": 657, "label": "unlabeled"}, {"type": "text", "expr": " Times are compared to the total time spent by Eigen and\nMKL, which do not support separate assembly and compute", "word_idx": 46428, "sentence_idx": 658, "label": "unlabeled"}, {"type": "text", "expr": " For MKL, we use\n mkl_sparse_spmm , which does not sort rows of the output matrix", "word_idx": 46540, "sentence_idx": 659, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:", "word_idx": 46621, "sentence_idx": 660, "label": "unlabeled"}, {"type": "text", "expr": "mkl_sparse_spmm", "word_idx": 46629, "sentence_idx": 661, "label": "unlabeled"}, {"type": "text", "expr": "bcsstk17 bcsstk17 rma10 cant consph", "word_idx": 46644, "sentence_idx": 662, "label": "unlabeled"}, {"type": "text", "expr": "bcsstk17", "word_idx": 46679, "sentence_idx": 663, "label": "unlabeled"}, {"type": "text", "expr": "rma10", "word_idx": 46687, "sentence_idx": 664, "label": "unlabeled"}, {"type": "text", "expr": "consph", "word_idx": 46692, "sentence_idx": 665, "label": "unlabeled"}, {"type": "text", "expr": "Sorted (ms)", "word_idx": 46698, "sentence_idx": 666, "label": "unlabeled"}, {"type": "text", "expr": "Sorted (ms)", "word_idx": 46709, "sentence_idx": 667, "label": "unlabeled"}, {"type": "text", "expr": "assembly 331", "word_idx": 46720, "sentence_idx": 668, "label": "unlabeled"}, {"type": "text", "expr": "8 13204 8758 19979 41284", "word_idx": 46732, "sentence_idx": 669, "label": "unlabeled"}, {"type": "text", "expr": "13204", "word_idx": 46756, "sentence_idx": 670, "label": "unlabeled"}, {"type": "text", "expr": "19979", "word_idx": 46761, "sentence_idx": 671, "label": "unlabeled"}, {"type": "text", "expr": "41284", "word_idx": 46766, "sentence_idx": 672, "label": "unlabeled"}, {"type": "text", "expr": "compute 58", "word_idx": 46771, "sentence_idx": 673, "label": "unlabeled"}, {"type": "text", "expr": "07 2398 1742 4184 8480", "word_idx": 46781, "sentence_idx": 674, "label": "unlabeled"}, {"type": "text", "expr": "assembly+compute 389", "word_idx": 46803, "sentence_idx": 675, "label": "unlabeled"}, {"type": "text", "expr": "9 15602 10500 24163 49764", "word_idx": 46823, "sentence_idx": 676, "label": "unlabeled"}, {"type": "text", "expr": "15602", "word_idx": 46848, "sentence_idx": 677, "label": "unlabeled"}, {"type": "text", "expr": "10500", "word_idx": 46853, "sentence_idx": 678, "label": "unlabeled"}, {"type": "text", "expr": "24163", "word_idx": 46858, "sentence_idx": 679, "label": "unlabeled"}, {"type": "text", "expr": "49764", "word_idx": 46863, "sentence_idx": 680, "label": "unlabeled"}, {"type": "text", "expr": "fused 380", "word_idx": 46868, "sentence_idx": 681, "label": "unlabeled"}, {"type": "text", "expr": "5 15141 10279 23398 48106", "word_idx": 46877, "sentence_idx": 682, "label": "unlabeled"}, {"type": "text", "expr": "15141", "word_idx": 46902, "sentence_idx": 683, "label": "unlabeled"}, {"type": "text", "expr": "10279", "word_idx": 46907, "sentence_idx": 684, "label": "unlabeled"}, {"type": "text", "expr": "23398", "word_idx": 46912, "sentence_idx": 685, "label": "unlabeled"}, {"type": "text", "expr": "48106", "word_idx": 46917, "sentence_idx": 686, "label": "unlabeled"}, {"type": "text", "expr": "Eigen 1015 36555 28585 64706 230695", "word_idx": 46922, "sentence_idx": 687, "label": "unlabeled"}, {"type": "text", "expr": "36555", "word_idx": 46957, "sentence_idx": 688, "label": "unlabeled"}, {"type": "text", "expr": "28585", "word_idx": 46962, "sentence_idx": 689, "label": "unlabeled"}, {"type": "text", "expr": "64706", "word_idx": 46967, "sentence_idx": 690, "label": "unlabeled"}, {"type": "text", "expr": "230695", "word_idx": 46972, "sentence_idx": 691, "label": "unlabeled"}, {"type": "text", "expr": "Unsorted (ms)", "word_idx": 46978, "sentence_idx": 692, "label": "unlabeled"}, {"type": "text", "expr": "Unsorted (ms)", "word_idx": 46991, "sentence_idx": 693, "label": "unlabeled"}, {"type": "text", "expr": "assembly 34", "word_idx": 47004, "sentence_idx": 694, "label": "unlabeled"}, {"type": "text", "expr": "79 1510 949", "word_idx": 47015, "sentence_idx": 695, "label": "unlabeled"}, {"type": "text", "expr": "9 2126 4412", "word_idx": 47026, "sentence_idx": 696, "label": "unlabeled"}, {"type": "text", "expr": "compute 58", "word_idx": 47037, "sentence_idx": 697, "label": "unlabeled"}, {"type": "text", "expr": "02 2328 1849 4459 9624", "word_idx": 47047, "sentence_idx": 698, "label": "unlabeled"}, {"type": "text", "expr": "assembly+compute 92", "word_idx": 47069, "sentence_idx": 699, "label": "unlabeled"}, {"type": "text", "expr": "81 3838 2798", "word_idx": 47088, "sentence_idx": 700, "label": "unlabeled"}, {"type": "text", "expr": "9 6585 14036", "word_idx": 47100, "sentence_idx": 701, "label": "unlabeled"}, {"type": "text", "expr": "14036", "word_idx": 47112, "sentence_idx": 702, "label": "unlabeled"}, {"type": "text", "expr": "fused 76", "word_idx": 47117, "sentence_idx": 703, "label": "unlabeled"}, {"type": "text", "expr": "42 3369 2408 5652 12080", "word_idx": 47125, "sentence_idx": 704, "label": "unlabeled"}, {"type": "text", "expr": "12080", "word_idx": 47148, "sentence_idx": 705, "label": "unlabeled"}, {"type": "text", "expr": "MKL 76", "word_idx": 47153, "sentence_idx": 706, "label": "unlabeled"}, {"type": "text", "expr": "78 3300 2279 5507 13231", "word_idx": 47159, "sentence_idx": 707, "label": "unlabeled"}, {"type": "text", "expr": "13231", "word_idx": 47182, "sentence_idx": 708, "label": "unlabeled"}, {"type": "text", "expr": "Fast sparse matrix multiplication (SpMM) algorithms use workspaces to store\nintermediate values\u00a0 ", "word_idx": 47187, "sentence_idx": 709, "label": "unlabeled"}, {"type": "text", "expr": " We compare our generated workspace\nalgorithm to the SpMM implementations in MKL and Eigen", "word_idx": 47284, "sentence_idx": 710, "label": "unlabeled"}, {"type": "text", "expr": " The approach we\ndescribed in prior work\u00a0  can in theory handle sparse matrix\nmultiplication by inserting into sparse results", "word_idx": 47374, "sentence_idx": 711, "label": "unlabeled"}, {"type": "text", "expr": " The current implementation,\nhowever, does not support this, so we do not compare against its merge-based\napproach", "word_idx": 47499, "sentence_idx": 712, "label": "unlabeled"}, {"type": "text", "expr": " We compute SpMM with two operands: a real-world matrix from\nTable\u00a0 1  and a synthetic matrix generated with a specific\ntarget sparsity, with uniform random placement of nonzeros", "word_idx": 47613, "sentence_idx": 713, "label": "unlabeled"}, {"type": "text", "expr": " Eigen implements a\n sorted  algorithm, which sorts the column entries within each row so\nthey are ordered, while MKL\u2019s  mkl_sparse_spmm  implements an  unsorted  algorithm\u2014the\ncolumn entries may appear in any order", "word_idx": 47791, "sentence_idx": 714, "label": "unlabeled"}, {"type": "text", "expr": " Because these two algorithms have very\ndifferent costs, we implement a workspace variant of each", "word_idx": 48006, "sentence_idx": 715, "label": "unlabeled"}, {"type": "text", "expr": " In addition, we\nimplement two variants of workspace algorithm: one that separates assembly and\ncomputation, and one that fuses the two operations", "word_idx": 48103, "sentence_idx": 716, "label": "unlabeled"}, {"type": "text", "expr": "2017b", "word_idx": 48249, "sentence_idx": 717, "label": "unlabeled"}, {"type": "text", "expr": "sorted", "word_idx": 48254, "sentence_idx": 718, "label": "unlabeled"}, {"type": "text", "expr": "mkl_sparse_spmm", "word_idx": 48260, "sentence_idx": 719, "label": "unlabeled"}, {"type": "text", "expr": "unsorted", "word_idx": 48275, "sentence_idx": 720, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a0 11  shows the running times of matrix multiplication\nfor each matrix in Table\u00a0 1  multiplied by two matrices of\ndifferent densities (1E-4 and 2", "word_idx": 48283, "sentence_idx": 721, "label": "unlabeled"}, {"type": "text", "expr": "5E-3), using our fused workspace implementation", "word_idx": 48434, "sentence_idx": 722, "label": "unlabeled"}, {"type": "text", "expr": "\nOn average, Eigen is slower than our approach, which generates a variant of\nGustavson\u2019s matrix multiplication algorithm, by 2", "word_idx": 48481, "sentence_idx": 723, "label": "unlabeled"}, {"type": "text", "expr": "9 $\\times$  and 3", "word_idx": 48607, "sentence_idx": 724, "label": "unlabeled"}, {"type": "text", "expr": "1 $\\times$ \nrespectively for the two sparsity levels", "word_idx": 48624, "sentence_idx": 725, "label": "unlabeled"}, {"type": "text", "expr": " For the unsorted algorithm, we\ncompare against Intel MKL, and find that our performance is essentially the\nsame on average, with our algorithm being up to 22% faster in some cases", "word_idx": 48676, "sentence_idx": 726, "label": "unlabeled"}, {"type": "text", "expr": "\nOther than one outlier that is 32% slower than MKL, the workspace algorithm is\nnever more than 6% slower than MKL\u2019s hand-optimized SpMM implementation", "word_idx": 48856, "sentence_idx": 727, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 49007, "sentence_idx": 728, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 49013, "sentence_idx": 729, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a0 2  breaks down the running times for the different\ncodes for multiplying with a matrix of density 2", "word_idx": 49019, "sentence_idx": 730, "label": "unlabeled"}, {"type": "text", "expr": " Due to sorting, assembly\ntimes for the sorted algorithm are quite large; however, the compute time is\noccasionally faster than the unsorted compute time, due to improved locality\nwhen accumulating workspace entries into the result matrix", "word_idx": 49125, "sentence_idx": 731, "label": "unlabeled"}, {"type": "text", "expr": " The fused\nalgorithm is also faster when not using sorting, because otherwise the sort (we\nuse the standard C  qsort ) dominates the time", "word_idx": 49363, "sentence_idx": 732, "label": "unlabeled"}, {"type": "text", "expr": "qsort", "word_idx": 49500, "sentence_idx": 733, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a012:  \nMatricized tensor times Khatri-Rao product (MTTKRP) running times,\nnormalized to the workspace algorithm running time", "word_idx": 49505, "sentence_idx": 734, "label": "unlabeled"}, {"type": "text", "expr": " Only compute times are\nshown; assembly times are negligible because the outputs are dense", "word_idx": 49635, "sentence_idx": 735, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a012:", "word_idx": 49725, "sentence_idx": 736, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a013:  \nMTTKRP compute time as we vary the density of the matrix operands, for the\nthree test tensors", "word_idx": 49735, "sentence_idx": 737, "label": "unlabeled"}, {"type": "text", "expr": " We compare MTTKRP computed with a workspace when the\nmatrix operands are passed in as dense matrices against an implementation\nthat takes sparse matrices as inputs and outputs a sparse matrix", "word_idx": 49841, "sentence_idx": 738, "label": "unlabeled"}, {"type": "text", "expr": " In all\ncases, the tensor is passed in using a sparse format", "word_idx": 50033, "sentence_idx": 739, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a013:", "word_idx": 50093, "sentence_idx": 740, "label": "unlabeled"}, {"type": "text", "expr": "3  Matricized Tensor Times Khatri-Rao Product", "word_idx": 50103, "sentence_idx": 741, "label": "unlabeled"}, {"type": "text", "expr": "Matricized tensor times Khatri-Rao product (MTTKRP) is used to compute\ngeneralizations of SVD factorization for tensors in data analytics", "word_idx": 50148, "sentence_idx": 742, "label": "unlabeled"}, {"type": "text", "expr": " It takes\nas input a sparse 3-tensor and two matrices, and outputs a matrix", "word_idx": 50285, "sentence_idx": 743, "label": "unlabeled"}, {"type": "text", "expr": "\nFigure\u00a0 12  shows the results for our workspace algorithm\non three input tensors, compared to  taco  and the hand-coded SPLATT library", "word_idx": 50360, "sentence_idx": 744, "label": "unlabeled"}, {"type": "text", "expr": "\nWe show only compute times, as the assembly times are negligible because the\noutputs are dense", "word_idx": 50495, "sentence_idx": 745, "label": "unlabeled"}, {"type": "text", "expr": "For the NELL-1 and NELL-2 tensors, the workspace algorithm outperforms the\nmerge-based algorithm in  taco  and is within 16% and 12% of the hand-coded\nperformance of SPLATT", "word_idx": 50590, "sentence_idx": 746, "label": "unlabeled"}, {"type": "text", "expr": " On the smaller Facebook dataset, the merge algorithm is\nfaster than both our implementation and SPLATT\u2019s", "word_idx": 50762, "sentence_idx": 747, "label": "unlabeled"}, {"type": "text", "expr": " That is, different inputs\nperform better with different algorithms, which demonstrates the advantage of\nbeing able to generate both versions of the algorithm", "word_idx": 50867, "sentence_idx": 748, "label": "unlabeled"}, {"type": "text", "expr": "4  Matricized Tensor Times Khatri-Rao Product with Sparse Matrices", "word_idx": 51025, "sentence_idx": 749, "label": "unlabeled"}, {"type": "text", "expr": "It is useful to support MTTKRP where both the tensor and matrix operands are\nsparse\u00a0 ", "word_idx": 51091, "sentence_idx": 750, "label": "unlabeled"}, {"type": "text", "expr": " If the result is also sparse, then the MTTKRP can be\nmust faster since it only needs to iterate over nonzeros", "word_idx": 51176, "sentence_idx": 751, "label": "unlabeled"}, {"type": "text", "expr": " The code is tricky\nto write, however, and cannot be generated by the current version of  taco ,\nalthough the prior merge-based theory supports it", "word_idx": 51286, "sentence_idx": 752, "label": "unlabeled"}, {"type": "text", "expr": " In this section, we use a\nworkspace implementation of sparse MTTKRP enabled by the operator split\noptimization", "word_idx": 51432, "sentence_idx": 753, "label": "unlabeled"}, {"type": "text", "expr": " As far as we are aware, ours is the first implementation of an\nMTTKRP algorithm where all operands are sparse and the output is a sparse\nmatrix", "word_idx": 51543, "sentence_idx": 754, "label": "unlabeled"}, {"type": "text", "expr": "2017a", "word_idx": 51687, "sentence_idx": 755, "label": "unlabeled"}, {"type": "text", "expr": "Which version is faster depends on the density of the sparse operands", "word_idx": 51692, "sentence_idx": 756, "label": "unlabeled"}, {"type": "text", "expr": "\nFigure\u00a0 13  shows experiments that compares the compute\ntimes for MTTKRP with sparse matrices against MTTKRP with dense matrices, as we\nvary the density of the randomly generated input matrices", "word_idx": 51761, "sentence_idx": 757, "label": "unlabeled"}, {"type": "text", "expr": " Note that the dense\nmatrix version should have the same performance regardless of sparsity and any\nvariation is likely due to system noise", "word_idx": 51955, "sentence_idx": 758, "label": "unlabeled"}, {"type": "text", "expr": " For each of the tensors, the\ncrossover point is at about 50% nonzero values, showing that such a sparse\nalgorithm can be faster even with only a modest amount of sparsity in the\ninputs", "word_idx": 52094, "sentence_idx": 759, "label": "unlabeled"}, {"type": "text", "expr": " At the extreme, matrix operands that are 99", "word_idx": 52279, "sentence_idx": 760, "label": "unlabeled"}, {"type": "text", "expr": "99% sparse can result in\nspeedups of 4", "word_idx": 52323, "sentence_idx": 761, "label": "unlabeled"}, {"type": "text", "expr": "5\u201311 $\\times$  for our three test tensors", "word_idx": 52361, "sentence_idx": 762, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 52402, "sentence_idx": 763, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a014:  Scaling plot showing the time to assemble and compute  $n$  matrix additions with Eigen,\nMKL, taco binary operations, a single multi-operand taco function, and workspaces", "word_idx": 52408, "sentence_idx": 764, "label": "unlabeled"}, {"type": "text", "expr": "\nThe matrices are described in Table\u00a0 3 ", "word_idx": 52590, "sentence_idx": 765, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a014:", "word_idx": 52630, "sentence_idx": 766, "label": "unlabeled"}, {"type": "text", "expr": "5  Sparse Matrix Addition", "word_idx": 52640, "sentence_idx": 767, "label": "unlabeled"}, {"type": "text", "expr": "To demonstrate the utility of workspaces for sparse matrix addition (SpAdd), we\nshow that the algorithm scales as we increase the number of operands", "word_idx": 52665, "sentence_idx": 768, "label": "unlabeled"}, {"type": "text", "expr": " In\nFigure\u00a0 14 , we compare the workspace algorithm to  taco \nusing binary operations (as a library would be implemented),  taco  generating\na single function for the additions, Intel MKL (using its inspector-executor\nSpAdd implementation), and Eigen", "word_idx": 52813, "sentence_idx": 769, "label": "unlabeled"}, {"type": "text", "expr": " We pre-generate  $k$  matrices with the target\nsparsities chosen uniformly randomly from the range  $1E^{-4}$ \u20130", "word_idx": 53063, "sentence_idx": 770, "label": "unlabeled"}, {"type": "text", "expr": "04 and always add in\nthe same order and with the same matrices for each library", "word_idx": 53176, "sentence_idx": 771, "label": "unlabeled"}, {"type": "math", "expr": "$$1E^{-4}$$", "word_idx": 53255, "sentence_idx": 772, "label": "unlabeled"}, {"type": "text", "expr": "The results of this experiment show two things", "word_idx": 53262, "sentence_idx": 773, "label": "unlabeled"}, {"type": "text", "expr": " First, that the libraries are\nhampered by the restriction that they perform addition two operands at a time,\nhaving to construct and compute multiple temporaries, resulting in less\nperformance than is possible using code generation", "word_idx": 53308, "sentence_idx": 774, "label": "unlabeled"}, {"type": "text", "expr": " Even given this approach,\n taco  is faster than Intel MKL by 2", "word_idx": 53540, "sentence_idx": 775, "label": "unlabeled"}, {"type": "text", "expr": "8 $\\times$  on average, while Eigen and\n taco  show competitive performance", "word_idx": 53603, "sentence_idx": 776, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 53678, "sentence_idx": 777, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a03:   Breakdown of sparse matrix addition time\nin ms for adding 7 matrices, for all codes\nThe operands are randomly-generated sparse matrices of density\n2", "word_idx": 53684, "sentence_idx": 778, "label": "unlabeled"}, {"type": "text", "expr": "56E-02,\n1", "word_idx": 53843, "sentence_idx": 779, "label": "unlabeled"}, {"type": "text", "expr": "68E-03,\n2", "word_idx": 53852, "sentence_idx": 780, "label": "unlabeled"}, {"type": "text", "expr": "89E-04,\n2", "word_idx": 53861, "sentence_idx": 781, "label": "unlabeled"}, {"type": "text", "expr": "50E-03,\n2", "word_idx": 53870, "sentence_idx": 782, "label": "unlabeled"}, {"type": "text", "expr": "92E-03,\n2", "word_idx": 53879, "sentence_idx": 783, "label": "unlabeled"}, {"type": "text", "expr": "96E-02,\n1", "word_idx": 53888, "sentence_idx": 784, "label": "unlabeled"}, {"type": "text", "expr": "06E-02,\nrespectively", "word_idx": 53897, "sentence_idx": 785, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a03:", "word_idx": 53917, "sentence_idx": 786, "label": "unlabeled"}, {"type": "text", "expr": "Code Assembly Compute", "word_idx": 53925, "sentence_idx": 787, "label": "unlabeled"}, {"type": "text", "expr": "Assembly", "word_idx": 53946, "sentence_idx": 788, "label": "unlabeled"}, {"type": "text", "expr": "Compute", "word_idx": 53954, "sentence_idx": 789, "label": "unlabeled"}, {"type": "text", "expr": "taco binop 247 211", "word_idx": 53961, "sentence_idx": 790, "label": "unlabeled"}, {"type": "text", "expr": "taco 190 182", "word_idx": 53979, "sentence_idx": 791, "label": "unlabeled"}, {"type": "text", "expr": "workspace 190 93", "word_idx": 53991, "sentence_idx": 792, "label": "unlabeled"}, {"type": "text", "expr": "Eigen 436", "word_idx": 54007, "sentence_idx": 793, "label": "unlabeled"}, {"type": "text", "expr": "MKL 1141", "word_idx": 54016, "sentence_idx": 794, "label": "unlabeled"}, {"type": "text", "expr": "Secondly, the experiment shows the value of being able to produce both\nmerge-based and workspace-based implementations of SpAdd", "word_idx": 54024, "sentence_idx": 795, "label": "unlabeled"}, {"type": "text", "expr": " At up to four\nadditions, the two versions are competitive, with the merge-based code being\nslightly faster", "word_idx": 54151, "sentence_idx": 796, "label": "unlabeled"}, {"type": "text", "expr": " However, with increasing numbers of additions, the workspace\ncode begins to outperform the  taco  implementation, showing an increasing gap\nas more operands are added", "word_idx": 54258, "sentence_idx": 797, "label": "unlabeled"}, {"type": "text", "expr": " Table\u00a0 3  breaks down the\nperformance of adding 7 operands, separating out assembly time for the\n taco -based and workspace implementations", "word_idx": 54425, "sentence_idx": 798, "label": "unlabeled"}, {"type": "text", "expr": " For this experiment, we reuse the\nmatrix assembly code produced by taco to assemble the output, but compute\nusing a workspace", "word_idx": 54565, "sentence_idx": 799, "label": "unlabeled"}, {"type": "text", "expr": " Most of the time is spent in assembly, which is unsurprising,\ngiven that assembly requires memory allocations, while the computation performs only\npoint-wise work without the kinds of reductions found in MTTKRP and SpMM", "word_idx": 54691, "sentence_idx": 800, "label": "unlabeled"}, {"type": "text", "expr": "8  Related Work", "word_idx": 54911, "sentence_idx": 801, "label": "unlabeled"}, {"type": "text", "expr": "Related work is divided into work on tensor algebra compilation, work on manual\nworkspace optimizations of matrix and tensor kernels, and work on general loop\noptimization", "word_idx": 54926, "sentence_idx": 802, "label": "unlabeled"}, {"type": "text", "expr": "Related work is divided into work on tensor algebra compilation, work on manual\nworkspace optimizations of matrix and tensor kernels, and work on general loop\noptimization", "word_idx": 55097, "sentence_idx": 803, "label": "unlabeled"}, {"type": "text", "expr": "There have been much work on optimizing dense matrix and tensor\ncomputations\u00a0 ", "word_idx": 55268, "sentence_idx": 804, "label": "unlabeled"}, {"type": "text", "expr": " Researchers have also\nworked on compilation and code generation of sparse matrix computations,\nstarting with the work of  Bik and Wijshoff \u00a0  and the Bernoulli\nsystem\u00a0 ", "word_idx": 55346, "sentence_idx": 805, "label": "unlabeled"}, {"type": "text", "expr": " Recently, we proposed a tensor algebra compilation\ntheory built on an intermediate representation called iteration\ngraphs\u00a0  that are constructed from tensor index notation", "word_idx": 55515, "sentence_idx": 806, "label": "unlabeled"}, {"type": "text", "expr": " We\ngave an algorithm to generate code, but did not introduce any optimization\npasses", "word_idx": 55687, "sentence_idx": 807, "label": "unlabeled"}, {"type": "text", "expr": " The operator split optimization presented in this paper is, to the\nbest of our knowledge, the first optimization on iteration graphs", "word_idx": 55772, "sentence_idx": 808, "label": "unlabeled"}, {"type": "text", "expr": " As we have\nshown, it improves the performance of the kernels generated from many iteration\ngraphs by removing merges, hoisting loop invariant code, and handling\nscattering to sparse result tensors", "word_idx": 55905, "sentence_idx": 809, "label": "unlabeled"}, {"type": "text", "expr": "Bik and Wijshoff", "word_idx": 56102, "sentence_idx": 810, "label": "unlabeled"}, {"type": "text", "expr": "2017b", "word_idx": 56118, "sentence_idx": 811, "label": "unlabeled"}, {"type": "text", "expr": "The first use of dense workspaces for sparse matrix computations is\n Gustavson \u2019s sparse matrix multiplication implementation, that\nwe recreate with an operator split in\u00a0Figure\u00a0 7  to produce the code in\nand\u00a0Figure\u00a0 1 \u00a0 ", "word_idx": 56123, "sentence_idx": 812, "label": "unlabeled"}, {"type": "text", "expr": " A workspace used for\naccumulating temporary values is referred to as an expanded real accumulator\nin\u00a0  and as an abstract sparse accumulator data structure\nin\u00a0 ", "word_idx": 56343, "sentence_idx": 813, "label": "unlabeled"}, {"type": "text", "expr": " Dense workspaces and blocking are used to produce fast\nparallel code by\u00a0 Patwary et\u00a0al", "word_idx": 56504, "sentence_idx": 814, "label": "unlabeled"}, {"type": "text", "expr": " They also tried\na hash map workspace, but report that it did not have good performance for\ntheir use", "word_idx": 56591, "sentence_idx": 815, "label": "unlabeled"}, {"type": "text", "expr": " Furthermore,  Bulu\u00e7 et\u00a0al", "word_idx": 56692, "sentence_idx": 816, "label": "unlabeled"}, {"type": "text", "expr": "  use blocking and workspaces to\ndevelop sparse matrix-vector multiplication algorithms for the CSB data\nstructure that are equally fast for  $Ax$  and  $A^{T}x$ \u00a0 ", "word_idx": 56718, "sentence_idx": 817, "label": "unlabeled"}, {"type": "text", "expr": "\nFinally,  Smith et\u00a0al", "word_idx": 56882, "sentence_idx": 818, "label": "unlabeled"}, {"type": "text", "expr": "  uses a workspace to hoist loop-invariant code\nin their implementation of MTTKRP in the SPLATT library\u00a0 ", "word_idx": 56904, "sentence_idx": 819, "label": "unlabeled"}, {"type": "text", "expr": " We\nre-create this optimization with an operator split\nin\u00a0Figure\u00a0 9  and show the resulting source code\nin\u00a0Figure\u00a0 9 ", "word_idx": 57009, "sentence_idx": 820, "label": "unlabeled"}, {"type": "text", "expr": "Gustavson", "word_idx": 57126, "sentence_idx": 821, "label": "unlabeled"}, {"type": "text", "expr": "Patwary et\u00a0al", "word_idx": 57135, "sentence_idx": 822, "label": "unlabeled"}, {"type": "text", "expr": "Bulu\u00e7 et\u00a0al", "word_idx": 57148, "sentence_idx": 823, "label": "unlabeled"}, {"type": "math", "expr": "$$A^{T}x$$", "word_idx": 57159, "sentence_idx": 824, "label": "unlabeled"}, {"type": "text", "expr": "Smith et\u00a0al", "word_idx": 57165, "sentence_idx": 825, "label": "unlabeled"}, {"type": "text", "expr": "One use of operator splitting in loop nests, in addition to removing multi-way\nmerge code and scatters into sparse results, is to split apart computation that\nmay take place at different loop levels", "word_idx": 57176, "sentence_idx": 826, "label": "unlabeled"}, {"type": "text", "expr": " This results in operations being\nhoisted to a higher loop nest", "word_idx": 57374, "sentence_idx": 827, "label": "unlabeled"}, {"type": "text", "expr": " Loop invariant code motion has a long history\nin compilers, going back to the first FORTRAN compiler in\n1957\u00a0 ", "word_idx": 57437, "sentence_idx": 828, "label": "unlabeled"}, {"type": "text", "expr": " Recently, researchers have found new opportunities for\nremoving redundancy in loops by taking advantage of high-level algebraic\nknowledge\u00a0 ", "word_idx": 57548, "sentence_idx": 829, "label": "unlabeled"}, {"type": "text", "expr": " The operator split optimization is done on the\nhigh-level iteration graph intermediate representation that expose sparse\ndependencies", "word_idx": 57688, "sentence_idx": 830, "label": "unlabeled"}, {"type": "text", "expr": " Since it is applied prior to code generation, it avoids the need\nto analyze low-level code", "word_idx": 57822, "sentence_idx": 831, "label": "unlabeled"}, {"type": "text", "expr": " It can therefore apply loop invariant code motion\nto loops that implement sparse computations with many branches", "word_idx": 57913, "sentence_idx": 832, "label": "unlabeled"}, {"type": "text", "expr": "9  Conclusion", "word_idx": 58026, "sentence_idx": 833, "label": "unlabeled"}, {"type": "text", "expr": "This paper presented the first compiler optimization on the iteration graph\nintermediate representation for tensor computations", "word_idx": 58039, "sentence_idx": 834, "label": "unlabeled"}, {"type": "text", "expr": " A single\noperation\u2014operator split\u2014generalizes several manual optimizations described\nin the literature on sparse matrix and sparse tensor codes", "word_idx": 58166, "sentence_idx": 835, "label": "unlabeled"}, {"type": "text", "expr": " These\noptimizations remove expensive merge code, avoid the need to scatter into\nsparse results, and hoist partial sparse tensor computations out of inner\nloops", "word_idx": 58310, "sentence_idx": 836, "label": "unlabeled"}, {"type": "text", "expr": " Future work includes exploring trade-offs for different types of\nworkspaces and automating the decision of when to split", "word_idx": 58470, "sentence_idx": 837, "label": "unlabeled"}, {"type": "text", "expr": "This paper presented the first compiler optimization on the iteration graph\nintermediate representation for tensor computations", "word_idx": 58591, "sentence_idx": 838, "label": "unlabeled"}, {"type": "text", "expr": " A single\noperation\u2014operator split\u2014generalizes several manual optimizations described\nin the literature on sparse matrix and sparse tensor codes", "word_idx": 58718, "sentence_idx": 839, "label": "unlabeled"}, {"type": "text", "expr": " These\noptimizations remove expensive merge code, avoid the need to scatter into\nsparse results, and hoist partial sparse tensor computations out of inner\nloops", "word_idx": 58862, "sentence_idx": 840, "label": "unlabeled"}, {"type": "text", "expr": " Future work includes exploring trade-offs for different types of\nworkspaces and automating the decision of when to split", "word_idx": 59022, "sentence_idx": 841, "label": "unlabeled"}, {"type": "text", "expr": "References", "word_idx": 59143, "sentence_idx": 842, "label": "unlabeled"}, {"type": "text", "expr": "Abadi et\u00a0al", "word_idx": 59153, "sentence_idx": 843, "label": "unlabeled"}, {"type": "text", "expr": " (2016) \nMart\u00edn Abadi, Paul\nBarham, Jianmin Chen, Zhifeng Chen,\nAndy Davis, Jeffrey Dean,\nMatthieu Devin, Sanjay Ghemawat,\nGeoffrey Irving, Michael Isard,\nManjunath Kudlur, Josh Levenberg,\nRajat Monga, Sherry Moore,\nDerek\u00a0G", "word_idx": 59164, "sentence_idx": 844, "label": "unlabeled"}, {"type": "text", "expr": " Murray, Benoit Steiner,\nPaul Tucker, Vijay Vasudevan,\nPete Warden, Martin Wicke,\nYuan Yu, and Xiaoqiang Zheng", "word_idx": 59387, "sentence_idx": 845, "label": "unlabeled"}, {"type": "text", "expr": "\n2016", "word_idx": 59497, "sentence_idx": 846, "label": "unlabeled"}, {"type": "text", "expr": "Abadi et\u00a0al", "word_idx": 59502, "sentence_idx": 847, "label": "unlabeled"}, {"type": "text", "expr": " (2016)", "word_idx": 59513, "sentence_idx": 848, "label": "unlabeled"}, {"type": "text", "expr": "Mart\u00edn Abadi, Paul\nBarham, Jianmin Chen, Zhifeng Chen,\nAndy Davis, Jeffrey Dean,\nMatthieu Devin, Sanjay Ghemawat,\nGeoffrey Irving, Michael Isard,\nManjunath Kudlur, Josh Levenberg,\nRajat Monga, Sherry Moore,\nDerek\u00a0G", "word_idx": 59520, "sentence_idx": 849, "label": "unlabeled"}, {"type": "text", "expr": " Murray, Benoit Steiner,\nPaul Tucker, Vijay Vasudevan,\nPete Warden, Martin Wicke,\nYuan Yu, and Xiaoqiang Zheng", "word_idx": 59734, "sentence_idx": 850, "label": "unlabeled"}, {"type": "text", "expr": "\n2016", "word_idx": 59844, "sentence_idx": 851, "label": "unlabeled"}, {"type": "text", "expr": "TensorFlow: A System for Large-scale Machine\nLearning", "word_idx": 59849, "sentence_idx": 852, "label": "unlabeled"}, {"type": "text", "expr": " In  Proceedings of the 12th USENIX\nConference on Operating Systems Design and Implementation (OSDI\u201916) ", "word_idx": 59902, "sentence_idx": 853, "label": "unlabeled"}, {"type": "text", "expr": " USENIX Association,\nBerkeley, CA, USA, 265\u2013283", "word_idx": 60006, "sentence_idx": 854, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the 12th USENIX\nConference on Operating Systems Design and Implementation", "word_idx": 60053, "sentence_idx": 855, "label": "unlabeled"}, {"type": "text", "expr": "(OSDI\u201916)", "word_idx": 60141, "sentence_idx": 856, "label": "unlabeled"}, {"type": "text", "expr": "http://dl", "word_idx": 60150, "sentence_idx": 857, "label": "unlabeled"}, {"type": "text", "expr": "org/citation", "word_idx": 60159, "sentence_idx": 858, "label": "unlabeled"}, {"type": "text", "expr": "cfm?id=3026877", "word_idx": 60171, "sentence_idx": 859, "label": "unlabeled"}, {"type": "text", "expr": "3026899", "word_idx": 60185, "sentence_idx": 860, "label": "unlabeled"}, {"type": "text", "expr": "http://dl", "word_idx": 60192, "sentence_idx": 861, "label": "unlabeled"}, {"type": "text", "expr": "org/citation", "word_idx": 60201, "sentence_idx": 862, "label": "unlabeled"}, {"type": "text", "expr": "cfm?id=3026877", "word_idx": 60213, "sentence_idx": 863, "label": "unlabeled"}, {"type": "text", "expr": "3026899", "word_idx": 60227, "sentence_idx": 864, "label": "unlabeled"}, {"type": "text", "expr": "Anandkumar et\u00a0al", "word_idx": 60234, "sentence_idx": 865, "label": "unlabeled"}, {"type": "text", "expr": " (2014) \nAnimashree Anandkumar,\nRong Ge, Daniel Hsu,\nSham\u00a0M", "word_idx": 60250, "sentence_idx": 866, "label": "unlabeled"}, {"type": "text", "expr": " Kakade, and Matus Telgarsky", "word_idx": 60309, "sentence_idx": 867, "label": "unlabeled"}, {"type": "text", "expr": "\n2014", "word_idx": 60337, "sentence_idx": 868, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Tensor Decompositions for Learning Latent Variable\nModels", "word_idx": 60342, "sentence_idx": 869, "label": "unlabeled"}, {"type": "text", "expr": "Anandkumar et\u00a0al", "word_idx": 60402, "sentence_idx": 870, "label": "unlabeled"}, {"type": "text", "expr": " (2014)", "word_idx": 60418, "sentence_idx": 871, "label": "unlabeled"}, {"type": "text", "expr": "Animashree Anandkumar,\nRong Ge, Daniel Hsu,\nSham\u00a0M", "word_idx": 60425, "sentence_idx": 872, "label": "unlabeled"}, {"type": "text", "expr": " Kakade, and Matus Telgarsky", "word_idx": 60475, "sentence_idx": 873, "label": "unlabeled"}, {"type": "text", "expr": "\n2014", "word_idx": 60503, "sentence_idx": 874, "label": "unlabeled"}, {"type": "text", "expr": "Tensor Decompositions for Learning Latent Variable\nModels", "word_idx": 60508, "sentence_idx": 875, "label": "unlabeled"}, {"type": "text", "expr": " Mach", "word_idx": 60565, "sentence_idx": 876, "label": "unlabeled"}, {"type": "text", "expr": " Learn", "word_idx": 60570, "sentence_idx": 877, "label": "unlabeled"}, {"type": "text", "expr": "  15,\nArticle 1 (Jan", "word_idx": 60576, "sentence_idx": 878, "label": "unlabeled"}, {"type": "text", "expr": " 2014),\n60\u00a0pages", "word_idx": 60596, "sentence_idx": 879, "label": "unlabeled"}, {"type": "text", "expr": " Mach", "word_idx": 60612, "sentence_idx": 880, "label": "unlabeled"}, {"type": "text", "expr": " Learn", "word_idx": 60617, "sentence_idx": 881, "label": "unlabeled"}, {"type": "text", "expr": "Auer\net\u00a0al", "word_idx": 60623, "sentence_idx": 882, "label": "unlabeled"}, {"type": "text", "expr": " (2006) \nAlexander\u00a0A", "word_idx": 60633, "sentence_idx": 883, "label": "unlabeled"}, {"type": "text", "expr": " Auer, Gerald\nBaumgartner, David\u00a0E", "word_idx": 60653, "sentence_idx": 884, "label": "unlabeled"}, {"type": "text", "expr": " Bernholdt, Alina\nBibireata, Venkatesh Choppella, Daniel\nCociorva, Xiaoyang Gao, Robert Harrison,\nSriram Krishnamoorthy, Sandhya Krishnan,\nChi-Chung Lam, Qingda Lu,\nMarcel Nooijen, Russell Pitzer,\nJ", "word_idx": 60687, "sentence_idx": 885, "label": "unlabeled"}, {"type": "text", "expr": " Ramanujam, P", "word_idx": 60885, "sentence_idx": 886, "label": "unlabeled"}, {"type": "text", "expr": " Sadayappan, and\nAlexander Sibiryakov", "word_idx": 60898, "sentence_idx": 887, "label": "unlabeled"}, {"type": "text", "expr": " 2006", "word_idx": 60935, "sentence_idx": 888, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Automatic code generation for many-body electronic\nstructure methods: the tensor contraction engine", "word_idx": 60940, "sentence_idx": 889, "label": "unlabeled"}, {"type": "text", "expr": "Auer\net\u00a0al", "word_idx": 61042, "sentence_idx": 890, "label": "unlabeled"}, {"type": "text", "expr": " (2006)", "word_idx": 61052, "sentence_idx": 891, "label": "unlabeled"}, {"type": "text", "expr": "Alexander\u00a0A", "word_idx": 61059, "sentence_idx": 892, "label": "unlabeled"}, {"type": "text", "expr": " Auer, Gerald\nBaumgartner, David\u00a0E", "word_idx": 61070, "sentence_idx": 893, "label": "unlabeled"}, {"type": "text", "expr": " Bernholdt, Alina\nBibireata, Venkatesh Choppella, Daniel\nCociorva, Xiaoyang Gao, Robert Harrison,\nSriram Krishnamoorthy, Sandhya Krishnan,\nChi-Chung Lam, Qingda Lu,\nMarcel Nooijen, Russell Pitzer,\nJ", "word_idx": 61104, "sentence_idx": 894, "label": "unlabeled"}, {"type": "text", "expr": " Ramanujam, P", "word_idx": 61302, "sentence_idx": 895, "label": "unlabeled"}, {"type": "text", "expr": " Sadayappan, and\nAlexander Sibiryakov", "word_idx": 61315, "sentence_idx": 896, "label": "unlabeled"}, {"type": "text", "expr": " 2006", "word_idx": 61352, "sentence_idx": 897, "label": "unlabeled"}, {"type": "text", "expr": "Automatic code generation for many-body electronic\nstructure methods: the tensor contraction engine", "word_idx": 61357, "sentence_idx": 898, "label": "unlabeled"}, {"type": "text", "expr": "Molecular Physics  104,\n2 (2006), 211\u2013228", "word_idx": 61456, "sentence_idx": 899, "label": "unlabeled"}, {"type": "text", "expr": "Molecular Physics", "word_idx": 61497, "sentence_idx": 900, "label": "unlabeled"}, {"type": "text", "expr": "Backus (1978) \nJohn Backus", "word_idx": 61514, "sentence_idx": 901, "label": "unlabeled"}, {"type": "text", "expr": "\n1978", "word_idx": 61540, "sentence_idx": 902, "label": "unlabeled"}, {"type": "text", "expr": "Backus (1978)", "word_idx": 61545, "sentence_idx": 903, "label": "unlabeled"}, {"type": "text", "expr": "John Backus", "word_idx": 61558, "sentence_idx": 904, "label": "unlabeled"}, {"type": "text", "expr": "\n1978", "word_idx": 61569, "sentence_idx": 905, "label": "unlabeled"}, {"type": "text", "expr": "The history of FORTRAN I, II, and III", "word_idx": 61574, "sentence_idx": 906, "label": "unlabeled"}, {"type": "text", "expr": " In\n History of programming languages I ", "word_idx": 61611, "sentence_idx": 907, "label": "unlabeled"}, {"type": "text", "expr": " ACM,\n25\u201374", "word_idx": 61651, "sentence_idx": 908, "label": "unlabeled"}, {"type": "text", "expr": "History of programming languages I", "word_idx": 61662, "sentence_idx": 909, "label": "unlabeled"}, {"type": "text", "expr": "Bader\net\u00a0al", "word_idx": 61696, "sentence_idx": 910, "label": "unlabeled"}, {"type": "text", "expr": " (2008) \nBrett\u00a0W", "word_idx": 61707, "sentence_idx": 911, "label": "unlabeled"}, {"type": "text", "expr": " Bader, Michael\u00a0W", "word_idx": 61723, "sentence_idx": 912, "label": "unlabeled"}, {"type": "text", "expr": "\nBerry, and Murray Browne", "word_idx": 61740, "sentence_idx": 913, "label": "unlabeled"}, {"type": "text", "expr": "\n2008", "word_idx": 61765, "sentence_idx": 914, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Springer London, 147\u2013163", "word_idx": 61770, "sentence_idx": 915, "label": "unlabeled"}, {"type": "text", "expr": "Bader\net\u00a0al", "word_idx": 61797, "sentence_idx": 916, "label": "unlabeled"}, {"type": "text", "expr": " (2008)", "word_idx": 61808, "sentence_idx": 917, "label": "unlabeled"}, {"type": "text", "expr": "Brett\u00a0W", "word_idx": 61815, "sentence_idx": 918, "label": "unlabeled"}, {"type": "text", "expr": " Bader, Michael\u00a0W", "word_idx": 61822, "sentence_idx": 919, "label": "unlabeled"}, {"type": "text", "expr": "\nBerry, and Murray Browne", "word_idx": 61839, "sentence_idx": 920, "label": "unlabeled"}, {"type": "text", "expr": "\n2008", "word_idx": 61864, "sentence_idx": 921, "label": "unlabeled"}, {"type": "text", "expr": "Discussion Tracking in Enron Email Using\nPARAFAC ", "word_idx": 61869, "sentence_idx": 922, "label": "unlabeled"}, {"type": "text", "expr": "Discussion Tracking in Enron Email Using\nPARAFAC", "word_idx": 61918, "sentence_idx": 923, "label": "unlabeled"}, {"type": "text", "expr": "Springer London, 147\u2013163", "word_idx": 61966, "sentence_idx": 924, "label": "unlabeled"}, {"type": "text", "expr": "Bader and Kolda (2007) \nBrett\u00a0W Bader and\nTamara\u00a0G Kolda", "word_idx": 61990, "sentence_idx": 925, "label": "unlabeled"}, {"type": "text", "expr": " 2007", "word_idx": 62046, "sentence_idx": 926, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Efficient MATLAB computations with sparse and\nfactored tensors", "word_idx": 62051, "sentence_idx": 927, "label": "unlabeled"}, {"type": "text", "expr": "Bader and Kolda (2007)", "word_idx": 62116, "sentence_idx": 928, "label": "unlabeled"}, {"type": "text", "expr": "Brett\u00a0W Bader and\nTamara\u00a0G Kolda", "word_idx": 62138, "sentence_idx": 929, "label": "unlabeled"}, {"type": "text", "expr": " 2007", "word_idx": 62170, "sentence_idx": 930, "label": "unlabeled"}, {"type": "text", "expr": "Efficient MATLAB computations with sparse and\nfactored tensors", "word_idx": 62175, "sentence_idx": 931, "label": "unlabeled"}, {"type": "text", "expr": "SIAM Journal on Scientific Computing \n30, 1 (2007),\n205\u2013231", "word_idx": 62237, "sentence_idx": 932, "label": "unlabeled"}, {"type": "text", "expr": "SIAM Journal on Scientific Computing", "word_idx": 62296, "sentence_idx": 933, "label": "unlabeled"}, {"type": "text", "expr": "Bezanson et\u00a0al", "word_idx": 62332, "sentence_idx": 934, "label": "unlabeled"}, {"type": "text", "expr": " (2012) \nJeff Bezanson, Stefan\nKarpinski, Viral\u00a0B", "word_idx": 62346, "sentence_idx": 935, "label": "unlabeled"}, {"type": "text", "expr": " Shah, and Alan\nEdelman", "word_idx": 62395, "sentence_idx": 936, "label": "unlabeled"}, {"type": "text", "expr": " 2012", "word_idx": 62418, "sentence_idx": 937, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Julia: A Fast Dynamic Language for Technical\nComputing", "word_idx": 62423, "sentence_idx": 938, "label": "unlabeled"}, {"type": "text", "expr": "\n\n (2012)", "word_idx": 62480, "sentence_idx": 939, "label": "unlabeled"}, {"type": "text", "expr": "Bezanson et\u00a0al", "word_idx": 62489, "sentence_idx": 940, "label": "unlabeled"}, {"type": "text", "expr": " (2012)", "word_idx": 62503, "sentence_idx": 941, "label": "unlabeled"}, {"type": "text", "expr": "Jeff Bezanson, Stefan\nKarpinski, Viral\u00a0B", "word_idx": 62510, "sentence_idx": 942, "label": "unlabeled"}, {"type": "text", "expr": " Shah, and Alan\nEdelman", "word_idx": 62550, "sentence_idx": 943, "label": "unlabeled"}, {"type": "text", "expr": " 2012", "word_idx": 62573, "sentence_idx": 944, "label": "unlabeled"}, {"type": "text", "expr": "Julia: A Fast Dynamic Language for Technical\nComputing", "word_idx": 62578, "sentence_idx": 945, "label": "unlabeled"}, {"type": "text", "expr": "(2012)", "word_idx": 62632, "sentence_idx": 946, "label": "unlabeled"}, {"type": "text", "expr": "Bik and Wijshoff (1993) \nAart\u00a0JC Bik and Harry\u00a0AG\nWijshoff", "word_idx": 62638, "sentence_idx": 947, "label": "unlabeled"}, {"type": "text", "expr": " 1993", "word_idx": 62696, "sentence_idx": 948, "label": "unlabeled"}, {"type": "text", "expr": "Bik and Wijshoff (1993)", "word_idx": 62701, "sentence_idx": 949, "label": "unlabeled"}, {"type": "text", "expr": "Aart\u00a0JC Bik and Harry\u00a0AG\nWijshoff", "word_idx": 62724, "sentence_idx": 950, "label": "unlabeled"}, {"type": "text", "expr": " 1993", "word_idx": 62757, "sentence_idx": 951, "label": "unlabeled"}, {"type": "text", "expr": "Compilation techniques for sparse matrix\ncomputations", "word_idx": 62762, "sentence_idx": 952, "label": "unlabeled"}, {"type": "text", "expr": " In  Proceedings of the 7th\ninternational conference on Supercomputing ", "word_idx": 62815, "sentence_idx": 953, "label": "unlabeled"}, {"type": "text", "expr": " ACM, 416\u2013424", "word_idx": 62886, "sentence_idx": 954, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the 7th\ninternational conference on Supercomputing", "word_idx": 62899, "sentence_idx": 955, "label": "unlabeled"}, {"type": "text", "expr": "Bulu\u00e7 et\u00a0al", "word_idx": 62964, "sentence_idx": 956, "label": "unlabeled"}, {"type": "text", "expr": " (2009) \nAydin Bulu\u00e7,\nJeremy\u00a0T Fineman, Matteo Frigo,\nJohn\u00a0R Gilbert, and Charles\u00a0E\nLeiserson", "word_idx": 62975, "sentence_idx": 957, "label": "unlabeled"}, {"type": "text", "expr": " 2009", "word_idx": 63068, "sentence_idx": 958, "label": "unlabeled"}, {"type": "text", "expr": "Bulu\u00e7 et\u00a0al", "word_idx": 63073, "sentence_idx": 959, "label": "unlabeled"}, {"type": "text", "expr": " (2009)", "word_idx": 63084, "sentence_idx": 960, "label": "unlabeled"}, {"type": "text", "expr": "Aydin Bulu\u00e7,\nJeremy\u00a0T Fineman, Matteo Frigo,\nJohn\u00a0R Gilbert, and Charles\u00a0E\nLeiserson", "word_idx": 63091, "sentence_idx": 961, "label": "unlabeled"}, {"type": "text", "expr": " 2009", "word_idx": 63175, "sentence_idx": 962, "label": "unlabeled"}, {"type": "text", "expr": "Parallel sparse matrix-vector and\nmatrix-transpose-vector multiplication using compressed sparse blocks", "word_idx": 63180, "sentence_idx": 963, "label": "unlabeled"}, {"type": "text", "expr": " In\n Proceedings of the twenty-first annual symposium on\nParallelism in algorithms and architectures ", "word_idx": 63283, "sentence_idx": 964, "label": "unlabeled"}, {"type": "text", "expr": " ACM,\n233\u2013244", "word_idx": 63384, "sentence_idx": 965, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the twenty-first annual symposium on\nParallelism in algorithms and architectures", "word_idx": 63397, "sentence_idx": 966, "label": "unlabeled"}, {"type": "text", "expr": "Cichocki (2014) \nAndrzej Cichocki", "word_idx": 63492, "sentence_idx": 967, "label": "unlabeled"}, {"type": "text", "expr": "\n2014", "word_idx": 63525, "sentence_idx": 968, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Era of big data processing: A new approach via\ntensor networks and tensor decompositions", "word_idx": 63530, "sentence_idx": 969, "label": "unlabeled"}, {"type": "text", "expr": "Cichocki (2014)", "word_idx": 63621, "sentence_idx": 970, "label": "unlabeled"}, {"type": "text", "expr": "Andrzej Cichocki", "word_idx": 63636, "sentence_idx": 971, "label": "unlabeled"}, {"type": "text", "expr": "\n2014", "word_idx": 63652, "sentence_idx": 972, "label": "unlabeled"}, {"type": "text", "expr": "Era of big data processing: A new approach via\ntensor networks and tensor decompositions", "word_idx": 63657, "sentence_idx": 973, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1403", "word_idx": 63745, "sentence_idx": 974, "label": "unlabeled"}, {"type": "text", "expr": "2048 \n(2014)", "word_idx": 63770, "sentence_idx": 975, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1403", "word_idx": 63782, "sentence_idx": 976, "label": "unlabeled"}, {"type": "text", "expr": "Davis (2006) \nTimothy\u00a0A Davis", "word_idx": 63807, "sentence_idx": 977, "label": "unlabeled"}, {"type": "text", "expr": "\n2006", "word_idx": 63836, "sentence_idx": 978, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Siam", "word_idx": 63841, "sentence_idx": 979, "label": "unlabeled"}, {"type": "text", "expr": "Davis (2006)", "word_idx": 63848, "sentence_idx": 980, "label": "unlabeled"}, {"type": "text", "expr": "Timothy\u00a0A Davis", "word_idx": 63860, "sentence_idx": 981, "label": "unlabeled"}, {"type": "text", "expr": "\n2006", "word_idx": 63875, "sentence_idx": 982, "label": "unlabeled"}, {"type": "text", "expr": "Direct methods for sparse linear systems ", "word_idx": 63880, "sentence_idx": 983, "label": "unlabeled"}, {"type": "text", "expr": "Direct methods for sparse linear systems", "word_idx": 63921, "sentence_idx": 984, "label": "unlabeled"}, {"type": "text", "expr": "Davis and Hu (2011) \nTimothy\u00a0A", "word_idx": 63961, "sentence_idx": 985, "label": "unlabeled"}, {"type": "text", "expr": " Davis and\nYifan Hu", "word_idx": 63991, "sentence_idx": 986, "label": "unlabeled"}, {"type": "text", "expr": " 2011", "word_idx": 64010, "sentence_idx": 987, "label": "unlabeled"}, {"type": "text", "expr": "\n\n The University of Florida Sparse Matrix\nCollection", "word_idx": 64015, "sentence_idx": 988, "label": "unlabeled"}, {"type": "text", "expr": "Davis and Hu (2011)", "word_idx": 64068, "sentence_idx": 989, "label": "unlabeled"}, {"type": "text", "expr": "Timothy\u00a0A", "word_idx": 64087, "sentence_idx": 990, "label": "unlabeled"}, {"type": "text", "expr": " Davis and\nYifan Hu", "word_idx": 64096, "sentence_idx": 991, "label": "unlabeled"}, {"type": "text", "expr": " 2011", "word_idx": 64115, "sentence_idx": 992, "label": "unlabeled"}, {"type": "text", "expr": "The University of Florida Sparse Matrix\nCollection", "word_idx": 64120, "sentence_idx": 993, "label": "unlabeled"}, {"type": "text", "expr": "ACM Trans", "word_idx": 64170, "sentence_idx": 994, "label": "unlabeled"}, {"type": "text", "expr": " Math", "word_idx": 64179, "sentence_idx": 995, "label": "unlabeled"}, {"type": "text", "expr": " Softw", "word_idx": 64184, "sentence_idx": 996, "label": "unlabeled"}, {"type": "text", "expr": " \n38, 1, Article 1\n(Dec", "word_idx": 64190, "sentence_idx": 997, "label": "unlabeled"}, {"type": "text", "expr": " 2011)", "word_idx": 64213, "sentence_idx": 998, "label": "unlabeled"}, {"type": "text", "expr": "ACM Trans", "word_idx": 64219, "sentence_idx": 999, "label": "unlabeled"}, {"type": "text", "expr": " Math", "word_idx": 64228, "sentence_idx": 1000, "label": "unlabeled"}, {"type": "text", "expr": " Softw", "word_idx": 64233, "sentence_idx": 1001, "label": "unlabeled"}, {"type": "text", "expr": "Ding and Shen (2017) \nYufei Ding and Xipeng\nShen", "word_idx": 64239, "sentence_idx": 1002, "label": "unlabeled"}, {"type": "text", "expr": " 2017", "word_idx": 64287, "sentence_idx": 1003, "label": "unlabeled"}, {"type": "text", "expr": "\n\n GLORE: Generalized Loop Redundancy Elimination upon\nLER-Notation", "word_idx": 64292, "sentence_idx": 1004, "label": "unlabeled"}, {"type": "text", "expr": "\n\n (2017)", "word_idx": 64359, "sentence_idx": 1005, "label": "unlabeled"}, {"type": "text", "expr": "Ding and Shen (2017)", "word_idx": 64368, "sentence_idx": 1006, "label": "unlabeled"}, {"type": "text", "expr": "Yufei Ding and Xipeng\nShen", "word_idx": 64388, "sentence_idx": 1007, "label": "unlabeled"}, {"type": "text", "expr": " 2017", "word_idx": 64414, "sentence_idx": 1008, "label": "unlabeled"}, {"type": "text", "expr": "GLORE: Generalized Loop Redundancy Elimination upon\nLER-Notation", "word_idx": 64419, "sentence_idx": 1009, "label": "unlabeled"}, {"type": "text", "expr": "(2017)", "word_idx": 64483, "sentence_idx": 1010, "label": "unlabeled"}, {"type": "text", "expr": "Einstein (1916) \nAlbert", "word_idx": 64489, "sentence_idx": 1011, "label": "unlabeled"}, {"type": "text", "expr": " Einstein", "word_idx": 64512, "sentence_idx": 1012, "label": "unlabeled"}, {"type": "text", "expr": "\n1916", "word_idx": 64521, "sentence_idx": 1013, "label": "unlabeled"}, {"type": "text", "expr": "\n\n The Foundation of the General Theory of\nRelativity", "word_idx": 64526, "sentence_idx": 1014, "label": "unlabeled"}, {"type": "text", "expr": "Einstein (1916)", "word_idx": 64579, "sentence_idx": 1015, "label": "unlabeled"}, {"type": "text", "expr": "Albert", "word_idx": 64594, "sentence_idx": 1016, "label": "unlabeled"}, {"type": "text", "expr": " Einstein", "word_idx": 64600, "sentence_idx": 1017, "label": "unlabeled"}, {"type": "text", "expr": "\n1916", "word_idx": 64609, "sentence_idx": 1018, "label": "unlabeled"}, {"type": "text", "expr": "The Foundation of the General Theory of\nRelativity", "word_idx": 64614, "sentence_idx": 1019, "label": "unlabeled"}, {"type": "text", "expr": "Annalen der Physik  354\n(1916), 769\u2013822", "word_idx": 64664, "sentence_idx": 1020, "label": "unlabeled"}, {"type": "text", "expr": "Annalen der Physik", "word_idx": 64703, "sentence_idx": 1021, "label": "unlabeled"}, {"type": "text", "expr": "Feynman\net\u00a0al", "word_idx": 64721, "sentence_idx": 1022, "label": "unlabeled"}, {"type": "text", "expr": " (1963) \nRichard Feynman, Robert\u00a0B", "word_idx": 64734, "sentence_idx": 1023, "label": "unlabeled"}, {"type": "text", "expr": "\nLeighton, and Matthew\u00a0L", "word_idx": 64768, "sentence_idx": 1024, "label": "unlabeled"}, {"type": "text", "expr": " Sands", "word_idx": 64792, "sentence_idx": 1025, "label": "unlabeled"}, {"type": "text", "expr": "\n1963", "word_idx": 64798, "sentence_idx": 1026, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Addison-Wesley", "word_idx": 64803, "sentence_idx": 1027, "label": "unlabeled"}, {"type": "text", "expr": "Feynman\net\u00a0al", "word_idx": 64820, "sentence_idx": 1028, "label": "unlabeled"}, {"type": "text", "expr": " (1963)", "word_idx": 64833, "sentence_idx": 1029, "label": "unlabeled"}, {"type": "text", "expr": "Richard Feynman, Robert\u00a0B", "word_idx": 64840, "sentence_idx": 1030, "label": "unlabeled"}, {"type": "text", "expr": "\nLeighton, and Matthew\u00a0L", "word_idx": 64865, "sentence_idx": 1031, "label": "unlabeled"}, {"type": "text", "expr": " Sands", "word_idx": 64889, "sentence_idx": 1032, "label": "unlabeled"}, {"type": "text", "expr": "\n1963", "word_idx": 64895, "sentence_idx": 1033, "label": "unlabeled"}, {"type": "text", "expr": "The Feynman Lectures on Physics ", "word_idx": 64900, "sentence_idx": 1034, "label": "unlabeled"}, {"type": "text", "expr": "The Feynman Lectures on Physics", "word_idx": 64932, "sentence_idx": 1035, "label": "unlabeled"}, {"type": "text", "expr": "Addison-Wesley", "word_idx": 64963, "sentence_idx": 1036, "label": "unlabeled"}, {"type": "text", "expr": "Gilbert\net\u00a0al", "word_idx": 64977, "sentence_idx": 1037, "label": "unlabeled"}, {"type": "text", "expr": " (1992) \nJohn\u00a0R Gilbert, Cleve\nMoler, and Robert Schreiber", "word_idx": 64990, "sentence_idx": 1038, "label": "unlabeled"}, {"type": "text", "expr": "\n1992", "word_idx": 65048, "sentence_idx": 1039, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Sparse matrices in MATLAB: Design and\nimplementation", "word_idx": 65053, "sentence_idx": 1040, "label": "unlabeled"}, {"type": "text", "expr": "Gilbert\net\u00a0al", "word_idx": 65108, "sentence_idx": 1041, "label": "unlabeled"}, {"type": "text", "expr": " (1992)", "word_idx": 65121, "sentence_idx": 1042, "label": "unlabeled"}, {"type": "text", "expr": "John\u00a0R Gilbert, Cleve\nMoler, and Robert Schreiber", "word_idx": 65128, "sentence_idx": 1043, "label": "unlabeled"}, {"type": "text", "expr": "\n1992", "word_idx": 65177, "sentence_idx": 1044, "label": "unlabeled"}, {"type": "text", "expr": "Sparse matrices in MATLAB: Design and\nimplementation", "word_idx": 65182, "sentence_idx": 1045, "label": "unlabeled"}, {"type": "text", "expr": "SIAM J", "word_idx": 65234, "sentence_idx": 1046, "label": "unlabeled"}, {"type": "text", "expr": " Matrix Anal", "word_idx": 65240, "sentence_idx": 1047, "label": "unlabeled"}, {"type": "text", "expr": " Appl", "word_idx": 65252, "sentence_idx": 1048, "label": "unlabeled"}, {"type": "text", "expr": " \n13, 1 (1992),\n333\u2013356", "word_idx": 65257, "sentence_idx": 1049, "label": "unlabeled"}, {"type": "text", "expr": "SIAM J", "word_idx": 65280, "sentence_idx": 1050, "label": "unlabeled"}, {"type": "text", "expr": " Matrix Anal", "word_idx": 65286, "sentence_idx": 1051, "label": "unlabeled"}, {"type": "text", "expr": " Appl", "word_idx": 65298, "sentence_idx": 1052, "label": "unlabeled"}, {"type": "text", "expr": "Guennebaud\net\u00a0al", "word_idx": 65303, "sentence_idx": 1053, "label": "unlabeled"}, {"type": "text", "expr": " (2010) \nGa\u00ebl Guennebaud,\nBeno\u00eet Jacob, et\u00a0al", "word_idx": 65319, "sentence_idx": 1054, "label": "unlabeled"}, {"type": "text", "expr": "\n2010", "word_idx": 65364, "sentence_idx": 1055, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Eigen v3", "word_idx": 65369, "sentence_idx": 1056, "label": "unlabeled"}, {"type": "text", "expr": "\n\n http://eigen", "word_idx": 65380, "sentence_idx": 1057, "label": "unlabeled"}, {"type": "text", "expr": "tuxfamily", "word_idx": 65395, "sentence_idx": 1058, "label": "unlabeled"}, {"type": "text", "expr": "\n(2010)", "word_idx": 65404, "sentence_idx": 1059, "label": "unlabeled"}, {"type": "text", "expr": "Guennebaud\net\u00a0al", "word_idx": 65411, "sentence_idx": 1060, "label": "unlabeled"}, {"type": "text", "expr": " (2010)", "word_idx": 65427, "sentence_idx": 1061, "label": "unlabeled"}, {"type": "text", "expr": "Ga\u00ebl Guennebaud,\nBeno\u00eet Jacob, et\u00a0al", "word_idx": 65434, "sentence_idx": 1062, "label": "unlabeled"}, {"type": "text", "expr": "\n2010", "word_idx": 65470, "sentence_idx": 1063, "label": "unlabeled"}, {"type": "text", "expr": "Eigen v3", "word_idx": 65475, "sentence_idx": 1064, "label": "unlabeled"}, {"type": "text", "expr": "http://eigen", "word_idx": 65483, "sentence_idx": 1065, "label": "unlabeled"}, {"type": "text", "expr": "tuxfamily", "word_idx": 65495, "sentence_idx": 1066, "label": "unlabeled"}, {"type": "text", "expr": "\n(2010)", "word_idx": 65504, "sentence_idx": 1067, "label": "unlabeled"}, {"type": "text", "expr": "Gustavson (1978) \nFred\u00a0G", "word_idx": 65511, "sentence_idx": 1068, "label": "unlabeled"}, {"type": "text", "expr": " Gustavson", "word_idx": 65535, "sentence_idx": 1069, "label": "unlabeled"}, {"type": "text", "expr": "\n1978", "word_idx": 65545, "sentence_idx": 1070, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Two Fast Algorithms for Sparse Matrices:\nMultiplication and Permuted Transposition", "word_idx": 65550, "sentence_idx": 1071, "label": "unlabeled"}, {"type": "text", "expr": "Gustavson (1978)", "word_idx": 65635, "sentence_idx": 1072, "label": "unlabeled"}, {"type": "text", "expr": "Fred\u00a0G", "word_idx": 65651, "sentence_idx": 1073, "label": "unlabeled"}, {"type": "text", "expr": " Gustavson", "word_idx": 65657, "sentence_idx": 1074, "label": "unlabeled"}, {"type": "text", "expr": "\n1978", "word_idx": 65667, "sentence_idx": 1075, "label": "unlabeled"}, {"type": "text", "expr": "Two Fast Algorithms for Sparse Matrices:\nMultiplication and Permuted Transposition", "word_idx": 65672, "sentence_idx": 1076, "label": "unlabeled"}, {"type": "text", "expr": "ACM Trans", "word_idx": 65754, "sentence_idx": 1077, "label": "unlabeled"}, {"type": "text", "expr": " Math", "word_idx": 65763, "sentence_idx": 1078, "label": "unlabeled"}, {"type": "text", "expr": " Softw", "word_idx": 65768, "sentence_idx": 1079, "label": "unlabeled"}, {"type": "text", "expr": " \n4, 3 (1978)", "word_idx": 65774, "sentence_idx": 1080, "label": "unlabeled"}, {"type": "text", "expr": "ACM Trans", "word_idx": 65787, "sentence_idx": 1081, "label": "unlabeled"}, {"type": "text", "expr": " Math", "word_idx": 65796, "sentence_idx": 1082, "label": "unlabeled"}, {"type": "text", "expr": " Softw", "word_idx": 65801, "sentence_idx": 1083, "label": "unlabeled"}, {"type": "text", "expr": "Hansen (1970) \nPer\u00a0Brinch Hansen", "word_idx": 65807, "sentence_idx": 1084, "label": "unlabeled"}, {"type": "text", "expr": "\n1970", "word_idx": 65839, "sentence_idx": 1085, "label": "unlabeled"}, {"type": "text", "expr": "\n\n The Nucleus of a Multiprogramming System", "word_idx": 65844, "sentence_idx": 1086, "label": "unlabeled"}, {"type": "text", "expr": "Hansen (1970)", "word_idx": 65887, "sentence_idx": 1087, "label": "unlabeled"}, {"type": "text", "expr": "Per\u00a0Brinch Hansen", "word_idx": 65900, "sentence_idx": 1088, "label": "unlabeled"}, {"type": "text", "expr": "\n1970", "word_idx": 65917, "sentence_idx": 1089, "label": "unlabeled"}, {"type": "text", "expr": "The Nucleus of a Multiprogramming System", "word_idx": 65922, "sentence_idx": 1090, "label": "unlabeled"}, {"type": "text", "expr": "Commun", "word_idx": 65962, "sentence_idx": 1091, "label": "unlabeled"}, {"type": "text", "expr": " ACM  13,\n4 (April 1970),\n238\u2013241", "word_idx": 65968, "sentence_idx": 1092, "label": "unlabeled"}, {"type": "text", "expr": "Commun", "word_idx": 66001, "sentence_idx": 1093, "label": "unlabeled"}, {"type": "text", "expr": "https://doi", "word_idx": 66007, "sentence_idx": 1094, "label": "unlabeled"}, {"type": "text", "expr": "org/10", "word_idx": 66018, "sentence_idx": 1095, "label": "unlabeled"}, {"type": "text", "expr": "1145/362258", "word_idx": 66024, "sentence_idx": 1096, "label": "unlabeled"}, {"type": "text", "expr": "362278", "word_idx": 66035, "sentence_idx": 1097, "label": "unlabeled"}, {"type": "text", "expr": "https://doi", "word_idx": 66041, "sentence_idx": 1098, "label": "unlabeled"}, {"type": "text", "expr": "org/10", "word_idx": 66052, "sentence_idx": 1099, "label": "unlabeled"}, {"type": "text", "expr": "1145/362258", "word_idx": 66058, "sentence_idx": 1100, "label": "unlabeled"}, {"type": "text", "expr": "362278", "word_idx": 66069, "sentence_idx": 1101, "label": "unlabeled"}, {"type": "text", "expr": "Heath\net\u00a0al", "word_idx": 66075, "sentence_idx": 1102, "label": "unlabeled"}, {"type": "text", "expr": " (1991) \nMichael\u00a0T Heath, Esmond\nNg, and Barry\u00a0W Peyton", "word_idx": 66086, "sentence_idx": 1103, "label": "unlabeled"}, {"type": "text", "expr": "\n1991", "word_idx": 66141, "sentence_idx": 1104, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Parallel algorithms for sparse linear systems", "word_idx": 66146, "sentence_idx": 1105, "label": "unlabeled"}, {"type": "text", "expr": "Heath\net\u00a0al", "word_idx": 66194, "sentence_idx": 1106, "label": "unlabeled"}, {"type": "text", "expr": " (1991)", "word_idx": 66205, "sentence_idx": 1107, "label": "unlabeled"}, {"type": "text", "expr": "Michael\u00a0T Heath, Esmond\nNg, and Barry\u00a0W Peyton", "word_idx": 66212, "sentence_idx": 1108, "label": "unlabeled"}, {"type": "text", "expr": "\n1991", "word_idx": 66258, "sentence_idx": 1109, "label": "unlabeled"}, {"type": "text", "expr": "Parallel algorithms for sparse linear systems", "word_idx": 66263, "sentence_idx": 1110, "label": "unlabeled"}, {"type": "text", "expr": "SIAM review  33,\n3 (1991), 420\u2013460", "word_idx": 66308, "sentence_idx": 1111, "label": "unlabeled"}, {"type": "text", "expr": "SIAM review", "word_idx": 66342, "sentence_idx": 1112, "label": "unlabeled"}, {"type": "text", "expr": "Hitchcock (1927) \nFrank\u00a0L Hitchcock", "word_idx": 66353, "sentence_idx": 1113, "label": "unlabeled"}, {"type": "text", "expr": "\n1927", "word_idx": 66388, "sentence_idx": 1114, "label": "unlabeled"}, {"type": "text", "expr": "\n\n The expression of a tensor or a polyadic as a sum\nof products", "word_idx": 66393, "sentence_idx": 1115, "label": "unlabeled"}, {"type": "text", "expr": "Hitchcock (1927)", "word_idx": 66457, "sentence_idx": 1116, "label": "unlabeled"}, {"type": "text", "expr": "Frank\u00a0L Hitchcock", "word_idx": 66473, "sentence_idx": 1117, "label": "unlabeled"}, {"type": "text", "expr": "\n1927", "word_idx": 66490, "sentence_idx": 1118, "label": "unlabeled"}, {"type": "text", "expr": "The expression of a tensor or a polyadic as a sum\nof products", "word_idx": 66495, "sentence_idx": 1119, "label": "unlabeled"}, {"type": "text", "expr": "Studies in Applied Mathematics \n6, 1-4 (1927),\n164\u2013189", "word_idx": 66556, "sentence_idx": 1120, "label": "unlabeled"}, {"type": "text", "expr": "Studies in Applied Mathematics", "word_idx": 66610, "sentence_idx": 1121, "label": "unlabeled"}, {"type": "text", "expr": "Intel (2012) \nIntel", "word_idx": 66640, "sentence_idx": 1122, "label": "unlabeled"}, {"type": "text", "expr": " 2012", "word_idx": 66659, "sentence_idx": 1123, "label": "unlabeled"}, {"type": "text", "expr": "Intel (2012)", "word_idx": 66664, "sentence_idx": 1124, "label": "unlabeled"}, {"type": "text", "expr": "Intel", "word_idx": 66676, "sentence_idx": 1125, "label": "unlabeled"}, {"type": "text", "expr": " 2012", "word_idx": 66681, "sentence_idx": 1126, "label": "unlabeled"}, {"type": "text", "expr": "Intel math kernel library reference\nmanual ", "word_idx": 66686, "sentence_idx": 1127, "label": "unlabeled"}, {"type": "text", "expr": "Intel math kernel library reference\nmanual", "word_idx": 66729, "sentence_idx": 1128, "label": "unlabeled"}, {"type": "text", "expr": "Technical Report", "word_idx": 66771, "sentence_idx": 1129, "label": "unlabeled"}, {"type": "text", "expr": "\n630813-051US, 2012", "word_idx": 66787, "sentence_idx": 1130, "label": "unlabeled"}, {"type": "text", "expr": "\n http://software", "word_idx": 66806, "sentence_idx": 1131, "label": "unlabeled"}, {"type": "text", "expr": "intel", "word_idx": 66823, "sentence_idx": 1132, "label": "unlabeled"}, {"type": "text", "expr": "com/sites/products/documentation/hpc/mkl/mklman/mklman", "word_idx": 66828, "sentence_idx": 1133, "label": "unlabeled"}, {"type": "text", "expr": "http://software", "word_idx": 66882, "sentence_idx": 1134, "label": "unlabeled"}, {"type": "text", "expr": "intel", "word_idx": 66897, "sentence_idx": 1135, "label": "unlabeled"}, {"type": "text", "expr": "com/sites/products/documentation/hpc/mkl/mklman/mklman", "word_idx": 66902, "sentence_idx": 1136, "label": "unlabeled"}, {"type": "text", "expr": "Iverson (1962) \nKenneth\u00a0E", "word_idx": 66956, "sentence_idx": 1137, "label": "unlabeled"}, {"type": "text", "expr": " Iverson", "word_idx": 66981, "sentence_idx": 1138, "label": "unlabeled"}, {"type": "text", "expr": "\n1962", "word_idx": 66989, "sentence_idx": 1139, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Wiley", "word_idx": 66994, "sentence_idx": 1140, "label": "unlabeled"}, {"type": "text", "expr": "Iverson (1962)", "word_idx": 67002, "sentence_idx": 1141, "label": "unlabeled"}, {"type": "text", "expr": "Kenneth\u00a0E", "word_idx": 67016, "sentence_idx": 1142, "label": "unlabeled"}, {"type": "text", "expr": " Iverson", "word_idx": 67025, "sentence_idx": 1143, "label": "unlabeled"}, {"type": "text", "expr": "\n1962", "word_idx": 67033, "sentence_idx": 1144, "label": "unlabeled"}, {"type": "text", "expr": "A Programming Language ", "word_idx": 67038, "sentence_idx": 1145, "label": "unlabeled"}, {"type": "text", "expr": "A Programming Language", "word_idx": 67061, "sentence_idx": 1146, "label": "unlabeled"}, {"type": "text", "expr": "Wiley", "word_idx": 67083, "sentence_idx": 1147, "label": "unlabeled"}, {"type": "text", "expr": "Kjolstad et\u00a0al", "word_idx": 67088, "sentence_idx": 1148, "label": "unlabeled"}, {"type": "text", "expr": " (2017a) \nFredrik Kjolstad, Stephen\nChou, David Lugato, Shoaib Kamil, and\nSaman Amarasinghe", "word_idx": 67102, "sentence_idx": 1149, "label": "unlabeled"}, {"type": "text", "expr": " 2017a", "word_idx": 67193, "sentence_idx": 1150, "label": "unlabeled"}, {"type": "text", "expr": "Kjolstad et\u00a0al", "word_idx": 67199, "sentence_idx": 1151, "label": "unlabeled"}, {"type": "text", "expr": " (2017a)", "word_idx": 67213, "sentence_idx": 1152, "label": "unlabeled"}, {"type": "text", "expr": "Fredrik Kjolstad, Stephen\nChou, David Lugato, Shoaib Kamil, and\nSaman Amarasinghe", "word_idx": 67221, "sentence_idx": 1153, "label": "unlabeled"}, {"type": "text", "expr": " 2017a", "word_idx": 67302, "sentence_idx": 1154, "label": "unlabeled"}, {"type": "text", "expr": "taco: a tool to generate tensor algebra kernels", "word_idx": 67308, "sentence_idx": 1155, "label": "unlabeled"}, {"type": "text", "expr": "\nIn  Proceedings of the 32nd IEEE/ACM International\nConference on Automated Software Engineering ", "word_idx": 67355, "sentence_idx": 1156, "label": "unlabeled"}, {"type": "text", "expr": " IEEE Press,\n943\u2013948", "word_idx": 67452, "sentence_idx": 1157, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the 32nd IEEE/ACM International\nConference on Automated Software Engineering", "word_idx": 67472, "sentence_idx": 1158, "label": "unlabeled"}, {"type": "text", "expr": "Kjolstad et\u00a0al", "word_idx": 67563, "sentence_idx": 1159, "label": "unlabeled"}, {"type": "text", "expr": " (2017b) \nFredrik Kjolstad, Shoaib\nKamil, Stephen Chou, David Lugato, and\nSaman Amarasinghe", "word_idx": 67577, "sentence_idx": 1160, "label": "unlabeled"}, {"type": "text", "expr": " 2017b", "word_idx": 67668, "sentence_idx": 1161, "label": "unlabeled"}, {"type": "text", "expr": "\n\n The Tensor Algebra Compiler", "word_idx": 67674, "sentence_idx": 1162, "label": "unlabeled"}, {"type": "text", "expr": "Kjolstad et\u00a0al", "word_idx": 67704, "sentence_idx": 1163, "label": "unlabeled"}, {"type": "text", "expr": " (2017b)", "word_idx": 67718, "sentence_idx": 1164, "label": "unlabeled"}, {"type": "text", "expr": "Fredrik Kjolstad, Shoaib\nKamil, Stephen Chou, David Lugato, and\nSaman Amarasinghe", "word_idx": 67726, "sentence_idx": 1165, "label": "unlabeled"}, {"type": "text", "expr": " 2017b", "word_idx": 67807, "sentence_idx": 1166, "label": "unlabeled"}, {"type": "text", "expr": "The Tensor Algebra Compiler", "word_idx": 67813, "sentence_idx": 1167, "label": "unlabeled"}, {"type": "text", "expr": " ACM Program", "word_idx": 67840, "sentence_idx": 1168, "label": "unlabeled"}, {"type": "text", "expr": " Lang", "word_idx": 67852, "sentence_idx": 1169, "label": "unlabeled"}, {"type": "text", "expr": " \n1, OOPSLA, Article\n77 (Oct", "word_idx": 67857, "sentence_idx": 1170, "label": "unlabeled"}, {"type": "text", "expr": " 2017),\n29\u00a0pages", "word_idx": 67885, "sentence_idx": 1171, "label": "unlabeled"}, {"type": "text", "expr": " ACM Program", "word_idx": 67901, "sentence_idx": 1172, "label": "unlabeled"}, {"type": "text", "expr": " Lang", "word_idx": 67913, "sentence_idx": 1173, "label": "unlabeled"}, {"type": "text", "expr": "https://doi", "word_idx": 67918, "sentence_idx": 1174, "label": "unlabeled"}, {"type": "text", "expr": "org/10", "word_idx": 67929, "sentence_idx": 1175, "label": "unlabeled"}, {"type": "text", "expr": "1145/3133901", "word_idx": 67935, "sentence_idx": 1176, "label": "unlabeled"}, {"type": "text", "expr": "https://doi", "word_idx": 67947, "sentence_idx": 1177, "label": "unlabeled"}, {"type": "text", "expr": "org/10", "word_idx": 67958, "sentence_idx": 1178, "label": "unlabeled"}, {"type": "text", "expr": "1145/3133901", "word_idx": 67964, "sentence_idx": 1179, "label": "unlabeled"}, {"type": "text", "expr": "Kjolstad\net\u00a0al", "word_idx": 67976, "sentence_idx": 1180, "label": "unlabeled"}, {"type": "text", "expr": " (2016) \nFredrik Kjolstad, Shoaib\nKamil, Jonathan Ragan-Kelley, David\u00a0IW\nLevin, Shinjiro Sueda, Desai Chen,\nEtienne Vouga, Danny\u00a0M Kaufman,\nGurtej Kanwar, Wojciech Matusik,\net\u00a0al", "word_idx": 67990, "sentence_idx": 1181, "label": "unlabeled"}, {"type": "text", "expr": " 2016", "word_idx": 68168, "sentence_idx": 1182, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Simit: A language for physical simulation", "word_idx": 68173, "sentence_idx": 1183, "label": "unlabeled"}, {"type": "text", "expr": "Kjolstad\net\u00a0al", "word_idx": 68217, "sentence_idx": 1184, "label": "unlabeled"}, {"type": "text", "expr": " (2016)", "word_idx": 68231, "sentence_idx": 1185, "label": "unlabeled"}, {"type": "text", "expr": "Fredrik Kjolstad, Shoaib\nKamil, Jonathan Ragan-Kelley, David\u00a0IW\nLevin, Shinjiro Sueda, Desai Chen,\nEtienne Vouga, Danny\u00a0M Kaufman,\nGurtej Kanwar, Wojciech Matusik,\net\u00a0al", "word_idx": 68238, "sentence_idx": 1186, "label": "unlabeled"}, {"type": "text", "expr": " 2016", "word_idx": 68407, "sentence_idx": 1187, "label": "unlabeled"}, {"type": "text", "expr": "Simit: A language for physical simulation", "word_idx": 68412, "sentence_idx": 1188, "label": "unlabeled"}, {"type": "text", "expr": "ACM Transactions on Graphics (TOG) \n35, 2 (2016),\n20", "word_idx": 68453, "sentence_idx": 1189, "label": "unlabeled"}, {"type": "text", "expr": "ACM Transactions on Graphics (TOG)", "word_idx": 68505, "sentence_idx": 1190, "label": "unlabeled"}, {"type": "text", "expr": "Knuth (1973) \nDonald\u00a0Ervin Knuth", "word_idx": 68539, "sentence_idx": 1191, "label": "unlabeled"}, {"type": "text", "expr": "\n1973", "word_idx": 68571, "sentence_idx": 1192, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Pearson Education", "word_idx": 68576, "sentence_idx": 1193, "label": "unlabeled"}, {"type": "text", "expr": "Knuth (1973)", "word_idx": 68596, "sentence_idx": 1194, "label": "unlabeled"}, {"type": "text", "expr": "Donald\u00a0Ervin Knuth", "word_idx": 68608, "sentence_idx": 1195, "label": "unlabeled"}, {"type": "text", "expr": "\n1973", "word_idx": 68626, "sentence_idx": 1196, "label": "unlabeled"}, {"type": "text", "expr": "The art of computer programming: sorting\nand searching ", "word_idx": 68631, "sentence_idx": 1197, "label": "unlabeled"}, {"type": "text", "expr": "The art of computer programming: sorting\nand searching", "word_idx": 68686, "sentence_idx": 1198, "label": "unlabeled"}, {"type": "text", "expr": "Pearson Education", "word_idx": 68740, "sentence_idx": 1199, "label": "unlabeled"}, {"type": "text", "expr": "Kolda and Bader (2009) \nTamara\u00a0G Kolda and\nBrett\u00a0W Bader", "word_idx": 68757, "sentence_idx": 1200, "label": "unlabeled"}, {"type": "text", "expr": " 2009", "word_idx": 68813, "sentence_idx": 1201, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Tensor decompositions and applications", "word_idx": 68818, "sentence_idx": 1202, "label": "unlabeled"}, {"type": "text", "expr": "Kolda and Bader (2009)", "word_idx": 68859, "sentence_idx": 1203, "label": "unlabeled"}, {"type": "text", "expr": "Tamara\u00a0G Kolda and\nBrett\u00a0W Bader", "word_idx": 68881, "sentence_idx": 1204, "label": "unlabeled"}, {"type": "text", "expr": " 2009", "word_idx": 68913, "sentence_idx": 1205, "label": "unlabeled"}, {"type": "text", "expr": "Tensor decompositions and applications", "word_idx": 68918, "sentence_idx": 1206, "label": "unlabeled"}, {"type": "text", "expr": "SIAM review  51,\n3 (2009), 455\u2013500", "word_idx": 68956, "sentence_idx": 1207, "label": "unlabeled"}, {"type": "text", "expr": "SIAM review", "word_idx": 68990, "sentence_idx": 1208, "label": "unlabeled"}, {"type": "text", "expr": "Kolecki (2002) \nJoseph\u00a0C Kolecki", "word_idx": 69001, "sentence_idx": 1209, "label": "unlabeled"}, {"type": "text", "expr": "\n2002", "word_idx": 69033, "sentence_idx": 1210, "label": "unlabeled"}, {"type": "text", "expr": "\n\n An Introduction to Tensors for Students of Physics\nand Engineering", "word_idx": 69038, "sentence_idx": 1211, "label": "unlabeled"}, {"type": "text", "expr": "Kolecki (2002)", "word_idx": 69107, "sentence_idx": 1212, "label": "unlabeled"}, {"type": "text", "expr": "Joseph\u00a0C Kolecki", "word_idx": 69121, "sentence_idx": 1213, "label": "unlabeled"}, {"type": "text", "expr": "\n2002", "word_idx": 69137, "sentence_idx": 1214, "label": "unlabeled"}, {"type": "text", "expr": "An Introduction to Tensors for Students of Physics\nand Engineering", "word_idx": 69142, "sentence_idx": 1215, "label": "unlabeled"}, {"type": "text", "expr": "Unixenguaedu  7,\nSeptember (2002), 29", "word_idx": 69208, "sentence_idx": 1216, "label": "unlabeled"}, {"type": "text", "expr": "Unixenguaedu", "word_idx": 69245, "sentence_idx": 1217, "label": "unlabeled"}, {"type": "text", "expr": "Kotlyar\net\u00a0al", "word_idx": 69257, "sentence_idx": 1218, "label": "unlabeled"}, {"type": "text", "expr": " (1997) \nVladimir Kotlyar, Keshav\nPingali, and Paul Stodghill", "word_idx": 69270, "sentence_idx": 1219, "label": "unlabeled"}, {"type": "text", "expr": "\n1997", "word_idx": 69331, "sentence_idx": 1220, "label": "unlabeled"}, {"type": "text", "expr": "\n\n A relational approach to the compilation of sparse\nmatrix programs", "word_idx": 69336, "sentence_idx": 1221, "label": "unlabeled"}, {"type": "text", "expr": "Kotlyar\net\u00a0al", "word_idx": 69405, "sentence_idx": 1222, "label": "unlabeled"}, {"type": "text", "expr": " (1997)", "word_idx": 69418, "sentence_idx": 1223, "label": "unlabeled"}, {"type": "text", "expr": "Vladimir Kotlyar, Keshav\nPingali, and Paul Stodghill", "word_idx": 69425, "sentence_idx": 1224, "label": "unlabeled"}, {"type": "text", "expr": "\n1997", "word_idx": 69477, "sentence_idx": 1225, "label": "unlabeled"}, {"type": "text", "expr": "A relational approach to the compilation of sparse\nmatrix programs", "word_idx": 69482, "sentence_idx": 1226, "label": "unlabeled"}, {"type": "text", "expr": "In  Euro-Par\u201997 Parallel Processing ", "word_idx": 69548, "sentence_idx": 1227, "label": "unlabeled"}, {"type": "text", "expr": "\nSpringer, 318\u2013327", "word_idx": 69584, "sentence_idx": 1228, "label": "unlabeled"}, {"type": "text", "expr": "Euro-Par\u201997 Parallel Processing", "word_idx": 69602, "sentence_idx": 1229, "label": "unlabeled"}, {"type": "text", "expr": "Leskovec and\nKrevl (2014) \nJure Leskovec and Andrej\nKrevl", "word_idx": 69633, "sentence_idx": 1230, "label": "unlabeled"}, {"type": "text", "expr": " 2014", "word_idx": 69690, "sentence_idx": 1231, "label": "unlabeled"}, {"type": "text", "expr": "\n\n SNAP Datasets: Stanford Large Network Dataset\nCollection", "word_idx": 69695, "sentence_idx": 1232, "label": "unlabeled"}, {"type": "text", "expr": "Leskovec and\nKrevl (2014)", "word_idx": 69754, "sentence_idx": 1233, "label": "unlabeled"}, {"type": "text", "expr": "Jure Leskovec and Andrej\nKrevl", "word_idx": 69779, "sentence_idx": 1234, "label": "unlabeled"}, {"type": "text", "expr": " 2014", "word_idx": 69809, "sentence_idx": 1235, "label": "unlabeled"}, {"type": "text", "expr": "SNAP Datasets: Stanford Large Network Dataset\nCollection", "word_idx": 69814, "sentence_idx": 1236, "label": "unlabeled"}, {"type": "text", "expr": "http://snap", "word_idx": 69870, "sentence_idx": 1237, "label": "unlabeled"}, {"type": "text", "expr": "stanford", "word_idx": 69881, "sentence_idx": 1238, "label": "unlabeled"}, {"type": "text", "expr": "edu/data ", "word_idx": 69889, "sentence_idx": 1239, "label": "unlabeled"}, {"type": "text", "expr": "\n(June 2014)", "word_idx": 69898, "sentence_idx": 1240, "label": "unlabeled"}, {"type": "text", "expr": "http://snap", "word_idx": 69910, "sentence_idx": 1241, "label": "unlabeled"}, {"type": "text", "expr": "stanford", "word_idx": 69921, "sentence_idx": 1242, "label": "unlabeled"}, {"type": "text", "expr": "edu/data", "word_idx": 69929, "sentence_idx": 1243, "label": "unlabeled"}, {"type": "text", "expr": "MATLAB (2014) \nMATLAB", "word_idx": 69937, "sentence_idx": 1244, "label": "unlabeled"}, {"type": "text", "expr": " 2014", "word_idx": 69958, "sentence_idx": 1245, "label": "unlabeled"}, {"type": "text", "expr": "\n\n The MathWorks Inc", "word_idx": 69963, "sentence_idx": 1246, "label": "unlabeled"}, {"type": "text", "expr": ", Natick,\nMassachusetts", "word_idx": 69983, "sentence_idx": 1247, "label": "unlabeled"}, {"type": "text", "expr": "MATLAB (2014)", "word_idx": 70006, "sentence_idx": 1248, "label": "unlabeled"}, {"type": "text", "expr": "MATLAB", "word_idx": 70019, "sentence_idx": 1249, "label": "unlabeled"}, {"type": "text", "expr": " 2014", "word_idx": 70025, "sentence_idx": 1250, "label": "unlabeled"}, {"type": "text", "expr": "version 8", "word_idx": 70030, "sentence_idx": 1251, "label": "unlabeled"}, {"type": "text", "expr": "0 (R2014a) ", "word_idx": 70039, "sentence_idx": 1252, "label": "unlabeled"}, {"type": "text", "expr": "version 8", "word_idx": 70050, "sentence_idx": 1253, "label": "unlabeled"}, {"type": "text", "expr": "0 (R2014a)", "word_idx": 70059, "sentence_idx": 1254, "label": "unlabeled"}, {"type": "text", "expr": "The MathWorks Inc", "word_idx": 70069, "sentence_idx": 1255, "label": "unlabeled"}, {"type": "text", "expr": ", Natick,\nMassachusetts", "word_idx": 70086, "sentence_idx": 1256, "label": "unlabeled"}, {"type": "text", "expr": "McAuley and\nLeskovec (2013) \nJulian McAuley and Jure\nLeskovec", "word_idx": 70109, "sentence_idx": 1257, "label": "unlabeled"}, {"type": "text", "expr": " 2013", "word_idx": 70170, "sentence_idx": 1258, "label": "unlabeled"}, {"type": "text", "expr": "McAuley and\nLeskovec (2013)", "word_idx": 70175, "sentence_idx": 1259, "label": "unlabeled"}, {"type": "text", "expr": "Julian McAuley and Jure\nLeskovec", "word_idx": 70202, "sentence_idx": 1260, "label": "unlabeled"}, {"type": "text", "expr": " 2013", "word_idx": 70234, "sentence_idx": 1261, "label": "unlabeled"}, {"type": "text", "expr": "Hidden factors and hidden topics: understanding\nrating dimensions with review text", "word_idx": 70239, "sentence_idx": 1262, "label": "unlabeled"}, {"type": "text", "expr": " In  Proceedings\nof the 7th ACM conference on Recommender systems ", "word_idx": 70321, "sentence_idx": 1263, "label": "unlabeled"}, {"type": "text", "expr": " ACM,\n165\u2013172", "word_idx": 70387, "sentence_idx": 1264, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings\nof the 7th ACM conference on Recommender systems", "word_idx": 70400, "sentence_idx": 1265, "label": "unlabeled"}, {"type": "text", "expr": "McKinley\net\u00a0al", "word_idx": 70460, "sentence_idx": 1266, "label": "unlabeled"}, {"type": "text", "expr": " (1996) \nKathryn\u00a0S McKinley, Steve\nCarr, and Chau-Wen Tseng", "word_idx": 70474, "sentence_idx": 1267, "label": "unlabeled"}, {"type": "text", "expr": "\n1996", "word_idx": 70533, "sentence_idx": 1268, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Improving data locality with loop transformations", "word_idx": 70538, "sentence_idx": 1269, "label": "unlabeled"}, {"type": "text", "expr": "McKinley\net\u00a0al", "word_idx": 70590, "sentence_idx": 1270, "label": "unlabeled"}, {"type": "text", "expr": " (1996)", "word_idx": 70604, "sentence_idx": 1271, "label": "unlabeled"}, {"type": "text", "expr": "Kathryn\u00a0S McKinley, Steve\nCarr, and Chau-Wen Tseng", "word_idx": 70611, "sentence_idx": 1272, "label": "unlabeled"}, {"type": "text", "expr": "\n1996", "word_idx": 70661, "sentence_idx": 1273, "label": "unlabeled"}, {"type": "text", "expr": "Improving data locality with loop transformations", "word_idx": 70666, "sentence_idx": 1274, "label": "unlabeled"}, {"type": "text", "expr": "ACM Transactions on Programming Languages and\nSystems (TOPLAS)  18, 4\n(1996), 424\u2013453", "word_idx": 70715, "sentence_idx": 1275, "label": "unlabeled"}, {"type": "text", "expr": "ACM Transactions on Programming Languages and\nSystems (TOPLAS)", "word_idx": 70800, "sentence_idx": 1276, "label": "unlabeled"}, {"type": "text", "expr": "M\u00f6cks (1988) \nJoachim M\u00f6cks", "word_idx": 70862, "sentence_idx": 1277, "label": "unlabeled"}, {"type": "text", "expr": "\n1988", "word_idx": 70889, "sentence_idx": 1278, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Topographic components model for event-related\npotentials and some biophysical considerations", "word_idx": 70894, "sentence_idx": 1279, "label": "unlabeled"}, {"type": "text", "expr": "M\u00f6cks (1988)", "word_idx": 70990, "sentence_idx": 1280, "label": "unlabeled"}, {"type": "text", "expr": "Joachim M\u00f6cks", "word_idx": 71002, "sentence_idx": 1281, "label": "unlabeled"}, {"type": "text", "expr": "\n1988", "word_idx": 71015, "sentence_idx": 1282, "label": "unlabeled"}, {"type": "text", "expr": "Topographic components model for event-related\npotentials and some biophysical considerations", "word_idx": 71020, "sentence_idx": 1283, "label": "unlabeled"}, {"type": "text", "expr": "IEEE transactions on biomedical engineering \n35, 6 (1988),\n482\u2013484", "word_idx": 71113, "sentence_idx": 1284, "label": "unlabeled"}, {"type": "text", "expr": "IEEE transactions on biomedical engineering", "word_idx": 71179, "sentence_idx": 1285, "label": "unlabeled"}, {"type": "text", "expr": "Patwary et\u00a0al", "word_idx": 71222, "sentence_idx": 1286, "label": "unlabeled"}, {"type": "text", "expr": " (2015) \nMd", "word_idx": 71235, "sentence_idx": 1287, "label": "unlabeled"}, {"type": "text", "expr": " Mostofa\u00a0Ali Patwary,\nNadathur\u00a0Rajagopalan Satish, Narayanan\nSundaram, Jongsoo Park, Michael\u00a0J\nAnderson, Satya\u00a0Gautam Vadlamudi,\nDipankar Das, Sergey\u00a0G Pudov,\nVadim\u00a0O Pirogov, and Pradeep Dubey", "word_idx": 71246, "sentence_idx": 1288, "label": "unlabeled"}, {"type": "text", "expr": "\n2015", "word_idx": 71439, "sentence_idx": 1289, "label": "unlabeled"}, {"type": "text", "expr": "Patwary et\u00a0al", "word_idx": 71444, "sentence_idx": 1290, "label": "unlabeled"}, {"type": "text", "expr": " (2015)", "word_idx": 71457, "sentence_idx": 1291, "label": "unlabeled"}, {"type": "text", "expr": " Mostofa\u00a0Ali Patwary,\nNadathur\u00a0Rajagopalan Satish, Narayanan\nSundaram, Jongsoo Park, Michael\u00a0J\nAnderson, Satya\u00a0Gautam Vadlamudi,\nDipankar Das, Sergey\u00a0G Pudov,\nVadim\u00a0O Pirogov, and Pradeep Dubey", "word_idx": 71464, "sentence_idx": 1292, "label": "unlabeled"}, {"type": "text", "expr": "\n2015", "word_idx": 71657, "sentence_idx": 1293, "label": "unlabeled"}, {"type": "text", "expr": "Parallel efficient sparse matrix-matrix\nmultiplication on multicore platforms", "word_idx": 71662, "sentence_idx": 1294, "label": "unlabeled"}, {"type": "text", "expr": " In\n International Conference on High Performance\nComputing ", "word_idx": 71739, "sentence_idx": 1295, "label": "unlabeled"}, {"type": "text", "expr": " Springer, 48\u201357", "word_idx": 71799, "sentence_idx": 1296, "label": "unlabeled"}, {"type": "text", "expr": "International Conference on High Performance\nComputing", "word_idx": 71815, "sentence_idx": 1297, "label": "unlabeled"}, {"type": "text", "expr": "Phan and Cichocki (2010) \nAnh\u00a0Huy Phan and Andrzej\nCichocki", "word_idx": 71869, "sentence_idx": 1298, "label": "unlabeled"}, {"type": "text", "expr": " 2010", "word_idx": 71928, "sentence_idx": 1299, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Tensor decompositions for feature extraction and\nclassification of high dimensional datasets", "word_idx": 71933, "sentence_idx": 1300, "label": "unlabeled"}, {"type": "text", "expr": "Phan and Cichocki (2010)", "word_idx": 72028, "sentence_idx": 1301, "label": "unlabeled"}, {"type": "text", "expr": "Anh\u00a0Huy Phan and Andrzej\nCichocki", "word_idx": 72052, "sentence_idx": 1302, "label": "unlabeled"}, {"type": "text", "expr": " 2010", "word_idx": 72085, "sentence_idx": 1303, "label": "unlabeled"}, {"type": "text", "expr": "Tensor decompositions for feature extraction and\nclassification of high dimensional datasets", "word_idx": 72090, "sentence_idx": 1304, "label": "unlabeled"}, {"type": "text", "expr": "Nonlinear theory and its applications,\nIEICE  1, 1 (2010),\n37\u201368", "word_idx": 72182, "sentence_idx": 1305, "label": "unlabeled"}, {"type": "text", "expr": "Nonlinear theory and its applications,\nIEICE", "word_idx": 72246, "sentence_idx": 1306, "label": "unlabeled"}, {"type": "text", "expr": "Pissanetzky (1984) \nSergio Pissanetzky", "word_idx": 72290, "sentence_idx": 1307, "label": "unlabeled"}, {"type": "text", "expr": "\n1984", "word_idx": 72328, "sentence_idx": 1308, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Academic Press", "word_idx": 72333, "sentence_idx": 1309, "label": "unlabeled"}, {"type": "text", "expr": "Pissanetzky (1984)", "word_idx": 72350, "sentence_idx": 1310, "label": "unlabeled"}, {"type": "text", "expr": "Sergio Pissanetzky", "word_idx": 72368, "sentence_idx": 1311, "label": "unlabeled"}, {"type": "text", "expr": "\n1984", "word_idx": 72386, "sentence_idx": 1312, "label": "unlabeled"}, {"type": "text", "expr": "Sparse Matrix Technology-electronic\nedition ", "word_idx": 72391, "sentence_idx": 1313, "label": "unlabeled"}, {"type": "text", "expr": "Sparse Matrix Technology-electronic\nedition", "word_idx": 72435, "sentence_idx": 1314, "label": "unlabeled"}, {"type": "text", "expr": "Academic Press", "word_idx": 72478, "sentence_idx": 1315, "label": "unlabeled"}, {"type": "text", "expr": "Ragan-Kelley et\u00a0al", "word_idx": 72492, "sentence_idx": 1316, "label": "unlabeled"}, {"type": "text", "expr": " (2012) \nJonathan Ragan-Kelley,\nAndrew Adams, Sylvain Paris,\nMarc Levoy, Saman Amarasinghe, and\nFr\u00e9do Durand", "word_idx": 72510, "sentence_idx": 1317, "label": "unlabeled"}, {"type": "text", "expr": " 2012", "word_idx": 72618, "sentence_idx": 1318, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Decoupling algorithms from schedules for easy\noptimization of image processing pipelines", "word_idx": 72623, "sentence_idx": 1319, "label": "unlabeled"}, {"type": "text", "expr": "\n\n (2012)", "word_idx": 72714, "sentence_idx": 1320, "label": "unlabeled"}, {"type": "text", "expr": "Ragan-Kelley et\u00a0al", "word_idx": 72723, "sentence_idx": 1321, "label": "unlabeled"}, {"type": "text", "expr": " (2012)", "word_idx": 72741, "sentence_idx": 1322, "label": "unlabeled"}, {"type": "text", "expr": "Jonathan Ragan-Kelley,\nAndrew Adams, Sylvain Paris,\nMarc Levoy, Saman Amarasinghe, and\nFr\u00e9do Durand", "word_idx": 72748, "sentence_idx": 1323, "label": "unlabeled"}, {"type": "text", "expr": " 2012", "word_idx": 72847, "sentence_idx": 1324, "label": "unlabeled"}, {"type": "text", "expr": "Decoupling algorithms from schedules for easy\noptimization of image processing pipelines", "word_idx": 72852, "sentence_idx": 1325, "label": "unlabeled"}, {"type": "text", "expr": "(2012)", "word_idx": 72940, "sentence_idx": 1326, "label": "unlabeled"}, {"type": "text", "expr": "Shashua and Levin (2001) \nAmnon Shashua and Anat\nLevin", "word_idx": 72946, "sentence_idx": 1327, "label": "unlabeled"}, {"type": "text", "expr": " 2001", "word_idx": 73000, "sentence_idx": 1328, "label": "unlabeled"}, {"type": "text", "expr": "Shashua and Levin (2001)", "word_idx": 73005, "sentence_idx": 1329, "label": "unlabeled"}, {"type": "text", "expr": "Amnon Shashua and Anat\nLevin", "word_idx": 73029, "sentence_idx": 1330, "label": "unlabeled"}, {"type": "text", "expr": " 2001", "word_idx": 73057, "sentence_idx": 1331, "label": "unlabeled"}, {"type": "text", "expr": "Linear image coding for regression and\nclassification using the tensor-rank principle", "word_idx": 73062, "sentence_idx": 1332, "label": "unlabeled"}, {"type": "text", "expr": " In\n Computer Vision and Pattern Recognition, 2001", "word_idx": 73147, "sentence_idx": 1333, "label": "unlabeled"}, {"type": "text", "expr": " CVPR\n2001", "word_idx": 73197, "sentence_idx": 1334, "label": "unlabeled"}, {"type": "text", "expr": " Proceedings of the 2001 IEEE Computer Society Conference on ,\nVol", "word_idx": 73207, "sentence_idx": 1335, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, I\u2013I", "word_idx": 73273, "sentence_idx": 1336, "label": "unlabeled"}, {"type": "text", "expr": "Computer Vision and Pattern Recognition, 2001", "word_idx": 73283, "sentence_idx": 1337, "label": "unlabeled"}, {"type": "text", "expr": " CVPR\n2001", "word_idx": 73328, "sentence_idx": 1338, "label": "unlabeled"}, {"type": "text", "expr": " Proceedings of the 2001 IEEE Computer Society Conference on", "word_idx": 73338, "sentence_idx": 1339, "label": "unlabeled"}, {"type": "text", "expr": "Smith\net\u00a0al", "word_idx": 73398, "sentence_idx": 1340, "label": "unlabeled"}, {"type": "text", "expr": " (2017a) \nShaden Smith, Alec Beri,\nand George Karypis", "word_idx": 73409, "sentence_idx": 1341, "label": "unlabeled"}, {"type": "text", "expr": " 2017a", "word_idx": 73462, "sentence_idx": 1342, "label": "unlabeled"}, {"type": "text", "expr": "Smith\net\u00a0al", "word_idx": 73468, "sentence_idx": 1343, "label": "unlabeled"}, {"type": "text", "expr": " (2017a)", "word_idx": 73479, "sentence_idx": 1344, "label": "unlabeled"}, {"type": "text", "expr": "Shaden Smith, Alec Beri,\nand George Karypis", "word_idx": 73487, "sentence_idx": 1345, "label": "unlabeled"}, {"type": "text", "expr": " 2017a", "word_idx": 73530, "sentence_idx": 1346, "label": "unlabeled"}, {"type": "text", "expr": "Constrained Tensor Factorization with Accelerated\nAO-ADMM", "word_idx": 73536, "sentence_idx": 1347, "label": "unlabeled"}, {"type": "text", "expr": " In  Parallel Processing (ICPP), 2017 46th\nInternational Conference on ", "word_idx": 73593, "sentence_idx": 1348, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 111\u2013120", "word_idx": 73664, "sentence_idx": 1349, "label": "unlabeled"}, {"type": "text", "expr": "Parallel Processing (ICPP), 2017 46th\nInternational Conference on", "word_idx": 73678, "sentence_idx": 1350, "label": "unlabeled"}, {"type": "text", "expr": "Smith et\u00a0al", "word_idx": 73743, "sentence_idx": 1351, "label": "unlabeled"}, {"type": "text", "expr": " (2017b) \nShaden Smith, Jee\u00a0W", "word_idx": 73754, "sentence_idx": 1352, "label": "unlabeled"}, {"type": "text", "expr": "\nChoi, Jiajia Li, Richard Vuduc,\nJongsoo Park, Xing Liu, and\nGeorge Karypis", "word_idx": 73783, "sentence_idx": 1353, "label": "unlabeled"}, {"type": "text", "expr": " 2017b", "word_idx": 73858, "sentence_idx": 1354, "label": "unlabeled"}, {"type": "text", "expr": "\n\n FROSTT: The Formidable Repository of Open Sparse\nTensors and Tools", "word_idx": 73864, "sentence_idx": 1355, "label": "unlabeled"}, {"type": "text", "expr": "\n\n (2017)", "word_idx": 73933, "sentence_idx": 1356, "label": "unlabeled"}, {"type": "text", "expr": "Smith et\u00a0al", "word_idx": 73942, "sentence_idx": 1357, "label": "unlabeled"}, {"type": "text", "expr": " (2017b)", "word_idx": 73953, "sentence_idx": 1358, "label": "unlabeled"}, {"type": "text", "expr": "Shaden Smith, Jee\u00a0W", "word_idx": 73961, "sentence_idx": 1359, "label": "unlabeled"}, {"type": "text", "expr": "\nChoi, Jiajia Li, Richard Vuduc,\nJongsoo Park, Xing Liu, and\nGeorge Karypis", "word_idx": 73980, "sentence_idx": 1360, "label": "unlabeled"}, {"type": "text", "expr": " 2017b", "word_idx": 74055, "sentence_idx": 1361, "label": "unlabeled"}, {"type": "text", "expr": "FROSTT: The Formidable Repository of Open Sparse\nTensors and Tools", "word_idx": 74061, "sentence_idx": 1362, "label": "unlabeled"}, {"type": "text", "expr": "(2017)", "word_idx": 74127, "sentence_idx": 1363, "label": "unlabeled"}, {"type": "text", "expr": "http://frostt", "word_idx": 74133, "sentence_idx": 1364, "label": "unlabeled"}, {"type": "text", "expr": "http://frostt", "word_idx": 74146, "sentence_idx": 1365, "label": "unlabeled"}, {"type": "text", "expr": "Smith et\u00a0al", "word_idx": 74159, "sentence_idx": 1366, "label": "unlabeled"}, {"type": "text", "expr": " (2015) \nShaden Smith, Niranjay\nRavindran, Nicholas Sidiropoulos, and\nGeorge Karypis", "word_idx": 74170, "sentence_idx": 1367, "label": "unlabeled"}, {"type": "text", "expr": " 2015", "word_idx": 74254, "sentence_idx": 1368, "label": "unlabeled"}, {"type": "text", "expr": "Smith et\u00a0al", "word_idx": 74259, "sentence_idx": 1369, "label": "unlabeled"}, {"type": "text", "expr": " (2015)", "word_idx": 74270, "sentence_idx": 1370, "label": "unlabeled"}, {"type": "text", "expr": "Shaden Smith, Niranjay\nRavindran, Nicholas Sidiropoulos, and\nGeorge Karypis", "word_idx": 74277, "sentence_idx": 1371, "label": "unlabeled"}, {"type": "text", "expr": " 2015", "word_idx": 74352, "sentence_idx": 1372, "label": "unlabeled"}, {"type": "text", "expr": "SPLATT: Efficient and Parallel Sparse Tensor-Matrix\nMultiplication", "word_idx": 74357, "sentence_idx": 1373, "label": "unlabeled"}, {"type": "text", "expr": " In  2015 IEEE International\nParallel and Distributed Processing Symposium (IPDPS) ", "word_idx": 74423, "sentence_idx": 1374, "label": "unlabeled"}, {"type": "text", "expr": "\n61\u201370", "word_idx": 74506, "sentence_idx": 1375, "label": "unlabeled"}, {"type": "text", "expr": "2015 IEEE International\nParallel and Distributed Processing Symposium (IPDPS)", "word_idx": 74512, "sentence_idx": 1376, "label": "unlabeled"}, {"type": "text", "expr": "Tinney and Walker (1967) \nWilliam\u00a0F Tinney and\nJohn\u00a0W Walker", "word_idx": 74589, "sentence_idx": 1377, "label": "unlabeled"}, {"type": "text", "expr": " 1967", "word_idx": 74649, "sentence_idx": 1378, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Direct solutions of sparse network equations by\noptimally ordered triangular factorization", "word_idx": 74654, "sentence_idx": 1379, "label": "unlabeled"}, {"type": "text", "expr": "Tinney and Walker (1967)", "word_idx": 74747, "sentence_idx": 1380, "label": "unlabeled"}, {"type": "text", "expr": "William\u00a0F Tinney and\nJohn\u00a0W Walker", "word_idx": 74771, "sentence_idx": 1381, "label": "unlabeled"}, {"type": "text", "expr": " 1967", "word_idx": 74805, "sentence_idx": 1382, "label": "unlabeled"}, {"type": "text", "expr": "Direct solutions of sparse network equations by\noptimally ordered triangular factorization", "word_idx": 74810, "sentence_idx": 1383, "label": "unlabeled"}, {"type": "text", "expr": " IEEE  55,\n11 (1967), 1801\u20131809", "word_idx": 74900, "sentence_idx": 1384, "label": "unlabeled"}, {"type": "text", "expr": " IEEE", "word_idx": 74931, "sentence_idx": 1385, "label": "unlabeled"}, {"type": "text", "expr": "Wolfe (1982) \nMichael\u00a0Joseph Wolfe", "word_idx": 74936, "sentence_idx": 1386, "label": "unlabeled"}, {"type": "text", "expr": "\n1982", "word_idx": 74970, "sentence_idx": 1387, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Ph", "word_idx": 74975, "sentence_idx": 1388, "label": "unlabeled"}, {"type": "text", "expr": " Dissertation", "word_idx": 74980, "sentence_idx": 1389, "label": "unlabeled"}, {"type": "text", "expr": " University\nof Illinois at Urbana-Champaign, Champaign, IL, USA", "word_idx": 74993, "sentence_idx": 1390, "label": "unlabeled"}, {"type": "text", "expr": "\n\n AAI8303027", "word_idx": 75056, "sentence_idx": 1391, "label": "unlabeled"}, {"type": "text", "expr": "Wolfe (1982)", "word_idx": 75069, "sentence_idx": 1392, "label": "unlabeled"}, {"type": "text", "expr": "Michael\u00a0Joseph Wolfe", "word_idx": 75081, "sentence_idx": 1393, "label": "unlabeled"}, {"type": "text", "expr": "\n1982", "word_idx": 75101, "sentence_idx": 1394, "label": "unlabeled"}, {"type": "text", "expr": "Optimizing Supercompilers for Supercomputers ", "word_idx": 75106, "sentence_idx": 1395, "label": "unlabeled"}, {"type": "text", "expr": "Optimizing Supercompilers for Supercomputers", "word_idx": 75151, "sentence_idx": 1396, "label": "unlabeled"}, {"type": "text", "expr": " Dissertation", "word_idx": 75195, "sentence_idx": 1397, "label": "unlabeled"}, {"type": "text", "expr": " University\nof Illinois at Urbana-Champaign, Champaign, IL, USA", "word_idx": 75208, "sentence_idx": 1398, "label": "unlabeled"}, {"type": "text", "expr": "AAI8303027", "word_idx": 75271, "sentence_idx": 1399, "label": "unlabeled"}, {"type": "text", "expr": "Wulf et\u00a0al", "word_idx": 75281, "sentence_idx": 1400, "label": "unlabeled"}, {"type": "text", "expr": " (1974) \nW", "word_idx": 75291, "sentence_idx": 1401, "label": "unlabeled"}, {"type": "text", "expr": " Wulf, E", "word_idx": 75301, "sentence_idx": 1402, "label": "unlabeled"}, {"type": "text", "expr": " Cohen,\nW", "word_idx": 75309, "sentence_idx": 1403, "label": "unlabeled"}, {"type": "text", "expr": " Corwin, A", "word_idx": 75318, "sentence_idx": 1404, "label": "unlabeled"}, {"type": "text", "expr": " Jones, R", "word_idx": 75328, "sentence_idx": 1405, "label": "unlabeled"}, {"type": "text", "expr": "\nLevin, C", "word_idx": 75337, "sentence_idx": 1406, "label": "unlabeled"}, {"type": "text", "expr": " Pierson, and F", "word_idx": 75346, "sentence_idx": 1407, "label": "unlabeled"}, {"type": "text", "expr": " Pollack", "word_idx": 75361, "sentence_idx": 1408, "label": "unlabeled"}, {"type": "text", "expr": "\n1974", "word_idx": 75369, "sentence_idx": 1409, "label": "unlabeled"}, {"type": "text", "expr": "\n\n HYDRA: The Kernel of a Multiprocessor Operating\nSystem", "word_idx": 75374, "sentence_idx": 1410, "label": "unlabeled"}, {"type": "text", "expr": "Wulf et\u00a0al", "word_idx": 75431, "sentence_idx": 1411, "label": "unlabeled"}, {"type": "text", "expr": " (1974)", "word_idx": 75441, "sentence_idx": 1412, "label": "unlabeled"}, {"type": "text", "expr": " Wulf, E", "word_idx": 75448, "sentence_idx": 1413, "label": "unlabeled"}, {"type": "text", "expr": " Cohen,\nW", "word_idx": 75456, "sentence_idx": 1414, "label": "unlabeled"}, {"type": "text", "expr": " Corwin, A", "word_idx": 75465, "sentence_idx": 1415, "label": "unlabeled"}, {"type": "text", "expr": " Jones, R", "word_idx": 75475, "sentence_idx": 1416, "label": "unlabeled"}, {"type": "text", "expr": "\nLevin, C", "word_idx": 75484, "sentence_idx": 1417, "label": "unlabeled"}, {"type": "text", "expr": " Pierson, and F", "word_idx": 75493, "sentence_idx": 1418, "label": "unlabeled"}, {"type": "text", "expr": " Pollack", "word_idx": 75508, "sentence_idx": 1419, "label": "unlabeled"}, {"type": "text", "expr": "\n1974", "word_idx": 75516, "sentence_idx": 1420, "label": "unlabeled"}, {"type": "text", "expr": "HYDRA: The Kernel of a Multiprocessor Operating\nSystem", "word_idx": 75521, "sentence_idx": 1421, "label": "unlabeled"}, {"type": "text", "expr": "Commun", "word_idx": 75575, "sentence_idx": 1422, "label": "unlabeled"}, {"type": "text", "expr": " ACM  17,\n6 (June 1974),\n337\u2013345", "word_idx": 75581, "sentence_idx": 1423, "label": "unlabeled"}, {"type": "text", "expr": "Commun", "word_idx": 75613, "sentence_idx": 1424, "label": "unlabeled"}, {"type": "text", "expr": "https://doi", "word_idx": 75619, "sentence_idx": 1425, "label": "unlabeled"}, {"type": "text", "expr": "org/10", "word_idx": 75630, "sentence_idx": 1426, "label": "unlabeled"}, {"type": "text", "expr": "1145/355616", "word_idx": 75636, "sentence_idx": 1427, "label": "unlabeled"}, {"type": "text", "expr": "364017", "word_idx": 75647, "sentence_idx": 1428, "label": "unlabeled"}, {"type": "text", "expr": "https://doi", "word_idx": 75653, "sentence_idx": 1429, "label": "unlabeled"}, {"type": "text", "expr": "org/10", "word_idx": 75664, "sentence_idx": 1430, "label": "unlabeled"}, {"type": "text", "expr": "1145/355616", "word_idx": 75670, "sentence_idx": 1431, "label": "unlabeled"}, {"type": "text", "expr": "364017", "word_idx": 75681, "sentence_idx": 1432, "label": "unlabeled"}, {"type": "text", "expr": "Zhao (2014) \nHuasha Zhao", "word_idx": 75687, "sentence_idx": 1433, "label": "unlabeled"}, {"type": "text", "expr": "\n2014", "word_idx": 75711, "sentence_idx": 1434, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Ph", "word_idx": 75716, "sentence_idx": 1435, "label": "unlabeled"}, {"type": "text", "expr": " Dissertation", "word_idx": 75721, "sentence_idx": 1436, "label": "unlabeled"}, {"type": "text", "expr": " EECS\nDepartment, University of California, Berkeley", "word_idx": 75734, "sentence_idx": 1437, "label": "unlabeled"}, {"type": "text", "expr": "Zhao (2014)", "word_idx": 75786, "sentence_idx": 1438, "label": "unlabeled"}, {"type": "text", "expr": "Huasha Zhao", "word_idx": 75797, "sentence_idx": 1439, "label": "unlabeled"}, {"type": "text", "expr": "\n2014", "word_idx": 75808, "sentence_idx": 1440, "label": "unlabeled"}, {"type": "text", "expr": "High Performance Machine Learning through\nCodesign and Rooflining ", "word_idx": 75813, "sentence_idx": 1441, "label": "unlabeled"}, {"type": "text", "expr": "High Performance Machine Learning through\nCodesign and Rooflining", "word_idx": 75879, "sentence_idx": 1442, "label": "unlabeled"}, {"type": "text", "expr": " Dissertation", "word_idx": 75944, "sentence_idx": 1443, "label": "unlabeled"}, {"type": "text", "expr": " EECS\nDepartment, University of California, Berkeley", "word_idx": 75957, "sentence_idx": 1444, "label": "unlabeled"}, {"type": "text", "expr": "Generated  on Wed Mar  7 13:37:22 2018 by", "word_idx": 76009, "sentence_idx": 1445, "label": "unlabeled"}, {"type": "text", "expr": "LaTeXML", "word_idx": 76050, "sentence_idx": 1446, "label": "unlabeled"}], "underlying_plans": [{"type": "text", "expr": "Discovering Underlying Plans Based on Shallow Models", "word_idx": 0, "sentence_idx": 0, "label": "unlabeled"}, {"type": "text", "expr": "Discovering Underlying Plans Based on Shallow Models", "word_idx": 52, "sentence_idx": 1, "label": "unlabeled"}, {"type": "text", "expr": "Hankz Hankui Zhuo", "word_idx": 104, "sentence_idx": 2, "label": "unlabeled"}, {"type": "text", "expr": "Hankz Hankui Zhuo", "word_idx": 121, "sentence_idx": 3, "label": "unlabeled"}, {"type": "text", "expr": "Hankz Hankui Zhuo Sun Yat-Sen University \n Yantian Zha Arizona State University \n Subbarao Kambhampati Arizona State University", "word_idx": 138, "sentence_idx": 4, "label": "unlabeled"}, {"type": "text", "expr": "2 email:  zhuohank@mail", "word_idx": 265, "sentence_idx": 5, "label": "unlabeled"}, {"type": "text", "expr": "email:", "word_idx": 288, "sentence_idx": 6, "label": "unlabeled"}, {"type": "text", "expr": "4 email:  yantian", "word_idx": 294, "sentence_idx": 7, "label": "unlabeled"}, {"type": "text", "expr": "zha@asu", "word_idx": 311, "sentence_idx": 8, "label": "unlabeled"}, {"type": "text", "expr": "email:", "word_idx": 318, "sentence_idx": 9, "label": "unlabeled"}, {"type": "text", "expr": "6 email:  rao@asu", "word_idx": 324, "sentence_idx": 10, "label": "unlabeled"}, {"type": "text", "expr": "email:", "word_idx": 341, "sentence_idx": 11, "label": "unlabeled"}, {"type": "text", "expr": "Yantian Zha", "word_idx": 347, "sentence_idx": 12, "label": "unlabeled"}, {"type": "text", "expr": "Yantian Zha", "word_idx": 358, "sentence_idx": 13, "label": "unlabeled"}, {"type": "text", "expr": "Hankz Hankui Zhuo Sun Yat-Sen University \n Yantian Zha Arizona State University \n Subbarao Kambhampati Arizona State University", "word_idx": 369, "sentence_idx": 14, "label": "unlabeled"}, {"type": "text", "expr": "2 email:  zhuohank@mail", "word_idx": 496, "sentence_idx": 15, "label": "unlabeled"}, {"type": "text", "expr": "email:", "word_idx": 519, "sentence_idx": 16, "label": "unlabeled"}, {"type": "text", "expr": "4 email:  yantian", "word_idx": 525, "sentence_idx": 17, "label": "unlabeled"}, {"type": "text", "expr": "zha@asu", "word_idx": 542, "sentence_idx": 18, "label": "unlabeled"}, {"type": "text", "expr": "email:", "word_idx": 549, "sentence_idx": 19, "label": "unlabeled"}, {"type": "text", "expr": "6 email:  rao@asu", "word_idx": 555, "sentence_idx": 20, "label": "unlabeled"}, {"type": "text", "expr": "email:", "word_idx": 572, "sentence_idx": 21, "label": "unlabeled"}, {"type": "text", "expr": "Subbarao Kambhampati", "word_idx": 578, "sentence_idx": 22, "label": "unlabeled"}, {"type": "text", "expr": "Subbarao Kambhampati", "word_idx": 598, "sentence_idx": 23, "label": "unlabeled"}, {"type": "text", "expr": "Hankz Hankui Zhuo Sun Yat-Sen University \n Yantian Zha Arizona State University \n Subbarao Kambhampati Arizona State University", "word_idx": 618, "sentence_idx": 24, "label": "unlabeled"}, {"type": "text", "expr": "2 email:  zhuohank@mail", "word_idx": 745, "sentence_idx": 25, "label": "unlabeled"}, {"type": "text", "expr": "email:", "word_idx": 768, "sentence_idx": 26, "label": "unlabeled"}, {"type": "text", "expr": "4 email:  yantian", "word_idx": 774, "sentence_idx": 27, "label": "unlabeled"}, {"type": "text", "expr": "zha@asu", "word_idx": 791, "sentence_idx": 28, "label": "unlabeled"}, {"type": "text", "expr": "email:", "word_idx": 798, "sentence_idx": 29, "label": "unlabeled"}, {"type": "text", "expr": "6 email:  rao@asu", "word_idx": 804, "sentence_idx": 30, "label": "unlabeled"}, {"type": "text", "expr": "email:", "word_idx": 821, "sentence_idx": 31, "label": "unlabeled"}, {"type": "text", "expr": "Abstract", "word_idx": 827, "sentence_idx": 32, "label": "unlabeled"}, {"type": "text", "expr": "Abstract", "word_idx": 835, "sentence_idx": 33, "label": "unlabeled"}, {"type": "text", "expr": "Plan recognition aims to discover target plans (i", "word_idx": 843, "sentence_idx": 34, "label": "unlabeled"}, {"type": "text", "expr": ", sequences of actions) behind observed actions, with history plan libraries or domain models in hand", "word_idx": 892, "sentence_idx": 35, "label": "unlabeled"}, {"type": "text", "expr": " Previous approaches either discover plans by maximally \u201cmatching\u201d observed actions to plan libraries, assuming target plans are from plan libraries, or infer plans by executing domain models to best explain the observed actions, assuming that complete domain models are available", "word_idx": 993, "sentence_idx": 36, "label": "unlabeled"}, {"type": "text", "expr": " In real world applications, however, target plans are often not from plan libraries, and complete domain models are often not available, since building complete sets of plans and complete domain models are often difficult or expensive", "word_idx": 1273, "sentence_idx": 37, "label": "unlabeled"}, {"type": "text", "expr": " In this paper we view plan libraries as corpora and learn vector representations of actions using the corpora; we then discover target plans based on the vector representations", "word_idx": 1508, "sentence_idx": 38, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, we propose two approaches,  DUP  and  RNNPlanner , to discover target plans based on vector representations of actions", "word_idx": 1685, "sentence_idx": 39, "label": "unlabeled"}, {"type": "text", "expr": "  DUP  explores the EM-style framework to capture local contexts of actions and discover target plans by optimizing the probability of target plans, while  RNNPlanner  aims to leverage long-short term contexts of actions based on RNNs (recurrent neural networks) framework to help recognize target plans", "word_idx": 1818, "sentence_idx": 40, "label": "unlabeled"}, {"type": "text", "expr": " In the experiments, we empirically show that our approaches are capable of discovering underlying plans that are not from plan libraries, without requiring domain models provided", "word_idx": 2121, "sentence_idx": 41, "label": "unlabeled"}, {"type": "text", "expr": " We demonstrate the effectiveness of our approaches by comparing its performance to traditional plan recognition approaches in three planning domains", "word_idx": 2300, "sentence_idx": 42, "label": "unlabeled"}, {"type": "text", "expr": " We also compare  DUP  and  RNNPlanner  to see their advantages and disadvantages", "word_idx": 2449, "sentence_idx": 43, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 2530, "sentence_idx": 44, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 2540, "sentence_idx": 45, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 2550, "sentence_idx": 46, "label": "unlabeled"}, {"type": "text", "expr": "Keywords: Plan Recognition Distributed Representation Shallow Model AI Planning Action Model Learning", "word_idx": 2560, "sentence_idx": 47, "label": "unlabeled"}, {"type": "text", "expr": "Keywords:", "word_idx": 2661, "sentence_idx": 48, "label": "unlabeled"}, {"type": "text", "expr": "1  Introduction", "word_idx": 2670, "sentence_idx": 49, "label": "unlabeled"}, {"type": "text", "expr": "As computer-aided cooperative work scenarios become increasingly popular, human-in-the-loop planning and decision support has become a critical planning challenge (c", "word_idx": 2685, "sentence_idx": 50, "label": "unlabeled"}, {"type": "text", "expr": " An important aspect of such a support  (15)  is recognizing what plans the human in the loop is making, and provide appropriate suggestions about their next actions  (2) ", "word_idx": 2850, "sentence_idx": 51, "label": "unlabeled"}, {"type": "text", "expr": "\nAlthough there is a lot of work on plan recognition, much of it has\ntraditionally depended on the availability of a complete domain model\n ", "word_idx": 3021, "sentence_idx": 52, "label": "unlabeled"}, {"type": "text", "expr": " As has been argued elsewhere\n (15) , such models are hard to get\nin human-in-the-loop planning scenarios", "word_idx": 3161, "sentence_idx": 53, "label": "unlabeled"}, {"type": "text", "expr": " Here, the decision support\nsystems have to make themselves useful without insisting on complete\naction models of the domain", "word_idx": 3266, "sentence_idx": 54, "label": "unlabeled"}, {"type": "text", "expr": " The situation here is akin to that faced\nby search engines and other tools for computer supported\ncooperate work, and is thus a significant departure for the \u201cplanning\nas pure inference\u201d mindset of the automated planning community", "word_idx": 3390, "sentence_idx": 55, "label": "unlabeled"}, {"type": "text", "expr": " As\nsuch, the problem\ncalls for plan recognition with \u201cshallow\u201d models of the domain\n(c", "word_idx": 3621, "sentence_idx": 56, "label": "unlabeled"}, {"type": "text", "expr": "  (13) ), that can be easily learned\nautomatically", "word_idx": 3708, "sentence_idx": 57, "label": "unlabeled"}, {"type": "text", "expr": " Compared to learning action models (\u201ccomplex\u201d models correspondingly) of the domain from limited training data, learning shallow models can avoid the overfitting issue", "word_idx": 3758, "sentence_idx": 58, "label": "unlabeled"}, {"type": "text", "expr": " One key difference between \u201cshallow\u201d and \u201ccomplex\u201d models is the size of parameters of both models is distinguish, which is comparable to learning models in machine learning community, i", "word_idx": 3926, "sentence_idx": 59, "label": "unlabeled"}, {"type": "text", "expr": ", complex models with large parameters require much more training data for learning parameter values compared to \u201cshallow\u201d models", "word_idx": 4113, "sentence_idx": 60, "label": "unlabeled"}, {"type": "text", "expr": "There has been very little work on learning such shallow models to support human-in-the-loop planning", "word_idx": 4242, "sentence_idx": 61, "label": "unlabeled"}, {"type": "text", "expr": " Some examples include the work on Woogle system  (7)  that aimed to provide support to humans in web-service composition", "word_idx": 4343, "sentence_idx": 62, "label": "unlabeled"}, {"type": "text", "expr": " That work however relied on very primitive understanding of the actions (web services in their case) that consisted merely of learning the input/output types of individual services", "word_idx": 4464, "sentence_idx": 63, "label": "unlabeled"}, {"type": "text", "expr": " In this paper, we focus on learning more informative models that can help recognize the plans under construction by the humans, and provide active support by suggesting relevant actions", "word_idx": 4645, "sentence_idx": 64, "label": "unlabeled"}, {"type": "text", "expr": " To drive this process, we propose two approaches to learning informative models, namely  DUP , standing for  D iscovering  U nderlying  P lans based on action-vector representations, and  RNNPlanner , standing for  R ecurrent  N eural  N etwork based  Planner ", "word_idx": 4831, "sentence_idx": 65, "label": "unlabeled"}, {"type": "text", "expr": " The framework of  DUP  and  RNNPlanner  is shown in Figure  1 , where we take as input a set of plans (or a  plan library ) and learn the distributed representations of actions (namely  action vectors )", "word_idx": 5092, "sentence_idx": 66, "label": "unlabeled"}, {"type": "text", "expr": " After that, our  DUP  approach exploits an EM-Style framework to discover underlying plans based on the learnt action vectors, while our  RNNPlanner  approach exploits an RNN-Style framework to generate plans to best explain observations (i", "word_idx": 5295, "sentence_idx": 67, "label": "unlabeled"}, {"type": "text", "expr": ", discover underlying plans behind the observed actions) based on the learnt action vectors", "word_idx": 5536, "sentence_idx": 68, "label": "unlabeled"}, {"type": "text", "expr": " In  DUP  we consider local contexts (with a limited window size) of actions being recognized, while in  RNNPlanner  we explore the potential influence from long and short-term actions, which can be modelled by RNN, to help recognize unknown actions", "word_idx": 5627, "sentence_idx": 69, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 5876, "sentence_idx": 70, "label": "unlabeled"}, {"type": "text", "expr": "Planner", "word_idx": 5886, "sentence_idx": 71, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 5893, "sentence_idx": 72, "label": "unlabeled"}, {"type": "text", "expr": "plan library", "word_idx": 5903, "sentence_idx": 73, "label": "unlabeled"}, {"type": "text", "expr": "action vectors", "word_idx": 5915, "sentence_idx": 74, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 5929, "sentence_idx": 75, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 5939, "sentence_idx": 76, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:  The framework of our shallow models  DUP  and  RNNPlanner", "word_idx": 5949, "sentence_idx": 77, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:", "word_idx": 6017, "sentence_idx": 78, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 6026, "sentence_idx": 79, "label": "unlabeled"}, {"type": "text", "expr": "In summary, the contributions of the paper are shown below", "word_idx": 6036, "sentence_idx": 80, "label": "unlabeled"}, {"type": "text", "expr": "In summary, the contributions of the paper are shown below", "word_idx": 6094, "sentence_idx": 81, "label": "unlabeled"}, {"type": "text", "expr": "In  (28) , we presented a version of  DUP ", "word_idx": 6152, "sentence_idx": 82, "label": "unlabeled"}, {"type": "text", "expr": " In this paper we extend  (28)  with more details to elaborate the approach", "word_idx": 6194, "sentence_idx": 83, "label": "unlabeled"}, {"type": "text", "expr": "We propose a novel model  RNNPlanner  based on RNN to explore the influence of actions from long and short-term contexts", "word_idx": 6269, "sentence_idx": 84, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 6389, "sentence_idx": 85, "label": "unlabeled"}, {"type": "text", "expr": "We compare  RNNPlanner  to  DUP  to exhibit the advantage and disadvantage of leveraging information from long and short-term contexts", "word_idx": 6399, "sentence_idx": 86, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 6533, "sentence_idx": 87, "label": "unlabeled"}, {"type": "text", "expr": "In the sequel, we first formulate our plan recognition problem, and then address the details of our approaches  DUP  and  RNNPlanner ", "word_idx": 6543, "sentence_idx": 88, "label": "unlabeled"}, {"type": "text", "expr": " After that, we empirically demonstrate that it does capture a surprising amount of structure in the observed plan sequences, leading to effective plan recognition", "word_idx": 6676, "sentence_idx": 89, "label": "unlabeled"}, {"type": "text", "expr": " We further compare its performance to traditional plan recognition techniques, including one that uses the same plan traces to learn the STRIPS-style action models, and use the learned model to support plan recognition", "word_idx": 6839, "sentence_idx": 90, "label": "unlabeled"}, {"type": "text", "expr": " We also compare  RNNPlanner  with  DUP  to see the advantage and disadvantage of leveraging long and short-term contexts of actions in different scenarios", "word_idx": 7058, "sentence_idx": 91, "label": "unlabeled"}, {"type": "text", "expr": " We finally review previous approaches related to our work and conclude our paper with further work", "word_idx": 7213, "sentence_idx": 92, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 7312, "sentence_idx": 93, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 7322, "sentence_idx": 94, "label": "unlabeled"}, {"type": "text", "expr": "2  Problem Formulation", "word_idx": 7332, "sentence_idx": 95, "label": "unlabeled"}, {"type": "text", "expr": "A plan library, denoted by  $\\mathcal{L}$ , is composed of a set of plans  $\\{p\\}$ , where  $p$  is a sequence of actions, i", "word_idx": 7354, "sentence_idx": 96, "label": "unlabeled"}, {"type": "text", "expr": ",  $p=\\langle a_{1},a_{2},\\ldots,a_{n}\\rangle$  where  $a_{i}$ ,  $1\\leq i\\leq n$ , is an action name (without any parameter) represented by a string", "word_idx": 7478, "sentence_idx": 97, "label": "unlabeled"}, {"type": "text", "expr": " For example, a string  unstack-A-B  is an action meaning that  a robot unstacks block A from block B ", "word_idx": 7627, "sentence_idx": 98, "label": "unlabeled"}, {"type": "text", "expr": " We denote the set of all possible actions by  $\\bar{\\mathcal{A}}$  which is assumed to be known beforehand", "word_idx": 7729, "sentence_idx": 99, "label": "unlabeled"}, {"type": "text", "expr": " For ease of presentation, we assume that there is an empty action,  $\\phi$ , indicating an unknown or not observed action, i", "word_idx": 7836, "sentence_idx": 100, "label": "unlabeled"}, {"type": "text", "expr": ",  $\\mathcal{A}=\\bar{\\mathcal{A}}\\cup\\{\\phi\\}$ ", "word_idx": 7961, "sentence_idx": 101, "label": "unlabeled"}, {"type": "text", "expr": " An observation of an  unknown  plan  $\\tilde{p}$  is denoted by  $\\mathcal{O}=\\langle o_{1},o_{2},\\ldots,o_{M}\\rangle$ , where  $o_{i}\\in\\mathcal{A}$ ,  $1\\leq i\\leq M$ , is either an action in  $\\bar{\\mathcal{A}}$  or an empty action  $\\phi$  indicating the corresponding action is missing or not observed", "word_idx": 8008, "sentence_idx": 102, "label": "unlabeled"}, {"type": "text", "expr": " Note that  $\\tilde{p}$  is not necessarily in the plan library  $\\mathcal{L}$ , which makes the plan recognition problem more challenging, since matching the observation to the plan library will not work any more", "word_idx": 8315, "sentence_idx": 103, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}$$", "word_idx": 8528, "sentence_idx": 104, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{p\\}$$", "word_idx": 8539, "sentence_idx": 105, "label": "unlabeled"}, {"type": "math", "expr": "$$p=\\langle a_{1},a_{2},\\ldots,a_{n}\\rangle$$", "word_idx": 8544, "sentence_idx": 106, "label": "unlabeled"}, {"type": "math", "expr": "$$a_{i}$$", "word_idx": 8585, "sentence_idx": 107, "label": "unlabeled"}, {"type": "math", "expr": "$$1\\leq i\\leq n$$", "word_idx": 8590, "sentence_idx": 108, "label": "unlabeled"}, {"type": "text", "expr": "unstack-A-B", "word_idx": 8603, "sentence_idx": 109, "label": "unlabeled"}, {"type": "text", "expr": "a robot unstacks block A from block B", "word_idx": 8614, "sentence_idx": 110, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{\\mathcal{A}}$$", "word_idx": 8651, "sentence_idx": 111, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 8668, "sentence_idx": 112, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{A}=\\bar{\\mathcal{A}}\\cup\\{\\phi\\}$$", "word_idx": 8672, "sentence_idx": 113, "label": "unlabeled"}, {"type": "text", "expr": "unknown", "word_idx": 8713, "sentence_idx": 114, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{p}$$", "word_idx": 8720, "sentence_idx": 115, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}=\\langle o_{1},o_{2},\\ldots,o_{M}\\rangle$$", "word_idx": 8729, "sentence_idx": 116, "label": "unlabeled"}, {"type": "math", "expr": "$$o_{i}\\in\\mathcal{A}$$", "word_idx": 8780, "sentence_idx": 117, "label": "unlabeled"}, {"type": "math", "expr": "$$1\\leq i\\leq M$$", "word_idx": 8799, "sentence_idx": 118, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{\\mathcal{A}}$$", "word_idx": 8812, "sentence_idx": 119, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 8829, "sentence_idx": 120, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{p}$$", "word_idx": 8833, "sentence_idx": 121, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}$$", "word_idx": 8842, "sentence_idx": 122, "label": "unlabeled"}, {"type": "text", "expr": "We assume that the human is making a plan of at most length  $M$ ", "word_idx": 8853, "sentence_idx": 123, "label": "unlabeled"}, {"type": "text", "expr": " We also\nassume that at any given point, the planner is able to observe  $M-k$  of\nthese actions", "word_idx": 8918, "sentence_idx": 124, "label": "unlabeled"}, {"type": "text", "expr": " The  $k$  unobserved actions might either be in the suffiix of the plan, or in the middle", "word_idx": 9014, "sentence_idx": 125, "label": "unlabeled"}, {"type": "text", "expr": "\nOur aim is to suggest, for each of the  $k$  unobserved actions,  $m$ \npossible choices from which the user can select the action", "word_idx": 9104, "sentence_idx": 126, "label": "unlabeled"}, {"type": "text", "expr": " (Note\nthat we would like to keep  $m$  small, ideally close to 1, so as not to\noverwhelm users)", "word_idx": 9234, "sentence_idx": 127, "label": "unlabeled"}, {"type": "text", "expr": "\nAccordingly, we will evaluate the effectiveness of the decision\nsupport in terms of whether or not the user\u2019s best/intended action is\nwithin the suggested  $m$  actions", "word_idx": 9330, "sentence_idx": 128, "label": "unlabeled"}, {"type": "math", "expr": "$$M-k$$", "word_idx": 9499, "sentence_idx": 129, "label": "unlabeled"}, {"type": "text", "expr": "Specifically, our recognition problem can be represented by a triple  $\\Re=(\\mathcal{L},\\mathcal{O},\\mathcal{A})$ ", "word_idx": 9502, "sentence_idx": 130, "label": "unlabeled"}, {"type": "text", "expr": " The solution to  $\\Re$  is to discover the unknown plan  $\\tilde{p}$ , which is a plan with unkwown observations, that best explains  $\\mathcal{O}$  given  $\\mathcal{L}$  and  $\\mathcal{A}$ ", "word_idx": 9616, "sentence_idx": 131, "label": "unlabeled"}, {"type": "text", "expr": " We have the following assumptions  A1-A3 :", "word_idx": 9807, "sentence_idx": 132, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Re=(\\mathcal{L},\\mathcal{O},\\mathcal{A})$$", "word_idx": 9850, "sentence_idx": 133, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Re$$", "word_idx": 9891, "sentence_idx": 134, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{p}$$", "word_idx": 9894, "sentence_idx": 135, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 9903, "sentence_idx": 136, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}$$", "word_idx": 9914, "sentence_idx": 137, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{A}$$", "word_idx": 9925, "sentence_idx": 138, "label": "unlabeled"}, {"type": "text", "expr": "A1-A3", "word_idx": 9936, "sentence_idx": 139, "label": "unlabeled"}, {"type": "text", "expr": "The length of the underlying plan to be discovered is known, which releases us from searching unlimited length of plans", "word_idx": 9941, "sentence_idx": 140, "label": "unlabeled"}, {"type": "text", "expr": "The length of the underlying plan to be discovered is known, which releases us from searching unlimited length of plans", "word_idx": 10060, "sentence_idx": 141, "label": "unlabeled"}, {"type": "text", "expr": "The positions of missing actions in the underlying plan is known in advance, which releases us from searching missing actions in between observed actions", "word_idx": 10179, "sentence_idx": 142, "label": "unlabeled"}, {"type": "text", "expr": "The positions of missing actions in the underlying plan is known in advance, which releases us from searching missing actions in between observed actions", "word_idx": 10332, "sentence_idx": 143, "label": "unlabeled"}, {"type": "text", "expr": "All actions observed are assumed to be correct, which indicates there is no need to criticize or rectify the observed actions", "word_idx": 10485, "sentence_idx": 144, "label": "unlabeled"}, {"type": "text", "expr": "All actions observed are assumed to be correct, which indicates there is no need to criticize or rectify the observed actions", "word_idx": 10610, "sentence_idx": 145, "label": "unlabeled"}, {"type": "text", "expr": "An example of our plan recognition problem in the  blocks  domain is shown below", "word_idx": 10735, "sentence_idx": 146, "label": "unlabeled"}, {"type": "text", "expr": "blocks", "word_idx": 10815, "sentence_idx": 147, "label": "unlabeled"}, {"type": "text", "expr": "1 http://www", "word_idx": 10821, "sentence_idx": 148, "label": "unlabeled"}, {"type": "text", "expr": "toronto", "word_idx": 10833, "sentence_idx": 149, "label": "unlabeled"}, {"type": "text", "expr": "edu/aips2000/", "word_idx": 10840, "sentence_idx": 150, "label": "unlabeled"}, {"type": "text", "expr": "Example:  A plan library  $\\mathcal{L}$  in the  blocks  domain is assumed to have four plans as shown below:", "word_idx": 10853, "sentence_idx": 151, "label": "unlabeled"}, {"type": "text", "expr": "Example:", "word_idx": 10962, "sentence_idx": 152, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}$$", "word_idx": 10970, "sentence_idx": 153, "label": "unlabeled"}, {"type": "text", "expr": "blocks", "word_idx": 10981, "sentence_idx": 154, "label": "unlabeled"}, {"type": "text", "expr": "plan 1 :  pick-up-B stack-B-A pick-up-D stack-D-C", "word_idx": 10987, "sentence_idx": 155, "label": "unlabeled"}, {"type": "text", "expr": "plan 1", "word_idx": 11036, "sentence_idx": 156, "label": "unlabeled"}, {"type": "text", "expr": "pick-up-B stack-B-A pick-up-D stack-D-C", "word_idx": 11042, "sentence_idx": 157, "label": "unlabeled"}, {"type": "text", "expr": "plan 2 :  unstack-B-A put-down-B unstack-D-C put-down-D", "word_idx": 11081, "sentence_idx": 158, "label": "unlabeled"}, {"type": "text", "expr": "plan 2", "word_idx": 11136, "sentence_idx": 159, "label": "unlabeled"}, {"type": "text", "expr": "unstack-B-A put-down-B unstack-D-C put-down-D", "word_idx": 11142, "sentence_idx": 160, "label": "unlabeled"}, {"type": "text", "expr": "plan 3 :  pick-up-B stack-B-A pick-up-C stack-C-B pick-up-D stack-D-C", "word_idx": 11187, "sentence_idx": 161, "label": "unlabeled"}, {"type": "text", "expr": "plan 3", "word_idx": 11256, "sentence_idx": 162, "label": "unlabeled"}, {"type": "text", "expr": "pick-up-B stack-B-A pick-up-C stack-C-B pick-up-D stack-D-C", "word_idx": 11262, "sentence_idx": 163, "label": "unlabeled"}, {"type": "text", "expr": "plan 4 :  unstack-D-C put-down-D unstack-C-B put-down-C unstack-B-A put-down-B", "word_idx": 11321, "sentence_idx": 164, "label": "unlabeled"}, {"type": "text", "expr": "plan 4", "word_idx": 11399, "sentence_idx": 165, "label": "unlabeled"}, {"type": "text", "expr": "unstack-D-C put-down-D unstack-C-B put-down-C unstack-B-A put-down-B", "word_idx": 11405, "sentence_idx": 166, "label": "unlabeled"}, {"type": "text", "expr": "An observation  $\\mathcal{O}$  of action sequence is shown below:", "word_idx": 11473, "sentence_idx": 167, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 11538, "sentence_idx": 168, "label": "unlabeled"}, {"type": "text", "expr": "observation:", "word_idx": 11549, "sentence_idx": 169, "label": "unlabeled"}, {"type": "text", "expr": "observation:", "word_idx": 11561, "sentence_idx": 170, "label": "unlabeled"}, {"type": "text", "expr": "pick-up-B  $\\phi$  unstack-D-C put-down-D  $\\phi$  stack-C-B  $\\phi$   $\\phi$", "word_idx": 11573, "sentence_idx": 171, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 11650, "sentence_idx": 172, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 11654, "sentence_idx": 173, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 11658, "sentence_idx": 174, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 11662, "sentence_idx": 175, "label": "unlabeled"}, {"type": "text", "expr": "Given the above input, our  DUP  algorithm outputs plans as follows:", "word_idx": 11666, "sentence_idx": 176, "label": "unlabeled"}, {"type": "text", "expr": "pick-up-B stack-B-A unstack-D-C put-down-D pick-up-C stack-C-B pick-up-D stack-D-C", "word_idx": 11734, "sentence_idx": 177, "label": "unlabeled"}, {"type": "text", "expr": "pick-up-B stack-B-A unstack-D-C put-down-D pick-up-C stack-C-B pick-up-D stack-D-C", "word_idx": 11816, "sentence_idx": 178, "label": "unlabeled"}, {"type": "text", "expr": "pick-up-B stack-B-A unstack-D-C put-down-D pick-up-C stack-C-B pick-up-D stack-D-C", "word_idx": 11898, "sentence_idx": 179, "label": "unlabeled"}, {"type": "text", "expr": "Although the \u201cplan completion\u201d problem seems to differ superficially\nfrom the traditional \u201cplan recognition\u201d problem, we point out that\nmany earlier works on plan recognition do in fact evaluate their\nrecognition algorithms in terms of completion tasks, e", "word_idx": 11980, "sentence_idx": 180, "label": "unlabeled"}, {"type": "text", "expr": " While\nthese earlier efforts use different problem settings, taking either a\nplan library or action models as input, they share one common\ncharacteristic: they all aim to look for a plan that can best explain\n(or complete) the observed actions", "word_idx": 12235, "sentence_idx": 181, "label": "unlabeled"}, {"type": "text", "expr": " This is exactly the same as our\nproblem we aim to solve", "word_idx": 12478, "sentence_idx": 182, "label": "unlabeled"}, {"type": "text", "expr": "3  Learning the distributed representations of actions", "word_idx": 12534, "sentence_idx": 183, "label": "unlabeled"}, {"type": "text", "expr": "Since actions are denoted by a name string, actions can be viewed as words, and a plan can be viewed as a sentence", "word_idx": 12588, "sentence_idx": 184, "label": "unlabeled"}, {"type": "text", "expr": " Furthermore, the plan library  $\\mathcal{L}$  can be seen as a corpus, and the set of all possible actions  $\\mathcal{A}$  is the vocabulary", "word_idx": 12702, "sentence_idx": 185, "label": "unlabeled"}, {"type": "text", "expr": " Given a plan corpus, we can exploit off-the-shelf approaches, e", "word_idx": 12843, "sentence_idx": 186, "label": "unlabeled"}, {"type": "text", "expr": ", the Skip-gram model  (20) , for learning vector representations for actions", "word_idx": 12907, "sentence_idx": 187, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}$$", "word_idx": 12984, "sentence_idx": 188, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{A}$$", "word_idx": 12995, "sentence_idx": 189, "label": "unlabeled"}, {"type": "text", "expr": "The objective of the Skip-gram model is to learn vector representations for predicting the surrounding words in a sentence or document", "word_idx": 13006, "sentence_idx": 190, "label": "unlabeled"}, {"type": "text", "expr": " Given a corpus  $\\mathcal{C}$ , composed of a sequence of training words  $\\langle w_{1},w_{2},\\ldots,w_{T}\\rangle$ , where  $T=|\\mathcal{C}|$ , the Skip-gram model maximizes the average log probability", "word_idx": 13140, "sentence_idx": 191, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{C}$$", "word_idx": 13343, "sentence_idx": 192, "label": "unlabeled"}, {"type": "math", "expr": "$$\\langle w_{1},w_{2},\\ldots,w_{T}\\rangle$$", "word_idx": 13354, "sentence_idx": 193, "label": "unlabeled"}, {"type": "math", "expr": "$$T=|\\mathcal{C}|$$", "word_idx": 13393, "sentence_idx": 194, "label": "unlabeled"}, {"type": "text", "expr": "$\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{-c\\leq j\\leq c,j\\neq 0}\\log p(w_{t+j}|w_{t})$", "word_idx": 13408, "sentence_idx": 195, "label": "unlabeled"}, {"type": "math", "expr": "$$\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{-c\\leq j\\leq c,j\\neq 0}\\log p(w_{t+j}|w_{t})$$", "word_idx": 13485, "sentence_idx": 196, "label": "unlabeled"}, {"type": "text", "expr": "where  $c$  is the size of the training window or context", "word_idx": 13560, "sentence_idx": 197, "label": "unlabeled"}, {"type": "text", "expr": "where", "word_idx": 13617, "sentence_idx": 198, "label": "unlabeled"}, {"type": "text", "expr": "The basic probability  $p(w_{t+j}|w_{t})$  is defined by the hierarchical softmax, which uses a binary tree representation of the output layer with the  $K$  words as its leaves and for each node, explicitly represents the relative probabilities of its child nodes  (20) ", "word_idx": 13622, "sentence_idx": 199, "label": "unlabeled"}, {"type": "text", "expr": " For each leaf node, there is an unique path from the root to the node, and this path is used to estimate the probability of the word represented by the leaf node", "word_idx": 13893, "sentence_idx": 200, "label": "unlabeled"}, {"type": "text", "expr": " There are no explicit output vector representations for words", "word_idx": 14055, "sentence_idx": 201, "label": "unlabeled"}, {"type": "text", "expr": " Instead, each inner node has an output vector  $v^{\\prime}_{n(w,j)}$ , and the probability of a word being the output word is defined by", "word_idx": 14117, "sentence_idx": 202, "label": "unlabeled"}, {"type": "math", "expr": "$$p(w_{t+j}|w_{t})$$", "word_idx": 14254, "sentence_idx": 203, "label": "unlabeled"}, {"type": "math", "expr": "$$v^{\\prime}_{n(w,j)}$$", "word_idx": 14270, "sentence_idx": 204, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle p(w_{t+j}|w_{t})=\\prod_{i=1}^{L(w_{t+j})-1}\\Big{\\{}\\sigma(%\n\\mathbb{I}(n(w_{t+j},i+1)=$", "word_idx": 14289, "sentence_idx": 205, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle p(w_{t+j}|w_{t})=\\prod_{i=1}^{L(w_{t+j})-1}\\Big{\\{}\\sigma(%\n\\mathbb{I}(n(w_{t+j},i+1)=$$", "word_idx": 14391, "sentence_idx": 206, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle child(n(w_{t+j},i)))\\cdot v_{n(w_{t+j},i)}\\cdot v_{w_{t}})\\Big{%\n\\}},$", "word_idx": 14491, "sentence_idx": 207, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle child(n(w_{t+j},i)))\\cdot v_{n(w_{t+j},i)}\\cdot v_{w_{t}})\\Big{%\n\\}},$$", "word_idx": 14576, "sentence_idx": 208, "label": "unlabeled"}, {"type": "text", "expr": "where", "word_idx": 14659, "sentence_idx": 209, "label": "unlabeled"}, {"type": "text", "expr": "$\\sigma(x)=1/(1+\\exp(-x))$", "word_idx": 14664, "sentence_idx": 210, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma(x)=1/(1+\\exp(-x)).$$", "word_idx": 14690, "sentence_idx": 211, "label": "unlabeled"}, {"type": "text", "expr": "$L(w)$  is the length from the root to the word  $w$  in the binary tree, e", "word_idx": 14715, "sentence_idx": 212, "label": "unlabeled"}, {"type": "text", "expr": ",  $L(w)=4$  if there are four nodes from the root to  $w$ ", "word_idx": 14790, "sentence_idx": 213, "label": "unlabeled"}, {"type": "text", "expr": "  $n(w,i)$  is the  $i$ th node from the root to  $w$ , e", "word_idx": 14849, "sentence_idx": 214, "label": "unlabeled"}, {"type": "text", "expr": ",  $n(w,1)=root$  and  $n(w,L(w))=w$ ", "word_idx": 14906, "sentence_idx": 215, "label": "unlabeled"}, {"type": "text", "expr": "  $child(n)$  is a fixed child (e", "word_idx": 14943, "sentence_idx": 216, "label": "unlabeled"}, {"type": "text", "expr": ", left child) of node  $n$ ", "word_idx": 14976, "sentence_idx": 217, "label": "unlabeled"}, {"type": "text", "expr": "  $v_{n}$  is the vector representation of the inner node  $n$ ", "word_idx": 15003, "sentence_idx": 218, "label": "unlabeled"}, {"type": "text", "expr": "  $v_{w_{t}}$  is the input vector representation of word  $w_{t}$ ", "word_idx": 15066, "sentence_idx": 219, "label": "unlabeled"}, {"type": "text", "expr": " The identity function  $\\mathbb{I}(x)$  is 1 if  $x$  is true; otherwise it is -1", "word_idx": 15133, "sentence_idx": 220, "label": "unlabeled"}, {"type": "math", "expr": "$$L(w)$$", "word_idx": 15215, "sentence_idx": 221, "label": "unlabeled"}, {"type": "math", "expr": "$$L(w)=4$$", "word_idx": 15219, "sentence_idx": 222, "label": "unlabeled"}, {"type": "math", "expr": "$$n(w,i)$$", "word_idx": 15225, "sentence_idx": 223, "label": "unlabeled"}, {"type": "math", "expr": "$$n(w,1)=root$$", "word_idx": 15231, "sentence_idx": 224, "label": "unlabeled"}, {"type": "math", "expr": "$$n(w,L(w))=w$$", "word_idx": 15242, "sentence_idx": 225, "label": "unlabeled"}, {"type": "math", "expr": "$$child(n)$$", "word_idx": 15253, "sentence_idx": 226, "label": "unlabeled"}, {"type": "math", "expr": "$$v_{n}$$", "word_idx": 15261, "sentence_idx": 227, "label": "unlabeled"}, {"type": "math", "expr": "$$v_{w_{t}}$$", "word_idx": 15266, "sentence_idx": 228, "label": "unlabeled"}, {"type": "math", "expr": "$$w_{t}$$", "word_idx": 15275, "sentence_idx": 229, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbb{I}(x)$$", "word_idx": 15280, "sentence_idx": 230, "label": "unlabeled"}, {"type": "text", "expr": "We can thus build vector representations of actions by maximizing Equation ( 1 ) with corpora or plan libraries  $\\mathcal{L}$  as input", "word_idx": 15293, "sentence_idx": 231, "label": "unlabeled"}, {"type": "text", "expr": " We will exploit the vector representations to discover the unknown plan  $\\tilde{p}$  in the next subsection", "word_idx": 15429, "sentence_idx": 232, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}$$", "word_idx": 15538, "sentence_idx": 233, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{p}$$", "word_idx": 15549, "sentence_idx": 234, "label": "unlabeled"}, {"type": "text", "expr": "4  Our  DUP  Algorithm", "word_idx": 15558, "sentence_idx": 235, "label": "unlabeled"}, {"type": "text", "expr": "Our  DUP  approach to the recognition problem  $\\Re$  functions by two phases", "word_idx": 15580, "sentence_idx": 236, "label": "unlabeled"}, {"type": "text", "expr": " We first learn vector representations of actions using the plan library  $\\mathcal{L}$ ", "word_idx": 15657, "sentence_idx": 237, "label": "unlabeled"}, {"type": "text", "expr": " We then iteratively sample actions for unobserved actions  $o_{i}$  by maximizing the probability of the unknown plan  $\\tilde{p}$  via the EM framework", "word_idx": 15745, "sentence_idx": 238, "label": "unlabeled"}, {"type": "text", "expr": " We present  DUP  in detail in the following subsections", "word_idx": 15898, "sentence_idx": 239, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Re$$", "word_idx": 15954, "sentence_idx": 240, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}$$", "word_idx": 15957, "sentence_idx": 241, "label": "unlabeled"}, {"type": "math", "expr": "$$o_{i}$$", "word_idx": 15968, "sentence_idx": 242, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{p}$$", "word_idx": 15973, "sentence_idx": 243, "label": "unlabeled"}, {"type": "text", "expr": "1  Maximizing Probability of Unknown Plans", "word_idx": 15982, "sentence_idx": 244, "label": "unlabeled"}, {"type": "text", "expr": "With the vector representations learnt in the last subsection, a straightforward way to discover the unknown plan  $\\tilde{p}$  is to explore all possible actions in  $\\bar{\\mathcal{A}}$  such that  $\\tilde{p}$  has the highest probability, which can be defined similar to Equation ( 1 ), i", "word_idx": 16024, "sentence_idx": 245, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{p}$$", "word_idx": 16314, "sentence_idx": 246, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{\\mathcal{A}}$$", "word_idx": 16323, "sentence_idx": 247, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{p}$$", "word_idx": 16340, "sentence_idx": 248, "label": "unlabeled"}, {"type": "text", "expr": "$\\mathcal{F}(\\tilde{p})=\\sum_{k=1}^{M}\\sum_{-c\\leq j\\leq c,j\\neq 0}\\log p(w_{k+%\nj}|w_{k})$", "word_idx": 16349, "sentence_idx": 249, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{F}(\\tilde{p})=\\sum_{k=1}^{M}\\sum_{-c\\leq j\\leq c,j\\neq 0}\\log p(w_{k+%\nj}|w_{k})$$", "word_idx": 16440, "sentence_idx": 250, "label": "unlabeled"}, {"type": "text", "expr": "where  $w_{k}$  denotes the  $k$ th action of  $\\tilde{p}$  and  $M$  is the length of  $\\tilde{p}$ ", "word_idx": 16529, "sentence_idx": 251, "label": "unlabeled"}, {"type": "text", "expr": " As we can see, this approach is exponentially hard with respect to the size of  $\\bar{\\mathcal{A}}$  and number of unobserved actions", "word_idx": 16629, "sentence_idx": 252, "label": "unlabeled"}, {"type": "text", "expr": " We thus design an approximate approach in the Expectation-Maximization framework to estimate an unknown plan  $\\tilde{p}$  that best explains the observation  $\\mathcal{O}$ ", "word_idx": 16763, "sentence_idx": 253, "label": "unlabeled"}, {"type": "math", "expr": "$$w_{k}$$", "word_idx": 16937, "sentence_idx": 254, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{p}$$", "word_idx": 16942, "sentence_idx": 255, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{p}$$", "word_idx": 16951, "sentence_idx": 256, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{\\mathcal{A}}$$", "word_idx": 16960, "sentence_idx": 257, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{p}$$", "word_idx": 16977, "sentence_idx": 258, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 16986, "sentence_idx": 259, "label": "unlabeled"}, {"type": "text", "expr": "To do this, we introduce new parameters to capture \u201cweights\u201d of values for each unobserved action", "word_idx": 16997, "sentence_idx": 260, "label": "unlabeled"}, {"type": "text", "expr": " Specifically speaking, assuming there are  $X$  unobserved actions in  $\\mathcal{O}$ , i", "word_idx": 17094, "sentence_idx": 261, "label": "unlabeled"}, {"type": "text", "expr": ", the number of  $\\phi$ s in  $\\mathcal{O}$  is  $X$ , we denote these unobserved actions by  $\\bar{a}_{1},,\\bar{a}_{x},,\\bar{a}_{X}$ , where the indices indicate the order they appear in  $\\mathcal{O}$ ", "word_idx": 17183, "sentence_idx": 262, "label": "unlabeled"}, {"type": "text", "expr": " Note that each  $\\bar{a}_{x}$  can be any action in  $\\bar{\\mathcal{A}}$ ", "word_idx": 17386, "sentence_idx": 263, "label": "unlabeled"}, {"type": "text", "expr": " We associate each possible value of  $\\bar{a}_{x}$  with a weight, denoted by  $\\bar{\\Gamma}_{\\bar{a}_{x},x}$ ", "word_idx": 17460, "sentence_idx": 264, "label": "unlabeled"}, {"type": "text", "expr": "  $\\bar{\\Gamma}$  is a  $|\\bar{\\mathcal{A}}|\\times X$  matrix, satisfying", "word_idx": 17571, "sentence_idx": 265, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 17644, "sentence_idx": 266, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 17655, "sentence_idx": 267, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 17659, "sentence_idx": 268, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{a}_{1},...,\\bar{a}_{x},...,\\bar{a}_{X}$$", "word_idx": 17670, "sentence_idx": 269, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 17713, "sentence_idx": 270, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{a}_{x}$$", "word_idx": 17724, "sentence_idx": 271, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{\\mathcal{A}}$$", "word_idx": 17735, "sentence_idx": 272, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{a}_{x}$$", "word_idx": 17752, "sentence_idx": 273, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{\\Gamma}_{\\bar{a}_{x},x}$$", "word_idx": 17763, "sentence_idx": 274, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{\\Gamma}$$", "word_idx": 17791, "sentence_idx": 275, "label": "unlabeled"}, {"type": "math", "expr": "$$|\\bar{\\mathcal{A}}|\\times X$$", "word_idx": 17803, "sentence_idx": 276, "label": "unlabeled"}, {"type": "text", "expr": "$\\sum_{o\\in\\bar{\\mathcal{A}}}\\bar{\\Gamma}_{o,x}=1,$", "word_idx": 17830, "sentence_idx": 277, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sum_{o\\in\\bar{\\mathcal{A}}}\\bar{\\Gamma}_{o,x}=1,$$", "word_idx": 17881, "sentence_idx": 278, "label": "unlabeled"}, {"type": "text", "expr": "where  $\\bar{\\Gamma}_{o,x}\\geq 0$  for each  $x$ ", "word_idx": 17930, "sentence_idx": 279, "label": "unlabeled"}, {"type": "text", "expr": " For the ease of specification, we extend  $\\bar{\\Gamma}$  to a bigger matrix with a size of  $|\\bar{\\mathcal{A}}|\\times M$ , denoted by  $\\Gamma$ , such that  $\\Gamma_{o,y}=\\bar{\\Gamma}_{o,x}$  if  $y$  is the index of the  $x$ th unobserved action in  $\\mathcal{O}$ , for all  $o\\in\\bar{\\mathcal{A}}$ ; otherwise,  $\\Gamma_{o,y}=1$  and  $\\Gamma_{o^{\\prime},y}=0$  for all  $o^{\\prime}\\in\\bar{\\mathcal{A}}\\wedge o^{\\prime}\\neq o$ ", "word_idx": 17979, "sentence_idx": 280, "label": "unlabeled"}, {"type": "text", "expr": " Our intuition is to estimate the unknown plan  $\\tilde{p}$  by selecting actions with the highest weights", "word_idx": 18411, "sentence_idx": 281, "label": "unlabeled"}, {"type": "text", "expr": " We thus introduce the weights to Equation ( 3 ), as shown below,", "word_idx": 18517, "sentence_idx": 282, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{\\Gamma}_{o,x}\\geq 0$$", "word_idx": 18582, "sentence_idx": 283, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{\\Gamma}$$", "word_idx": 18606, "sentence_idx": 284, "label": "unlabeled"}, {"type": "math", "expr": "$$|\\bar{\\mathcal{A}}|\\times M$$", "word_idx": 18618, "sentence_idx": 285, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma$$", "word_idx": 18645, "sentence_idx": 286, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{o,y}=\\bar{\\Gamma}_{o,x}$$", "word_idx": 18651, "sentence_idx": 287, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 18682, "sentence_idx": 288, "label": "unlabeled"}, {"type": "math", "expr": "$$o\\in\\bar{\\mathcal{A}}$$", "word_idx": 18693, "sentence_idx": 289, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{o,y}=1$$", "word_idx": 18714, "sentence_idx": 290, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{o^{\\prime},y}=0$$", "word_idx": 18728, "sentence_idx": 291, "label": "unlabeled"}, {"type": "math", "expr": "$$o^{\\prime}\\in\\bar{\\mathcal{A}}\\wedge o^{\\prime}\\neq o$$", "word_idx": 18751, "sentence_idx": 292, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{p}$$", "word_idx": 18804, "sentence_idx": 293, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle p(w_{k+j}|w_{k})=\\prod_{i=1}^{L(w_{k+j})-1}\\Big{\\{}\\sigma(%\n\\mathbb{I}(n(w_{k+j},i+1)=$", "word_idx": 18813, "sentence_idx": 294, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle p(w_{k+j}|w_{k})=\\prod_{i=1}^{L(w_{k+j})-1}\\Big{\\{}\\sigma(%\n\\mathbb{I}(n(w_{k+j},i+1)=$$", "word_idx": 18915, "sentence_idx": 295, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle child(n(w_{k+j},i)))\\cdot av_{n(w_{k+j},i)}\\cdot bv_{w_{k}})\\Big%\n{\\}},$", "word_idx": 19015, "sentence_idx": 296, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle child(n(w_{k+j},i)))\\cdot av_{n(w_{k+j},i)}\\cdot bv_{w_{k}})\\Big%\n{\\}},$$", "word_idx": 19102, "sentence_idx": 297, "label": "unlabeled"}, {"type": "text", "expr": "where  $a=\\Gamma_{w_{k+j},k+j}$  and  $b=\\Gamma_{w_{k},k}$ ", "word_idx": 19187, "sentence_idx": 298, "label": "unlabeled"}, {"type": "text", "expr": " We can see that the impact of  $w_{k+j}$  and  $w_{k}$  is penalized by weights  $a$  and  $b$  if they are unobserved actions, and stays unchanged, otherwise (since both  $a$  and  $b$  equal to 1 if they are observed actions)", "word_idx": 19246, "sentence_idx": 299, "label": "unlabeled"}, {"type": "math", "expr": "$$a=\\Gamma_{w_{k+j},k+j}$$", "word_idx": 19474, "sentence_idx": 300, "label": "unlabeled"}, {"type": "math", "expr": "$$b=\\Gamma_{w_{k},k}$$", "word_idx": 19496, "sentence_idx": 301, "label": "unlabeled"}, {"type": "math", "expr": "$$w_{k+j}$$", "word_idx": 19514, "sentence_idx": 302, "label": "unlabeled"}, {"type": "math", "expr": "$$w_{k}$$", "word_idx": 19521, "sentence_idx": 303, "label": "unlabeled"}, {"type": "text", "expr": "We assume  $X\\sim Multinomial(\\Gamma_{\\cdot,x})$ , i", "word_idx": 19526, "sentence_idx": 304, "label": "unlabeled"}, {"type": "text", "expr": ",  $p(X=o)=\\Gamma_{o,x}$ , where  $\\Gamma_{o,x}\\geq 0$  and  $\\sum_{a\\in\\bar{\\mathcal{A}}}\\eta^{a}=1$ ", "word_idx": 19578, "sentence_idx": 305, "label": "unlabeled"}, {"type": "text", "expr": "  $P(\\tilde{p}|\\Gamma)=\\prod_{x}\\Gamma_{o,x}$", "word_idx": 19680, "sentence_idx": 306, "label": "unlabeled"}, {"type": "math", "expr": "$$X\\sim Multinomial(\\Gamma_{\\cdot,x})$$", "word_idx": 19725, "sentence_idx": 307, "label": "unlabeled"}, {"type": "math", "expr": "$$p(X=o)=\\Gamma_{o,x}$$", "word_idx": 19760, "sentence_idx": 308, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{o,x}\\geq 0$$", "word_idx": 19779, "sentence_idx": 309, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sum_{a\\in\\bar{\\mathcal{A}}}\\eta^{a}=1$$", "word_idx": 19797, "sentence_idx": 310, "label": "unlabeled"}, {"type": "math", "expr": "$$P(\\tilde{p}|\\Gamma)=\\prod_{x}\\Gamma_{o,x}$$", "word_idx": 19835, "sentence_idx": 311, "label": "unlabeled"}, {"type": "text", "expr": "We redefine the objective function as shown below,", "word_idx": 19876, "sentence_idx": 312, "label": "unlabeled"}, {"type": "text", "expr": "We redefine the objective function as shown below,", "word_idx": 19926, "sentence_idx": 313, "label": "unlabeled"}, {"type": "text", "expr": "$\\mathcal{F}(\\tilde{p},\\Gamma)=\\sum_{k=1}^{M}\\sum_{-c\\leq j\\leq c,j\\neq 0}\\log p%\n(w_{k+j}|w_{k}),$", "word_idx": 19976, "sentence_idx": 314, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{F}(\\tilde{p},\\Gamma)=\\sum_{k=1}^{M}\\sum_{-c\\leq j\\leq c,j\\neq 0}\\log p%\n(w_{k+j}|w_{k}),$$", "word_idx": 20075, "sentence_idx": 315, "label": "unlabeled"}, {"type": "text", "expr": "where  $p(w_{k+j}|w_{k})$  is defined by Equation ( 4", "word_idx": 20172, "sentence_idx": 316, "label": "unlabeled"}, {"type": "text", "expr": " The gradient of Equation  5  is shown below,", "word_idx": 20225, "sentence_idx": 317, "label": "unlabeled"}, {"type": "math", "expr": "$$p(w_{k+j}|w_{k})$$", "word_idx": 20270, "sentence_idx": 318, "label": "unlabeled"}, {"type": "text", "expr": "$\\frac{\\partial\\mathcal{F}}{\\partial\\Gamma_{o,x}}=\\frac{4c(L(o)-1)}{\\Gamma_{o,x%\n}}$", "word_idx": 20286, "sentence_idx": 319, "label": "unlabeled"}, {"type": "math", "expr": "$$\\frac{\\partial\\mathcal{F}}{\\partial\\Gamma_{o,x}}=\\frac{4c(L(o)-1)}{\\Gamma_{o,x%\n}}.$$", "word_idx": 20370, "sentence_idx": 320, "label": "unlabeled"}, {"type": "text", "expr": "The only parameters needed to be updated are  $\\Gamma$ , which can be easily done by gradient descent, as shown below,", "word_idx": 20453, "sentence_idx": 321, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma$$", "word_idx": 20571, "sentence_idx": 322, "label": "unlabeled"}, {"type": "text", "expr": "$\\Gamma_{o,x}=\\Gamma_{o,x}+\\delta\\frac{\\partial\\mathcal{F}}{\\partial\\Gamma_{o,x%\n}},$", "word_idx": 20577, "sentence_idx": 323, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{o,x}=\\Gamma_{o,x}+\\delta\\frac{\\partial\\mathcal{F}}{\\partial\\Gamma_{o,x%\n}},$$", "word_idx": 20662, "sentence_idx": 324, "label": "unlabeled"}, {"type": "text", "expr": "if  $x$  is the index of unobserved action in  $\\mathcal{O}$ ; otherwise,  $\\Gamma_{o,x}$  stays unchanged, i", "word_idx": 20745, "sentence_idx": 325, "label": "unlabeled"}, {"type": "text", "expr": ",  $\\Gamma_{o,x}=1$ ", "word_idx": 20854, "sentence_idx": 326, "label": "unlabeled"}, {"type": "text", "expr": " Note that  $\\delta$  is a learning constant", "word_idx": 20874, "sentence_idx": 327, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 20918, "sentence_idx": 328, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{o,x}$$", "word_idx": 20929, "sentence_idx": 329, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{o,x}=1$$", "word_idx": 20941, "sentence_idx": 330, "label": "unlabeled"}, {"type": "math", "expr": "$$\\delta$$", "word_idx": 20955, "sentence_idx": 331, "label": "unlabeled"}, {"type": "text", "expr": "With Equation ( 7 ), we can design an EM algorithm by repeatedly sampling an unknown plan according to  $\\Gamma$  and updating  $\\Gamma$  based on Equation ( 7 ) until reaching convergence (e", "word_idx": 20961, "sentence_idx": 332, "label": "unlabeled"}, {"type": "text", "expr": ", a constant number of repetitions is reached)", "word_idx": 21152, "sentence_idx": 333, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma$$", "word_idx": 21198, "sentence_idx": 334, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma$$", "word_idx": 21204, "sentence_idx": 335, "label": "unlabeled"}, {"type": "text", "expr": "2  Overview of our  DUP  approach", "word_idx": 21210, "sentence_idx": 336, "label": "unlabeled"}, {"type": "text", "expr": "An overview of our  DUP  algorithm is shown in Algorithm  1 ", "word_idx": 21243, "sentence_idx": 337, "label": "unlabeled"}, {"type": "text", "expr": " In Step 2 of Algorithm  1 , we initialize  $\\Gamma_{o,k}=1/M$  for all  $o\\in\\bar{\\mathcal{A}}$ , if  $k$  is an index of unobserved actions in  $\\mathcal{O}$ ; and otherwise,  $\\Gamma_{o,k}=1$  and  $\\Gamma_{o^{\\prime},k}=0$  for all  $o^{\\prime}\\in\\bar{\\mathcal{A}}\\wedge o^{\\prime}\\neq o$ ", "word_idx": 21303, "sentence_idx": 338, "label": "unlabeled"}, {"type": "text", "expr": " In Step 4, we view  $\\Gamma_{\\cdot,k}$  as a probability distribution, and sample an action from  $\\bar{\\mathcal{A}}$  based on  $\\Gamma_{\\cdot,k}$  if  $k$  is an unobserved action index in  $\\mathcal{O}$ ", "word_idx": 21596, "sentence_idx": 339, "label": "unlabeled"}, {"type": "text", "expr": " In Step 5, we only update  $\\Gamma_{\\cdot,k}$  where  $k$  is an unobserved action index", "word_idx": 21803, "sentence_idx": 340, "label": "unlabeled"}, {"type": "text", "expr": " In Step 6, we linearly project all elements of the updated  $\\Gamma$  to between 0 and 1, such that we can do sampling directly based on  $\\Gamma$  in Step 4", "word_idx": 21892, "sentence_idx": 341, "label": "unlabeled"}, {"type": "text", "expr": " In Step 8, we simply select  $\\bar{a}_{x}$  based on", "word_idx": 22050, "sentence_idx": 342, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{o,k}=1/M$$", "word_idx": 22103, "sentence_idx": 343, "label": "unlabeled"}, {"type": "math", "expr": "$$o\\in\\bar{\\mathcal{A}}$$", "word_idx": 22119, "sentence_idx": 344, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 22140, "sentence_idx": 345, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{o,k}=1$$", "word_idx": 22151, "sentence_idx": 346, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{o^{\\prime},k}=0$$", "word_idx": 22165, "sentence_idx": 347, "label": "unlabeled"}, {"type": "math", "expr": "$$o^{\\prime}\\in\\bar{\\mathcal{A}}\\wedge o^{\\prime}\\neq o$$", "word_idx": 22188, "sentence_idx": 348, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{\\cdot,k}$$", "word_idx": 22241, "sentence_idx": 349, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{\\mathcal{A}}$$", "word_idx": 22257, "sentence_idx": 350, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{\\cdot,k}$$", "word_idx": 22274, "sentence_idx": 351, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 22290, "sentence_idx": 352, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{\\cdot,k}$$", "word_idx": 22301, "sentence_idx": 353, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma$$", "word_idx": 22317, "sentence_idx": 354, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma$$", "word_idx": 22323, "sentence_idx": 355, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{a}_{x}$$", "word_idx": 22329, "sentence_idx": 356, "label": "unlabeled"}, {"type": "text", "expr": "$\\bar{a}_{x}=\\arg\\max_{o\\in\\bar{\\mathcal{A}}}\\Gamma_{o,x},$", "word_idx": 22340, "sentence_idx": 357, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{a}_{x}=\\arg\\max_{o\\in\\bar{\\mathcal{A}}}\\Gamma_{o,x},$$", "word_idx": 22399, "sentence_idx": 358, "label": "unlabeled"}, {"type": "text", "expr": "for all unobserved action index  $x$ ", "word_idx": 22456, "sentence_idx": 359, "label": "unlabeled"}, {"type": "text", "expr": "Algorithm\u00a01  Framework of our  DUP  algorithm", "word_idx": 22493, "sentence_idx": 360, "label": "unlabeled"}, {"type": "text", "expr": "Algorithm\u00a01", "word_idx": 22538, "sentence_idx": 361, "label": "unlabeled"}, {"type": "text", "expr": "Input:  plan library  $\\mathcal{L}$ , observed actions  $\\mathcal{O}$ Output:  plan  $\\tilde{p}$", "word_idx": 22549, "sentence_idx": 362, "label": "unlabeled"}, {"type": "text", "expr": "Input:", "word_idx": 22645, "sentence_idx": 363, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}$$", "word_idx": 22651, "sentence_idx": 364, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 22662, "sentence_idx": 365, "label": "unlabeled"}, {"type": "text", "expr": "Output:", "word_idx": 22673, "sentence_idx": 366, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{p}$$", "word_idx": 22680, "sentence_idx": 367, "label": "unlabeled"}, {"type": "text", "expr": "1: \u00a0\u00a0learn vector representation of actions", "word_idx": 22689, "sentence_idx": 368, "label": "unlabeled"}, {"type": "text", "expr": "2: \u00a0\u00a0initialize  $\\Gamma_{o,k}$  with  $1/M$  for all  $o\\in\\bar{\\mathcal{A}}$ , when  $k$  is an unobserved action index", "word_idx": 22732, "sentence_idx": 369, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{o,k}$$", "word_idx": 22853, "sentence_idx": 370, "label": "unlabeled"}, {"type": "math", "expr": "$$1/M$$", "word_idx": 22865, "sentence_idx": 371, "label": "unlabeled"}, {"type": "math", "expr": "$$o\\in\\bar{\\mathcal{A}}$$", "word_idx": 22868, "sentence_idx": 372, "label": "unlabeled"}, {"type": "text", "expr": "3: \u00a0\u00a0 while \u00a0the maximal number of repetitions is not reached\u00a0 do", "word_idx": 22889, "sentence_idx": 373, "label": "unlabeled"}, {"type": "text", "expr": "while", "word_idx": 22954, "sentence_idx": 374, "label": "unlabeled"}, {"type": "text", "expr": "4: \u00a0\u00a0\u00a0\u00a0\u00a0sample unobserved actions in  $\\mathcal{O}$  based on  $\\Gamma$", "word_idx": 22959, "sentence_idx": 375, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 23030, "sentence_idx": 376, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma$$", "word_idx": 23041, "sentence_idx": 377, "label": "unlabeled"}, {"type": "text", "expr": "5: \u00a0\u00a0\u00a0\u00a0\u00a0update  $\\Gamma$  based on Equation ( 7 )", "word_idx": 23047, "sentence_idx": 378, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma$$", "word_idx": 23096, "sentence_idx": 379, "label": "unlabeled"}, {"type": "text", "expr": "6: \u00a0\u00a0\u00a0\u00a0\u00a0project  $\\Gamma$  to [0,1]", "word_idx": 23102, "sentence_idx": 380, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma$$", "word_idx": 23137, "sentence_idx": 381, "label": "unlabeled"}, {"type": "text", "expr": "7: \u00a0\u00a0 end \u00a0 while", "word_idx": 23143, "sentence_idx": 382, "label": "unlabeled"}, {"type": "text", "expr": "while", "word_idx": 23160, "sentence_idx": 383, "label": "unlabeled"}, {"type": "text", "expr": "8: \u00a0\u00a0select actions for unobserved actions with the largest weights in  $\\Gamma$", "word_idx": 23165, "sentence_idx": 384, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma$$", "word_idx": 23245, "sentence_idx": 385, "label": "unlabeled"}, {"type": "text", "expr": "9: \u00a0\u00a0 return  \u00a0 $\\tilde{p}$", "word_idx": 23251, "sentence_idx": 386, "label": "unlabeled"}, {"type": "text", "expr": "return", "word_idx": 23278, "sentence_idx": 387, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{p}$$", "word_idx": 23284, "sentence_idx": 388, "label": "unlabeled"}, {"type": "text", "expr": "Our  DUP  algorithm framework belongs to a family of policy gradient\nalgorithms, which have been successfully applied to complex problems,\ne", "word_idx": 23293, "sentence_idx": 389, "label": "unlabeled"}, {"type": "text", "expr": ", robot control  (21) , natural language processing\n (4) ", "word_idx": 23433, "sentence_idx": 390, "label": "unlabeled"}, {"type": "text", "expr": " Our formulation is unique in how it\nrecognizes plans, in comparison to the existing methods in the planning\ncommunity", "word_idx": 23490, "sentence_idx": 391, "label": "unlabeled"}, {"type": "text", "expr": "Note that our current study shows that even direct application of word\nvector learning methods provide competitive performance for plan\ncompletion tasks", "word_idx": 23608, "sentence_idx": 392, "label": "unlabeled"}, {"type": "text", "expr": " We believe we can further improve the performance by\nusing the planning specific structural information in the EM phase", "word_idx": 23760, "sentence_idx": 393, "label": "unlabeled"}, {"type": "text", "expr": " In\nother words, if we are provided with additional planning structural\ninformation as input, we can exploit the structural information to\nfilter candidate plans to be recognized in the EM procedure", "word_idx": 23880, "sentence_idx": 394, "label": "unlabeled"}, {"type": "text", "expr": "Note that our current study shows that even direct application of word\nvector learning methods provide competitive performance for plan\ncompletion tasks", "word_idx": 24078, "sentence_idx": 395, "label": "unlabeled"}, {"type": "text", "expr": " We believe we can further improve the performance by\nusing the planning specific structural information in the EM phase", "word_idx": 24230, "sentence_idx": 396, "label": "unlabeled"}, {"type": "text", "expr": " In\nother words, if we are provided with additional planning structural\ninformation as input, we can exploit the structural information to\nfilter candidate plans to be recognized in the EM procedure", "word_idx": 24350, "sentence_idx": 397, "label": "unlabeled"}, {"type": "text", "expr": "5  Our  RNNPlanner  approach", "word_idx": 24548, "sentence_idx": 398, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 24576, "sentence_idx": 399, "label": "unlabeled"}, {"type": "text", "expr": "Instead of using the EM-style framework, in this section we present another approach which is based on Recurrent Neural Networks (RNNs), specifically Long Short-term Memory networks (LSTMs), with the distributed representations of actions introduced in Section  3 ", "word_idx": 24586, "sentence_idx": 400, "label": "unlabeled"}, {"type": "text", "expr": " LSTM is a specific kind of RNN that works by leveraging long-short term contexts", "word_idx": 24850, "sentence_idx": 401, "label": "unlabeled"}, {"type": "text", "expr": " As we model our plan recognition problem as an action-sequence generation problem, our aim of exploring RNN-LSTM architecture is to leverage longer-horizon of action contexts to help improve the accuracy of generating new actions based on previously observed or generated actions", "word_idx": 24931, "sentence_idx": 402, "label": "unlabeled"}, {"type": "text", "expr": " We will first introduce the RNN-LSTM architecture, and then introduce our  RNNPlanner  model", "word_idx": 25211, "sentence_idx": 403, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 25304, "sentence_idx": 404, "label": "unlabeled"}, {"type": "text", "expr": "1  The RNN Model", "word_idx": 25314, "sentence_idx": 405, "label": "unlabeled"}, {"type": "text", "expr": "Specifically, the RNN architecture can be defined in the following way", "word_idx": 25330, "sentence_idx": 406, "label": "unlabeled"}, {"type": "text", "expr": " Given an input action  $x_{t}$  at the step  $t$ , RNN accepts it with weighted connections to  $N$  hidden layers that are stacked together", "word_idx": 25400, "sentence_idx": 407, "label": "unlabeled"}, {"type": "text", "expr": " And from the hidden layer stack, there is a connection to the output layer  $y_{t}$ , as well as a cyclic weighted connection going into the hidden layer stack", "word_idx": 25541, "sentence_idx": 408, "label": "unlabeled"}, {"type": "text", "expr": " And if we unroll this RNN cell along  $T$  steps, it could accept an action input sequence  $\\mathbf{x}=(x_{1},x_{T})$ , and compute a sequence of hidden states  $\\mathbf{h}=(h_{1}^{N},h_{2}^{N},,h_{T}^{N})$ ", "word_idx": 25701, "sentence_idx": 409, "label": "unlabeled"}, {"type": "text", "expr": " For each of these hidden states  $h_{t}$  ( $1\\leq t\\leq T$ ), it contributes to predicting the next step output  $y_{t+1}$ , and thus RNN computes an output vector sequence  $\\mathbf{y}=(y_{1},,y_{T})$ , by concatenating outputs from all steps together", "word_idx": 25910, "sentence_idx": 410, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t}$$", "word_idx": 26164, "sentence_idx": 411, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}$$", "word_idx": 26169, "sentence_idx": 412, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}=(x_{1}...,x_{T})$$", "word_idx": 26174, "sentence_idx": 413, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{h}=(h_{1}^{N},h_{2}^{N},...,h_{T}^{N})$$", "word_idx": 26201, "sentence_idx": 414, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{t}$$", "word_idx": 26247, "sentence_idx": 415, "label": "unlabeled"}, {"type": "math", "expr": "$$1\\leq t\\leq T$$", "word_idx": 26252, "sentence_idx": 416, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t+1}$$", "word_idx": 26265, "sentence_idx": 417, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{y}=(y_{1},...,y_{T})$$", "word_idx": 26272, "sentence_idx": 418, "label": "unlabeled"}, {"type": "text", "expr": "Given an input sequence  $\\mathbf{x}$ , an RNN model could predict an output sequence  $\\mathbf{y}$ , in which output  $y_{t}$  at each step depends on the input  $x_{t}$  at that step, and the hidden state at the previous step", "word_idx": 26300, "sentence_idx": 419, "label": "unlabeled"}, {"type": "text", "expr": " The RNN could also be utilized to directly generate, in principle, infinitely long future outputs (actions), given a single input  $x_{t}$ ", "word_idx": 26527, "sentence_idx": 420, "label": "unlabeled"}, {"type": "text", "expr": " The sequence of future actions could be generated by directly feeding the output  $y_{t}$  at a step  $t$ , to the input  $x_{t+1}$  at the next step  $t+1$ ", "word_idx": 26667, "sentence_idx": 421, "label": "unlabeled"}, {"type": "text", "expr": " This way, RNN \u201cassumes\u201d what it predicts that would happen at next step is reliable ( $y_{t}=x_{t+1}$ )", "word_idx": 26825, "sentence_idx": 422, "label": "unlabeled"}, {"type": "text", "expr": " As for training the RNN as a sequence generation model, we could utilize  $y_{t}$  to parameterize a predictive distribution  $P(x_{t+1}|y_{t})$  over all of the possible next inputs  $x_{t+1}$ , and thus we could minimize the loss:", "word_idx": 26929, "sentence_idx": 423, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}$$", "word_idx": 27162, "sentence_idx": 424, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{y}$$", "word_idx": 27172, "sentence_idx": 425, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}$$", "word_idx": 27182, "sentence_idx": 426, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t}$$", "word_idx": 27187, "sentence_idx": 427, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t}$$", "word_idx": 27192, "sentence_idx": 428, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}$$", "word_idx": 27197, "sentence_idx": 429, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t+1}$$", "word_idx": 27202, "sentence_idx": 430, "label": "unlabeled"}, {"type": "math", "expr": "$$t+1$$", "word_idx": 27209, "sentence_idx": 431, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}=x_{t+1}$$", "word_idx": 27212, "sentence_idx": 432, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}$$", "word_idx": 27225, "sentence_idx": 433, "label": "unlabeled"}, {"type": "math", "expr": "$$P(x_{t+1}|y_{t})$$", "word_idx": 27230, "sentence_idx": 434, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t+1}$$", "word_idx": 27246, "sentence_idx": 435, "label": "unlabeled"}, {"type": "text", "expr": "$\\mathcal{L}(\\mathbf{x})=-\\sum_{t=1}^{T}logP(x_{t+1}|y_{t})$", "word_idx": 27253, "sentence_idx": 436, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}(\\mathbf{x})=-\\sum_{t=1}^{T}logP(x_{t+1}|y_{t}).$$", "word_idx": 27313, "sentence_idx": 437, "label": "unlabeled"}, {"type": "text", "expr": "where  $T$  is the number of steps of an observed plan trace,  $x_{t+1}$  is the observed action at step  $t+1$ , and  $y_{t}$  is the output at step  $t$  as well as the prediction of what would happen at step  $t+1$ ", "word_idx": 27372, "sentence_idx": 438, "label": "unlabeled"}, {"type": "text", "expr": " To estimate  $y_{t}$  based on  $x_{1},\\ldots,x_{t}$ , we exploit the Long Short-term Memory (LSTM) model, which has been demonstrated effective on generating sequences  , to leverage long term information prior to  $x_{t}$  and predict  $y_{t}$  based on current input  $x_{t}$ ", "word_idx": 27590, "sentence_idx": 439, "label": "unlabeled"}, {"type": "text", "expr": " We can thus rewrite Equation ( 8 ) as:", "word_idx": 27870, "sentence_idx": 440, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t+1}$$", "word_idx": 27909, "sentence_idx": 441, "label": "unlabeled"}, {"type": "math", "expr": "$$t+1$$", "word_idx": 27916, "sentence_idx": 442, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}$$", "word_idx": 27919, "sentence_idx": 443, "label": "unlabeled"}, {"type": "math", "expr": "$$t+1$$", "word_idx": 27924, "sentence_idx": 444, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}$$", "word_idx": 27927, "sentence_idx": 445, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{1},\\ldots,x_{t}$$", "word_idx": 27932, "sentence_idx": 446, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t}$$", "word_idx": 27950, "sentence_idx": 447, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}$$", "word_idx": 27955, "sentence_idx": 448, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t}$$", "word_idx": 27960, "sentence_idx": 449, "label": "unlabeled"}, {"type": "text", "expr": "$\\mathcal{L}(\\mathbf{x})=-\\sum_{t=1}^{T}logP(x_{t+1}|y_{t})\\mathrm{LSTM}(y_{t}|%\nx_{1:t}),$", "word_idx": 27965, "sentence_idx": 450, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}(\\mathbf{x})=-\\sum_{t=1}^{T}logP(x_{t+1}|y_{t})\\mathrm{LSTM}(y_{t}|%\nx_{1:t}),$$", "word_idx": 28056, "sentence_idx": 451, "label": "unlabeled"}, {"type": "text", "expr": "where  $\\mathrm{LSTM}(y_{t}|x_{1:t})$  indicates the LSTM model estimates  $y_{t}$  based on current input  $x_{t}$  and memories of previous input prior to  $x_{t}$ ", "word_idx": 28145, "sentence_idx": 452, "label": "unlabeled"}, {"type": "text", "expr": " The framework of LSTM   is shown in Figure  2 , where  $x_{t}$  is the  $t$ th input,  $h_{t}$  is the  $t$ th hidden state", "word_idx": 28311, "sentence_idx": 453, "label": "unlabeled"}, {"type": "text", "expr": "  $i_{t}$ ,  $f_{t}$ ,  $o_{t}$  and  $c_{t}$  are the  $t$ th  input gate ,  forget gate ,  output gate ,  cell  and  cell input  activation vectors, respectively, whose dimensions are the same as the hidden vector  $h_{t}$ ", "word_idx": 28435, "sentence_idx": 454, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{LSTM}(y_{t}|x_{1:t})$$", "word_idx": 28660, "sentence_idx": 455, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}$$", "word_idx": 28688, "sentence_idx": 456, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t}$$", "word_idx": 28693, "sentence_idx": 457, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t}$$", "word_idx": 28698, "sentence_idx": 458, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t}$$", "word_idx": 28703, "sentence_idx": 459, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{t}$$", "word_idx": 28708, "sentence_idx": 460, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{t}$$", "word_idx": 28713, "sentence_idx": 461, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{t}$$", "word_idx": 28718, "sentence_idx": 462, "label": "unlabeled"}, {"type": "math", "expr": "$$o_{t}$$", "word_idx": 28723, "sentence_idx": 463, "label": "unlabeled"}, {"type": "math", "expr": "$$c_{t}$$", "word_idx": 28728, "sentence_idx": 464, "label": "unlabeled"}, {"type": "text", "expr": "input gate", "word_idx": 28733, "sentence_idx": 465, "label": "unlabeled"}, {"type": "text", "expr": "forget gate", "word_idx": 28743, "sentence_idx": 466, "label": "unlabeled"}, {"type": "text", "expr": "output gate", "word_idx": 28754, "sentence_idx": 467, "label": "unlabeled"}, {"type": "text", "expr": "cell input", "word_idx": 28765, "sentence_idx": 468, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{t}$$", "word_idx": 28775, "sentence_idx": 469, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:  Long Short-term Memory (LSTM) cell", "word_idx": 28780, "sentence_idx": 470, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:", "word_idx": 28825, "sentence_idx": 471, "label": "unlabeled"}, {"type": "text", "expr": "LSTM is implemented by the following functions:", "word_idx": 28834, "sentence_idx": 472, "label": "unlabeled"}, {"type": "text", "expr": "LSTM is implemented by the following functions:", "word_idx": 28881, "sentence_idx": 473, "label": "unlabeled"}, {"type": "text", "expr": "= (10)", "word_idx": 28928, "sentence_idx": 474, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle i_{t}$", "word_idx": 28934, "sentence_idx": 475, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle i_{t}$$", "word_idx": 28955, "sentence_idx": 476, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=$", "word_idx": 28974, "sentence_idx": 477, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=$$", "word_idx": 28990, "sentence_idx": 478, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\sigma(W_{xi}x_{t}+W_{hi}h_{t-1}+W_{ci}c_{t-1}+b_{i})$", "word_idx": 29004, "sentence_idx": 479, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\sigma(W_{xi}x_{t}+W_{hi}h_{t-1}+W_{ci}c_{t-1}+b_{i})$$", "word_idx": 29072, "sentence_idx": 480, "label": "unlabeled"}, {"type": "text", "expr": "= (11)", "word_idx": 29138, "sentence_idx": 481, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle f_{t}$", "word_idx": 29144, "sentence_idx": 482, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle f_{t}$$", "word_idx": 29165, "sentence_idx": 483, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=$", "word_idx": 29184, "sentence_idx": 484, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=$$", "word_idx": 29200, "sentence_idx": 485, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\sigma(W_{xf}x_{t}+W_{hf}h_{t-1}+W_{cf}c_{t-1}+b_{f})$", "word_idx": 29214, "sentence_idx": 486, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\sigma(W_{xf}x_{t}+W_{hf}h_{t-1}+W_{cf}c_{t-1}+b_{f})$$", "word_idx": 29282, "sentence_idx": 487, "label": "unlabeled"}, {"type": "text", "expr": "= (12)", "word_idx": 29348, "sentence_idx": 488, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle c_{t}$", "word_idx": 29354, "sentence_idx": 489, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle c_{t}$$", "word_idx": 29375, "sentence_idx": 490, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=$", "word_idx": 29394, "sentence_idx": 491, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=$$", "word_idx": 29410, "sentence_idx": 492, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle f_{t}\\circ c_{t-1}+i_{t}\\circ\\tanh(W_{xc}x_{t}+W_{hc}h_{t-1}+b_{%\nc})$", "word_idx": 29424, "sentence_idx": 493, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle f_{t}\\circ c_{t-1}+i_{t}\\circ\\tanh(W_{xc}x_{t}+W_{hc}h_{t-1}+b_{%\nc})$$", "word_idx": 29509, "sentence_idx": 494, "label": "unlabeled"}, {"type": "text", "expr": "= (13)", "word_idx": 29592, "sentence_idx": 495, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle o_{t}$", "word_idx": 29598, "sentence_idx": 496, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle o_{t}$$", "word_idx": 29619, "sentence_idx": 497, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=$", "word_idx": 29638, "sentence_idx": 498, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=$$", "word_idx": 29654, "sentence_idx": 499, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\sigma(W_{xo}x_{t}+W_{ho}h_{t-1}+W_{co}c_{t}+b_{o})$", "word_idx": 29668, "sentence_idx": 500, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\sigma(W_{xo}x_{t}+W_{ho}h_{t-1}+W_{co}c_{t}+b_{o})$$", "word_idx": 29734, "sentence_idx": 501, "label": "unlabeled"}, {"type": "text", "expr": "= (14)", "word_idx": 29798, "sentence_idx": 502, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle h_{t}$", "word_idx": 29804, "sentence_idx": 503, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle h_{t}$$", "word_idx": 29825, "sentence_idx": 504, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=$", "word_idx": 29844, "sentence_idx": 505, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=$$", "word_idx": 29860, "sentence_idx": 506, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle o_{t}\\circ\\tanh(c_{t})$", "word_idx": 29874, "sentence_idx": 507, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle o_{t}\\circ\\tanh(c_{t})$$", "word_idx": 29912, "sentence_idx": 508, "label": "unlabeled"}, {"type": "text", "expr": "where  $\\circ$  indicates the Hadamard product,  $\\sigma$  is the logistic sigmoid function,  $W_{xi}$  is an input-input gate matrix,  $W_{hi}$  is a hidden-input gate matrix,  $W_{ci}$  is a cell-input gate matrix,  $W_{xf}$  is an input-forget gate matrix,  $W_{hf}$  is a hidden-forget gate matrix,  $W_{cf}$  is a cell-forget gate matrix,  $W_{xc}$  is an input-cell gate matrix,  $W_{hc}$  is a hidden-cell gate matrix,  $W_{xo}$  is an input-output gate matrix,  $W_{ho}$  is a hidden-output gate matrix,  $W_{co}$  is a cell-output gate matrix", "word_idx": 29948, "sentence_idx": 509, "label": "unlabeled"}, {"type": "text", "expr": "  $b_{i}$ ,  $b_{f}$ ,  $b_{c}$ , and  $b_{o}$  are bias terms", "word_idx": 30499, "sentence_idx": 510, "label": "unlabeled"}, {"type": "text", "expr": " Note that the matrices from cell to gate vectors (i", "word_idx": 30561, "sentence_idx": 511, "label": "unlabeled"}, {"type": "text", "expr": ",  $W_{ci}$ ,  $W_{cf}$  and  $W_{co}$ ) are diagonal, such that each element  $e$  in each gate vector only receives input of element  $e$  of the cell vector", "word_idx": 30613, "sentence_idx": 512, "label": "unlabeled"}, {"type": "text", "expr": " The major innovation of LSTM is its memory cell  $c_{t}$  which essentially acts as an accumulator of the state information", "word_idx": 30772, "sentence_idx": 513, "label": "unlabeled"}, {"type": "text", "expr": "  $c_{t}$  is accessed, written and cleared by self-parameterized controlling gates, i", "word_idx": 30896, "sentence_idx": 514, "label": "unlabeled"}, {"type": "text", "expr": ",  input, forget, output  gates", "word_idx": 30982, "sentence_idx": 515, "label": "unlabeled"}, {"type": "text", "expr": " Each time a new input  $x_{t}$  comes, its information is accumulated to the memory cell if the input gate  $i_{t}$  is activated", "word_idx": 31013, "sentence_idx": 516, "label": "unlabeled"}, {"type": "text", "expr": " The past cell status  $c_{t-1}$  could be forgotten in this process if the forget gate  $f_{t}$  is activated", "word_idx": 31143, "sentence_idx": 517, "label": "unlabeled"}, {"type": "text", "expr": " Whether the latest cell output  $c_{t}$  is propagated to the final state  $h_{t}$  is further controlled by the output gate  $o_{t}$ ", "word_idx": 31253, "sentence_idx": 518, "label": "unlabeled"}, {"type": "text", "expr": " The benefit of using the memory cell and gates to control information flow is the gradient is trapped in the cell and prevented from vanishing too quickly", "word_idx": 31388, "sentence_idx": 519, "label": "unlabeled"}, {"type": "math", "expr": "$$\\circ$$", "word_idx": 31543, "sentence_idx": 520, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma$$", "word_idx": 31548, "sentence_idx": 521, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{xi}$$", "word_idx": 31554, "sentence_idx": 522, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{hi}$$", "word_idx": 31560, "sentence_idx": 523, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{ci}$$", "word_idx": 31566, "sentence_idx": 524, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{xf}$$", "word_idx": 31572, "sentence_idx": 525, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{hf}$$", "word_idx": 31578, "sentence_idx": 526, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{cf}$$", "word_idx": 31584, "sentence_idx": 527, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{xc}$$", "word_idx": 31590, "sentence_idx": 528, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{hc}$$", "word_idx": 31596, "sentence_idx": 529, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{xo}$$", "word_idx": 31602, "sentence_idx": 530, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{ho}$$", "word_idx": 31608, "sentence_idx": 531, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{co}$$", "word_idx": 31614, "sentence_idx": 532, "label": "unlabeled"}, {"type": "math", "expr": "$$b_{i}$$", "word_idx": 31620, "sentence_idx": 533, "label": "unlabeled"}, {"type": "math", "expr": "$$b_{f}$$", "word_idx": 31625, "sentence_idx": 534, "label": "unlabeled"}, {"type": "math", "expr": "$$b_{c}$$", "word_idx": 31630, "sentence_idx": 535, "label": "unlabeled"}, {"type": "math", "expr": "$$b_{o}$$", "word_idx": 31635, "sentence_idx": 536, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{ci}$$", "word_idx": 31640, "sentence_idx": 537, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{cf}$$", "word_idx": 31646, "sentence_idx": 538, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{co}$$", "word_idx": 31652, "sentence_idx": 539, "label": "unlabeled"}, {"type": "math", "expr": "$$c_{t}$$", "word_idx": 31658, "sentence_idx": 540, "label": "unlabeled"}, {"type": "math", "expr": "$$c_{t}$$", "word_idx": 31663, "sentence_idx": 541, "label": "unlabeled"}, {"type": "text", "expr": "input, forget, output", "word_idx": 31668, "sentence_idx": 542, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t}$$", "word_idx": 31689, "sentence_idx": 543, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{t}$$", "word_idx": 31694, "sentence_idx": 544, "label": "unlabeled"}, {"type": "math", "expr": "$$c_{t-1}$$", "word_idx": 31699, "sentence_idx": 545, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{t}$$", "word_idx": 31706, "sentence_idx": 546, "label": "unlabeled"}, {"type": "math", "expr": "$$c_{t}$$", "word_idx": 31711, "sentence_idx": 547, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{t}$$", "word_idx": 31716, "sentence_idx": 548, "label": "unlabeled"}, {"type": "math", "expr": "$$o_{t}$$", "word_idx": 31721, "sentence_idx": 549, "label": "unlabeled"}, {"type": "text", "expr": "2  Discovering Underlying Plans with the RNN Model", "word_idx": 31726, "sentence_idx": 550, "label": "unlabeled"}, {"type": "text", "expr": "With the distributed representations of actions addressed in Section 3, we can view each plan in the plan library as a sequence of actions, and the plan library as a set of action sequences which can be utilized to train the RNN model", "word_idx": 31776, "sentence_idx": 551, "label": "unlabeled"}, {"type": "text", "expr": " The framework of RNN with sequences of actions can be seen from Figure  3 ", "word_idx": 32010, "sentence_idx": 552, "label": "unlabeled"}, {"type": "text", "expr": " The bottom row in Figure  3  is an example action sequence \u201cpick-up-B, stack-B-A, pick-up-C, stack-C-B, pick-up-D, \u2026\u201d, which corresponds to an input sequence  $\\mathbf{x}$ ", "word_idx": 32085, "sentence_idx": 553, "label": "unlabeled"}, {"type": "text", "expr": " Once an action among the bottom row is fed into the RNN, that action is assigned with an index, and an embedding layer is trained to find a vector representation based on that index", "word_idx": 32258, "sentence_idx": 554, "label": "unlabeled"}, {"type": "text", "expr": " The action vector from the embedding layer is the feature that can be used by the LSTM cell", "word_idx": 32440, "sentence_idx": 555, "label": "unlabeled"}, {"type": "text", "expr": " How the LSTM cell works has been explained in Equation  14 ", "word_idx": 32532, "sentence_idx": 556, "label": "unlabeled"}, {"type": "text", "expr": " Similar to a classic RNN cell, the LSTM cell feeds its output to both itself as a hidden state, and the softmax layer to obtain a probability distribution over all actions in the action vocabulary  $\\mathcal{A}$ ", "word_idx": 32592, "sentence_idx": 557, "label": "unlabeled"}, {"type": "text", "expr": " From the perspective of the LSTM cell at the next step, it receives a hidden state from the previous step  $h_{t-1}$ , an action vector at the current step  $x_{t}$ ", "word_idx": 32805, "sentence_idx": 558, "label": "unlabeled"}, {"type": "text", "expr": " To obtain the index of most possible action, our model samples over the action distribution output from softmax layer", "word_idx": 32971, "sentence_idx": 559, "label": "unlabeled"}, {"type": "text", "expr": " That retrieved index could be mapped to an action in the vocabulary  $\\bar{\\mathcal{A}}$ ", "word_idx": 33089, "sentence_idx": 560, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}$$", "word_idx": 33179, "sentence_idx": 561, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{A}$$", "word_idx": 33189, "sentence_idx": 562, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{t-1}$$", "word_idx": 33200, "sentence_idx": 563, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t}$$", "word_idx": 33207, "sentence_idx": 564, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{\\mathcal{A}}$$", "word_idx": 33212, "sentence_idx": 565, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:  The framework of our  RNNPlanner  approach (with one hidden layer)", "word_idx": 33229, "sentence_idx": 566, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:", "word_idx": 33306, "sentence_idx": 567, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 33315, "sentence_idx": 568, "label": "unlabeled"}, {"type": "text", "expr": "The top row in Figure  3  is the output sequence, which is denoted by \u201cOUT1, OUT2, OUT3, OUT4, OUT5, \u2026\u201d, which corresponds to the estimated sequence \u201c $y_{1}$ ,  $y_{2}$ ,  $y_{3}$ ,  $y_{4}$ ,  $y_{5}$ ,  $\\ldots$ \u201d in Equation ( 9 )", "word_idx": 33325, "sentence_idx": 569, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{1}$$", "word_idx": 33559, "sentence_idx": 570, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{2}$$", "word_idx": 33564, "sentence_idx": 571, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{3}$$", "word_idx": 33569, "sentence_idx": 572, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{4}$$", "word_idx": 33574, "sentence_idx": 573, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{5}$$", "word_idx": 33579, "sentence_idx": 574, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ldots$$", "word_idx": 33584, "sentence_idx": 575, "label": "unlabeled"}, {"type": "text", "expr": "Note that we exploit the dotted arrow to indicate two folds of meanings in Figure  3 ", "word_idx": 33590, "sentence_idx": 576, "label": "unlabeled"}, {"type": "text", "expr": " When training the RNN model, the one pointed by the head of the dotted row (the embedding of input) is used to compute the cross entropy error with the output at tail (output of LSTM cell at the previous step), and next-step observation as the input at the head, to train the model", "word_idx": 33675, "sentence_idx": 577, "label": "unlabeled"}, {"type": "text", "expr": " When using the trained RNN model to discover unknown actions, the model \u201cimagines\u201d what it predicts is the real next input, and takes it to continue its prediction", "word_idx": 33957, "sentence_idx": 578, "label": "unlabeled"}, {"type": "text", "expr": " Thus the one pointed by the head is copied and identical to the one denoted by the tail", "word_idx": 34121, "sentence_idx": 579, "label": "unlabeled"}, {"type": "text", "expr": " For example, the embedding of \u201cstack-B-A\u201d is copied from the prediction vector of \u201cOUT1\u201d if the input \u201cstack-B-A\u201d was unknown", "word_idx": 34209, "sentence_idx": 580, "label": "unlabeled"}, {"type": "text", "expr": " In addition, the arrows between each of two LSTM cells shows the unrolling of a LSTM cell", "word_idx": 34335, "sentence_idx": 581, "label": "unlabeled"}, {"type": "text", "expr": " The horizontal dashed line suggests that we obtain the action output at each step, by sampling from probability distribution, provided by the softmax layer", "word_idx": 34425, "sentence_idx": 582, "label": "unlabeled"}, {"type": "text", "expr": "With the trained RNN model, we can discover underlying actions by simply exploiting the RNN model to generate unknown actions based on observed or already discovered actions", "word_idx": 34581, "sentence_idx": 583, "label": "unlabeled"}, {"type": "text", "expr": " For example, given the observation:", "word_idx": 34754, "sentence_idx": 584, "label": "unlabeled"}, {"type": "text", "expr": "With the trained RNN model, we can discover underlying actions by simply exploiting the RNN model to generate unknown actions based on observed or already discovered actions", "word_idx": 34790, "sentence_idx": 585, "label": "unlabeled"}, {"type": "text", "expr": " For example, given the observation:", "word_idx": 34963, "sentence_idx": 586, "label": "unlabeled"}, {"type": "text", "expr": "pick-up-B  $\\phi$  unstack-D-C put-down-D  $\\phi$  stack-C-B  $\\phi$   $\\phi$", "word_idx": 34999, "sentence_idx": 587, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 35076, "sentence_idx": 588, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 35080, "sentence_idx": 589, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 35084, "sentence_idx": 590, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 35088, "sentence_idx": 591, "label": "unlabeled"}, {"type": "text", "expr": "we can generate the first  $\\phi$  based on  pick-up-B , the second  $\\phi$  based on actions from  pick-up-B  to  put-down-D , the third  $\\phi$  based on actions from  pick-up-B  to  stack-C-B , and the last  $\\phi$  based on all previously actions (including generated actions at where there was a  $\\phi$ )", "word_idx": 35092, "sentence_idx": 592, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 35402, "sentence_idx": 593, "label": "unlabeled"}, {"type": "text", "expr": "pick-up-B", "word_idx": 35406, "sentence_idx": 594, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 35415, "sentence_idx": 595, "label": "unlabeled"}, {"type": "text", "expr": "pick-up-B", "word_idx": 35419, "sentence_idx": 596, "label": "unlabeled"}, {"type": "text", "expr": "put-down-D", "word_idx": 35428, "sentence_idx": 597, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 35438, "sentence_idx": 598, "label": "unlabeled"}, {"type": "text", "expr": "pick-up-B", "word_idx": 35442, "sentence_idx": 599, "label": "unlabeled"}, {"type": "text", "expr": "stack-C-B", "word_idx": 35451, "sentence_idx": 600, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 35460, "sentence_idx": 601, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 35464, "sentence_idx": 602, "label": "unlabeled"}, {"type": "text", "expr": "6  Experiments", "word_idx": 35468, "sentence_idx": 603, "label": "unlabeled"}, {"type": "text", "expr": "In this section, we evaluate our  DUP  and  RNNPlanner  algorithms in three planning domains from International Planning Competition, i", "word_idx": 35482, "sentence_idx": 604, "label": "unlabeled"}, {"type": "text", "expr": ", blocks ${}^{1}$ , depots , and driverlog ${}^{2}$ ", "word_idx": 35617, "sentence_idx": 605, "label": "unlabeled"}, {"type": "text", "expr": " To generate training and testing data, we randomly created 5000 planning problems for each domain, and solved these planning problems with a planning solver, such as FF , to produce 5000 plans", "word_idx": 35669, "sentence_idx": 606, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 35862, "sentence_idx": 607, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{1}$$", "word_idx": 35872, "sentence_idx": 608, "label": "unlabeled"}, {"type": "text", "expr": "2 http://www", "word_idx": 35878, "sentence_idx": 609, "label": "unlabeled"}, {"type": "text", "expr": "edu/afs/cs/project/jair/pub/volume20/long03a-html/JAIRIPC", "word_idx": 35890, "sentence_idx": 610, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{2}$$", "word_idx": 35947, "sentence_idx": 611, "label": "unlabeled"}, {"type": "text", "expr": "3 https://fai", "word_idx": 35953, "sentence_idx": 612, "label": "unlabeled"}, {"type": "text", "expr": "uni-saarland", "word_idx": 35966, "sentence_idx": 613, "label": "unlabeled"}, {"type": "text", "expr": "de/hoffmann/ff", "word_idx": 35978, "sentence_idx": 614, "label": "unlabeled"}, {"type": "text", "expr": "We define the accuracy of our  DUP  and  RNNPlanner  algorithms as follows", "word_idx": 35992, "sentence_idx": 615, "label": "unlabeled"}, {"type": "text", "expr": " For each unobserved action  $\\bar{a}_{x}$ ,  DUP  and  RNNPlanner  suggest a set of possible actions  $S_{x}$  which have the highest value of  $\\Gamma_{\\bar{a}_{x},x}$  for all  $\\bar{a}_{x}\\in\\bar{\\mathcal{A}}$ ", "word_idx": 36066, "sentence_idx": 616, "label": "unlabeled"}, {"type": "text", "expr": " If  $S_{x}$  covers the  truth  action  $a_{truth}$ , i", "word_idx": 36280, "sentence_idx": 617, "label": "unlabeled"}, {"type": "text", "expr": ",  $a_{truth}\\in S_{x}$ , we increase the number of correct suggestions by 1", "word_idx": 36336, "sentence_idx": 618, "label": "unlabeled"}, {"type": "text", "expr": " We thus define the accuracy  $acc$  as shown below:", "word_idx": 36412, "sentence_idx": 619, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 36464, "sentence_idx": 620, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{a}_{x}$$", "word_idx": 36474, "sentence_idx": 621, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 36485, "sentence_idx": 622, "label": "unlabeled"}, {"type": "math", "expr": "$$S_{x}$$", "word_idx": 36495, "sentence_idx": 623, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{\\bar{a}_{x},x}$$", "word_idx": 36500, "sentence_idx": 624, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{a}_{x}\\in\\bar{\\mathcal{A}}$$", "word_idx": 36522, "sentence_idx": 625, "label": "unlabeled"}, {"type": "math", "expr": "$$S_{x}$$", "word_idx": 36553, "sentence_idx": 626, "label": "unlabeled"}, {"type": "text", "expr": "truth", "word_idx": 36558, "sentence_idx": 627, "label": "unlabeled"}, {"type": "math", "expr": "$$a_{truth}$$", "word_idx": 36563, "sentence_idx": 628, "label": "unlabeled"}, {"type": "math", "expr": "$$a_{truth}\\in S_{x}$$", "word_idx": 36572, "sentence_idx": 629, "label": "unlabeled"}, {"type": "math", "expr": "$$acc$$", "word_idx": 36590, "sentence_idx": 630, "label": "unlabeled"}, {"type": "text", "expr": "$acc=\\frac{1}{T}\\sum_{i=1}^{T}\\frac{\\#\\langle correct\\textrm{-}suggestions%\n\\rangle_{i}}{K_{i}},$", "word_idx": 36593, "sentence_idx": 631, "label": "unlabeled"}, {"type": "math", "expr": "$$acc=\\frac{1}{T}\\sum_{i=1}^{T}\\frac{\\#\\langle correct\\textrm{-}suggestions%\n\\rangle_{i}}{K_{i}},$$", "word_idx": 36690, "sentence_idx": 632, "label": "unlabeled"}, {"type": "text", "expr": "where  $T$  is the size of testing set,  $\\#\\langle correct\\textrm{-}suggestions\\rangle_{i}$  is the number of correct suggestions for the  $i$ th testing plan,  $K_{i}$  is the number of unobserved actions in the  $i$ th testing plan", "word_idx": 36785, "sentence_idx": 633, "label": "unlabeled"}, {"type": "text", "expr": " We can see that the accuracy  $acc$  may be influenced by  $S_{x}$ ", "word_idx": 37019, "sentence_idx": 634, "label": "unlabeled"}, {"type": "text", "expr": " We will test different size of  $S_{x}$  in the experiment", "word_idx": 37087, "sentence_idx": 635, "label": "unlabeled"}, {"type": "math", "expr": "$$\\#\\langle correct\\textrm{-}suggestions\\rangle_{i}$$", "word_idx": 37146, "sentence_idx": 636, "label": "unlabeled"}, {"type": "math", "expr": "$$K_{i}$$", "word_idx": 37195, "sentence_idx": 637, "label": "unlabeled"}, {"type": "math", "expr": "$$acc$$", "word_idx": 37200, "sentence_idx": 638, "label": "unlabeled"}, {"type": "math", "expr": "$$S_{x}$$", "word_idx": 37203, "sentence_idx": 639, "label": "unlabeled"}, {"type": "math", "expr": "$$S_{x}$$", "word_idx": 37208, "sentence_idx": 640, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:  Features of datasets", "word_idx": 37213, "sentence_idx": 641, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:", "word_idx": 37243, "sentence_idx": 642, "label": "unlabeled"}, {"type": "text", "expr": "domain #plan #word #vocabulary", "word_idx": 37251, "sentence_idx": 643, "label": "unlabeled"}, {"type": "text", "expr": "blocks 5000 292250 1250", "word_idx": 37281, "sentence_idx": 644, "label": "unlabeled"}, {"type": "text", "expr": "292250", "word_idx": 37304, "sentence_idx": 645, "label": "unlabeled"}, {"type": "text", "expr": "depots 5000 209711 2273", "word_idx": 37310, "sentence_idx": 646, "label": "unlabeled"}, {"type": "text", "expr": "209711", "word_idx": 37333, "sentence_idx": 647, "label": "unlabeled"}, {"type": "text", "expr": "driverlog 5000 179621 1441", "word_idx": 37339, "sentence_idx": 648, "label": "unlabeled"}, {"type": "text", "expr": "179621", "word_idx": 37365, "sentence_idx": 649, "label": "unlabeled"}, {"type": "text", "expr": "State-of-the-art plan recognition approaches with plan libraries as input aim at finding a plan from plan libraries to best explain the observed actions  (10) , which we denote by  MatchPlan ", "word_idx": 37371, "sentence_idx": 650, "label": "unlabeled"}, {"type": "text", "expr": " We develop a  MatchPlan  system based on the idea of  (10)  and compare our  DUP  algorithm to  MatchPlan  with respect to different percentages of unobserved actions  $\\xi$  and different sizes of suggestion or recommendation set  $S_{x}$ ", "word_idx": 37562, "sentence_idx": 651, "label": "unlabeled"}, {"type": "text", "expr": " Another baseline is action-models based plan recognition approach  (24)  (denoted by  PRP , short for Plan Recognition as Planning)", "word_idx": 37803, "sentence_idx": 652, "label": "unlabeled"}, {"type": "text", "expr": " Since we do not have action models as input in our  DUP  algorithm, we exploited the action model learning system  ARMS   (30)  to learn action models from the plan library and feed the action models to the  PRP  approach", "word_idx": 37935, "sentence_idx": 653, "label": "unlabeled"}, {"type": "text", "expr": " We call this hybrid plan recognition approach  ARMS+PRP ", "word_idx": 38157, "sentence_idx": 654, "label": "unlabeled"}, {"type": "text", "expr": " To learn action models,  ARMS  requires state information of plans as input", "word_idx": 38214, "sentence_idx": 655, "label": "unlabeled"}, {"type": "text", "expr": " We thus added extra information, i", "word_idx": 38290, "sentence_idx": 656, "label": "unlabeled"}, {"type": "text", "expr": ", initial state and goal of each plan in the plan library, to  ARMS+PRP ", "word_idx": 38325, "sentence_idx": 657, "label": "unlabeled"}, {"type": "text", "expr": " In addition,  PRP  requires as input a set of candidate goals  $\\mathcal{G}$  for each plan to be recognized in the testing set, which was also generated and fed to  PRP  when testing", "word_idx": 38397, "sentence_idx": 658, "label": "unlabeled"}, {"type": "text", "expr": " In summary, the hybrid plan recognition approach  ARMS+PRP  has more input information, i", "word_idx": 38581, "sentence_idx": 659, "label": "unlabeled"}, {"type": "text", "expr": ", initial states and goals in plan library and candidate goals  $\\mathcal{G}$  for each testing example, than our  DUP  approach", "word_idx": 38671, "sentence_idx": 660, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 38799, "sentence_idx": 661, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 38808, "sentence_idx": 662, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 38817, "sentence_idx": 663, "label": "unlabeled"}, {"type": "math", "expr": "$$\\xi$$", "word_idx": 38826, "sentence_idx": 664, "label": "unlabeled"}, {"type": "math", "expr": "$$S_{x}$$", "word_idx": 38829, "sentence_idx": 665, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 38834, "sentence_idx": 666, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 38842, "sentence_idx": 667, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{G}$$", "word_idx": 38850, "sentence_idx": 668, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 38861, "sentence_idx": 669, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{G}$$", "word_idx": 38869, "sentence_idx": 670, "label": "unlabeled"}, {"type": "text", "expr": "To evaluate DUP, we compared it with several baselines that we elaborated above, i", "word_idx": 38880, "sentence_idx": 671, "label": "unlabeled"}, {"type": "text", "expr": ",  MatchPlan , and  ARMS+PRP ", "word_idx": 38962, "sentence_idx": 672, "label": "unlabeled"}, {"type": "text", "expr": " We randomly divided the plans into ten folds, with 500 plans in each fold", "word_idx": 38991, "sentence_idx": 673, "label": "unlabeled"}, {"type": "text", "expr": " We ran our  DUP  algorithm ten times to calculate an average of accuracies, each time with one fold for testing and the rest for training", "word_idx": 39065, "sentence_idx": 674, "label": "unlabeled"}, {"type": "text", "expr": " In the testing data, we randomly removed actions from each testing plan (i", "word_idx": 39203, "sentence_idx": 675, "label": "unlabeled"}, {"type": "text", "expr": ",  $\\mathcal{O}$ ) with a specific percentage  $\\xi$  of the plan length", "word_idx": 39278, "sentence_idx": 676, "label": "unlabeled"}, {"type": "text", "expr": " Features of datasets are shown in Table  1 , where the second column is the number of plans generated, the third column is the total number of words (or actions) of all plans, and the last column is the size of vocabulary used in all plans", "word_idx": 39350, "sentence_idx": 677, "label": "unlabeled"}, {"type": "text", "expr": "\nTo evaluate our  RNNPlanner  algorithm, we directly compared  RNNPlanner  to  DUP ", "word_idx": 39590, "sentence_idx": 678, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 39673, "sentence_idx": 679, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 39682, "sentence_idx": 680, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 39690, "sentence_idx": 681, "label": "unlabeled"}, {"type": "math", "expr": "$$\\xi$$", "word_idx": 39701, "sentence_idx": 682, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 39704, "sentence_idx": 683, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 39714, "sentence_idx": 684, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:  Accuracies of  DUP  and  ARMS+PRP  with respect to different percentage of unobserved actions", "word_idx": 39724, "sentence_idx": 685, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:", "word_idx": 39828, "sentence_idx": 686, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 39837, "sentence_idx": 687, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a05:  Accuracies of  DUP  and  ARMS+PRP  with respect to different size of recommendations", "word_idx": 39845, "sentence_idx": 688, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a05:", "word_idx": 39940, "sentence_idx": 689, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 39949, "sentence_idx": 690, "label": "unlabeled"}, {"type": "text", "expr": "1  Comparison between  DUP  and  ARMS+PRP", "word_idx": 39957, "sentence_idx": 691, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 39998, "sentence_idx": 692, "label": "unlabeled"}, {"type": "text", "expr": "We first compare our  DUP  algorithm to  ARMS+PRP  to see the advantage of  DUP ", "word_idx": 40006, "sentence_idx": 693, "label": "unlabeled"}, {"type": "text", "expr": " We varied the percentage of unobserved actions and the size of recommended actions to see the change of accuracies, respectively", "word_idx": 40086, "sentence_idx": 694, "label": "unlabeled"}, {"type": "text", "expr": " The results are shown below", "word_idx": 40215, "sentence_idx": 695, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 40243, "sentence_idx": 696, "label": "unlabeled"}, {"type": "text", "expr": "1  Varying Percentage of Unobserved Actions", "word_idx": 40251, "sentence_idx": 697, "label": "unlabeled"}, {"type": "text", "expr": "In this experiment we would like to see the change of accuracies of both our  DUP  algorithm and  ARMS+PRP  with respect to  $\\xi$  in  $\\mathcal{O}$ ", "word_idx": 40294, "sentence_idx": 698, "label": "unlabeled"}, {"type": "text", "expr": " We set the window of training context  $c$  in Equation ( 1 ) to be three, the number of iterations in Algorithm  1  to be 1500, the size of recommendations to be ten, and the learning constant  $\\delta$  in Equation ( 7 ) to be 0", "word_idx": 40444, "sentence_idx": 699, "label": "unlabeled"}, {"type": "text", "expr": " For  ARMS+PRP , we generated 20 candidate goals for each testing example including the ground-truth goal which corresponds to the ground-truth plan to be recognized", "word_idx": 40675, "sentence_idx": 700, "label": "unlabeled"}, {"type": "text", "expr": " The results are shown in Figure  4 ", "word_idx": 40840, "sentence_idx": 701, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 40876, "sentence_idx": 702, "label": "unlabeled"}, {"type": "math", "expr": "$$\\xi$$", "word_idx": 40884, "sentence_idx": 703, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 40887, "sentence_idx": 704, "label": "unlabeled"}, {"type": "math", "expr": "$$\\delta$$", "word_idx": 40898, "sentence_idx": 705, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 40904, "sentence_idx": 706, "label": "unlabeled"}, {"type": "text", "expr": "From Figure  4 , we can see that in all three domains, the accuracy of our  DUP  algorithm is generally higher  ARMS+PRP , which verifies that our  DUP  algorithm can indeed capture relations among actions better than the model-based approach  ARMS+PRP ", "word_idx": 40912, "sentence_idx": 707, "label": "unlabeled"}, {"type": "text", "expr": " The rationale is that we explore global plan information from the plan library to learn a \u201cshallow\u201d model (distributed representations of actions) and use this model with global information to best explain the observed actions", "word_idx": 41165, "sentence_idx": 708, "label": "unlabeled"}, {"type": "text", "expr": " While  ARMS+PRP  tries to leverage global plan information from the plan library to learn action models and uses the models to recognize observed actions, it enforces itself to extract \u201cexact\u201d models represented by planning models which are often with noise", "word_idx": 41392, "sentence_idx": 709, "label": "unlabeled"}, {"type": "text", "expr": " When feeding those noisy models to  PRP , since  PRP  that uses planning techniques to recognize plans is very sensitive to noise of planning models, the recognition accuracy is lower than  DUP , even though  ARMS+PRP  has more input information (i", "word_idx": 41650, "sentence_idx": 710, "label": "unlabeled"}, {"type": "text", "expr": ", initial states and candidate goals) than our  DUP  algorithm", "word_idx": 41899, "sentence_idx": 711, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 41961, "sentence_idx": 712, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 41969, "sentence_idx": 713, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 41977, "sentence_idx": 714, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 41985, "sentence_idx": 715, "label": "unlabeled"}, {"type": "text", "expr": "Looking at the changes of accuracies with respect to the percentage of\nunobserved actions, we can see that our  DUP  algorithm performs\nfairly well even when the percentage of unobserved action reaches\n25%", "word_idx": 41993, "sentence_idx": 716, "label": "unlabeled"}, {"type": "text", "expr": " In contrast,  ARMS+PRP  is sensitive to the percentage of unobserved\nactions, i", "word_idx": 42198, "sentence_idx": 717, "label": "unlabeled"}, {"type": "text", "expr": ", the accuracy goes down when more actions are\nunobserved", "word_idx": 42278, "sentence_idx": 718, "label": "unlabeled"}, {"type": "text", "expr": " This is because the noise of planning models induces more\nuncertain information, which harms the recognition accuracy, when the\npercentage of unobserved actions becomes larger", "word_idx": 42335, "sentence_idx": 719, "label": "unlabeled"}, {"type": "text", "expr": " Comparing accuracies\nof different domains, we can see that our  DUP  algorithm functions\nbetter in the  blocks  domain than the other two domains", "word_idx": 42511, "sentence_idx": 720, "label": "unlabeled"}, {"type": "text", "expr": " This is\nbecause the ratio of #word over #vocabulary in the  blocks \ndomain is much larger than the other two domains, as shown in Table\n 1 ", "word_idx": 42657, "sentence_idx": 721, "label": "unlabeled"}, {"type": "text", "expr": " We would conjecture that increasing the ratio could\nimprove the accuracy of  DUP ", "word_idx": 42797, "sentence_idx": 722, "label": "unlabeled"}, {"type": "text", "expr": " From Figure  4  (as well as Figure  6 ), we can see that it appears that the accuracy of DUP is not\naffected by increasing percentages of unobserved actions", "word_idx": 42879, "sentence_idx": 723, "label": "unlabeled"}, {"type": "text", "expr": " The rationale is (1) the percentage of unobserved actions is low,\nless than 25%, i", "word_idx": 43036, "sentence_idx": 724, "label": "unlabeled"}, {"type": "text", "expr": ",\nthere is at most one unobserved action over four continuous actions; (2) the window size of\ncontext in DUP is set to be 3, which ensures that DUP generally has \u201dstable\u201d context information to estimate\nthe unobserved action when the percentage of unobserved actions is less than 25%, resulting in the stable accuracy\nin Figure  4  (likewise for Figure  6 )", "word_idx": 43119, "sentence_idx": 725, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 43476, "sentence_idx": 726, "label": "unlabeled"}, {"type": "text", "expr": "blocks", "word_idx": 43484, "sentence_idx": 727, "label": "unlabeled"}, {"type": "text", "expr": "blocks", "word_idx": 43490, "sentence_idx": 728, "label": "unlabeled"}, {"type": "text", "expr": "2  Varying Size of Recommendation Set", "word_idx": 43496, "sentence_idx": 729, "label": "unlabeled"}, {"type": "text", "expr": "We next evaluate the performance of our  DUP  algorithm with respect to the size of recommendation set  $S_{x}$ ", "word_idx": 43533, "sentence_idx": 730, "label": "unlabeled"}, {"type": "text", "expr": " We evaluate the influence of the recommendation set by varying the size from 1 to 10", "word_idx": 43645, "sentence_idx": 731, "label": "unlabeled"}, {"type": "text", "expr": " The size of recommendation set is much smaller than the complete set", "word_idx": 43730, "sentence_idx": 732, "label": "unlabeled"}, {"type": "text", "expr": " For example, the size of complete set in the blocks domain is 1250 (shown in Table 1)", "word_idx": 43799, "sentence_idx": 733, "label": "unlabeled"}, {"type": "text", "expr": " It is less than 1% even though we recommend 10 actions for each unobserved action", "word_idx": 43885, "sentence_idx": 734, "label": "unlabeled"}, {"type": "text", "expr": " We set the context window  $c$  used in Equation ( 1 ) to be three, the percentage of unobserved actions to be 0", "word_idx": 43967, "sentence_idx": 735, "label": "unlabeled"}, {"type": "text", "expr": "25, and the learning constant  $\\delta$  in Equation ( 7 ) to be 0", "word_idx": 44080, "sentence_idx": 736, "label": "unlabeled"}, {"type": "text", "expr": " For  ARMS+PRP , the number of candidate goals for each testing example is set to 20", "word_idx": 44146, "sentence_idx": 737, "label": "unlabeled"}, {"type": "text", "expr": "  ARMS+PRP  aims to recognize plans that are optimal with respect to the cost of actions", "word_idx": 44230, "sentence_idx": 738, "label": "unlabeled"}, {"type": "text", "expr": " We relax  ARMS+PRP  to output  $|S_{x}|$  optimal plans, some of which might be suboptimal", "word_idx": 44318, "sentence_idx": 739, "label": "unlabeled"}, {"type": "text", "expr": " The results are shown in Figure  5 ", "word_idx": 44409, "sentence_idx": 740, "label": "unlabeled"}, {"type": "math", "expr": "$$S_{x}$$", "word_idx": 44445, "sentence_idx": 741, "label": "unlabeled"}, {"type": "math", "expr": "$$\\delta$$", "word_idx": 44450, "sentence_idx": 742, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 44456, "sentence_idx": 743, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 44464, "sentence_idx": 744, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 44472, "sentence_idx": 745, "label": "unlabeled"}, {"type": "math", "expr": "$$|S_{x}|$$", "word_idx": 44480, "sentence_idx": 746, "label": "unlabeled"}, {"type": "text", "expr": "From Figure  5 , we find that accuracies of the three approaches generally become larger when the size of the recommended set increases in all three domains", "word_idx": 44487, "sentence_idx": 747, "label": "unlabeled"}, {"type": "text", "expr": " This is consistent with our intuition, since the larger the recommended set is, the higher the possibility for the  truth  action to be in the recommended set", "word_idx": 44643, "sentence_idx": 748, "label": "unlabeled"}, {"type": "text", "expr": " We can also see that the accuracy of our  DUP  algorithm are generally larger than  ARMS+PRP  in all three domains, which verifies that our  DUP  algorithm can indeed better capture relations among actions and thus recognize unobserved actions better than the model-learning based approach  ARMS+PRP ", "word_idx": 44802, "sentence_idx": 749, "label": "unlabeled"}, {"type": "text", "expr": " The reason is similar to the one given for Figure  4  in the previous section", "word_idx": 45103, "sentence_idx": 750, "label": "unlabeled"}, {"type": "text", "expr": " That is, the \u201cshallow\u201d model learnt by our  DUP  algorithm is better for recognizing plans than both the \u201cexact\u201d planning model learnt by  ARMS  for recognizing plans with planning techniques", "word_idx": 45181, "sentence_idx": 751, "label": "unlabeled"}, {"type": "text", "expr": " Furthermore, the advantage of  DUP  becomes even larger when the size of recommended action set increases, which suggests our vector representation based learning approach can better capture action relations when the size of recommended action set is larger", "word_idx": 45373, "sentence_idx": 752, "label": "unlabeled"}, {"type": "text", "expr": " The possibility of actions correctly recognized by  DUP  becomes much larger than  ARMS+PRP  when the size of recommendations increases", "word_idx": 45631, "sentence_idx": 753, "label": "unlabeled"}, {"type": "text", "expr": "truth", "word_idx": 45767, "sentence_idx": 754, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 45772, "sentence_idx": 755, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 45780, "sentence_idx": 756, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 45788, "sentence_idx": 757, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a06:  Accuracies of  DUP  and  MatchPlan  with respect to different percentage of unobserved actions", "word_idx": 45796, "sentence_idx": 758, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a06:", "word_idx": 45901, "sentence_idx": 759, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 45910, "sentence_idx": 760, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a07:  Accuracies of  DUP  and  MatchPlan  with respect to different size of recommendations", "word_idx": 45919, "sentence_idx": 761, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a07:", "word_idx": 46015, "sentence_idx": 762, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 46024, "sentence_idx": 763, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a08:  Accuracies of  DUP  and  MatchPlan  with respect to different size of training set", "word_idx": 46033, "sentence_idx": 764, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a08:", "word_idx": 46126, "sentence_idx": 765, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 46135, "sentence_idx": 766, "label": "unlabeled"}, {"type": "text", "expr": "2  Comparison between  DUP  and  MatchPlan", "word_idx": 46144, "sentence_idx": 767, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 46186, "sentence_idx": 768, "label": "unlabeled"}, {"type": "text", "expr": "In this experiment we compare  DUP  to  MatchPlan  which is built based on the idea of  (10) ", "word_idx": 46195, "sentence_idx": 769, "label": "unlabeled"}, {"type": "text", "expr": " Likewise we varied the percentage of unobserved actions and the size of recommended actions to see the change of accuracies of both algorithms", "word_idx": 46288, "sentence_idx": 770, "label": "unlabeled"}, {"type": "text", "expr": " The results are exhibited below", "word_idx": 46431, "sentence_idx": 771, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 46463, "sentence_idx": 772, "label": "unlabeled"}, {"type": "text", "expr": "1  Varying Percentage of Unobserved Actions", "word_idx": 46472, "sentence_idx": 773, "label": "unlabeled"}, {"type": "text", "expr": "To compare our  DUP  algorithm with  MatchPlan  with respect to different percentage of unobserved actions, we set the window of training context  $c$  in Equation ( 1 ) of  DUP  to be three, the number of iterations in Algorithm  1  to be 1500, the size of recommendations to be ten, and the learning constant  $\\delta$  in Equation ( 7 ) to be 0", "word_idx": 46515, "sentence_idx": 774, "label": "unlabeled"}, {"type": "text", "expr": "\nTo make fair the comparison (with  MatchPlan ), we set the matching window of  MatchPlan  to be three, the same as the training context  $c$  of  DUP , when searching plans from plan libraries  $\\mathcal{L}$ ", "word_idx": 46862, "sentence_idx": 775, "label": "unlabeled"}, {"type": "text", "expr": " In other words, to estimate an unobserved action  $\\bar{a}_{x}$  in  $\\mathcal{O}$ ,  MatchPlan  matches previous three actions and subsequent three actions of  $\\bar{a}_{x}$  to plans in  $\\mathcal{L}$ , and recommends ten actions with the maximal number of matched actions, considering observed actions in the context of  $\\bar{a}_{x}$  and actions in  $\\mathcal{L}$  as a successful matching", "word_idx": 47071, "sentence_idx": 776, "label": "unlabeled"}, {"type": "text", "expr": " The results are shown in Figure  6 ", "word_idx": 47466, "sentence_idx": 777, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 47502, "sentence_idx": 778, "label": "unlabeled"}, {"type": "math", "expr": "$$\\delta$$", "word_idx": 47511, "sentence_idx": 779, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 47517, "sentence_idx": 780, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 47526, "sentence_idx": 781, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}$$", "word_idx": 47535, "sentence_idx": 782, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{a}_{x}$$", "word_idx": 47546, "sentence_idx": 783, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 47557, "sentence_idx": 784, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 47568, "sentence_idx": 785, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{a}_{x}$$", "word_idx": 47577, "sentence_idx": 786, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}$$", "word_idx": 47588, "sentence_idx": 787, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{a}_{x}$$", "word_idx": 47599, "sentence_idx": 788, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}$$", "word_idx": 47610, "sentence_idx": 789, "label": "unlabeled"}, {"type": "text", "expr": "From Figure  6 , we find that the accuracy of  DUP  is much better than  MatchPlan , which indicates that our  DUP  algorithm can better learn knowledge from plan libraries than the local matching approach  MatchPlan ", "word_idx": 47621, "sentence_idx": 790, "label": "unlabeled"}, {"type": "text", "expr": " This is because we take advantage of global plan information of the plan library when learning the \u201cshallow\u201d model, i", "word_idx": 47838, "sentence_idx": 791, "label": "unlabeled"}, {"type": "text", "expr": ", distributed representations of actions, and the model with global information can best explain the observed actions", "word_idx": 47956, "sentence_idx": 792, "label": "unlabeled"}, {"type": "text", "expr": "\nIn contrast,  MatchPlan  just utilizes local plan information when matching the observed actions to the plan library, which results in lower accuracies", "word_idx": 48073, "sentence_idx": 793, "label": "unlabeled"}, {"type": "text", "expr": " Looking at all three different domains, we can see that both algorithms perform the best in the  blocks  domain", "word_idx": 48225, "sentence_idx": 794, "label": "unlabeled"}, {"type": "text", "expr": " The reason is similar to the one provided in the last subsection (for Figure  4 ), i", "word_idx": 48337, "sentence_idx": 795, "label": "unlabeled"}, {"type": "text", "expr": ", the number of words over the number of vocabulary in the  blocks  domain is relatively larger than the other two domains, which gives us the hint that it is possible to improve accuracies by increasing the ratio of the number of words over the number of vocabularies", "word_idx": 48422, "sentence_idx": 796, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 48690, "sentence_idx": 797, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 48699, "sentence_idx": 798, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 48708, "sentence_idx": 799, "label": "unlabeled"}, {"type": "text", "expr": "blocks", "word_idx": 48717, "sentence_idx": 800, "label": "unlabeled"}, {"type": "text", "expr": "blocks", "word_idx": 48723, "sentence_idx": 801, "label": "unlabeled"}, {"type": "text", "expr": "2  Varying Size of Recommendation Set", "word_idx": 48729, "sentence_idx": 802, "label": "unlabeled"}, {"type": "text", "expr": "Likewise, we also would like to evaluate the change of accuracies when increasing the size of recommended actions", "word_idx": 48766, "sentence_idx": 803, "label": "unlabeled"}, {"type": "text", "expr": " We used the same experimental setting as done by previous subsection", "word_idx": 48879, "sentence_idx": 804, "label": "unlabeled"}, {"type": "text", "expr": " That is, we set the window of training context  $c$  of  DUP  to be three, the learning constant  $\\delta$  to be 0", "word_idx": 48948, "sentence_idx": 805, "label": "unlabeled"}, {"type": "text", "expr": "1, the number of iterations in Algorithm  1  to be 1500, the matching window of  MatchPlan  to be three", "word_idx": 49064, "sentence_idx": 806, "label": "unlabeled"}, {"type": "text", "expr": " In addition, we fix the percentage of unobserved actions to be 0", "word_idx": 49167, "sentence_idx": 807, "label": "unlabeled"}, {"type": "text", "expr": " The results are shown in Figure  7 ", "word_idx": 49232, "sentence_idx": 808, "label": "unlabeled"}, {"type": "math", "expr": "$$\\delta$$", "word_idx": 49268, "sentence_idx": 809, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 49274, "sentence_idx": 810, "label": "unlabeled"}, {"type": "text", "expr": "We can observe that the accuracy of our  DUP  algorithm are generally larger than  MatchPlan  in all three domains in Figure  7 , which suggests that our  DUP  algorithm can indeed better capture relations among actions and thus recognize unobserved actions better than the matching approach  MatchPlan ", "word_idx": 49283, "sentence_idx": 811, "label": "unlabeled"}, {"type": "text", "expr": " The reason behind this is similar to previous experiments, i", "word_idx": 49586, "sentence_idx": 812, "label": "unlabeled"}, {"type": "text", "expr": ", the global information captured from plan libraries by  DUP  can indeed better improve accuracies than local information exploited by  MatchPlan ", "word_idx": 49647, "sentence_idx": 813, "label": "unlabeled"}, {"type": "text", "expr": " In addition, looking at the trends of the curves of both  DUP  and  MatchPlan , we can see the performance of  DUP  becomes much better than  MatchPlan  when the size of recommendations increases", "word_idx": 49794, "sentence_idx": 814, "label": "unlabeled"}, {"type": "text", "expr": " This indicates the influence of global information becomes much larger when the size of recommendations increasing", "word_idx": 49990, "sentence_idx": 815, "label": "unlabeled"}, {"type": "text", "expr": " In other words, larger size of recommendations provides better chance for \u201cshallow\u201d models learnt by  DUP  to perform better", "word_idx": 50105, "sentence_idx": 816, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 50230, "sentence_idx": 817, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 50239, "sentence_idx": 818, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 50248, "sentence_idx": 819, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 50257, "sentence_idx": 820, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 50266, "sentence_idx": 821, "label": "unlabeled"}, {"type": "text", "expr": "3  Varying Size of Training Set", "word_idx": 50275, "sentence_idx": 822, "label": "unlabeled"}, {"type": "text", "expr": "To see the effect of size of training set, we ran both  DUP  and  MatchPlan  with different size of training set", "word_idx": 50306, "sentence_idx": 823, "label": "unlabeled"}, {"type": "text", "expr": " We used the same setting as done by last subsection except fixing the size of recommendations to be 10, when running both algorithms", "word_idx": 50418, "sentence_idx": 824, "label": "unlabeled"}, {"type": "text", "expr": " We varied the size of training set from 2500 to 4500", "word_idx": 50551, "sentence_idx": 825, "label": "unlabeled"}, {"type": "text", "expr": " The results are shown in Figure  8 ", "word_idx": 50604, "sentence_idx": 826, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 50640, "sentence_idx": 827, "label": "unlabeled"}, {"type": "text", "expr": "We observed that accuracies of both  DUP  and  MatchPlan  generally become higher when the size of training set increases", "word_idx": 50649, "sentence_idx": 828, "label": "unlabeled"}, {"type": "text", "expr": " This is consistent with our intuition, since the larger the size of training set is, the richer the information is available for improving the accuracies", "word_idx": 50770, "sentence_idx": 829, "label": "unlabeled"}, {"type": "text", "expr": " Comparing the curves of  DUP  and  MatchPlan , we can see that  DUP  performs much better than  MatchPlan ", "word_idx": 50924, "sentence_idx": 830, "label": "unlabeled"}, {"type": "text", "expr": " This further verifies the benefit of exploiting global information of plan libraries when learning the shallow models as done by  DUP ", "word_idx": 51031, "sentence_idx": 831, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 51166, "sentence_idx": 832, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 51175, "sentence_idx": 833, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 51184, "sentence_idx": 834, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a09:  Accuracy with respect to different number of iterations in the blocks domain", "word_idx": 51193, "sentence_idx": 835, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a09:", "word_idx": 51280, "sentence_idx": 836, "label": "unlabeled"}, {"type": "text", "expr": "3  Accuracy w", "word_idx": 51289, "sentence_idx": 837, "label": "unlabeled"}, {"type": "text", "expr": " Iterations", "word_idx": 51302, "sentence_idx": 838, "label": "unlabeled"}, {"type": "text", "expr": "In the previous experiments, we set the number of iterations in Algorithm  1  to be 1500", "word_idx": 51313, "sentence_idx": 839, "label": "unlabeled"}, {"type": "text", "expr": " In this experiment, we would like to see the influence of iterations of our  DUP  algorithm when running the EM-style procedure", "word_idx": 51401, "sentence_idx": 840, "label": "unlabeled"}, {"type": "text", "expr": " We changed the number of iterations from 300 to 3000 to see the trend of accuracy", "word_idx": 51529, "sentence_idx": 841, "label": "unlabeled"}, {"type": "text", "expr": " We exhibit the experimental results in the  blocks  domain (the results of the other two domains are similar) in Figure  9 ", "word_idx": 51611, "sentence_idx": 842, "label": "unlabeled"}, {"type": "text", "expr": "blocks", "word_idx": 51735, "sentence_idx": 843, "label": "unlabeled"}, {"type": "text", "expr": "From Figure  9 , we can see the accuracy becomes higher at the beginning and stays flat when reaching the size of 1500", "word_idx": 51741, "sentence_idx": 844, "label": "unlabeled"}, {"type": "text", "expr": " This exhibits that the EM procedure converges and has stable accuracies after the iteration reaches 1500", "word_idx": 51859, "sentence_idx": 845, "label": "unlabeled"}, {"type": "text", "expr": " Similar results can also be found in the other two domains", "word_idx": 51964, "sentence_idx": 846, "label": "unlabeled"}, {"type": "text", "expr": "4  Comparison between  RNNPlanner  and  DUP", "word_idx": 52023, "sentence_idx": 847, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 52066, "sentence_idx": 848, "label": "unlabeled"}, {"type": "text", "expr": "In this section we compare  RNNPlanner  with  DUP  to see the change of performance with respect to different distributions of missing actions in the underlying plans to be discovered", "word_idx": 52076, "sentence_idx": 849, "label": "unlabeled"}, {"type": "text", "expr": " In this experiment, we are interested in evaluating the performance on consecutive missing actions in the underlying plans since these scenarios often exist in many applications such as surveillance  (1) ", "word_idx": 52259, "sentence_idx": 850, "label": "unlabeled"}, {"type": "text", "expr": " We first test the performance of both  RNNPlanner  and  DUP  in discovering underlying plans with only consecutive missing actions in the \u201cmiddle\u201d of the plans, i", "word_idx": 52464, "sentence_idx": 851, "label": "unlabeled"}, {"type": "text", "expr": ", actions are not missing at the end or in the front, which indicates missing actions can be inferred from both previously and subsequently observed actions", "word_idx": 52627, "sentence_idx": 852, "label": "unlabeled"}, {"type": "text", "expr": " Then we evaluate both  RNNPlanner  and  DUP  in discovering underlying plans with only consecutive missing actions at the end of the plans, which indicates missing actions can only be inferred from previously observed actions", "word_idx": 52783, "sentence_idx": 853, "label": "unlabeled"}, {"type": "text", "expr": " After that, we also evaluate the performance of our  RNNPlanner  and  DUP  approaches with respect to the size of recommendation set", "word_idx": 53009, "sentence_idx": 854, "label": "unlabeled"}, {"type": "text", "expr": " In the following subsections, we present the experimental results regarding those three aspects", "word_idx": 53142, "sentence_idx": 855, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 53238, "sentence_idx": 856, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 53248, "sentence_idx": 857, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 53258, "sentence_idx": 858, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 53268, "sentence_idx": 859, "label": "unlabeled"}, {"type": "text", "expr": "1  Performance with missing actions in the middle", "word_idx": 53278, "sentence_idx": 860, "label": "unlabeled"}, {"type": "text", "expr": "To see the performance of  RNNPlanner  and  DUP  in cases when actions are missing in the middle of the underlying plan to be discovered, we vary the number of consecutive missing actions from 1 to 10, to see the change of accuracies of both  RNNPlanner  and  DUP ", "word_idx": 53327, "sentence_idx": 861, "label": "unlabeled"}, {"type": "text", "expr": " We set the window size to be 1 and the recommendation size to be 10", "word_idx": 53591, "sentence_idx": 862, "label": "unlabeled"}, {"type": "text", "expr": " The results are shown in Figure  10 ", "word_idx": 53659, "sentence_idx": 863, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 53696, "sentence_idx": 864, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 53706, "sentence_idx": 865, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a010:  Accuracy with respect to missing actions in the middle", "word_idx": 53716, "sentence_idx": 866, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a010:", "word_idx": 53782, "sentence_idx": 867, "label": "unlabeled"}, {"type": "text", "expr": "From Figure  10 , we can see that the accuracies of both  RNNPlanner  and  DUP  generally become lower when the number of consecutive unobserved actions increasing", "word_idx": 53792, "sentence_idx": 868, "label": "unlabeled"}, {"type": "text", "expr": " This is consistent with our intuition since the more actions are missing, the less information can be used to help infer the unobserved actions, which results in low accuracies", "word_idx": 53955, "sentence_idx": 869, "label": "unlabeled"}, {"type": "text", "expr": " Comparing the curves of  RNNPlanner  and  DUP , we can see that the accuracy of  DUP  is higher than  RNNPlanner  at the beginning", "word_idx": 54132, "sentence_idx": 870, "label": "unlabeled"}, {"type": "text", "expr": " This is because  DUP  exploits information of both observed actions before and after missing actions to infer the missing actions, while  RNNPlanner  just exploits observed actions before missing actions", "word_idx": 54263, "sentence_idx": 871, "label": "unlabeled"}, {"type": "text", "expr": " When the number of missing actions is larger than 3, the accuracies of  DUP  and  RNNPlanner  are both low (i", "word_idx": 54467, "sentence_idx": 872, "label": "unlabeled"}, {"type": "text", "expr": ", lower than 0", "word_idx": 54577, "sentence_idx": 873, "label": "unlabeled"}, {"type": "text", "expr": " This is because the window size of  DUP  and  RNNPlanner  is set to be 1, which indicates we exploit one action before the missing actions and one action after the missing actions to estimate the missing actions", "word_idx": 54591, "sentence_idx": 874, "label": "unlabeled"}, {"type": "text", "expr": " When the consecutive missing actions are more than 1, there may not be sufficient context information for inferring the missing actions, resulting in low accuracies", "word_idx": 54803, "sentence_idx": 875, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 54968, "sentence_idx": 876, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 54978, "sentence_idx": 877, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 54988, "sentence_idx": 878, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 54998, "sentence_idx": 879, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 55008, "sentence_idx": 880, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 55018, "sentence_idx": 881, "label": "unlabeled"}, {"type": "text", "expr": "2  Performance with missing actions at the end", "word_idx": 55028, "sentence_idx": 882, "label": "unlabeled"}, {"type": "text", "expr": "We also would like to see the performance of  RNNPlanner  and  DUP  in discovering missing actions at the end, which is prevalent in application domains that aim at discovering/predicting future actions", "word_idx": 55074, "sentence_idx": 883, "label": "unlabeled"}, {"type": "text", "expr": " Similar to previous experiments, we vary the number of consecutive unobserved or missing actions to see the change of accuracies of both  RNNPlanner  and  DUP ", "word_idx": 55276, "sentence_idx": 884, "label": "unlabeled"}, {"type": "text", "expr": " We set the window size to be 1 and the recommendation size to be 10 as well", "word_idx": 55436, "sentence_idx": 885, "label": "unlabeled"}, {"type": "text", "expr": " The experimental results are shown in Figure  11 ", "word_idx": 55512, "sentence_idx": 886, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 55562, "sentence_idx": 887, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 55572, "sentence_idx": 888, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a011:  Accuracy with respect to missing actions in the end", "word_idx": 55582, "sentence_idx": 889, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a011:", "word_idx": 55645, "sentence_idx": 890, "label": "unlabeled"}, {"type": "text", "expr": "From Figure  11 , we can observe that the accuracies of both  RNNPlanner  and  DUP  generally get decreasing when the number of consecutive missing actions increases", "word_idx": 55655, "sentence_idx": 891, "label": "unlabeled"}, {"type": "text", "expr": " This is similar to previous experimental results", "word_idx": 55820, "sentence_idx": 892, "label": "unlabeled"}, {"type": "text", "expr": " That is, the more actions are missing, the less information is available for estimating the missing actions, which results in lower accuracy", "word_idx": 55869, "sentence_idx": 893, "label": "unlabeled"}, {"type": "text", "expr": " In addition, we can also see that  RNNPlanner  generally performs better than  DUP , which indicates that the RNNs-based approach, i", "word_idx": 56010, "sentence_idx": 894, "label": "unlabeled"}, {"type": "text", "expr": ",  RNNPlanner , can indeed better exploit observed actions to predict future missing actions, since RNNs are capable of flexibly leveraging long or short-term information to help predict missing actions", "word_idx": 56143, "sentence_idx": 895, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 56345, "sentence_idx": 896, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 56355, "sentence_idx": 897, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 56365, "sentence_idx": 898, "label": "unlabeled"}, {"type": "text", "expr": "3  Performance with respect to different recommendation size", "word_idx": 56375, "sentence_idx": 899, "label": "unlabeled"}, {"type": "text", "expr": "To see the change with respect to different recommendation size, we vary the size of recommendation sets from 1 to 10 and calculate their corresponding accuracies", "word_idx": 56435, "sentence_idx": 900, "label": "unlabeled"}, {"type": "text", "expr": " We test our approaches with four cases: A", "word_idx": 56597, "sentence_idx": 901, "label": "unlabeled"}, {"type": "text", "expr": " there are five actions missing at the end; B", "word_idx": 56639, "sentence_idx": 902, "label": "unlabeled"}, {"type": "text", "expr": " there are five actions missing in the middle; C", "word_idx": 56684, "sentence_idx": 903, "label": "unlabeled"}, {"type": "text", "expr": " there is one action missing at the end; D", "word_idx": 56732, "sentence_idx": 904, "label": "unlabeled"}, {"type": "text", "expr": " there is one action missing in the middle", "word_idx": 56774, "sentence_idx": 905, "label": "unlabeled"}, {"type": "text", "expr": " The results are shown in Figures  12 - 15  corresponding to cases A-D, respectively", "word_idx": 56816, "sentence_idx": 906, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a012:  Case A: accuracy with respect to different size of recommendations", "word_idx": 56900, "sentence_idx": 907, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a012:", "word_idx": 56978, "sentence_idx": 908, "label": "unlabeled"}, {"type": "text", "expr": "Case A:", "word_idx": 56988, "sentence_idx": 909, "label": "unlabeled"}, {"type": "text", "expr": "As shown in Figure  12 ,  RNNPlanner  performs better than  DUP  mostly, except for when the recommendation set size is larger/equal to eight in blocks domain", "word_idx": 56995, "sentence_idx": 910, "label": "unlabeled"}, {"type": "text", "expr": " This is because  RNNPlanner , which contains LSTM cells, is able to actively remember or forget past observations (inputs) and computations (hidden states)", "word_idx": 57153, "sentence_idx": 911, "label": "unlabeled"}, {"type": "text", "expr": " For example, if in a set of sequences, a pattern  $A^{**}$  follows  $A^{*}$  after three words (i", "word_idx": 57309, "sentence_idx": 912, "label": "unlabeled"}, {"type": "text", "expr": ",  $,A^{*},w_{i},w_{i+1},w_{i+2},A^{**},$ ), where  $w_{i},w_{i+1},w_{i+2}$  could be any word from the vocabulary except for  $A^{*}$  and  $A^{**}$ ", "word_idx": 57408, "sentence_idx": 913, "label": "unlabeled"}, {"type": "text", "expr": " And if the window size of  DUP  is smaller than three, then  DUP  is not able to utilize this pattern to predict  $A^{**}$  mainly based on  $A^{*}$ ", "word_idx": 57558, "sentence_idx": 914, "label": "unlabeled"}, {"type": "text", "expr": " When predicting the  $A^{**}$ , the  DUP  with context size one, works by searching for the most similar word to the  $w_{i+2}$ ", "word_idx": 57708, "sentence_idx": 915, "label": "unlabeled"}, {"type": "text", "expr": " One would yet argue that we can set a larger window size for  DUP ", "word_idx": 57837, "sentence_idx": 916, "label": "unlabeled"}, {"type": "text", "expr": " Larger window size does not necessarily lead to higher accuracy, since using a larger window size also add more noise in the training of  DUP ", "word_idx": 57904, "sentence_idx": 917, "label": "unlabeled"}, {"type": "text", "expr": " Remember that the word2vec model treats equally all possible word pair samples within its context window", "word_idx": 58047, "sentence_idx": 918, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 58152, "sentence_idx": 919, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 58162, "sentence_idx": 920, "label": "unlabeled"}, {"type": "math", "expr": "$$A^{**}$$", "word_idx": 58172, "sentence_idx": 921, "label": "unlabeled"}, {"type": "math", "expr": "$$A^{*}$$", "word_idx": 58178, "sentence_idx": 922, "label": "unlabeled"}, {"type": "math", "expr": "$$...,A^{*},w_{i},w_{i+1},w_{i+2},A^{**},...$$", "word_idx": 58183, "sentence_idx": 923, "label": "unlabeled"}, {"type": "math", "expr": "$$w_{i},w_{i+1},w_{i+2}$$", "word_idx": 58225, "sentence_idx": 924, "label": "unlabeled"}, {"type": "math", "expr": "$$A^{*}$$", "word_idx": 58246, "sentence_idx": 925, "label": "unlabeled"}, {"type": "math", "expr": "$$A^{**}$$", "word_idx": 58251, "sentence_idx": 926, "label": "unlabeled"}, {"type": "math", "expr": "$$A^{**}$$", "word_idx": 58257, "sentence_idx": 927, "label": "unlabeled"}, {"type": "math", "expr": "$$A^{*}$$", "word_idx": 58263, "sentence_idx": 928, "label": "unlabeled"}, {"type": "math", "expr": "$$A^{**}$$", "word_idx": 58268, "sentence_idx": 929, "label": "unlabeled"}, {"type": "math", "expr": "$$w_{i+2}$$", "word_idx": 58274, "sentence_idx": 930, "label": "unlabeled"}, {"type": "text", "expr": "In addition, observing the accuracies (in terms of the size of recommendation  $S_{x}$ ) of all three domains , we can see only in the blocks domain that  DUP  outperforms  RNNPlanner , when  $S_{x}$  is larger than eight", "word_idx": 58281, "sentence_idx": 931, "label": "unlabeled"}, {"type": "text", "expr": " Also in the  blocks  domain,  DUP  has the best performance, comparing to how  DUP  functions in other two domains", "word_idx": 58502, "sentence_idx": 932, "label": "unlabeled"}, {"type": "text", "expr": " This is because plans from the  blocks  domain has an overall higher ratio of #word to #vocabulary, which increases the possibility that the word pattern outside a context window, would reappear inside the window, and consequently help  DUP  recognize actions in the missing positions", "word_idx": 58617, "sentence_idx": 933, "label": "unlabeled"}, {"type": "text", "expr": " Coming back to the example when we have a plan like  $,A^{*},w_{i},w_{i+1},w_{i+2},A^{**},$ , in  blocks  domain, it\u2019s more possible the word  $A^{*}$  happens again in one of  $w_{i}$ ,  $w_{i+1}$ , and  $w_{i+2}$ ", "word_idx": 58902, "sentence_idx": 934, "label": "unlabeled"}, {"type": "math", "expr": "$$S_{x}$$", "word_idx": 59118, "sentence_idx": 935, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 59123, "sentence_idx": 936, "label": "unlabeled"}, {"type": "math", "expr": "$$S_{x}$$", "word_idx": 59133, "sentence_idx": 937, "label": "unlabeled"}, {"type": "text", "expr": "blocks", "word_idx": 59138, "sentence_idx": 938, "label": "unlabeled"}, {"type": "text", "expr": "blocks", "word_idx": 59144, "sentence_idx": 939, "label": "unlabeled"}, {"type": "math", "expr": "$$...,A^{*},w_{i},w_{i+1},w_{i+2},A^{**},...$$", "word_idx": 59150, "sentence_idx": 940, "label": "unlabeled"}, {"type": "text", "expr": "blocks", "word_idx": 59192, "sentence_idx": 941, "label": "unlabeled"}, {"type": "math", "expr": "$$A^{*}$$", "word_idx": 59198, "sentence_idx": 942, "label": "unlabeled"}, {"type": "math", "expr": "$$w_{i}$$", "word_idx": 59203, "sentence_idx": 943, "label": "unlabeled"}, {"type": "math", "expr": "$$w_{i+1}$$", "word_idx": 59208, "sentence_idx": 944, "label": "unlabeled"}, {"type": "math", "expr": "$$w_{i+2}$$", "word_idx": 59215, "sentence_idx": 945, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a013:  Case B: accuracy with respect to different size of recommendations", "word_idx": 59222, "sentence_idx": 946, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a013:", "word_idx": 59300, "sentence_idx": 947, "label": "unlabeled"}, {"type": "text", "expr": "Case B:", "word_idx": 59310, "sentence_idx": 948, "label": "unlabeled"}, {"type": "text", "expr": "What we can observe here from Figure  13 , is similar to our observations in  case A ", "word_idx": 59317, "sentence_idx": 949, "label": "unlabeled"}, {"type": "text", "expr": "  RNNPlanner  generally performs better than  DUP , except for when the size of recommendation  $S_{x}$  is larger or equal to nine in  blocks  domain", "word_idx": 59402, "sentence_idx": 950, "label": "unlabeled"}, {"type": "text", "expr": " It could also be observed that both  RNNPlanner  and  DUP  have the best accuracy performance in the  blocks  domain", "word_idx": 59552, "sentence_idx": 951, "label": "unlabeled"}, {"type": "text", "expr": "case A", "word_idx": 59669, "sentence_idx": 952, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 59675, "sentence_idx": 953, "label": "unlabeled"}, {"type": "math", "expr": "$$S_{x}$$", "word_idx": 59685, "sentence_idx": 954, "label": "unlabeled"}, {"type": "text", "expr": "blocks", "word_idx": 59690, "sentence_idx": 955, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 59696, "sentence_idx": 956, "label": "unlabeled"}, {"type": "text", "expr": "blocks", "word_idx": 59706, "sentence_idx": 957, "label": "unlabeled"}, {"type": "text", "expr": "And by comparing the Figure  13  in  case B  (five removed actions in the middle) with Figure  12  in  case A  (five removed actions at the end), we can see that, the accuracy difference between  DUP  and  RNNPlanner  at each size of recommendation along the x-axis, is smaller in  case B ", "word_idx": 59712, "sentence_idx": 958, "label": "unlabeled"}, {"type": "text", "expr": " This is because,  RNNPlanner  only leverages the observed actions before a missing position, whereas  DUP  has the advantage of additionally using the observation after a missing position", "word_idx": 60001, "sentence_idx": 959, "label": "unlabeled"}, {"type": "text", "expr": "case B", "word_idx": 60189, "sentence_idx": 960, "label": "unlabeled"}, {"type": "text", "expr": "case A", "word_idx": 60195, "sentence_idx": 961, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 60201, "sentence_idx": 962, "label": "unlabeled"}, {"type": "text", "expr": "case B", "word_idx": 60211, "sentence_idx": 963, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 60217, "sentence_idx": 964, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a014:  Case C: accuracy with respect to different size of recommendations", "word_idx": 60227, "sentence_idx": 965, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a014:", "word_idx": 60305, "sentence_idx": 966, "label": "unlabeled"}, {"type": "text", "expr": "Case C:", "word_idx": 60315, "sentence_idx": 967, "label": "unlabeled"}, {"type": "text", "expr": "From Figure  14 , we can see that both  RNNPlanner  and  DUP  could outperform each other in certain domains and recommendation set sizes ( $S_{x}$ )", "word_idx": 60322, "sentence_idx": 968, "label": "unlabeled"}, {"type": "text", "expr": " In  blocks  domain,  DUP  is better when  $S_{x}$  is larger than five", "word_idx": 60471, "sentence_idx": 969, "label": "unlabeled"}, {"type": "text", "expr": " In  depots  domain,  RNNPlanner  is overwhelmingly better than  DUP ", "word_idx": 60542, "sentence_idx": 970, "label": "unlabeled"}, {"type": "text", "expr": " In  driverlog  domain,  DUP  performs overall better except that, when there is only one recommendation,  DUP  is as good as  RNNPlanner ", "word_idx": 60611, "sentence_idx": 971, "label": "unlabeled"}, {"type": "text", "expr": " To explain this observation, if the number of consecutively removed action is less or equal to context window size (e", "word_idx": 60749, "sentence_idx": 972, "label": "unlabeled"}, {"type": "text", "expr": ", window size is one, and number of missing actions is one, in our  case C ), then the fixed, and short context window of  DUP , is competitive enough", "word_idx": 60867, "sentence_idx": 973, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 61017, "sentence_idx": 974, "label": "unlabeled"}, {"type": "math", "expr": "$$S_{x}$$", "word_idx": 61027, "sentence_idx": 975, "label": "unlabeled"}, {"type": "text", "expr": "blocks", "word_idx": 61032, "sentence_idx": 976, "label": "unlabeled"}, {"type": "math", "expr": "$$S_{x}$$", "word_idx": 61038, "sentence_idx": 977, "label": "unlabeled"}, {"type": "text", "expr": "depots", "word_idx": 61043, "sentence_idx": 978, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 61049, "sentence_idx": 979, "label": "unlabeled"}, {"type": "text", "expr": "driverlog", "word_idx": 61059, "sentence_idx": 980, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 61068, "sentence_idx": 981, "label": "unlabeled"}, {"type": "text", "expr": "case C", "word_idx": 61078, "sentence_idx": 982, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a015:  Case D: accuracy with respect to different size of recommendations", "word_idx": 61084, "sentence_idx": 983, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a015:", "word_idx": 61162, "sentence_idx": 984, "label": "unlabeled"}, {"type": "text", "expr": "Case D:", "word_idx": 61172, "sentence_idx": 985, "label": "unlabeled"}, {"type": "text", "expr": "From the results in Figure  15 , we can see that  DUP  functions better than  RNNPlanner  over all three domains, whereas  DUP  is worse in  case A  and  case B , and could occasionally be better than  RNNPlanner  in  case C ", "word_idx": 61179, "sentence_idx": 986, "label": "unlabeled"}, {"type": "text", "expr": " It makes sense in that, on the one hand, within the fixed and short context window, if there is very less positions with removed actions,  DUP  would have an improved performance", "word_idx": 61404, "sentence_idx": 987, "label": "unlabeled"}, {"type": "text", "expr": " On the other hand,  RNNPlanner  is not able to leverage the information from both sides of a position with a missing action", "word_idx": 61583, "sentence_idx": 988, "label": "unlabeled"}, {"type": "text", "expr": " Therefore, in  case D ,  DUP  gains the benefit from both assumptions that there is only one missing action, and the position of that action is randomly chosen in the middle of a plan", "word_idx": 61707, "sentence_idx": 989, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 61891, "sentence_idx": 990, "label": "unlabeled"}, {"type": "text", "expr": "case A", "word_idx": 61901, "sentence_idx": 991, "label": "unlabeled"}, {"type": "text", "expr": "case B", "word_idx": 61907, "sentence_idx": 992, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 61913, "sentence_idx": 993, "label": "unlabeled"}, {"type": "text", "expr": "case C", "word_idx": 61923, "sentence_idx": 994, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 61929, "sentence_idx": 995, "label": "unlabeled"}, {"type": "text", "expr": "case D", "word_idx": 61939, "sentence_idx": 996, "label": "unlabeled"}, {"type": "text", "expr": "7  Related work", "word_idx": 61945, "sentence_idx": 997, "label": "unlabeled"}, {"type": "text", "expr": "Our work is related to planning with incomplete domain models (or model-lite planning  )", "word_idx": 61960, "sentence_idx": 998, "label": "unlabeled"}, {"type": "text", "expr": "\nFigure  16  shows the schematic view of incomplete models and their relationships in the spectrum of incompleteness", "word_idx": 62048, "sentence_idx": 999, "label": "unlabeled"}, {"type": "text", "expr": " In a full model, we know exactly the dynamics of the model (i", "word_idx": 62164, "sentence_idx": 1000, "label": "unlabeled"}, {"type": "text", "expr": ", state transitions)", "word_idx": 62226, "sentence_idx": 1001, "label": "unlabeled"}, {"type": "text", "expr": " Approximate models are the closest to full\nmodels and their representations are similar\nexcept that there can be incomplete knowledge of action descriptions", "word_idx": 62246, "sentence_idx": 1002, "label": "unlabeled"}, {"type": "text", "expr": " To enable approximate planners to\nperform more (e", "word_idx": 62403, "sentence_idx": 1003, "label": "unlabeled"}, {"type": "text", "expr": ", providing robust plans),\nplanners are assumed to have access\nto additional knowledge circumscribing the\nincompleteness  (29) ", "word_idx": 62453, "sentence_idx": 1004, "label": "unlabeled"}, {"type": "text", "expr": " Partial models are one level further down the line in terms of the degree of incompleteness", "word_idx": 62580, "sentence_idx": 1005, "label": "unlabeled"}, {"type": "text", "expr": " While approximate models can encode incompleteness in the precondition/effect descriptions of the individual actions, partial models can completely abstract portions of a plan without providing details for them", "word_idx": 62672, "sentence_idx": 1006, "label": "unlabeled"}, {"type": "text", "expr": " In such cases, even though providing complete plans is infeasible, partial models can provide \u201cplanning guidance\u201d for agents  (31) ", "word_idx": 62883, "sentence_idx": 1007, "label": "unlabeled"}, {"type": "text", "expr": " Shallow models are essentially just a step above having no planning model", "word_idx": 63015, "sentence_idx": 1008, "label": "unlabeled"}, {"type": "text", "expr": " They provide interesting contrasts to the standard precondition and effect based action models used in automated planning community", "word_idx": 63089, "sentence_idx": 1009, "label": "unlabeled"}, {"type": "text", "expr": " Our work in this paper belongs to the class of shallow models", "word_idx": 63221, "sentence_idx": 1010, "label": "unlabeled"}, {"type": "text", "expr": " In developing shallow models, we are interested in planning technology that helps humans develop plans, even in the absence of any structured models or plan traces", "word_idx": 63283, "sentence_idx": 1011, "label": "unlabeled"}, {"type": "text", "expr": " In such cases, the best that we can hope for is to learn local structures of the planning model to provide planning support, similar to providing spell-check in writing", "word_idx": 63447, "sentence_idx": 1012, "label": "unlabeled"}, {"type": "text", "expr": " While some work in web-service composition (c", "word_idx": 63616, "sentence_idx": 1013, "label": "unlabeled"}, {"type": "text", "expr": "  (8) ) did focus on this type of planning support, they were hobbled by being limited to simple input/output type comparison", "word_idx": 63662, "sentence_idx": 1014, "label": "unlabeled"}, {"type": "text", "expr": " In contrast, we expect shallow models to be useful in \u201ccritiquing\u201d the plans being generated by the humans (e", "word_idx": 63787, "sentence_idx": 1015, "label": "unlabeled"}, {"type": "text", "expr": " detecting that an action introduced by the human is not consistent with the model), and \u201cexplaining/justifying\u201d the suggestions generated by humans", "word_idx": 63897, "sentence_idx": 1016, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a016:  Schematic view of incomplete models and their relationships in the spectrum of incompleteness", "word_idx": 64045, "sentence_idx": 1017, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a016:", "word_idx": 64150, "sentence_idx": 1018, "label": "unlabeled"}, {"type": "text", "expr": "Our work is also related to plan recognition", "word_idx": 64160, "sentence_idx": 1019, "label": "unlabeled"}, {"type": "text", "expr": " Kautz and Allen proposed an approach to recognizing plans based on parsing observed actions as sequences of subactions and essentially model this knowledge as a context-free rule in an \u201caction grammar\u201d  (16) ", "word_idx": 64204, "sentence_idx": 1020, "label": "unlabeled"}, {"type": "text", "expr": " All actions, plans are uniformly referred to as goals, and a recognizer\u2019s knowledge is represented by a set of first-order statements called event hierarchy encoded in first-order logic, which defines abstraction, decomposition and functional relationships between types of events", "word_idx": 64413, "sentence_idx": 1021, "label": "unlabeled"}, {"type": "text", "expr": " Lesh and Etzioni further presented methods in scaling up activity recognition to scale up his work computationally  (18) ", "word_idx": 64694, "sentence_idx": 1022, "label": "unlabeled"}, {"type": "text", "expr": " They automatically constructed plan-library from domain primitives, which was different from  (16)  where the plan library was explicitly represented", "word_idx": 64816, "sentence_idx": 1023, "label": "unlabeled"}, {"type": "text", "expr": " In these approaches, the problem of combinatorial explosion of plan execution models impedes its application to real-world domains", "word_idx": 64966, "sentence_idx": 1024, "label": "unlabeled"}, {"type": "text", "expr": " Kabanza and Filion  (12)  proposed an anytime plan recognition algorithm to reduce the number of generated plan execution models based on weighted model counting", "word_idx": 65097, "sentence_idx": 1025, "label": "unlabeled"}, {"type": "text", "expr": " These approaches are, however, difficult to represent uncertainty", "word_idx": 65259, "sentence_idx": 1026, "label": "unlabeled"}, {"type": "text", "expr": " They offer no mechanism for preferring one consistent approach to another and incapable of deciding whether one particular plan is more likely than another, as long as both of them can be consistent enough to explain the actions observed", "word_idx": 65325, "sentence_idx": 1027, "label": "unlabeled"}, {"type": "text", "expr": " Although we exploit a library of plans in our  DUP  and  RNNPlanner  approaches, we aim to learning shallow models and utilize the shallow models to recognize plans that are not necessarily in the plan library, which is different from previous approaches that assume the plans to be recognized are from the plan library", "word_idx": 65563, "sentence_idx": 1028, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 65883, "sentence_idx": 1029, "label": "unlabeled"}, {"type": "text", "expr": "Instead of using a library of plans, Ramirez and Geffner  (24)  proposed an approach to solving the plan recognition problem using slightly modified planning algorithms, assuming the action models were given as input (note that action models can be created by experts or learnt by previous systems  )", "word_idx": 65893, "sentence_idx": 1030, "label": "unlabeled"}, {"type": "text", "expr": " Except previous work   on the plan recognition problem presented in the introduction section,  Saria and Mahadevan presented a hierarchical multi-agent markov processes as a framework for hierarchical probabilistic plan recognition in cooperative multi-agent systems  (26) ", "word_idx": 66193, "sentence_idx": 1031, "label": "unlabeled"}, {"type": "text", "expr": " Amir and Gal addressed a plan recognition approach to recognizing student behaviors using virtual science laboratories  (3) ", "word_idx": 66467, "sentence_idx": 1032, "label": "unlabeled"}, {"type": "text", "expr": " Ramirez and Geffner exploited off-the-shelf classical planners to recognize probabilistic plans  (25) ", "word_idx": 66592, "sentence_idx": 1033, "label": "unlabeled"}, {"type": "text", "expr": " Different from those approaches, we do not require any domain model knowledge provided as input", "word_idx": 66695, "sentence_idx": 1034, "label": "unlabeled"}, {"type": "text", "expr": " Instead, we automatically learn shallow domain models from previous plan cases for recognizing unknown plans that may not be identical to previous cases", "word_idx": 66791, "sentence_idx": 1035, "label": "unlabeled"}, {"type": "text", "expr": "8  Conclusion and Discussion", "word_idx": 66944, "sentence_idx": 1036, "label": "unlabeled"}, {"type": "text", "expr": "In this paper we present two novel plan recognition approaches,  DUP  and  RNNPlanner ,\nbased on vector representation of actions", "word_idx": 66972, "sentence_idx": 1037, "label": "unlabeled"}, {"type": "text", "expr": " For  DUP , we first learn the vector\nrepresentations of actions from plan libraries using the Skip-gram\nmodel which has been demonstrated to be effective", "word_idx": 67101, "sentence_idx": 1038, "label": "unlabeled"}, {"type": "text", "expr": " We then discover\nunobserved actions with the vector representations by repeatedly\nsampling actions and optimizing the probability of potential plans to\nbe recognized", "word_idx": 67255, "sentence_idx": 1039, "label": "unlabeled"}, {"type": "text", "expr": " For  RNNPlanner , we let the neural network itself to learn the word embedding, which would then be utilized by higher LSTM layers", "word_idx": 67421, "sentence_idx": 1040, "label": "unlabeled"}, {"type": "text", "expr": " We also empirically exhibit the effectiveness of our\napproaches", "word_idx": 67552, "sentence_idx": 1041, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 67616, "sentence_idx": 1042, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 67626, "sentence_idx": 1043, "label": "unlabeled"}, {"type": "text", "expr": "While we focused on a one-shot recognition task in this paper, in\npractice, human-in-the-loop planning will consist of multiple\niterations, with DUP recognizing the plan and suggesting action\naddition alternatives; the human making a selection and revising the\nplan", "word_idx": 67636, "sentence_idx": 1044, "label": "unlabeled"}, {"type": "text", "expr": " The aim is to provide a form of flexible plan completion tool,\nakin to auto-completers for search engine queries", "word_idx": 67901, "sentence_idx": 1045, "label": "unlabeled"}, {"type": "text", "expr": " To do this\nefficiently, we need to make the DUP recognition algorithm \u201cincremental", "word_idx": 68014, "sentence_idx": 1046, "label": "unlabeled"}, {"type": "text", "expr": "While we focused on a one-shot recognition task in this paper, in\npractice, human-in-the-loop planning will consist of multiple\niterations, with DUP recognizing the plan and suggesting action\naddition alternatives; the human making a selection and revising the\nplan", "word_idx": 68097, "sentence_idx": 1047, "label": "unlabeled"}, {"type": "text", "expr": " The aim is to provide a form of flexible plan completion tool,\nakin to auto-completers for search engine queries", "word_idx": 68362, "sentence_idx": 1048, "label": "unlabeled"}, {"type": "text", "expr": " To do this\nefficiently, we need to make the DUP recognition algorithm \u201cincremental", "word_idx": 68475, "sentence_idx": 1049, "label": "unlabeled"}, {"type": "text", "expr": "The word-vector based domain model we developed in this paper provides\ninteresting contrasts to the standard precondition and effect based\naction models used in automated planning community", "word_idx": 68558, "sentence_idx": 1050, "label": "unlabeled"}, {"type": "text", "expr": " One of our future\naims is to provide a more systematic comparison of the tradeoffs\noffered by these models", "word_idx": 68747, "sentence_idx": 1051, "label": "unlabeled"}, {"type": "text", "expr": " Although we have\nfocused on the \u201cplan recognition\u201d aspects of this model until now,\nand assumed that \u201cplanning support\u201d will be limited to suggesting\npotential actions to the humans", "word_idx": 68854, "sentence_idx": 1052, "label": "unlabeled"}, {"type": "text", "expr": " In future, we will also consider\n\u201ccritiquing\u201d the plans being generated by the humans (e", "word_idx": 69036, "sentence_idx": 1053, "label": "unlabeled"}, {"type": "text", "expr": " detecting\nthat an action introduced by the human is not consistent with the\nmodel learned by DUP), and \u201cexplaining/justifying\u201d the suggestions\ngenerated by humans", "word_idx": 69125, "sentence_idx": 1054, "label": "unlabeled"}, {"type": "text", "expr": " Here, we cannot expect causal explanations of the\nsorts that can be generated with the help of complete action models\n(e", "word_idx": 69288, "sentence_idx": 1055, "label": "unlabeled"}, {"type": "text", "expr": "  (22) ), and will have to develop justifications\nanalogous to those used in recommendation systems", "word_idx": 69409, "sentence_idx": 1056, "label": "unlabeled"}, {"type": "text", "expr": "Another potential application for this type of distributed action representations proposed in this paper is social media analysis", "word_idx": 69508, "sentence_idx": 1057, "label": "unlabeled"}, {"type": "text", "expr": " In particular, work such as  (17)  shows that identification of action-outcome relationships can significantly improve the analysis of social media threads", "word_idx": 69637, "sentence_idx": 1058, "label": "unlabeled"}, {"type": "text", "expr": " The challenge of course is that such action-outcome models have to be learned from raw and noisy social media text containing mere fragments of plans", "word_idx": 69793, "sentence_idx": 1059, "label": "unlabeled"}, {"type": "text", "expr": " We believe that action vector models of the type we proposed in this paper provide a promising way of handling this challenge", "word_idx": 69943, "sentence_idx": 1060, "label": "unlabeled"}, {"type": "text", "expr": "Acknowledgements", "word_idx": 70069, "sentence_idx": 1061, "label": "unlabeled"}, {"type": "text", "expr": " \nZhuo thanks the support of the National Key Research and Development Program of China (2016YFB0201900), National Natural Science Foundation of China (U1611262), Guangdong Natural Science Funds for Distinguished Young Scholar (2017A030306028), Pearl River Science and Technology New Star of Guangzhou, and Guangdong Province Key Laboratory of Big Data Analysis and Processing for the support of this research", "word_idx": 70085, "sentence_idx": 1062, "label": "unlabeled"}, {"type": "text", "expr": " Kambhampati\u2019s research is supported in part by the ARO grant\nW911NF-13-1-0023, the ONR grants N00014-13-1-0176, N00014-09-1-0017\nand N00014-07-1-1049, and the NSF grant IIS201330813", "word_idx": 70494, "sentence_idx": 1063, "label": "unlabeled"}, {"type": "text", "expr": "Acknowledgements", "word_idx": 70676, "sentence_idx": 1064, "label": "unlabeled"}, {"type": "text", "expr": "References", "word_idx": 70692, "sentence_idx": 1065, "label": "unlabeled"}, {"type": "text", "expr": "(1) \nAbidi, B", "word_idx": 70702, "sentence_idx": 1066, "label": "unlabeled"}, {"type": "text", "expr": ", Aragam, N", "word_idx": 70715, "sentence_idx": 1067, "label": "unlabeled"}, {"type": "text", "expr": ", Yao, Y", "word_idx": 70726, "sentence_idx": 1068, "label": "unlabeled"}, {"type": "text", "expr": ", Abidi, M", "word_idx": 70734, "sentence_idx": 1069, "label": "unlabeled"}, {"type": "text", "expr": ": Survey and analysis of\nmultimodal sensor planning and integration for wide area surveillance", "word_idx": 70744, "sentence_idx": 1070, "label": "unlabeled"}, {"type": "text", "expr": "Abidi, B", "word_idx": 70838, "sentence_idx": 1071, "label": "unlabeled"}, {"type": "text", "expr": ", Aragam, N", "word_idx": 70846, "sentence_idx": 1072, "label": "unlabeled"}, {"type": "text", "expr": ", Yao, Y", "word_idx": 70857, "sentence_idx": 1073, "label": "unlabeled"}, {"type": "text", "expr": ", Abidi, M", "word_idx": 70865, "sentence_idx": 1074, "label": "unlabeled"}, {"type": "text", "expr": ": Survey and analysis of\nmultimodal sensor planning and integration for wide area surveillance", "word_idx": 70875, "sentence_idx": 1075, "label": "unlabeled"}, {"type": "text", "expr": "ACM Comput", "word_idx": 70969, "sentence_idx": 1076, "label": "unlabeled"}, {"type": "text", "expr": " Surv", "word_idx": 70979, "sentence_idx": 1077, "label": "unlabeled"}, {"type": "text", "expr": "  41 (1), 7:1\u20137:36 (2009)", "word_idx": 70984, "sentence_idx": 1078, "label": "unlabeled"}, {"type": "text", "expr": "(2) \nAlbrecht, S", "word_idx": 71009, "sentence_idx": 1079, "label": "unlabeled"}, {"type": "text", "expr": ", Ramamoorthy, S", "word_idx": 71025, "sentence_idx": 1080, "label": "unlabeled"}, {"type": "text", "expr": ": Are you doing what I think you are doing?\ncriticising uncertain agent models", "word_idx": 71041, "sentence_idx": 1081, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of the Thirty-First Conference on Uncertainty in\nArtificial Intelligence, UAI 2015, July 12-16, 2015, Amsterdam, The\nNetherlands, pp", "word_idx": 71119, "sentence_idx": 1082, "label": "unlabeled"}, {"type": "text", "expr": " 52\u201361 (2015)", "word_idx": 71270, "sentence_idx": 1083, "label": "unlabeled"}, {"type": "text", "expr": "Albrecht, S", "word_idx": 71283, "sentence_idx": 1084, "label": "unlabeled"}, {"type": "text", "expr": ", Ramamoorthy, S", "word_idx": 71294, "sentence_idx": 1085, "label": "unlabeled"}, {"type": "text", "expr": ": Are you doing what I think you are doing?\ncriticising uncertain agent models", "word_idx": 71310, "sentence_idx": 1086, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of the Thirty-First Conference on Uncertainty in\nArtificial Intelligence, UAI 2015, July 12-16, 2015, Amsterdam, The\nNetherlands, pp", "word_idx": 71388, "sentence_idx": 1087, "label": "unlabeled"}, {"type": "text", "expr": " 52\u201361 (2015)", "word_idx": 71536, "sentence_idx": 1088, "label": "unlabeled"}, {"type": "text", "expr": "(3) \nAmir, O", "word_idx": 71549, "sentence_idx": 1089, "label": "unlabeled"}, {"type": "text", "expr": ", Gal, Y", "word_idx": 71561, "sentence_idx": 1090, "label": "unlabeled"}, {"type": "text", "expr": ": Plan recognition in virtual laboratories", "word_idx": 71569, "sentence_idx": 1091, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of IJCAI, pp", "word_idx": 71611, "sentence_idx": 1092, "label": "unlabeled"}, {"type": "text", "expr": " 2392\u20132397 (2011)", "word_idx": 71642, "sentence_idx": 1093, "label": "unlabeled"}, {"type": "text", "expr": "Amir, O", "word_idx": 71659, "sentence_idx": 1094, "label": "unlabeled"}, {"type": "text", "expr": ", Gal, Y", "word_idx": 71666, "sentence_idx": 1095, "label": "unlabeled"}, {"type": "text", "expr": ": Plan recognition in virtual laboratories", "word_idx": 71674, "sentence_idx": 1096, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of IJCAI, pp", "word_idx": 71716, "sentence_idx": 1097, "label": "unlabeled"}, {"type": "text", "expr": " 2392\u20132397 (2011)", "word_idx": 71744, "sentence_idx": 1098, "label": "unlabeled"}, {"type": "text", "expr": "(4) \nBranavan, S", "word_idx": 71761, "sentence_idx": 1099, "label": "unlabeled"}, {"type": "text", "expr": ", Kushman, N", "word_idx": 71777, "sentence_idx": 1100, "label": "unlabeled"}, {"type": "text", "expr": ", Lei, T", "word_idx": 71789, "sentence_idx": 1101, "label": "unlabeled"}, {"type": "text", "expr": ", Barzilay, R", "word_idx": 71797, "sentence_idx": 1102, "label": "unlabeled"}, {"type": "text", "expr": ": Learning high-level planning\nfrom text", "word_idx": 71810, "sentence_idx": 1103, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of ACL-12 (2012)", "word_idx": 71850, "sentence_idx": 1104, "label": "unlabeled"}, {"type": "text", "expr": "Branavan, S", "word_idx": 71885, "sentence_idx": 1105, "label": "unlabeled"}, {"type": "text", "expr": ", Kushman, N", "word_idx": 71896, "sentence_idx": 1106, "label": "unlabeled"}, {"type": "text", "expr": ", Lei, T", "word_idx": 71908, "sentence_idx": 1107, "label": "unlabeled"}, {"type": "text", "expr": ", Barzilay, R", "word_idx": 71916, "sentence_idx": 1108, "label": "unlabeled"}, {"type": "text", "expr": ": Learning high-level planning\nfrom text", "word_idx": 71929, "sentence_idx": 1109, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of ACL-12 (2012)", "word_idx": 71969, "sentence_idx": 1110, "label": "unlabeled"}, {"type": "text", "expr": "(5) \nBui, H", "word_idx": 72001, "sentence_idx": 1111, "label": "unlabeled"}, {"type": "text", "expr": ": A general model for online probabilistic plan recognition", "word_idx": 72012, "sentence_idx": 1112, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of IJCAI, pp", "word_idx": 72071, "sentence_idx": 1113, "label": "unlabeled"}, {"type": "text", "expr": " 1309\u20131318 (2003)", "word_idx": 72102, "sentence_idx": 1114, "label": "unlabeled"}, {"type": "text", "expr": "Bui, H", "word_idx": 72119, "sentence_idx": 1115, "label": "unlabeled"}, {"type": "text", "expr": ": A general model for online probabilistic plan recognition", "word_idx": 72125, "sentence_idx": 1116, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of IJCAI, pp", "word_idx": 72184, "sentence_idx": 1117, "label": "unlabeled"}, {"type": "text", "expr": " 1309\u20131318 (2003)", "word_idx": 72212, "sentence_idx": 1118, "label": "unlabeled"}, {"type": "text", "expr": "(6) \nCohen, P", "word_idx": 72229, "sentence_idx": 1119, "label": "unlabeled"}, {"type": "text", "expr": ", Kaiser, E", "word_idx": 72242, "sentence_idx": 1120, "label": "unlabeled"}, {"type": "text", "expr": ", Buchanan, M", "word_idx": 72253, "sentence_idx": 1121, "label": "unlabeled"}, {"type": "text", "expr": ", Lind, S", "word_idx": 72266, "sentence_idx": 1122, "label": "unlabeled"}, {"type": "text", "expr": ", Corrigan, M", "word_idx": 72275, "sentence_idx": 1123, "label": "unlabeled"}, {"type": "text", "expr": ", Wesson,\nR", "word_idx": 72288, "sentence_idx": 1124, "label": "unlabeled"}, {"type": "text", "expr": ": Sketch-thru-plan: a multimodal interface for command and control", "word_idx": 72299, "sentence_idx": 1125, "label": "unlabeled"}, {"type": "text", "expr": "Cohen, P", "word_idx": 72365, "sentence_idx": 1126, "label": "unlabeled"}, {"type": "text", "expr": ", Kaiser, E", "word_idx": 72373, "sentence_idx": 1127, "label": "unlabeled"}, {"type": "text", "expr": ", Buchanan, M", "word_idx": 72384, "sentence_idx": 1128, "label": "unlabeled"}, {"type": "text", "expr": ", Lind, S", "word_idx": 72397, "sentence_idx": 1129, "label": "unlabeled"}, {"type": "text", "expr": ", Corrigan, M", "word_idx": 72406, "sentence_idx": 1130, "label": "unlabeled"}, {"type": "text", "expr": ", Wesson,\nR", "word_idx": 72419, "sentence_idx": 1131, "label": "unlabeled"}, {"type": "text", "expr": ": Sketch-thru-plan: a multimodal interface for command and control", "word_idx": 72430, "sentence_idx": 1132, "label": "unlabeled"}, {"type": "text", "expr": "Commun", "word_idx": 72496, "sentence_idx": 1133, "label": "unlabeled"}, {"type": "text", "expr": " ACM  58 (4), 56\u201365 (2015)", "word_idx": 72502, "sentence_idx": 1134, "label": "unlabeled"}, {"type": "text", "expr": "(7) \nDong, X", "word_idx": 72528, "sentence_idx": 1135, "label": "unlabeled"}, {"type": "text", "expr": ", Halevy, A", "word_idx": 72540, "sentence_idx": 1136, "label": "unlabeled"}, {"type": "text", "expr": ", Madhavan, J", "word_idx": 72551, "sentence_idx": 1137, "label": "unlabeled"}, {"type": "text", "expr": ", Nemes, E", "word_idx": 72564, "sentence_idx": 1138, "label": "unlabeled"}, {"type": "text", "expr": ", Zhang, J", "word_idx": 72574, "sentence_idx": 1139, "label": "unlabeled"}, {"type": "text", "expr": ": Simlarity search\nfor web services", "word_idx": 72584, "sentence_idx": 1140, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: (e)Proceedings of the Thirtieth International Conference on Very\nLarge Data Bases, pp", "word_idx": 72619, "sentence_idx": 1141, "label": "unlabeled"}, {"type": "text", "expr": " 372\u2013383 (2004)", "word_idx": 72711, "sentence_idx": 1142, "label": "unlabeled"}, {"type": "text", "expr": "Dong, X", "word_idx": 72726, "sentence_idx": 1143, "label": "unlabeled"}, {"type": "text", "expr": ", Halevy, A", "word_idx": 72733, "sentence_idx": 1144, "label": "unlabeled"}, {"type": "text", "expr": ", Madhavan, J", "word_idx": 72744, "sentence_idx": 1145, "label": "unlabeled"}, {"type": "text", "expr": ", Nemes, E", "word_idx": 72757, "sentence_idx": 1146, "label": "unlabeled"}, {"type": "text", "expr": ", Zhang, J", "word_idx": 72767, "sentence_idx": 1147, "label": "unlabeled"}, {"type": "text", "expr": ": Simlarity search\nfor web services", "word_idx": 72777, "sentence_idx": 1148, "label": "unlabeled"}, {"type": "text", "expr": "In: (e)Proceedings of the Thirtieth International Conference on Very\nLarge Data Bases, pp", "word_idx": 72812, "sentence_idx": 1149, "label": "unlabeled"}, {"type": "text", "expr": " 372\u2013383 (2004)", "word_idx": 72901, "sentence_idx": 1150, "label": "unlabeled"}, {"type": "text", "expr": "(8) \nDong, X", "word_idx": 72916, "sentence_idx": 1151, "label": "unlabeled"}, {"type": "text", "expr": ", Halevy, A", "word_idx": 72928, "sentence_idx": 1152, "label": "unlabeled"}, {"type": "text", "expr": ", Madhavan, J", "word_idx": 72939, "sentence_idx": 1153, "label": "unlabeled"}, {"type": "text", "expr": ", Nemes, E", "word_idx": 72952, "sentence_idx": 1154, "label": "unlabeled"}, {"type": "text", "expr": ", Zhang, J", "word_idx": 72962, "sentence_idx": 1155, "label": "unlabeled"}, {"type": "text", "expr": ": Simlarity search\nfor web services", "word_idx": 72972, "sentence_idx": 1156, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of VLDB, pp", "word_idx": 73007, "sentence_idx": 1157, "label": "unlabeled"}, {"type": "text", "expr": " 372\u2013383 (2004)", "word_idx": 73037, "sentence_idx": 1158, "label": "unlabeled"}, {"type": "text", "expr": "Dong, X", "word_idx": 73052, "sentence_idx": 1159, "label": "unlabeled"}, {"type": "text", "expr": ", Halevy, A", "word_idx": 73059, "sentence_idx": 1160, "label": "unlabeled"}, {"type": "text", "expr": ", Madhavan, J", "word_idx": 73070, "sentence_idx": 1161, "label": "unlabeled"}, {"type": "text", "expr": ", Nemes, E", "word_idx": 73083, "sentence_idx": 1162, "label": "unlabeled"}, {"type": "text", "expr": ", Zhang, J", "word_idx": 73093, "sentence_idx": 1163, "label": "unlabeled"}, {"type": "text", "expr": ": Simlarity search\nfor web services", "word_idx": 73103, "sentence_idx": 1164, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of VLDB, pp", "word_idx": 73138, "sentence_idx": 1165, "label": "unlabeled"}, {"type": "text", "expr": " 372\u2013383 (2004)", "word_idx": 73165, "sentence_idx": 1166, "label": "unlabeled"}, {"type": "text", "expr": "(9) \nGeib, C", "word_idx": 73180, "sentence_idx": 1167, "label": "unlabeled"}, {"type": "text", "expr": ", Goldman, R", "word_idx": 73192, "sentence_idx": 1168, "label": "unlabeled"}, {"type": "text", "expr": ": A probabilistic plan recognition algorithm based on\nplan tree grammars", "word_idx": 73204, "sentence_idx": 1169, "label": "unlabeled"}, {"type": "text", "expr": "Geib, C", "word_idx": 73276, "sentence_idx": 1170, "label": "unlabeled"}, {"type": "text", "expr": ", Goldman, R", "word_idx": 73283, "sentence_idx": 1171, "label": "unlabeled"}, {"type": "text", "expr": ": A probabilistic plan recognition algorithm based on\nplan tree grammars", "word_idx": 73295, "sentence_idx": 1172, "label": "unlabeled"}, {"type": "text", "expr": "Artificial Intelligence  173 (11), 1101\u20131132 (2009)", "word_idx": 73367, "sentence_idx": 1173, "label": "unlabeled"}, {"type": "text", "expr": "(10) \nGeib, C", "word_idx": 73418, "sentence_idx": 1174, "label": "unlabeled"}, {"type": "text", "expr": ", Steedman, M", "word_idx": 73431, "sentence_idx": 1175, "label": "unlabeled"}, {"type": "text", "expr": ": On natural language processing and plan recognition", "word_idx": 73444, "sentence_idx": 1176, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: IJCAI 2007, Proceedings of the 20th International Joint\nConference on Artificial Intelligence, Hyderabad, India, January 6-12, 2007,\npp", "word_idx": 73497, "sentence_idx": 1177, "label": "unlabeled"}, {"type": "text", "expr": " 1612\u20131617 (2007)", "word_idx": 73639, "sentence_idx": 1178, "label": "unlabeled"}, {"type": "text", "expr": "Geib, C", "word_idx": 73656, "sentence_idx": 1179, "label": "unlabeled"}, {"type": "text", "expr": ", Steedman, M", "word_idx": 73663, "sentence_idx": 1180, "label": "unlabeled"}, {"type": "text", "expr": ": On natural language processing and plan recognition", "word_idx": 73676, "sentence_idx": 1181, "label": "unlabeled"}, {"type": "text", "expr": "In: IJCAI 2007, Proceedings of the 20th International Joint\nConference on Artificial Intelligence, Hyderabad, India, January 6-12, 2007,\npp", "word_idx": 73729, "sentence_idx": 1182, "label": "unlabeled"}, {"type": "text", "expr": " 1612\u20131617 (2007)", "word_idx": 73868, "sentence_idx": 1183, "label": "unlabeled"}, {"type": "text", "expr": "(11) \nGraves, A", "word_idx": 73885, "sentence_idx": 1184, "label": "unlabeled"}, {"type": "text", "expr": ": Generating sequences with recurrent neural networks", "word_idx": 73900, "sentence_idx": 1185, "label": "unlabeled"}, {"type": "text", "expr": "\n\n URL http://arxiv", "word_idx": 73953, "sentence_idx": 1186, "label": "unlabeled"}, {"type": "text", "expr": "org/abs/1308", "word_idx": 73972, "sentence_idx": 1187, "label": "unlabeled"}, {"type": "text", "expr": "Graves, A", "word_idx": 73984, "sentence_idx": 1188, "label": "unlabeled"}, {"type": "text", "expr": ": Generating sequences with recurrent neural networks", "word_idx": 73993, "sentence_idx": 1189, "label": "unlabeled"}, {"type": "text", "expr": "CoRR  abs/1308", "word_idx": 74046, "sentence_idx": 1190, "label": "unlabeled"}, {"type": "text", "expr": "0850  (2013)", "word_idx": 74060, "sentence_idx": 1191, "label": "unlabeled"}, {"type": "text", "expr": "abs/1308", "word_idx": 74072, "sentence_idx": 1192, "label": "unlabeled"}, {"type": "text", "expr": "URL http://arxiv", "word_idx": 74080, "sentence_idx": 1193, "label": "unlabeled"}, {"type": "text", "expr": "org/abs/1308", "word_idx": 74096, "sentence_idx": 1194, "label": "unlabeled"}, {"type": "text", "expr": "(12) \nKabanza, F", "word_idx": 74108, "sentence_idx": 1195, "label": "unlabeled"}, {"type": "text", "expr": ", Filion, J", "word_idx": 74124, "sentence_idx": 1196, "label": "unlabeled"}, {"type": "text", "expr": ", Benaskeur, A", "word_idx": 74135, "sentence_idx": 1197, "label": "unlabeled"}, {"type": "text", "expr": ", Irandoust, H", "word_idx": 74149, "sentence_idx": 1198, "label": "unlabeled"}, {"type": "text", "expr": ": Controlling the\nhypothesis space in probabilistic plan recognition", "word_idx": 74163, "sentence_idx": 1199, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: IJCAI (2013)", "word_idx": 74231, "sentence_idx": 1200, "label": "unlabeled"}, {"type": "text", "expr": "Kabanza, F", "word_idx": 74250, "sentence_idx": 1201, "label": "unlabeled"}, {"type": "text", "expr": ", Filion, J", "word_idx": 74260, "sentence_idx": 1202, "label": "unlabeled"}, {"type": "text", "expr": ", Benaskeur, A", "word_idx": 74271, "sentence_idx": 1203, "label": "unlabeled"}, {"type": "text", "expr": ", Irandoust, H", "word_idx": 74285, "sentence_idx": 1204, "label": "unlabeled"}, {"type": "text", "expr": ": Controlling the\nhypothesis space in probabilistic plan recognition", "word_idx": 74299, "sentence_idx": 1205, "label": "unlabeled"}, {"type": "text", "expr": "In: IJCAI (2013)", "word_idx": 74367, "sentence_idx": 1206, "label": "unlabeled"}, {"type": "text", "expr": "(13) \nKambhampati, S", "word_idx": 74383, "sentence_idx": 1207, "label": "unlabeled"}, {"type": "text", "expr": ": Model-lite planning for the web age masses: The challenges of\nplanning with incomplete and evolving domain models", "word_idx": 74403, "sentence_idx": 1208, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of the Twenty-Second AAAI Conference on Artificial\nIntelligence, pp", "word_idx": 74518, "sentence_idx": 1209, "label": "unlabeled"}, {"type": "text", "expr": " 1601\u20131605 (2007)", "word_idx": 74604, "sentence_idx": 1210, "label": "unlabeled"}, {"type": "text", "expr": "Kambhampati, S", "word_idx": 74621, "sentence_idx": 1211, "label": "unlabeled"}, {"type": "text", "expr": ": Model-lite planning for the web age masses: The challenges of\nplanning with incomplete and evolving domain models", "word_idx": 74635, "sentence_idx": 1212, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of the Twenty-Second AAAI Conference on Artificial\nIntelligence, pp", "word_idx": 74750, "sentence_idx": 1213, "label": "unlabeled"}, {"type": "text", "expr": " 1601\u20131605 (2007)", "word_idx": 74833, "sentence_idx": 1214, "label": "unlabeled"}, {"type": "text", "expr": "(14) \nKambhampati, S", "word_idx": 74850, "sentence_idx": 1215, "label": "unlabeled"}, {"type": "text", "expr": ": Model-lite planning for the web age masses: The challenges of\nplanning with incomplete and evolving domain theories", "word_idx": 74870, "sentence_idx": 1216, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of AAAI, pp", "word_idx": 74987, "sentence_idx": 1217, "label": "unlabeled"}, {"type": "text", "expr": " 1601\u20131605 (2007)", "word_idx": 75017, "sentence_idx": 1218, "label": "unlabeled"}, {"type": "text", "expr": "Kambhampati, S", "word_idx": 75034, "sentence_idx": 1219, "label": "unlabeled"}, {"type": "text", "expr": ": Model-lite planning for the web age masses: The challenges of\nplanning with incomplete and evolving domain theories", "word_idx": 75048, "sentence_idx": 1220, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of AAAI, pp", "word_idx": 75165, "sentence_idx": 1221, "label": "unlabeled"}, {"type": "text", "expr": " 1601\u20131605 (2007)", "word_idx": 75192, "sentence_idx": 1222, "label": "unlabeled"}, {"type": "text", "expr": "(15) \nKambhampati, S", "word_idx": 75209, "sentence_idx": 1223, "label": "unlabeled"}, {"type": "text", "expr": ", Talamadupula, K", "word_idx": 75229, "sentence_idx": 1224, "label": "unlabeled"}, {"type": "text", "expr": ": Human-in-the-loop planning and decision\nsupport (2015)", "word_idx": 75246, "sentence_idx": 1225, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Rakaposhi", "word_idx": 75302, "sentence_idx": 1226, "label": "unlabeled"}, {"type": "text", "expr": "edu/hilp-tutorial", "word_idx": 75314, "sentence_idx": 1227, "label": "unlabeled"}, {"type": "text", "expr": "Kambhampati, S", "word_idx": 75331, "sentence_idx": 1228, "label": "unlabeled"}, {"type": "text", "expr": ", Talamadupula, K", "word_idx": 75345, "sentence_idx": 1229, "label": "unlabeled"}, {"type": "text", "expr": ": Human-in-the-loop planning and decision\nsupport (2015)", "word_idx": 75362, "sentence_idx": 1230, "label": "unlabeled"}, {"type": "text", "expr": "Rakaposhi", "word_idx": 75418, "sentence_idx": 1231, "label": "unlabeled"}, {"type": "text", "expr": "edu/hilp-tutorial", "word_idx": 75427, "sentence_idx": 1232, "label": "unlabeled"}, {"type": "text", "expr": "(16) \nKautz, H", "word_idx": 75444, "sentence_idx": 1233, "label": "unlabeled"}, {"type": "text", "expr": ", Allen, J", "word_idx": 75458, "sentence_idx": 1234, "label": "unlabeled"}, {"type": "text", "expr": ": Generalized plan recognition", "word_idx": 75468, "sentence_idx": 1235, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of AAAI, pp", "word_idx": 75498, "sentence_idx": 1236, "label": "unlabeled"}, {"type": "text", "expr": " 32\u201337 (1986)", "word_idx": 75528, "sentence_idx": 1237, "label": "unlabeled"}, {"type": "text", "expr": "Kautz, H", "word_idx": 75541, "sentence_idx": 1238, "label": "unlabeled"}, {"type": "text", "expr": ", Allen, J", "word_idx": 75549, "sentence_idx": 1239, "label": "unlabeled"}, {"type": "text", "expr": ": Generalized plan recognition", "word_idx": 75559, "sentence_idx": 1240, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of AAAI, pp", "word_idx": 75589, "sentence_idx": 1241, "label": "unlabeled"}, {"type": "text", "expr": " 32\u201337 (1986)", "word_idx": 75616, "sentence_idx": 1242, "label": "unlabeled"}, {"type": "text", "expr": "(17) \nK\u0131c\u0131man, E", "word_idx": 75629, "sentence_idx": 1243, "label": "unlabeled"}, {"type": "text", "expr": ", Richardson, M", "word_idx": 75645, "sentence_idx": 1244, "label": "unlabeled"}, {"type": "text", "expr": ": Towards decision support and goal\nachievement: Identifying action-outcome relationships from social media", "word_idx": 75660, "sentence_idx": 1245, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of the 21th ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, pp", "word_idx": 75767, "sentence_idx": 1246, "label": "unlabeled"}, {"type": "text", "expr": " 547\u2013556", "word_idx": 75876, "sentence_idx": 1247, "label": "unlabeled"}, {"type": "text", "expr": " ACM (2015)", "word_idx": 75884, "sentence_idx": 1248, "label": "unlabeled"}, {"type": "text", "expr": "K\u0131c\u0131man, E", "word_idx": 75895, "sentence_idx": 1249, "label": "unlabeled"}, {"type": "text", "expr": ", Richardson, M", "word_idx": 75905, "sentence_idx": 1250, "label": "unlabeled"}, {"type": "text", "expr": ": Towards decision support and goal\nachievement: Identifying action-outcome relationships from social media", "word_idx": 75920, "sentence_idx": 1251, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of the 21th ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, pp", "word_idx": 76027, "sentence_idx": 1252, "label": "unlabeled"}, {"type": "text", "expr": " 547\u2013556", "word_idx": 76133, "sentence_idx": 1253, "label": "unlabeled"}, {"type": "text", "expr": " ACM (2015)", "word_idx": 76141, "sentence_idx": 1254, "label": "unlabeled"}, {"type": "text", "expr": "(18) \nLesh, N", "word_idx": 76152, "sentence_idx": 1255, "label": "unlabeled"}, {"type": "text", "expr": ", Etzioni, O", "word_idx": 76165, "sentence_idx": 1256, "label": "unlabeled"}, {"type": "text", "expr": ": A sound and fast goal recognizer", "word_idx": 76177, "sentence_idx": 1257, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: IJCAI, pp", "word_idx": 76211, "sentence_idx": 1258, "label": "unlabeled"}, {"type": "text", "expr": " 1704\u20131710 (1995)", "word_idx": 76227, "sentence_idx": 1259, "label": "unlabeled"}, {"type": "text", "expr": "Lesh, N", "word_idx": 76244, "sentence_idx": 1260, "label": "unlabeled"}, {"type": "text", "expr": ", Etzioni, O", "word_idx": 76251, "sentence_idx": 1261, "label": "unlabeled"}, {"type": "text", "expr": ": A sound and fast goal recognizer", "word_idx": 76263, "sentence_idx": 1262, "label": "unlabeled"}, {"type": "text", "expr": "In: IJCAI, pp", "word_idx": 76297, "sentence_idx": 1263, "label": "unlabeled"}, {"type": "text", "expr": " 1704\u20131710 (1995)", "word_idx": 76310, "sentence_idx": 1264, "label": "unlabeled"}, {"type": "text", "expr": "(19) \nManikonda, L", "word_idx": 76327, "sentence_idx": 1265, "label": "unlabeled"}, {"type": "text", "expr": ", Chakraborti, T", "word_idx": 76345, "sentence_idx": 1266, "label": "unlabeled"}, {"type": "text", "expr": ", De, S", "word_idx": 76361, "sentence_idx": 1267, "label": "unlabeled"}, {"type": "text", "expr": ", Talamadupula, K", "word_idx": 76368, "sentence_idx": 1268, "label": "unlabeled"}, {"type": "text", "expr": ", Kambhampati, S", "word_idx": 76385, "sentence_idx": 1269, "label": "unlabeled"}, {"type": "text", "expr": ":\nAI-MIX: using automated planning to steer human workers towards better\ncrowdsourced plans", "word_idx": 76401, "sentence_idx": 1270, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of the Twenty-Eighth AAAI Conference on Artificial\nIntelligence, pp", "word_idx": 76492, "sentence_idx": 1271, "label": "unlabeled"}, {"type": "text", "expr": " 3004\u20133009 (2014)", "word_idx": 76578, "sentence_idx": 1272, "label": "unlabeled"}, {"type": "text", "expr": "Manikonda, L", "word_idx": 76595, "sentence_idx": 1273, "label": "unlabeled"}, {"type": "text", "expr": ", Chakraborti, T", "word_idx": 76607, "sentence_idx": 1274, "label": "unlabeled"}, {"type": "text", "expr": ", De, S", "word_idx": 76623, "sentence_idx": 1275, "label": "unlabeled"}, {"type": "text", "expr": ", Talamadupula, K", "word_idx": 76630, "sentence_idx": 1276, "label": "unlabeled"}, {"type": "text", "expr": ", Kambhampati, S", "word_idx": 76647, "sentence_idx": 1277, "label": "unlabeled"}, {"type": "text", "expr": ":\nAI-MIX: using automated planning to steer human workers towards better\ncrowdsourced plans", "word_idx": 76663, "sentence_idx": 1278, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of the Twenty-Eighth AAAI Conference on Artificial\nIntelligence, pp", "word_idx": 76754, "sentence_idx": 1279, "label": "unlabeled"}, {"type": "text", "expr": " 3004\u20133009 (2014)", "word_idx": 76837, "sentence_idx": 1280, "label": "unlabeled"}, {"type": "text", "expr": "(20) \nMikolov, T", "word_idx": 76854, "sentence_idx": 1281, "label": "unlabeled"}, {"type": "text", "expr": ", Sutskever, I", "word_idx": 76870, "sentence_idx": 1282, "label": "unlabeled"}, {"type": "text", "expr": ", Chen, K", "word_idx": 76884, "sentence_idx": 1283, "label": "unlabeled"}, {"type": "text", "expr": ", Corrado, G", "word_idx": 76893, "sentence_idx": 1284, "label": "unlabeled"}, {"type": "text", "expr": ", Dean, J", "word_idx": 76905, "sentence_idx": 1285, "label": "unlabeled"}, {"type": "text", "expr": ": Distributed\nrepresentations of words and phrases and their compositionality", "word_idx": 76914, "sentence_idx": 1286, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: NIPS, pp", "word_idx": 76991, "sentence_idx": 1287, "label": "unlabeled"}, {"type": "text", "expr": " 3111\u20133119 (2013)", "word_idx": 77006, "sentence_idx": 1288, "label": "unlabeled"}, {"type": "text", "expr": "Mikolov, T", "word_idx": 77023, "sentence_idx": 1289, "label": "unlabeled"}, {"type": "text", "expr": ", Sutskever, I", "word_idx": 77033, "sentence_idx": 1290, "label": "unlabeled"}, {"type": "text", "expr": ", Chen, K", "word_idx": 77047, "sentence_idx": 1291, "label": "unlabeled"}, {"type": "text", "expr": ", Corrado, G", "word_idx": 77056, "sentence_idx": 1292, "label": "unlabeled"}, {"type": "text", "expr": ", Dean, J", "word_idx": 77068, "sentence_idx": 1293, "label": "unlabeled"}, {"type": "text", "expr": ": Distributed\nrepresentations of words and phrases and their compositionality", "word_idx": 77077, "sentence_idx": 1294, "label": "unlabeled"}, {"type": "text", "expr": "In: NIPS, pp", "word_idx": 77154, "sentence_idx": 1295, "label": "unlabeled"}, {"type": "text", "expr": " 3111\u20133119 (2013)", "word_idx": 77166, "sentence_idx": 1296, "label": "unlabeled"}, {"type": "text", "expr": "(21) \nNg, A", "word_idx": 77183, "sentence_idx": 1297, "label": "unlabeled"}, {"type": "text", "expr": ", Kim, H", "word_idx": 77194, "sentence_idx": 1298, "label": "unlabeled"}, {"type": "text", "expr": ", Jordan, M", "word_idx": 77202, "sentence_idx": 1299, "label": "unlabeled"}, {"type": "text", "expr": ", Sastry, S", "word_idx": 77213, "sentence_idx": 1300, "label": "unlabeled"}, {"type": "text", "expr": ": Autonomous helicopter flight via\nreinforcement learning", "word_idx": 77224, "sentence_idx": 1301, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of NIPS-03 (2003)", "word_idx": 77281, "sentence_idx": 1302, "label": "unlabeled"}, {"type": "text", "expr": "Ng, A", "word_idx": 77317, "sentence_idx": 1303, "label": "unlabeled"}, {"type": "text", "expr": ", Kim, H", "word_idx": 77322, "sentence_idx": 1304, "label": "unlabeled"}, {"type": "text", "expr": ", Jordan, M", "word_idx": 77330, "sentence_idx": 1305, "label": "unlabeled"}, {"type": "text", "expr": ", Sastry, S", "word_idx": 77341, "sentence_idx": 1306, "label": "unlabeled"}, {"type": "text", "expr": ": Autonomous helicopter flight via\nreinforcement learning", "word_idx": 77352, "sentence_idx": 1307, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of NIPS-03 (2003)", "word_idx": 77409, "sentence_idx": 1308, "label": "unlabeled"}, {"type": "text", "expr": "(22) \nPetrie, C", "word_idx": 77442, "sentence_idx": 1309, "label": "unlabeled"}, {"type": "text", "expr": ": Constrained decision revision", "word_idx": 77457, "sentence_idx": 1310, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of the 10th National Conference on Artificial\nIntelligence", "word_idx": 77488, "sentence_idx": 1311, "label": "unlabeled"}, {"type": "text", "expr": " San Jose, CA, July 12-16, 1992", "word_idx": 77565, "sentence_idx": 1312, "label": "unlabeled"}, {"type": "text", "expr": " 393\u2013400 (1992)", "word_idx": 77596, "sentence_idx": 1313, "label": "unlabeled"}, {"type": "text", "expr": "Petrie, C", "word_idx": 77611, "sentence_idx": 1314, "label": "unlabeled"}, {"type": "text", "expr": ": Constrained decision revision", "word_idx": 77620, "sentence_idx": 1315, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of the 10th National Conference on Artificial\nIntelligence", "word_idx": 77651, "sentence_idx": 1316, "label": "unlabeled"}, {"type": "text", "expr": " San Jose, CA, July 12-16, 1992", "word_idx": 77725, "sentence_idx": 1317, "label": "unlabeled"}, {"type": "text", "expr": " 393\u2013400 (1992)", "word_idx": 77756, "sentence_idx": 1318, "label": "unlabeled"}, {"type": "text", "expr": "(23) \nRam\u00edrez, M", "word_idx": 77771, "sentence_idx": 1319, "label": "unlabeled"}, {"type": "text", "expr": ", Geffner, H", "word_idx": 77787, "sentence_idx": 1320, "label": "unlabeled"}, {"type": "text", "expr": ": Plan recognition as planning", "word_idx": 77799, "sentence_idx": 1321, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: IJCAI 2009, Proceedings of the 21st International Joint\nConference on Artificial Intelligence, Pasadena, California, USA, July 11-17,\n2009, pp", "word_idx": 77829, "sentence_idx": 1322, "label": "unlabeled"}, {"type": "text", "expr": " 1778\u20131783 (2009)", "word_idx": 77978, "sentence_idx": 1323, "label": "unlabeled"}, {"type": "text", "expr": "Ram\u00edrez, M", "word_idx": 77995, "sentence_idx": 1324, "label": "unlabeled"}, {"type": "text", "expr": ", Geffner, H", "word_idx": 78005, "sentence_idx": 1325, "label": "unlabeled"}, {"type": "text", "expr": ": Plan recognition as planning", "word_idx": 78017, "sentence_idx": 1326, "label": "unlabeled"}, {"type": "text", "expr": "In: IJCAI 2009, Proceedings of the 21st International Joint\nConference on Artificial Intelligence, Pasadena, California, USA, July 11-17,\n2009, pp", "word_idx": 78047, "sentence_idx": 1327, "label": "unlabeled"}, {"type": "text", "expr": " 1778\u20131783 (2009)", "word_idx": 78193, "sentence_idx": 1328, "label": "unlabeled"}, {"type": "text", "expr": "(24) \nRamirez, M", "word_idx": 78210, "sentence_idx": 1329, "label": "unlabeled"}, {"type": "text", "expr": ", Geffner, H", "word_idx": 78226, "sentence_idx": 1330, "label": "unlabeled"}, {"type": "text", "expr": ": Plan recognition as planning", "word_idx": 78238, "sentence_idx": 1331, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of IJCAI, pp", "word_idx": 78268, "sentence_idx": 1332, "label": "unlabeled"}, {"type": "text", "expr": " 1778\u20131783 (2009)", "word_idx": 78299, "sentence_idx": 1333, "label": "unlabeled"}, {"type": "text", "expr": "Ramirez, M", "word_idx": 78316, "sentence_idx": 1334, "label": "unlabeled"}, {"type": "text", "expr": ", Geffner, H", "word_idx": 78326, "sentence_idx": 1335, "label": "unlabeled"}, {"type": "text", "expr": ": Plan recognition as planning", "word_idx": 78338, "sentence_idx": 1336, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of IJCAI, pp", "word_idx": 78368, "sentence_idx": 1337, "label": "unlabeled"}, {"type": "text", "expr": " 1778\u20131783 (2009)", "word_idx": 78396, "sentence_idx": 1338, "label": "unlabeled"}, {"type": "text", "expr": "(25) \nRamirez, M", "word_idx": 78413, "sentence_idx": 1339, "label": "unlabeled"}, {"type": "text", "expr": ", Geffner, H", "word_idx": 78429, "sentence_idx": 1340, "label": "unlabeled"}, {"type": "text", "expr": ": Probabilistic plan recognition using off-the-shelf\nclassical planners", "word_idx": 78441, "sentence_idx": 1341, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of AAAI, pp", "word_idx": 78512, "sentence_idx": 1342, "label": "unlabeled"}, {"type": "text", "expr": " 1121\u20131126 (2010)", "word_idx": 78542, "sentence_idx": 1343, "label": "unlabeled"}, {"type": "text", "expr": "Ramirez, M", "word_idx": 78559, "sentence_idx": 1344, "label": "unlabeled"}, {"type": "text", "expr": ", Geffner, H", "word_idx": 78569, "sentence_idx": 1345, "label": "unlabeled"}, {"type": "text", "expr": ": Probabilistic plan recognition using off-the-shelf\nclassical planners", "word_idx": 78581, "sentence_idx": 1346, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of AAAI, pp", "word_idx": 78652, "sentence_idx": 1347, "label": "unlabeled"}, {"type": "text", "expr": " 1121\u20131126 (2010)", "word_idx": 78679, "sentence_idx": 1348, "label": "unlabeled"}, {"type": "text", "expr": "(26) \nSaria, S", "word_idx": 78696, "sentence_idx": 1349, "label": "unlabeled"}, {"type": "text", "expr": ", Mahadevan, S", "word_idx": 78710, "sentence_idx": 1350, "label": "unlabeled"}, {"type": "text", "expr": ": Probabilistic plan recognitionin multiagent systems", "word_idx": 78724, "sentence_idx": 1351, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of AAAI (2004)", "word_idx": 78777, "sentence_idx": 1352, "label": "unlabeled"}, {"type": "text", "expr": "Saria, S", "word_idx": 78810, "sentence_idx": 1353, "label": "unlabeled"}, {"type": "text", "expr": ", Mahadevan, S", "word_idx": 78818, "sentence_idx": 1354, "label": "unlabeled"}, {"type": "text", "expr": ": Probabilistic plan recognitionin multiagent systems", "word_idx": 78832, "sentence_idx": 1355, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of AAAI (2004)", "word_idx": 78885, "sentence_idx": 1356, "label": "unlabeled"}, {"type": "text", "expr": "(27) \nShi, X", "word_idx": 78915, "sentence_idx": 1357, "label": "unlabeled"}, {"type": "text", "expr": ", Chen, Z", "word_idx": 78927, "sentence_idx": 1358, "label": "unlabeled"}, {"type": "text", "expr": ", Wang, H", "word_idx": 78936, "sentence_idx": 1359, "label": "unlabeled"}, {"type": "text", "expr": ", Yeung, D", "word_idx": 78945, "sentence_idx": 1360, "label": "unlabeled"}, {"type": "text", "expr": ", Wong, W", "word_idx": 78955, "sentence_idx": 1361, "label": "unlabeled"}, {"type": "text", "expr": ", Woo, W", "word_idx": 78964, "sentence_idx": 1362, "label": "unlabeled"}, {"type": "text", "expr": ": Convolutional LSTM\nnetwork: A machine learning approach for precipitation nowcasting", "word_idx": 78972, "sentence_idx": 1363, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Advances in Neural Information Processing Systems 28: Annual\nConference on Neural Information Processing Systems 2015, December 7-12,\n2015, Montreal, Quebec, Canada, pp", "word_idx": 79058, "sentence_idx": 1364, "label": "unlabeled"}, {"type": "text", "expr": " 802\u2013810 (2015)", "word_idx": 79233, "sentence_idx": 1365, "label": "unlabeled"}, {"type": "text", "expr": "Shi, X", "word_idx": 79248, "sentence_idx": 1366, "label": "unlabeled"}, {"type": "text", "expr": ", Chen, Z", "word_idx": 79254, "sentence_idx": 1367, "label": "unlabeled"}, {"type": "text", "expr": ", Wang, H", "word_idx": 79263, "sentence_idx": 1368, "label": "unlabeled"}, {"type": "text", "expr": ", Yeung, D", "word_idx": 79272, "sentence_idx": 1369, "label": "unlabeled"}, {"type": "text", "expr": ", Wong, W", "word_idx": 79282, "sentence_idx": 1370, "label": "unlabeled"}, {"type": "text", "expr": ", Woo, W", "word_idx": 79291, "sentence_idx": 1371, "label": "unlabeled"}, {"type": "text", "expr": ": Convolutional LSTM\nnetwork: A machine learning approach for precipitation nowcasting", "word_idx": 79299, "sentence_idx": 1372, "label": "unlabeled"}, {"type": "text", "expr": "In: Advances in Neural Information Processing Systems 28: Annual\nConference on Neural Information Processing Systems 2015, December 7-12,\n2015, Montreal, Quebec, Canada, pp", "word_idx": 79385, "sentence_idx": 1373, "label": "unlabeled"}, {"type": "text", "expr": " 802\u2013810 (2015)", "word_idx": 79557, "sentence_idx": 1374, "label": "unlabeled"}, {"type": "text", "expr": "(28) \nTian, X", "word_idx": 79572, "sentence_idx": 1375, "label": "unlabeled"}, {"type": "text", "expr": ", Zhuo, H", "word_idx": 79585, "sentence_idx": 1376, "label": "unlabeled"}, {"type": "text", "expr": ", Kambhampati, S", "word_idx": 79594, "sentence_idx": 1377, "label": "unlabeled"}, {"type": "text", "expr": ": Discovering underlying plans based on\ndistributed representations of actions", "word_idx": 79610, "sentence_idx": 1378, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of the 2016 International Conference on Autonomous\nAgents & Multiagent Systems, Singapore, May 9-13, 2016, pp", "word_idx": 79688, "sentence_idx": 1379, "label": "unlabeled"}, {"type": "text", "expr": " 1135\u20131143\n(2016)", "word_idx": 79816, "sentence_idx": 1380, "label": "unlabeled"}, {"type": "text", "expr": "Tian, X", "word_idx": 79833, "sentence_idx": 1381, "label": "unlabeled"}, {"type": "text", "expr": ", Zhuo, H", "word_idx": 79840, "sentence_idx": 1382, "label": "unlabeled"}, {"type": "text", "expr": ", Kambhampati, S", "word_idx": 79849, "sentence_idx": 1383, "label": "unlabeled"}, {"type": "text", "expr": ": Discovering underlying plans based on\ndistributed representations of actions", "word_idx": 79865, "sentence_idx": 1384, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of the 2016 International Conference on Autonomous\nAgents & Multiagent Systems, Singapore, May 9-13, 2016, pp", "word_idx": 79943, "sentence_idx": 1385, "label": "unlabeled"}, {"type": "text", "expr": " 1135\u20131143\n(2016)", "word_idx": 80068, "sentence_idx": 1386, "label": "unlabeled"}, {"type": "text", "expr": "(29) \nTuan\u00a0Nguyen Subbarao\u00a0Kambhampati, M", "word_idx": 80085, "sentence_idx": 1387, "label": "unlabeled"}, {"type": "text", "expr": ": Synthesizing robust plans under\nincomplete domain models", "word_idx": 80126, "sentence_idx": 1388, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proc", "word_idx": 80184, "sentence_idx": 1389, "label": "unlabeled"}, {"type": "text", "expr": " AAAI Workshop on Generalized Planning (2011)", "word_idx": 80195, "sentence_idx": 1390, "label": "unlabeled"}, {"type": "text", "expr": "Tuan\u00a0Nguyen Subbarao\u00a0Kambhampati, M", "word_idx": 80240, "sentence_idx": 1391, "label": "unlabeled"}, {"type": "text", "expr": ": Synthesizing robust plans under\nincomplete domain models", "word_idx": 80275, "sentence_idx": 1392, "label": "unlabeled"}, {"type": "text", "expr": "In: Proc", "word_idx": 80333, "sentence_idx": 1393, "label": "unlabeled"}, {"type": "text", "expr": " AAAI Workshop on Generalized Planning (2011)", "word_idx": 80341, "sentence_idx": 1394, "label": "unlabeled"}, {"type": "text", "expr": "(30) \nYang, Q", "word_idx": 80386, "sentence_idx": 1395, "label": "unlabeled"}, {"type": "text", "expr": ", Wu, K", "word_idx": 80399, "sentence_idx": 1396, "label": "unlabeled"}, {"type": "text", "expr": ", Jiang, Y", "word_idx": 80406, "sentence_idx": 1397, "label": "unlabeled"}, {"type": "text", "expr": ": Learning action models from plan examples using\nweighted MAX-SAT", "word_idx": 80416, "sentence_idx": 1398, "label": "unlabeled"}, {"type": "text", "expr": "Yang, Q", "word_idx": 80482, "sentence_idx": 1399, "label": "unlabeled"}, {"type": "text", "expr": ", Wu, K", "word_idx": 80489, "sentence_idx": 1400, "label": "unlabeled"}, {"type": "text", "expr": ", Jiang, Y", "word_idx": 80496, "sentence_idx": 1401, "label": "unlabeled"}, {"type": "text", "expr": ": Learning action models from plan examples using\nweighted MAX-SAT", "word_idx": 80506, "sentence_idx": 1402, "label": "unlabeled"}, {"type": "text", "expr": "Artificial Intelligence Journal  171 , 107\u2013143 (2007)", "word_idx": 80572, "sentence_idx": 1403, "label": "unlabeled"}, {"type": "text", "expr": "(31) \nZhang, Y", "word_idx": 80625, "sentence_idx": 1404, "label": "unlabeled"}, {"type": "text", "expr": ", Sreedharan, S", "word_idx": 80639, "sentence_idx": 1405, "label": "unlabeled"}, {"type": "text", "expr": ", Kambhampati, S", "word_idx": 80654, "sentence_idx": 1406, "label": "unlabeled"}, {"type": "text", "expr": ": Capability models and their\napplications in planning", "word_idx": 80670, "sentence_idx": 1407, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of AAMAS, pp", "word_idx": 80724, "sentence_idx": 1408, "label": "unlabeled"}, {"type": "text", "expr": " 1151\u20131159 (2015)", "word_idx": 80755, "sentence_idx": 1409, "label": "unlabeled"}, {"type": "text", "expr": "Zhang, Y", "word_idx": 80772, "sentence_idx": 1410, "label": "unlabeled"}, {"type": "text", "expr": ", Sreedharan, S", "word_idx": 80780, "sentence_idx": 1411, "label": "unlabeled"}, {"type": "text", "expr": ", Kambhampati, S", "word_idx": 80795, "sentence_idx": 1412, "label": "unlabeled"}, {"type": "text", "expr": ": Capability models and their\napplications in planning", "word_idx": 80811, "sentence_idx": 1413, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of AAMAS, pp", "word_idx": 80865, "sentence_idx": 1414, "label": "unlabeled"}, {"type": "text", "expr": " 1151\u20131159 (2015)", "word_idx": 80893, "sentence_idx": 1415, "label": "unlabeled"}, {"type": "text", "expr": "(32) \nZhuo, H", "word_idx": 80910, "sentence_idx": 1416, "label": "unlabeled"}, {"type": "text", "expr": ", Kambhampati, S", "word_idx": 80923, "sentence_idx": 1417, "label": "unlabeled"}, {"type": "text", "expr": ": Model-lite planning: Case-based vs", "word_idx": 80939, "sentence_idx": 1418, "label": "unlabeled"}, {"type": "text", "expr": " model-based\napproaches", "word_idx": 80975, "sentence_idx": 1419, "label": "unlabeled"}, {"type": "text", "expr": "Zhuo, H", "word_idx": 80998, "sentence_idx": 1420, "label": "unlabeled"}, {"type": "text", "expr": ", Kambhampati, S", "word_idx": 81005, "sentence_idx": 1421, "label": "unlabeled"}, {"type": "text", "expr": ": Model-lite planning: Case-based vs", "word_idx": 81021, "sentence_idx": 1422, "label": "unlabeled"}, {"type": "text", "expr": " model-based\napproaches", "word_idx": 81057, "sentence_idx": 1423, "label": "unlabeled"}, {"type": "text", "expr": "Artif", "word_idx": 81080, "sentence_idx": 1424, "label": "unlabeled"}, {"type": "text", "expr": " Intell", "word_idx": 81085, "sentence_idx": 1425, "label": "unlabeled"}, {"type": "text", "expr": "  246 , 1\u201321 (2017)", "word_idx": 81092, "sentence_idx": 1426, "label": "unlabeled"}, {"type": "text", "expr": "(33) \nZhuo, H", "word_idx": 81111, "sentence_idx": 1427, "label": "unlabeled"}, {"type": "text", "expr": ", Li, L", "word_idx": 81124, "sentence_idx": 1428, "label": "unlabeled"}, {"type": "text", "expr": ": Multi-agent plan recognition with partial team traces and\nplan libraries", "word_idx": 81131, "sentence_idx": 1429, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of IJCAI, pp", "word_idx": 81205, "sentence_idx": 1430, "label": "unlabeled"}, {"type": "text", "expr": " 484\u2013489 (2011)", "word_idx": 81236, "sentence_idx": 1431, "label": "unlabeled"}, {"type": "text", "expr": "Zhuo, H", "word_idx": 81251, "sentence_idx": 1432, "label": "unlabeled"}, {"type": "text", "expr": ", Li, L", "word_idx": 81258, "sentence_idx": 1433, "label": "unlabeled"}, {"type": "text", "expr": ": Multi-agent plan recognition with partial team traces and\nplan libraries", "word_idx": 81265, "sentence_idx": 1434, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of IJCAI, pp", "word_idx": 81339, "sentence_idx": 1435, "label": "unlabeled"}, {"type": "text", "expr": " 484\u2013489 (2011)", "word_idx": 81367, "sentence_idx": 1436, "label": "unlabeled"}, {"type": "text", "expr": "(34) \nZhuo, H", "word_idx": 81382, "sentence_idx": 1437, "label": "unlabeled"}, {"type": "text", "expr": ", Nguyen, T", "word_idx": 81395, "sentence_idx": 1438, "label": "unlabeled"}, {"type": "text", "expr": ", Kambhampati, S", "word_idx": 81406, "sentence_idx": 1439, "label": "unlabeled"}, {"type": "text", "expr": ": Model-lite case-based planning", "word_idx": 81422, "sentence_idx": 1440, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: AAAI (2013)", "word_idx": 81454, "sentence_idx": 1441, "label": "unlabeled"}, {"type": "text", "expr": "Zhuo, H", "word_idx": 81472, "sentence_idx": 1442, "label": "unlabeled"}, {"type": "text", "expr": ", Nguyen, T", "word_idx": 81479, "sentence_idx": 1443, "label": "unlabeled"}, {"type": "text", "expr": ", Kambhampati, S", "word_idx": 81490, "sentence_idx": 1444, "label": "unlabeled"}, {"type": "text", "expr": ": Model-lite case-based planning", "word_idx": 81506, "sentence_idx": 1445, "label": "unlabeled"}, {"type": "text", "expr": "In: AAAI (2013)", "word_idx": 81538, "sentence_idx": 1446, "label": "unlabeled"}, {"type": "text", "expr": "(35) \nZhuo, H", "word_idx": 81553, "sentence_idx": 1447, "label": "unlabeled"}, {"type": "text", "expr": ", Yang, Q", "word_idx": 81566, "sentence_idx": 1448, "label": "unlabeled"}, {"type": "text", "expr": ", Hu, D", "word_idx": 81575, "sentence_idx": 1449, "label": "unlabeled"}, {"type": "text", "expr": ", Li, L", "word_idx": 81582, "sentence_idx": 1450, "label": "unlabeled"}, {"type": "text", "expr": ": Learning complex action models with\nquantifiers and implications", "word_idx": 81589, "sentence_idx": 1451, "label": "unlabeled"}, {"type": "text", "expr": "Zhuo, H", "word_idx": 81655, "sentence_idx": 1452, "label": "unlabeled"}, {"type": "text", "expr": ", Yang, Q", "word_idx": 81662, "sentence_idx": 1453, "label": "unlabeled"}, {"type": "text", "expr": ", Hu, D", "word_idx": 81671, "sentence_idx": 1454, "label": "unlabeled"}, {"type": "text", "expr": ", Li, L", "word_idx": 81678, "sentence_idx": 1455, "label": "unlabeled"}, {"type": "text", "expr": ": Learning complex action models with\nquantifiers and implications", "word_idx": 81685, "sentence_idx": 1456, "label": "unlabeled"}, {"type": "text", "expr": "Artificial Intelligence  174 (18), 1540\u20131569 (2010)", "word_idx": 81751, "sentence_idx": 1457, "label": "unlabeled"}, {"type": "text", "expr": "(36) \nZhuo, H", "word_idx": 81802, "sentence_idx": 1458, "label": "unlabeled"}, {"type": "text", "expr": ", Yang, Q", "word_idx": 81815, "sentence_idx": 1459, "label": "unlabeled"}, {"type": "text", "expr": ", Kambhampati, S", "word_idx": 81824, "sentence_idx": 1460, "label": "unlabeled"}, {"type": "text", "expr": ": Action-model based multi-agent plan\nrecognition", "word_idx": 81840, "sentence_idx": 1461, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of NIPS, pp", "word_idx": 81889, "sentence_idx": 1462, "label": "unlabeled"}, {"type": "text", "expr": " 377\u2013385 (2012)", "word_idx": 81919, "sentence_idx": 1463, "label": "unlabeled"}, {"type": "text", "expr": "Zhuo, H", "word_idx": 81934, "sentence_idx": 1464, "label": "unlabeled"}, {"type": "text", "expr": ", Yang, Q", "word_idx": 81941, "sentence_idx": 1465, "label": "unlabeled"}, {"type": "text", "expr": ", Kambhampati, S", "word_idx": 81950, "sentence_idx": 1466, "label": "unlabeled"}, {"type": "text", "expr": ": Action-model based multi-agent plan\nrecognition", "word_idx": 81966, "sentence_idx": 1467, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of NIPS, pp", "word_idx": 82015, "sentence_idx": 1468, "label": "unlabeled"}, {"type": "text", "expr": " 377\u2013385 (2012)", "word_idx": 82042, "sentence_idx": 1469, "label": "unlabeled"}, {"type": "text", "expr": "Generated  on Wed Mar  7 13:38:02 2018 by", "word_idx": 82057, "sentence_idx": 1470, "label": "unlabeled"}, {"type": "text", "expr": "LaTeXML", "word_idx": 82098, "sentence_idx": 1471, "label": "unlabeled"}], "faster_rcnn": [{"type": "text", "expr": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "word_idx": 0, "sentence_idx": 0, "label": "unlabeled"}, {"type": "text", "expr": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "word_idx": 78, "sentence_idx": 1, "label": "unlabeled"}, {"type": "text", "expr": "Shaoqing\u00a0Ren", "word_idx": 156, "sentence_idx": 2, "label": "unlabeled"}, {"type": "text", "expr": "Shaoqing\u00a0Ren", "word_idx": 168, "sentence_idx": 3, "label": "unlabeled"}, {"type": "text", "expr": "Kaiming\u00a0He", "word_idx": 180, "sentence_idx": 4, "label": "unlabeled"}, {"type": "text", "expr": "Kaiming\u00a0He", "word_idx": 190, "sentence_idx": 5, "label": "unlabeled"}, {"type": "text", "expr": "Ross Girshick", "word_idx": 200, "sentence_idx": 6, "label": "unlabeled"}, {"type": "text", "expr": "Ross Girshick", "word_idx": 213, "sentence_idx": 7, "label": "unlabeled"}, {"type": "text", "expr": "and\u00a0Jian\u00a0Sun \\IEEEcompsocitemizethanks \\IEEEcompsocthanksitem S", "word_idx": 226, "sentence_idx": 8, "label": "unlabeled"}, {"type": "text", "expr": " Ren is with University of Science and Technology of China, Hefei, China", "word_idx": 289, "sentence_idx": 9, "label": "unlabeled"}, {"type": "text", "expr": " This work was done when S", "word_idx": 361, "sentence_idx": 10, "label": "unlabeled"}, {"type": "text", "expr": " Ren was an intern at Microsoft Research", "word_idx": 387, "sentence_idx": 11, "label": "unlabeled"}, {"type": "text", "expr": " Email: sqren@mail", "word_idx": 427, "sentence_idx": 12, "label": "unlabeled"}, {"type": "text", "expr": "cn\n \\IEEEcompsocthanksitem K", "word_idx": 445, "sentence_idx": 13, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He and J", "word_idx": 473, "sentence_idx": 14, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sun are with Visual Computing Group, Microsoft Research", "word_idx": 482, "sentence_idx": 15, "label": "unlabeled"}, {"type": "text", "expr": " E-mail: {kahe,jiansun}@microsoft", "word_idx": 538, "sentence_idx": 16, "label": "unlabeled"}, {"type": "text", "expr": "com\n \\IEEEcompsocthanksitem R", "word_idx": 571, "sentence_idx": 17, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Girshick is with Facebook AI Research", "word_idx": 600, "sentence_idx": 18, "label": "unlabeled"}, {"type": "text", "expr": " The majority of this work was done when R", "word_idx": 638, "sentence_idx": 19, "label": "unlabeled"}, {"type": "text", "expr": " Girshick was with Microsoft Research", "word_idx": 680, "sentence_idx": 20, "label": "unlabeled"}, {"type": "text", "expr": " E-mail: rbg@fb", "word_idx": 717, "sentence_idx": 21, "label": "unlabeled"}, {"type": "text", "expr": "\\IEEEcompsocitemizethanks", "word_idx": 732, "sentence_idx": 22, "label": "unlabeled"}, {"type": "text", "expr": "\\IEEEcompsocthanksitem", "word_idx": 757, "sentence_idx": 23, "label": "unlabeled"}, {"type": "text", "expr": "\\IEEEcompsocthanksitem", "word_idx": 779, "sentence_idx": 24, "label": "unlabeled"}, {"type": "text", "expr": "\\IEEEcompsocthanksitem", "word_idx": 801, "sentence_idx": 25, "label": "unlabeled"}, {"type": "text", "expr": "Abstract", "word_idx": 823, "sentence_idx": 26, "label": "unlabeled"}, {"type": "text", "expr": "Abstract", "word_idx": 831, "sentence_idx": 27, "label": "unlabeled"}, {"type": "text", "expr": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations", "word_idx": 839, "sentence_idx": 28, "label": "unlabeled"}, {"type": "text", "expr": "\nAdvances like SPPnet   and Fast R-CNN   have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck", "word_idx": 950, "sentence_idx": 29, "label": "unlabeled"}, {"type": "text", "expr": "\nIn this work, we introduce a  Region Proposal Network  (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals", "word_idx": 1102, "sentence_idx": 30, "label": "unlabeled"}, {"type": "text", "expr": " An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position", "word_idx": 1285, "sentence_idx": 31, "label": "unlabeled"}, {"type": "text", "expr": " The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection", "word_idx": 1407, "sentence_idx": 32, "label": "unlabeled"}, {"type": "text", "expr": "\nWe further merge RPN and Fast R-CNN into a single network by sharing their convolutional features\u2014using the recently popular terminology of neural networks with \u201cattention\u201d mechanisms, the RPN component tells the unified network where to look", "word_idx": 1523, "sentence_idx": 33, "label": "unlabeled"}, {"type": "text", "expr": "\nFor the very deep VGG-16 model  , our detection system has a frame rate of 5fps ( including all steps ) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image", "word_idx": 1766, "sentence_idx": 34, "label": "unlabeled"}, {"type": "text", "expr": " In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks", "word_idx": 2020, "sentence_idx": 35, "label": "unlabeled"}, {"type": "text", "expr": " Code has been made publicly available", "word_idx": 2150, "sentence_idx": 36, "label": "unlabeled"}, {"type": "text", "expr": "Region Proposal Network", "word_idx": 2188, "sentence_idx": 37, "label": "unlabeled"}, {"type": "text", "expr": "including all steps", "word_idx": 2211, "sentence_idx": 38, "label": "unlabeled"}, {"type": "text", "expr": "\\IEEEcompsoctitleabstractindextext", "word_idx": 2230, "sentence_idx": 39, "label": "unlabeled"}, {"type": "text", "expr": "bject Detection, Region Proposal, Convolutional Neural Network", "word_idx": 2264, "sentence_idx": 40, "label": "unlabeled"}, {"type": "text", "expr": "bject Detection, Region Proposal, Convolutional Neural Network", "word_idx": 2326, "sentence_idx": 41, "label": "unlabeled"}, {"type": "text", "expr": "\\IEEEpeerreviewmaketitle", "word_idx": 2388, "sentence_idx": 42, "label": "unlabeled"}, {"type": "text", "expr": "1  Introduction", "word_idx": 2412, "sentence_idx": 43, "label": "unlabeled"}, {"type": "text", "expr": "Recent advances in object detection are driven by the success of region proposal methods ( \\eg ,  ) and region-based convolutional neural networks (R-CNNs)  ", "word_idx": 2427, "sentence_idx": 44, "label": "unlabeled"}, {"type": "text", "expr": " Although region-based CNNs were computationally expensive as originally developed in  , their cost has been drastically reduced thanks to sharing convolutions across proposals  ", "word_idx": 2584, "sentence_idx": 45, "label": "unlabeled"}, {"type": "text", "expr": " The latest incarnation, Fast R-CNN  , achieves near real-time rates using very deep networks  ,  when ignoring the time spent on region proposals ", "word_idx": 2762, "sentence_idx": 46, "label": "unlabeled"}, {"type": "text", "expr": " Now, proposals are the test-time computational bottleneck in state-of-the-art detection systems", "word_idx": 2909, "sentence_idx": 47, "label": "unlabeled"}, {"type": "text", "expr": "when ignoring the time spent on region proposals", "word_idx": 3005, "sentence_idx": 48, "label": "unlabeled"}, {"type": "text", "expr": "Region proposal methods typically rely on inexpensive features and economical inference schemes", "word_idx": 3053, "sentence_idx": 49, "label": "unlabeled"}, {"type": "text", "expr": "\nSelective Search  , one of the most popular methods, greedily merges superpixels based on engineered low-level features", "word_idx": 3148, "sentence_idx": 50, "label": "unlabeled"}, {"type": "text", "expr": " Yet when compared to efficient detection networks  , Selective Search is an order of magnitude slower, at 2 seconds per image in a CPU implementation", "word_idx": 3268, "sentence_idx": 51, "label": "unlabeled"}, {"type": "text", "expr": "\nEdgeBoxes   currently provides the best tradeoff between proposal quality and speed, at 0", "word_idx": 3418, "sentence_idx": 52, "label": "unlabeled"}, {"type": "text", "expr": "2 seconds per image", "word_idx": 3508, "sentence_idx": 53, "label": "unlabeled"}, {"type": "text", "expr": " Nevertheless, the region proposal step still consumes as much running time as the detection network", "word_idx": 3527, "sentence_idx": 54, "label": "unlabeled"}, {"type": "text", "expr": "One may note that fast region-based CNNs take advantage of GPUs, while the region proposal methods used in research are implemented on the CPU, making such runtime comparisons inequitable", "word_idx": 3627, "sentence_idx": 55, "label": "unlabeled"}, {"type": "text", "expr": " An obvious way to accelerate proposal computation is to re-implement it for the GPU", "word_idx": 3814, "sentence_idx": 56, "label": "unlabeled"}, {"type": "text", "expr": " This may be an effective engineering solution, but re-implementation ignores the down-stream detection network and therefore misses important opportunities for sharing computation", "word_idx": 3898, "sentence_idx": 57, "label": "unlabeled"}, {"type": "text", "expr": "One may note that fast region-based CNNs take advantage of GPUs, while the region proposal methods used in research are implemented on the CPU, making such runtime comparisons inequitable", "word_idx": 4078, "sentence_idx": 58, "label": "unlabeled"}, {"type": "text", "expr": " An obvious way to accelerate proposal computation is to re-implement it for the GPU", "word_idx": 4265, "sentence_idx": 59, "label": "unlabeled"}, {"type": "text", "expr": " This may be an effective engineering solution, but re-implementation ignores the down-stream detection network and therefore misses important opportunities for sharing computation", "word_idx": 4349, "sentence_idx": 60, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:  Different schemes for addressing multiple scales and sizes", "word_idx": 4529, "sentence_idx": 61, "label": "unlabeled"}, {"type": "text", "expr": " (a) Pyramids of images and feature maps are built, and the classifier is run at all scales", "word_idx": 4598, "sentence_idx": 62, "label": "unlabeled"}, {"type": "text", "expr": " (b) Pyramids of filters with multiple scales/sizes are run on the feature map", "word_idx": 4689, "sentence_idx": 63, "label": "unlabeled"}, {"type": "text", "expr": " (c) We use pyramids of reference boxes in the regression functions", "word_idx": 4767, "sentence_idx": 64, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:", "word_idx": 4834, "sentence_idx": 65, "label": "unlabeled"}, {"type": "text", "expr": "In this paper, we show that an algorithmic change\u2014computing proposals with a deep convolutional neural network\u2014leads to an elegant and effective solution where proposal computation is nearly cost-free given the detection network\u2019s computation", "word_idx": 4843, "sentence_idx": 66, "label": "unlabeled"}, {"type": "text", "expr": "\nTo this end, we introduce novel  Region Proposal Networks  (RPNs) that share convolutional layers with state-of-the-art object detection networks  ", "word_idx": 5085, "sentence_idx": 67, "label": "unlabeled"}, {"type": "text", "expr": " By sharing convolutions at test-time, the marginal cost for computing proposals is small ( \\eg , 10ms per image)", "word_idx": 5233, "sentence_idx": 68, "label": "unlabeled"}, {"type": "text", "expr": "Region Proposal Networks", "word_idx": 5346, "sentence_idx": 69, "label": "unlabeled"}, {"type": "text", "expr": "Our observation is that the convolutional feature maps used by region-based detectors, like Fast R-CNN, can also be used for generating region proposals", "word_idx": 5370, "sentence_idx": 70, "label": "unlabeled"}, {"type": "text", "expr": "\nOn top of these convolutional features, we construct an RPN by adding a few additional convolutional layers that simultaneously regress region bounds and objectness scores at each location on a regular grid", "word_idx": 5522, "sentence_idx": 71, "label": "unlabeled"}, {"type": "text", "expr": "\nThe RPN is thus a kind of fully convolutional network (FCN)   and can be trained end-to-end specifically for the task for generating detection proposals", "word_idx": 5729, "sentence_idx": 72, "label": "unlabeled"}, {"type": "text", "expr": "RPNs are designed to efficiently predict region proposals with a wide range of scales and aspect ratios", "word_idx": 5882, "sentence_idx": 73, "label": "unlabeled"}, {"type": "text", "expr": " In contrast to prevalent methods   that use pyramids of images (Figure\u00a0 1 , a) or pyramids of filters (Figure\u00a0 1 , b), we introduce novel \u201canchor\u201d boxes that serve as references at multiple scales and aspect ratios", "word_idx": 5985, "sentence_idx": 74, "label": "unlabeled"}, {"type": "text", "expr": " Our scheme can be thought of as a pyramid of regression references (Figure\u00a0 1 , c), which avoids enumerating images or filters of multiple scales or aspect ratios", "word_idx": 6200, "sentence_idx": 75, "label": "unlabeled"}, {"type": "text", "expr": " This model performs well when trained and tested using single-scale images and thus benefits running speed", "word_idx": 6363, "sentence_idx": 76, "label": "unlabeled"}, {"type": "text", "expr": "To unify RPNs with Fast R-CNN   object detection networks, we propose a training scheme that alternates between fine-tuning for the region proposal task and then fine-tuning for object detection, while keeping the proposals fixed", "word_idx": 6470, "sentence_idx": 77, "label": "unlabeled"}, {"type": "text", "expr": "\nThis scheme converges quickly and produces a unified network with convolutional features that are shared between both tasks", "word_idx": 6699, "sentence_idx": 78, "label": "unlabeled"}, {"type": "text", "expr": "1 Since the publication of the conference version of this paper  , we have also found that RPNs can be trained jointly with Fast R-CNN networks leading to less training time", "word_idx": 6823, "sentence_idx": 79, "label": "unlabeled"}, {"type": "text", "expr": "We comprehensively evaluate our method on the PASCAL VOC detection benchmarks   where RPNs with Fast R-CNNs produce detection accuracy better than the strong baseline of Selective Search with Fast R-CNNs", "word_idx": 6996, "sentence_idx": 80, "label": "unlabeled"}, {"type": "text", "expr": " Meanwhile, our method waives nearly all computational burdens of Selective Search at test-time\u2014the effective running time for proposals is just 10 milliseconds", "word_idx": 7199, "sentence_idx": 81, "label": "unlabeled"}, {"type": "text", "expr": "\nUsing the expensive very deep models of  , our detection method still has a frame rate of 5fps ( including all steps ) on a GPU, and thus is a practical object detection system in terms of both speed and accuracy", "word_idx": 7359, "sentence_idx": 82, "label": "unlabeled"}, {"type": "text", "expr": "\nWe also report results on the MS COCO dataset   and investigate the improvements on PASCAL VOC using the COCO data", "word_idx": 7572, "sentence_idx": 83, "label": "unlabeled"}, {"type": "text", "expr": "\nCode has been made publicly available at  https://github", "word_idx": 7687, "sentence_idx": 84, "label": "unlabeled"}, {"type": "text", "expr": "com/shaoqingren/faster_rcnn  (in MATLAB) and  https://github", "word_idx": 7744, "sentence_idx": 85, "label": "unlabeled"}, {"type": "text", "expr": "com/rbgirshick/py-faster-rcnn  (in Python)", "word_idx": 7804, "sentence_idx": 86, "label": "unlabeled"}, {"type": "text", "expr": "including all steps", "word_idx": 7846, "sentence_idx": 87, "label": "unlabeled"}, {"type": "text", "expr": "https://github", "word_idx": 7865, "sentence_idx": 88, "label": "unlabeled"}, {"type": "text", "expr": "com/shaoqingren/faster_rcnn", "word_idx": 7879, "sentence_idx": 89, "label": "unlabeled"}, {"type": "text", "expr": "https://github", "word_idx": 7906, "sentence_idx": 90, "label": "unlabeled"}, {"type": "text", "expr": "com/rbgirshick/py-faster-rcnn", "word_idx": 7920, "sentence_idx": 91, "label": "unlabeled"}, {"type": "text", "expr": "A preliminary version of this manuscript was published previously  ", "word_idx": 7949, "sentence_idx": 92, "label": "unlabeled"}, {"type": "text", "expr": " Since then, the frameworks of RPN and Faster R-CNN have been adopted and generalized to other methods, such as 3D object detection  , part-based detection  , instance segmentation  , and image captioning  ", "word_idx": 8016, "sentence_idx": 93, "label": "unlabeled"}, {"type": "text", "expr": " Our fast and effective object detection system has also been built in commercial systems such as at Pinterests  , with user engagement improvements reported", "word_idx": 8222, "sentence_idx": 94, "label": "unlabeled"}, {"type": "text", "expr": "In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the basis of several 1st-place entries   in the tracks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation", "word_idx": 8379, "sentence_idx": 95, "label": "unlabeled"}, {"type": "text", "expr": " RPNs completely learn to propose regions from data, and thus can easily benefit from deeper and more expressive features (such as the 101-layer residual nets adopted in  )", "word_idx": 8580, "sentence_idx": 96, "label": "unlabeled"}, {"type": "text", "expr": " Faster R-CNN and RPN are also used by several other leading entries in these competitions ", "word_idx": 8752, "sentence_idx": 97, "label": "unlabeled"}, {"type": "text", "expr": " These results suggest that our method is not only a cost-efficient solution for practical usage, but also an effective way of improving object detection accuracy", "word_idx": 8843, "sentence_idx": 98, "label": "unlabeled"}, {"type": "text", "expr": "2 http://image-net", "word_idx": 9005, "sentence_idx": 99, "label": "unlabeled"}, {"type": "text", "expr": "org/challenges/LSVRC/2015/results", "word_idx": 9023, "sentence_idx": 100, "label": "unlabeled"}, {"type": "text", "expr": "http://image-net", "word_idx": 9056, "sentence_idx": 101, "label": "unlabeled"}, {"type": "text", "expr": "org/challenges/LSVRC/2015/results", "word_idx": 9072, "sentence_idx": 102, "label": "unlabeled"}, {"type": "text", "expr": "2  Related Work", "word_idx": 9105, "sentence_idx": 103, "label": "unlabeled"}, {"type": "text", "expr": "Object Proposals", "word_idx": 9120, "sentence_idx": 104, "label": "unlabeled"}, {"type": "text", "expr": "  There is a large literature on object proposal methods", "word_idx": 9136, "sentence_idx": 105, "label": "unlabeled"}, {"type": "text", "expr": " Comprehensive surveys and comparisons of object proposal methods can be found in  ", "word_idx": 9192, "sentence_idx": 106, "label": "unlabeled"}, {"type": "text", "expr": " Widely used object proposal methods include those based on grouping super-pixels ( \\eg , Selective Search  , CPMC  , MCG  ) and those based on sliding windows ( \\eg , objectness in windows  , EdgeBoxes  )", "word_idx": 9275, "sentence_idx": 107, "label": "unlabeled"}, {"type": "text", "expr": " Object proposal methods were adopted as external modules independent of the detectors ( \\eg , Selective Search   object detectors, R-CNN  , and Fast R-CNN  )", "word_idx": 9480, "sentence_idx": 108, "label": "unlabeled"}, {"type": "text", "expr": "Object Proposals", "word_idx": 9638, "sentence_idx": 109, "label": "unlabeled"}, {"type": "text", "expr": "Deep Networks for Object Detection", "word_idx": 9654, "sentence_idx": 110, "label": "unlabeled"}, {"type": "text", "expr": "  The R-CNN method   trains CNNs end-to-end to classify the proposal regions into object categories or background", "word_idx": 9688, "sentence_idx": 111, "label": "unlabeled"}, {"type": "text", "expr": " R-CNN mainly plays as a classifier, and it does not predict object bounds (except for refining by bounding box regression)", "word_idx": 9801, "sentence_idx": 112, "label": "unlabeled"}, {"type": "text", "expr": "\nIts accuracy depends on the performance of the region proposal module (see comparisons in  )", "word_idx": 9924, "sentence_idx": 113, "label": "unlabeled"}, {"type": "text", "expr": "\nSeveral papers have proposed ways of using deep networks for predicting object bounding boxes  ", "word_idx": 10017, "sentence_idx": 114, "label": "unlabeled"}, {"type": "text", "expr": "\nIn the OverFeat method  , a fully-connected layer is trained to predict the box coordinates for the localization task that assumes a single object", "word_idx": 10113, "sentence_idx": 115, "label": "unlabeled"}, {"type": "text", "expr": " The fully-connected layer is then turned into a convolutional layer for detecting multiple class-specific objects", "word_idx": 10260, "sentence_idx": 116, "label": "unlabeled"}, {"type": "text", "expr": " The MultiBox methods   generate region proposals from a network whose last fully-connected layer simultaneously predicts multiple class-agnostic boxes, generalizing the \u201csingle-box\u201d fashion of OverFeat", "word_idx": 10374, "sentence_idx": 117, "label": "unlabeled"}, {"type": "text", "expr": " These class-agnostic boxes are used as proposals for R-CNN  ", "word_idx": 10576, "sentence_idx": 118, "label": "unlabeled"}, {"type": "text", "expr": "\nThe MultiBox proposal network is applied on a single image crop or multiple large image crops ( \\eg , 224 $\\times$ 224), in contrast to our fully convolutional scheme", "word_idx": 10637, "sentence_idx": 119, "label": "unlabeled"}, {"type": "text", "expr": " MultiBox does not share features between the proposal and detection networks", "word_idx": 10804, "sentence_idx": 120, "label": "unlabeled"}, {"type": "text", "expr": "\nWe discuss OverFeat and MultiBox in more depth later in context with our method", "word_idx": 10881, "sentence_idx": 121, "label": "unlabeled"}, {"type": "text", "expr": "\nConcurrent with our work, the DeepMask method   is developed for learning segmentation proposals", "word_idx": 10961, "sentence_idx": 122, "label": "unlabeled"}, {"type": "text", "expr": "Deep Networks for Object Detection", "word_idx": 11058, "sentence_idx": 123, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 11092, "sentence_idx": 124, "label": "unlabeled"}, {"type": "text", "expr": "Shared computation of convolutions   has been attracting increasing attention for efficient, yet accurate, visual recognition", "word_idx": 11098, "sentence_idx": 125, "label": "unlabeled"}, {"type": "text", "expr": " The OverFeat paper   computes convolutional features from an image pyramid for classification, localization, and detection", "word_idx": 11223, "sentence_idx": 126, "label": "unlabeled"}, {"type": "text", "expr": " Adaptively-sized pooling (SPP)   on shared convolutional feature maps is developed for efficient region-based object detection   and semantic segmentation  ", "word_idx": 11346, "sentence_idx": 127, "label": "unlabeled"}, {"type": "text", "expr": " Fast R-CNN   enables end-to-end detector training on shared convolutional features and shows compelling accuracy and speed", "word_idx": 11503, "sentence_idx": 128, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:  Faster R-CNN is a single, unified network for object detection", "word_idx": 11626, "sentence_idx": 129, "label": "unlabeled"}, {"type": "text", "expr": " The RPN module serves as the \u2018attention\u2019 of this unified network", "word_idx": 11699, "sentence_idx": 130, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:", "word_idx": 11764, "sentence_idx": 131, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:  Left : Region Proposal Network (RPN)", "word_idx": 11773, "sentence_idx": 132, "label": "unlabeled"}, {"type": "text", "expr": "  Right : Example detections using RPN proposals on PASCAL VOC 2007 test", "word_idx": 11820, "sentence_idx": 133, "label": "unlabeled"}, {"type": "text", "expr": " Our method detects objects in a wide range of scales and aspect ratios", "word_idx": 11892, "sentence_idx": 134, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:", "word_idx": 11963, "sentence_idx": 135, "label": "unlabeled"}, {"type": "text", "expr": "Right", "word_idx": 11972, "sentence_idx": 136, "label": "unlabeled"}, {"type": "text", "expr": "3  Faster R-CNN", "word_idx": 11977, "sentence_idx": 137, "label": "unlabeled"}, {"type": "text", "expr": "Our object detection system, called Faster R-CNN, is composed of two modules", "word_idx": 11992, "sentence_idx": 138, "label": "unlabeled"}, {"type": "text", "expr": " The first module is a deep fully convolutional network that proposes regions, and the second module is the Fast R-CNN detector   that uses the proposed regions", "word_idx": 12068, "sentence_idx": 139, "label": "unlabeled"}, {"type": "text", "expr": " The entire system is a single, unified network for object detection (Figure\u00a0 2 )", "word_idx": 12228, "sentence_idx": 140, "label": "unlabeled"}, {"type": "text", "expr": "\nUsing the recently popular terminology of neural networks with \u2018attention\u2019   mechanisms, the RPN module tells the Fast R-CNN module where to look", "word_idx": 12309, "sentence_idx": 141, "label": "unlabeled"}, {"type": "text", "expr": "\nIn Section\u00a0 3", "word_idx": 12455, "sentence_idx": 142, "label": "unlabeled"}, {"type": "text", "expr": "1  we introduce the designs and properties of the network for region proposal", "word_idx": 12469, "sentence_idx": 143, "label": "unlabeled"}, {"type": "text", "expr": " In Section\u00a0 3", "word_idx": 12546, "sentence_idx": 144, "label": "unlabeled"}, {"type": "text", "expr": "2  we develop algorithms for training both modules with features shared", "word_idx": 12560, "sentence_idx": 145, "label": "unlabeled"}, {"type": "text", "expr": "1  Region Proposal Networks", "word_idx": 12631, "sentence_idx": 146, "label": "unlabeled"}, {"type": "text", "expr": "A Region Proposal Network (RPN) takes an image (of any size) as input and outputs a set of rectangular object proposals, each with an objectness score", "word_idx": 12658, "sentence_idx": 147, "label": "unlabeled"}, {"type": "text", "expr": " \nWe model this process with a fully convolutional network  , which we describe in this section", "word_idx": 12808, "sentence_idx": 148, "label": "unlabeled"}, {"type": "text", "expr": "\nBecause our ultimate goal is to share computation with a Fast R-CNN object detection network  , we assume that both nets share a common set of convolutional layers", "word_idx": 12903, "sentence_idx": 149, "label": "unlabeled"}, {"type": "text", "expr": "\nIn our experiments, we investigate the Zeiler and Fergus model   (ZF), which has 5 shareable convolutional layers and the Simonyan and Zisserman model   (VGG-16), which has 13 shareable convolutional layers", "word_idx": 13067, "sentence_idx": 150, "label": "unlabeled"}, {"type": "text", "expr": "3 \u201cRegion\u201d is a generic term and in this paper we only consider  rectangular  regions, as is common for many methods ( \\eg ,  )", "word_idx": 13274, "sentence_idx": 151, "label": "unlabeled"}, {"type": "text", "expr": " \u201cObjectness\u201d measures membership to a set of object classes  \\vs \u00a0background", "word_idx": 13401, "sentence_idx": 152, "label": "unlabeled"}, {"type": "text", "expr": "rectangular", "word_idx": 13478, "sentence_idx": 153, "label": "unlabeled"}, {"type": "text", "expr": "To generate region proposals, we slide a small network over the convolutional feature map output by the last shared convolutional layer", "word_idx": 13489, "sentence_idx": 154, "label": "unlabeled"}, {"type": "text", "expr": "\nThis small network takes as input an  $n\\times n$  spatial window of the input convolutional feature map", "word_idx": 13624, "sentence_idx": 155, "label": "unlabeled"}, {"type": "text", "expr": "\nEach sliding window is mapped to a lower-dimensional feature (256-d for ZF and 512-d for VGG, with ReLU   following)", "word_idx": 13729, "sentence_idx": 156, "label": "unlabeled"}, {"type": "text", "expr": "\nThis feature is fed into two sibling fully-connected layers\u2014a box-regression layer ( reg ) and a box-classification layer ( cls )", "word_idx": 13846, "sentence_idx": 157, "label": "unlabeled"}, {"type": "text", "expr": "\nWe use  $n=3$  in this paper, noting that the effective receptive field on the input image is large (171 and 228 pixels for ZF and VGG, respectively)", "word_idx": 13976, "sentence_idx": 158, "label": "unlabeled"}, {"type": "text", "expr": "\nThis mini-network is illustrated at a single position in Figure\u00a0 3  (left)", "word_idx": 14126, "sentence_idx": 159, "label": "unlabeled"}, {"type": "text", "expr": "\nNote that because the mini-network operates in a sliding-window fashion, the fully-connected layers are shared across all spatial locations", "word_idx": 14201, "sentence_idx": 160, "label": "unlabeled"}, {"type": "text", "expr": "\nThis architecture is naturally implemented with an  $n\\times n$  convolutional layer followed by two sibling  $1\\times 1$  convolutional layers (for  reg  and  cls , respectively)", "word_idx": 14341, "sentence_idx": 161, "label": "unlabeled"}, {"type": "math", "expr": "$$n\\times n$$", "word_idx": 14521, "sentence_idx": 162, "label": "unlabeled"}, {"type": "math", "expr": "$$n=3$$", "word_idx": 14530, "sentence_idx": 163, "label": "unlabeled"}, {"type": "math", "expr": "$$n\\times n$$", "word_idx": 14533, "sentence_idx": 164, "label": "unlabeled"}, {"type": "math", "expr": "$$1\\times 1$$", "word_idx": 14542, "sentence_idx": 165, "label": "unlabeled"}, {"type": "text", "expr": "1  Anchors", "word_idx": 14551, "sentence_idx": 166, "label": "unlabeled"}, {"type": "text", "expr": "At each sliding-window location, we simultaneously predict multiple region proposals, where the number of maximum possible proposals for each location is denoted as  $k$ ", "word_idx": 14561, "sentence_idx": 167, "label": "unlabeled"}, {"type": "text", "expr": "\nSo the  reg  layer has  $4k$  outputs encoding the coordinates of  $k$  boxes, and the  cls  layer outputs  $2k$  scores that estimate probability of object or not object for each proposal ", "word_idx": 14731, "sentence_idx": 168, "label": "unlabeled"}, {"type": "text", "expr": " The  $k$  proposals are parameterized  relative  to  $k$  reference boxes, which we call  anchors ", "word_idx": 14921, "sentence_idx": 169, "label": "unlabeled"}, {"type": "text", "expr": " An anchor is centered at the sliding window in question, and is associated with a scale and aspect ratio (Figure\u00a0 3 , left)", "word_idx": 15020, "sentence_idx": 170, "label": "unlabeled"}, {"type": "text", "expr": " By default we use 3 scales and 3 aspect ratios, yielding  $k=9$  anchors at each sliding position", "word_idx": 15144, "sentence_idx": 171, "label": "unlabeled"}, {"type": "text", "expr": " For a convolutional feature map of a size  $W\\times H$  (typically  $\\sim$ 2,400), there are  $WHk$  anchors in total", "word_idx": 15242, "sentence_idx": 172, "label": "unlabeled"}, {"type": "text", "expr": "4 For simplicity we implement the  cls  layer as a two-class softmax layer", "word_idx": 15360, "sentence_idx": 173, "label": "unlabeled"}, {"type": "text", "expr": " Alternatively, one may use logistic regression to produce  $k$  scores", "word_idx": 15434, "sentence_idx": 174, "label": "unlabeled"}, {"type": "text", "expr": "relative", "word_idx": 15505, "sentence_idx": 175, "label": "unlabeled"}, {"type": "text", "expr": "anchors", "word_idx": 15513, "sentence_idx": 176, "label": "unlabeled"}, {"type": "math", "expr": "$$k=9$$", "word_idx": 15520, "sentence_idx": 177, "label": "unlabeled"}, {"type": "math", "expr": "$$W\\times H$$", "word_idx": 15523, "sentence_idx": 178, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sim$$", "word_idx": 15532, "sentence_idx": 179, "label": "unlabeled"}, {"type": "math", "expr": "$$WHk$$", "word_idx": 15536, "sentence_idx": 180, "label": "unlabeled"}, {"type": "text", "expr": "Translation-Invariant Anchors", "word_idx": 15539, "sentence_idx": 181, "label": "unlabeled"}, {"type": "text", "expr": "Translation-Invariant Anchors", "word_idx": 15568, "sentence_idx": 182, "label": "unlabeled"}, {"type": "text", "expr": "Translation-Invariant Anchors", "word_idx": 15597, "sentence_idx": 183, "label": "unlabeled"}, {"type": "text", "expr": "An important property of our approach is that it is  translation invariant , both in terms of the anchors and the functions that compute proposals relative to the anchors", "word_idx": 15626, "sentence_idx": 184, "label": "unlabeled"}, {"type": "text", "expr": "\nIf one translates an object in an image, the proposal should translate and the same function should be able to predict the proposal in either location", "word_idx": 15796, "sentence_idx": 185, "label": "unlabeled"}, {"type": "text", "expr": " This translation-invariant property is guaranteed by our method ", "word_idx": 15947, "sentence_idx": 186, "label": "unlabeled"}, {"type": "text", "expr": "\nAs a comparison, the MultiBox method   uses k-means to generate 800 anchors, which are  not  translation invariant", "word_idx": 16012, "sentence_idx": 187, "label": "unlabeled"}, {"type": "text", "expr": " So MultiBox does not guarantee that the same proposal is generated if an object is translated", "word_idx": 16127, "sentence_idx": 188, "label": "unlabeled"}, {"type": "text", "expr": "translation invariant", "word_idx": 16221, "sentence_idx": 189, "label": "unlabeled"}, {"type": "text", "expr": "5 As is the case of FCNs  , our network is translation invariant up to the network\u2019s total stride", "word_idx": 16242, "sentence_idx": 190, "label": "unlabeled"}, {"type": "text", "expr": "The translation-invariant property also reduces the model size", "word_idx": 16339, "sentence_idx": 191, "label": "unlabeled"}, {"type": "text", "expr": "\nMultiBox has a  $(4+1)\\times 800$ -dimensional fully-connected output layer, whereas our method has a  $(4+2)\\times 9$ -dimensional convolutional output layer in the case of  $k=9$  anchors", "word_idx": 16401, "sentence_idx": 192, "label": "unlabeled"}, {"type": "text", "expr": " As a result, our output layer has  $28\\times 10^{4}$  parameters ( $512\\times(4+2)\\times 9$  for VGG-16), two orders of magnitude fewer than MultiBox\u2019s output layer that has  $61\\times 10^{6}$  parameters ( $1536\\times(4+1)\\times 800$  for GoogleNet   in MultiBox  )", "word_idx": 16591, "sentence_idx": 193, "label": "unlabeled"}, {"type": "text", "expr": "\nIf considering the feature projection layers, our proposal layers still have an order of magnitude fewer parameters than MultiBox ", "word_idx": 16858, "sentence_idx": 194, "label": "unlabeled"}, {"type": "text", "expr": " We expect our method to have less risk of overfitting on small datasets, like PASCAL VOC", "word_idx": 16989, "sentence_idx": 195, "label": "unlabeled"}, {"type": "math", "expr": "$$(4+1)\\times 800$$", "word_idx": 17078, "sentence_idx": 196, "label": "unlabeled"}, {"type": "math", "expr": "$$(4+2)\\times 9$$", "word_idx": 17093, "sentence_idx": 197, "label": "unlabeled"}, {"type": "math", "expr": "$$k=9$$", "word_idx": 17106, "sentence_idx": 198, "label": "unlabeled"}, {"type": "math", "expr": "$$2.8\\times 10^{4}$$", "word_idx": 17109, "sentence_idx": 199, "label": "unlabeled"}, {"type": "math", "expr": "$$512\\times(4+2)\\times 9$$", "word_idx": 17125, "sentence_idx": 200, "label": "unlabeled"}, {"type": "math", "expr": "$$6.1\\times 10^{6}$$", "word_idx": 17147, "sentence_idx": 201, "label": "unlabeled"}, {"type": "math", "expr": "$$1536\\times(4+1)\\times 800$$", "word_idx": 17163, "sentence_idx": 202, "label": "unlabeled"}, {"type": "text", "expr": "6 Considering the feature projection layers, our proposal layers\u2019 parameter count is  $3\\times 3\\times 512\\times 512+512\\times 6\\times 9=24\\times 10^{6}$ ; MultiBox\u2019s proposal layers\u2019 parameter count is  $7\\times 7\\times(64+96+64+64)\\times 1536+1536\\times 5\\times 800=27\\times 10^{6}$ ", "word_idx": 17188, "sentence_idx": 203, "label": "unlabeled"}, {"type": "math", "expr": "$$3\\times 3\\times 512\\times 512+512\\times 6\\times 9=2.4\\times 10^{6}$$", "word_idx": 17473, "sentence_idx": 204, "label": "unlabeled"}, {"type": "math", "expr": "$$7\\times 7\\times(64+96+64+64)\\times 1536+1536\\times 5\\times 800=27\\times 10^{6}$$", "word_idx": 17539, "sentence_idx": 205, "label": "unlabeled"}, {"type": "text", "expr": "Multi-Scale Anchors as Regression References", "word_idx": 17617, "sentence_idx": 206, "label": "unlabeled"}, {"type": "text", "expr": "Multi-Scale Anchors as Regression References", "word_idx": 17661, "sentence_idx": 207, "label": "unlabeled"}, {"type": "text", "expr": "Multi-Scale Anchors as Regression References", "word_idx": 17705, "sentence_idx": 208, "label": "unlabeled"}, {"type": "text", "expr": "Our design of anchors presents a novel scheme for addressing multiple scales (and aspect ratios)", "word_idx": 17749, "sentence_idx": 209, "label": "unlabeled"}, {"type": "text", "expr": "\nAs shown in Figure\u00a0 1 , there have been two popular ways for multi-scale predictions", "word_idx": 17845, "sentence_idx": 210, "label": "unlabeled"}, {"type": "text", "expr": " The first way is based on image/feature pyramids,  \\eg , in DPM   and CNN-based methods  ", "word_idx": 17930, "sentence_idx": 211, "label": "unlabeled"}, {"type": "text", "expr": " The images are resized at multiple scales, and feature maps (HOG   or deep convolutional features  ) are computed for each scale (Figure\u00a0 1 (a))", "word_idx": 18020, "sentence_idx": 212, "label": "unlabeled"}, {"type": "text", "expr": " This way is often useful but is time-consuming", "word_idx": 18165, "sentence_idx": 213, "label": "unlabeled"}, {"type": "text", "expr": " The second way is to use sliding windows of multiple scales (and/or aspect ratios) on the feature maps", "word_idx": 18212, "sentence_idx": 214, "label": "unlabeled"}, {"type": "text", "expr": " For example, in DPM  , models of different aspect ratios are trained separately using different filter sizes (such as 5 $\\times$ 7 and 7 $\\times$ 5)", "word_idx": 18315, "sentence_idx": 215, "label": "unlabeled"}, {"type": "text", "expr": " If this way is used to address multiple scales, it can be thought of as a \u201cpyramid of filters\u201d (Figure\u00a0 1 (b))", "word_idx": 18464, "sentence_idx": 216, "label": "unlabeled"}, {"type": "text", "expr": " The second way is usually adopted jointly with the first way  ", "word_idx": 18575, "sentence_idx": 217, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 18638, "sentence_idx": 218, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 18644, "sentence_idx": 219, "label": "unlabeled"}, {"type": "text", "expr": "As a comparison, our anchor-based method is built on  a pyramid of anchors , which is more cost-efficient", "word_idx": 18650, "sentence_idx": 220, "label": "unlabeled"}, {"type": "text", "expr": " Our method classifies and regresses bounding boxes with reference to anchor boxes of multiple scales and aspect ratios", "word_idx": 18755, "sentence_idx": 221, "label": "unlabeled"}, {"type": "text", "expr": "\nIt only relies on images and feature maps of a single scale, and uses filters (sliding windows on the feature map) of a single size", "word_idx": 18874, "sentence_idx": 222, "label": "unlabeled"}, {"type": "text", "expr": " We show by experiments the effects of this scheme for addressing multiple scales and sizes (Table\u00a0 8 )", "word_idx": 19006, "sentence_idx": 223, "label": "unlabeled"}, {"type": "text", "expr": "a pyramid of anchors", "word_idx": 19109, "sentence_idx": 224, "label": "unlabeled"}, {"type": "text", "expr": "Because of this multi-scale design based on anchors, we can simply use the convolutional features computed on a single-scale image, as is also done by the Fast R-CNN detector  ", "word_idx": 19129, "sentence_idx": 225, "label": "unlabeled"}, {"type": "text", "expr": " The design of multi-scale anchors is a key component for sharing features without extra cost for addressing scales", "word_idx": 19305, "sentence_idx": 226, "label": "unlabeled"}, {"type": "text", "expr": "2  Loss Function", "word_idx": 19420, "sentence_idx": 227, "label": "unlabeled"}, {"type": "text", "expr": "For training RPNs, we assign a binary class label (of being an object or not) to each anchor", "word_idx": 19436, "sentence_idx": 228, "label": "unlabeled"}, {"type": "text", "expr": "\nWe assign a positive label to two kinds of anchors: (i) the anchor/anchors with the highest Intersection-over-Union (IoU) overlap with a ground-truth box,  or  (ii) an anchor that has an IoU overlap higher than 0", "word_idx": 19528, "sentence_idx": 229, "label": "unlabeled"}, {"type": "text", "expr": "7 with any ground-truth box", "word_idx": 19741, "sentence_idx": 230, "label": "unlabeled"}, {"type": "text", "expr": "\nNote that a single ground-truth box may assign positive labels to multiple anchors", "word_idx": 19768, "sentence_idx": 231, "label": "unlabeled"}, {"type": "text", "expr": " Usually the second condition is sufficient to determine the positive samples; but we still adopt the first condition for the reason that in some rare cases the second condition may find no positive sample", "word_idx": 19851, "sentence_idx": 232, "label": "unlabeled"}, {"type": "text", "expr": "\nWe assign a negative label to a non-positive anchor if its IoU ratio is lower than 0", "word_idx": 20056, "sentence_idx": 233, "label": "unlabeled"}, {"type": "text", "expr": "3 for all ground-truth boxes", "word_idx": 20141, "sentence_idx": 234, "label": "unlabeled"}, {"type": "text", "expr": "\nAnchors that are neither positive nor negative do not contribute to the training objective", "word_idx": 20169, "sentence_idx": 235, "label": "unlabeled"}, {"type": "text", "expr": "With these definitions, we minimize an objective function following the multi-task loss in Fast R-CNN  ", "word_idx": 20260, "sentence_idx": 236, "label": "unlabeled"}, {"type": "text", "expr": " Our loss function for an image is defined as:", "word_idx": 20363, "sentence_idx": 237, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle L(\\{p_{i}\\},\\{t_{i}\\})=\\frac{1}{N_{\\mathit{cls}}}\\sum_{i}L_{%\n\\mathit{cls}}(p_{i},p^{*}_{i})$", "word_idx": 20409, "sentence_idx": 238, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle L(\\{p_{i}\\},\\{t_{i}\\})=\\frac{1}{N_{\\mathit{cls}}}\\sum_{i}L_{%\n\\mathit{cls}}(p_{i},p^{*}_{i})$$", "word_idx": 20517, "sentence_idx": 239, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle+\\lambda\\frac{1}{N_{\\mathit{reg}}}\\sum_{i}p^{*}_{i}L_{\\mathit{reg%\n}}(t_{i},t^{*}_{i})$", "word_idx": 20623, "sentence_idx": 240, "label": "usecase"}, {"type": "math", "expr": "$$\\displaystyle+\\lambda\\frac{1}{N_{\\mathit{reg}}}\\sum_{i}p^{*}_{i}L_{\\mathit{reg%\n}}(t_{i},t^{*}_{i}).$$", "word_idx": 20724, "sentence_idx": 241, "label": "usecase"}, {"type": "text", "expr": "Here,  $i$  is the index of an anchor in a mini-batch and  $p_{i}$  is the predicted probability of anchor  $i$  being an object", "word_idx": 20824, "sentence_idx": 242, "label": "unlabeled"}, {"type": "text", "expr": " The ground-truth label  $p^{*}_{i}$  is 1 if the anchor is positive, and is 0 if the anchor is negative", "word_idx": 20952, "sentence_idx": 243, "label": "unlabeled"}, {"type": "text", "expr": "  $t_{i}$  is a vector representing the 4 parameterized coordinates of the predicted bounding box, and  $t^{*}_{i}$  is that of the ground-truth box associated with a positive anchor", "word_idx": 21056, "sentence_idx": 244, "label": "unlabeled"}, {"type": "text", "expr": "\nThe classification loss  $L_{\\mathit{cls}}$  is log loss over two classes (object  \\vs \u00a0not object)", "word_idx": 21238, "sentence_idx": 245, "label": "unlabeled"}, {"type": "text", "expr": "\nFor the regression loss, we use  $L_{\\mathit{reg}}(t_{i},t^{*}_{i})=R(t_{i}-t^{*}_{i})$  where  $R$  is the robust loss function (smooth L ${}_{1}$ ) defined in  ", "word_idx": 21338, "sentence_idx": 246, "label": "unlabeled"}, {"type": "text", "expr": " The term  $p^{*}_{i}L_{\\mathit{reg}}$  means the regression loss is activated only for positive anchors ( $p^{*}_{i}=1$ ) and is disabled otherwise ( $p^{*}_{i}=0$ )", "word_idx": 21501, "sentence_idx": 247, "label": "unlabeled"}, {"type": "text", "expr": " The outputs of the  cls  and  reg  layers consist of  $\\{p_{i}\\}$  and  $\\{t_{i}\\}$  respectively", "word_idx": 21667, "sentence_idx": 248, "label": "unlabeled"}, {"type": "math", "expr": "$$p_{i}$$", "word_idx": 21765, "sentence_idx": 249, "label": "unlabeled"}, {"type": "math", "expr": "$$p^{*}_{i}$$", "word_idx": 21770, "sentence_idx": 250, "label": "unlabeled"}, {"type": "math", "expr": "$$t_{i}$$", "word_idx": 21779, "sentence_idx": 251, "label": "unlabeled"}, {"type": "math", "expr": "$$t^{*}_{i}$$", "word_idx": 21784, "sentence_idx": 252, "label": "unlabeled"}, {"type": "math", "expr": "$$L_{\\mathit{cls}}$$", "word_idx": 21793, "sentence_idx": 253, "label": "unlabeled"}, {"type": "math", "expr": "$$L_{\\mathit{reg}}(t_{i},t^{*}_{i})=R(t_{i}-t^{*}_{i})$$", "word_idx": 21809, "sentence_idx": 254, "label": "unlabeled"}, {"type": "math", "expr": "$${}_{1}$$", "word_idx": 21861, "sentence_idx": 255, "label": "unlabeled"}, {"type": "math", "expr": "$$p^{*}_{i}L_{\\mathit{reg}}$$", "word_idx": 21867, "sentence_idx": 256, "label": "unlabeled"}, {"type": "math", "expr": "$$p^{*}_{i}=1$$", "word_idx": 21892, "sentence_idx": 257, "label": "unlabeled"}, {"type": "math", "expr": "$$p^{*}_{i}=0$$", "word_idx": 21903, "sentence_idx": 258, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{p_{i}\\}$$", "word_idx": 21914, "sentence_idx": 259, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{t_{i}\\}$$", "word_idx": 21923, "sentence_idx": 260, "label": "unlabeled"}, {"type": "text", "expr": "The two terms are normalized by  $N_{\\mathit{cls}}$  and  $N_{\\mathit{reg}}$  and weighted by a balancing parameter  $\\lambda$ ", "word_idx": 21932, "sentence_idx": 261, "label": "definition"}, {"type": "text", "expr": " In our current implementation (as in the released code), the  $cls$  term in Eqn", "word_idx": 22059, "sentence_idx": 262, "label": "unlabeled"}, {"type": "text", "expr": "( 1 ) is normalized by the mini-batch size ( \\ie ,  $N_{\\mathit{cls}}=256$ ) and the  $reg$  term is normalized by the number of anchor locations ( \\ie ,  $N_{\\mathit{reg}}\\sim 2,400$ )", "word_idx": 22140, "sentence_idx": 263, "label": "unlabeled"}, {"type": "text", "expr": " By default we set  $\\lambda=10$ , and thus both  cls  and  reg  terms are roughly equally weighted", "word_idx": 22325, "sentence_idx": 264, "label": "usecase"}, {"type": "text", "expr": " We show by experiments that the results are insensitive to the values of  $\\lambda$  in a wide range (Table\u00a0 9 )", "word_idx": 22424, "sentence_idx": 265, "label": "definition"}, {"type": "text", "expr": "\nWe also note that the normalization as above is not required and could be simplified", "word_idx": 22537, "sentence_idx": 266, "label": "unlabeled"}, {"type": "math", "expr": "$$N_{\\mathit{cls}}$$", "word_idx": 22622, "sentence_idx": 267, "label": "unlabeled"}, {"type": "math", "expr": "$$N_{\\mathit{reg}}$$", "word_idx": 22638, "sentence_idx": 268, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda$$", "word_idx": 22654, "sentence_idx": 269, "label": "none"}, {"type": "math", "expr": "$$cls$$", "word_idx": 22661, "sentence_idx": 270, "label": "unlabeled"}, {"type": "math", "expr": "$$N_{\\mathit{cls}}=256$$", "word_idx": 22664, "sentence_idx": 271, "label": "unlabeled"}, {"type": "math", "expr": "$$reg$$", "word_idx": 22684, "sentence_idx": 272, "label": "unlabeled"}, {"type": "math", "expr": "$$N_{\\mathit{reg}}\\sim 2,400$$", "word_idx": 22687, "sentence_idx": 273, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda=10$$", "word_idx": 22713, "sentence_idx": 274, "label": "none"}, {"type": "math", "expr": "$$\\lambda$$", "word_idx": 22723, "sentence_idx": 275, "label": "none"}, {"type": "text", "expr": "For bounding box regression, we adopt the parameterizations of the 4 coordinates following  :", "word_idx": 22730, "sentence_idx": 276, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle t_{\\textrm{x}}$", "word_idx": 22823, "sentence_idx": 277, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle t_{\\textrm{x}}$$", "word_idx": 22853, "sentence_idx": 278, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=(x-x_{\\textrm{a}})/w_{\\textrm{a}},\\quad t_{\\textrm{y}}=(y-y_{%\n\\textrm{a}})/h_{\\textrm{a}},$", "word_idx": 22881, "sentence_idx": 279, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=(x-x_{\\textrm{a}})/w_{\\textrm{a}},\\quad t_{\\textrm{y}}=(y-y_{%\n\\textrm{a}})/h_{\\textrm{a}},$$", "word_idx": 22988, "sentence_idx": 280, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle t_{\\textrm{w}}$", "word_idx": 23093, "sentence_idx": 281, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle t_{\\textrm{w}}$$", "word_idx": 23123, "sentence_idx": 282, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\log(w/w_{\\textrm{a}}),\\quad t_{\\textrm{h}}=\\log(h/h_{\\textrm{a}%\n}),$", "word_idx": 23151, "sentence_idx": 283, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\log(w/w_{\\textrm{a}}),\\quad t_{\\textrm{h}}=\\log(h/h_{\\textrm{a}%\n}),$$", "word_idx": 23236, "sentence_idx": 284, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle t^{*}_{\\textrm{x}}$", "word_idx": 23319, "sentence_idx": 285, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle t^{*}_{\\textrm{x}}$$", "word_idx": 23353, "sentence_idx": 286, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=(x^{*}-x_{\\textrm{a}})/w_{\\textrm{a}},\\quad t^{*}_{\\textrm{y}}=(%\ny^{*}-y_{\\textrm{a}})/h_{\\textrm{a}},$", "word_idx": 23385, "sentence_idx": 287, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=(x^{*}-x_{\\textrm{a}})/w_{\\textrm{a}},\\quad t^{*}_{\\textrm{y}}=(%\ny^{*}-y_{\\textrm{a}})/h_{\\textrm{a}},$$", "word_idx": 23504, "sentence_idx": 288, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle t^{*}_{\\textrm{w}}$", "word_idx": 23621, "sentence_idx": 289, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle t^{*}_{\\textrm{w}}$$", "word_idx": 23655, "sentence_idx": 290, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\log(w^{*}/w_{\\textrm{a}}),\\quad t^{*}_{\\textrm{h}}=\\log(h^{*}/h%\n_{\\textrm{a}}),$", "word_idx": 23687, "sentence_idx": 291, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\log(w^{*}/w_{\\textrm{a}}),\\quad t^{*}_{\\textrm{h}}=\\log(h^{*}/h%\n_{\\textrm{a}}),$$", "word_idx": 23784, "sentence_idx": 292, "label": "unlabeled"}, {"type": "text", "expr": "where  $x$ ,  $y$ ,  $w$ , and  $h$  denote the box\u2019s center coordinates and its width and height", "word_idx": 23879, "sentence_idx": 293, "label": "unlabeled"}, {"type": "text", "expr": "\nVariables  $x$ ,  $x_{\\textrm{a}}$ , and  $x^{*}$  are for the predicted box, anchor box, and ground-truth box respectively (likewise for  $y,w,h$ )", "word_idx": 23976, "sentence_idx": 294, "label": "unlabeled"}, {"type": "text", "expr": " This can be thought of as bounding-box regression from an anchor box to a nearby ground-truth box", "word_idx": 24125, "sentence_idx": 295, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{\\textrm{a}}$$", "word_idx": 24223, "sentence_idx": 296, "label": "unlabeled"}, {"type": "math", "expr": "$$x^{*}$$", "word_idx": 24237, "sentence_idx": 297, "label": "unlabeled"}, {"type": "math", "expr": "$$y,w,h$$", "word_idx": 24242, "sentence_idx": 298, "label": "unlabeled"}, {"type": "text", "expr": "Nevertheless, our method achieves bounding-box regression by a different manner from previous RoI-based (Region of Interest) methods  ", "word_idx": 24247, "sentence_idx": 299, "label": "unlabeled"}, {"type": "text", "expr": "\nIn  , bounding-box regression is performed on features pooled from  arbitrarily  sized RoIs, and the regression weights are  shared  by all region sizes", "word_idx": 24381, "sentence_idx": 300, "label": "unlabeled"}, {"type": "text", "expr": " In our formulation, the features used for regression are of the  same  spatial size ( $3\\times 3$ ) on the feature maps", "word_idx": 24534, "sentence_idx": 301, "label": "unlabeled"}, {"type": "text", "expr": " To account for varying sizes, a set of  $k$  bounding-box regressors are learned", "word_idx": 24654, "sentence_idx": 302, "label": "unlabeled"}, {"type": "text", "expr": " Each regressor is responsible for one scale and one aspect ratio, and the  $k$  regressors do  not  share weights", "word_idx": 24735, "sentence_idx": 303, "label": "unlabeled"}, {"type": "text", "expr": " As such, it is still possible to predict boxes of various sizes even though the features are of a fixed size/scale, thanks to the design of anchors", "word_idx": 24849, "sentence_idx": 304, "label": "unlabeled"}, {"type": "text", "expr": "arbitrarily", "word_idx": 24997, "sentence_idx": 305, "label": "unlabeled"}, {"type": "text", "expr": "shared", "word_idx": 25008, "sentence_idx": 306, "label": "unlabeled"}, {"type": "math", "expr": "$$3\\times 3$$", "word_idx": 25014, "sentence_idx": 307, "label": "unlabeled"}, {"type": "text", "expr": "3  Training RPNs", "word_idx": 25023, "sentence_idx": 308, "label": "unlabeled"}, {"type": "text", "expr": "The RPN can be trained end-to-end by back-propagation and stochastic gradient descent (SGD)  ", "word_idx": 25039, "sentence_idx": 309, "label": "unlabeled"}, {"type": "text", "expr": "\nWe follow the \u201cimage-centric\u201d sampling strategy from   to train this network", "word_idx": 25132, "sentence_idx": 310, "label": "unlabeled"}, {"type": "text", "expr": " Each mini-batch arises from a single image that contains many positive and negative example anchors", "word_idx": 25209, "sentence_idx": 311, "label": "unlabeled"}, {"type": "text", "expr": " It is possible to optimize for the loss functions of all anchors, but this will bias towards negative samples as they are dominate", "word_idx": 25309, "sentence_idx": 312, "label": "unlabeled"}, {"type": "text", "expr": " Instead, we randomly sample 256 anchors in an image to compute the loss function of a mini-batch, where the sampled positive and negative anchors have a ratio of  up to  1:1", "word_idx": 25440, "sentence_idx": 313, "label": "unlabeled"}, {"type": "text", "expr": " If there are fewer than 128 positive samples in an image, we pad the mini-batch with negative ones", "word_idx": 25614, "sentence_idx": 314, "label": "unlabeled"}, {"type": "text", "expr": "up to", "word_idx": 25713, "sentence_idx": 315, "label": "unlabeled"}, {"type": "text", "expr": "We randomly initialize all new layers by drawing weights from a zero-mean Gaussian distribution with standard deviation 0", "word_idx": 25718, "sentence_idx": 316, "label": "unlabeled"}, {"type": "text", "expr": " All other layers ( \\ie , the shared convolutional layers) are initialized by pre-training a model for ImageNet classification  , as is standard practice  ", "word_idx": 25839, "sentence_idx": 317, "label": "unlabeled"}, {"type": "text", "expr": " We tune all layers of the ZF net, and conv3 $\\_1$  and up for the VGG net to conserve memory  ", "word_idx": 25994, "sentence_idx": 318, "label": "unlabeled"}, {"type": "text", "expr": "\nWe use a learning rate of 0", "word_idx": 26089, "sentence_idx": 319, "label": "unlabeled"}, {"type": "text", "expr": "001 for 60k mini-batches, and 0", "word_idx": 26117, "sentence_idx": 320, "label": "unlabeled"}, {"type": "text", "expr": "0001 for the next 20k mini-batches on the PASCAL VOC dataset", "word_idx": 26148, "sentence_idx": 321, "label": "unlabeled"}, {"type": "text", "expr": " We use a momentum of 0", "word_idx": 26208, "sentence_idx": 322, "label": "unlabeled"}, {"type": "text", "expr": "9 and a weight decay of 0", "word_idx": 26231, "sentence_idx": 323, "label": "unlabeled"}, {"type": "text", "expr": "0005  ", "word_idx": 26256, "sentence_idx": 324, "label": "unlabeled"}, {"type": "text", "expr": " Our implementation uses Caffe  ", "word_idx": 26262, "sentence_idx": 325, "label": "unlabeled"}, {"type": "math", "expr": "$$\\_1$$", "word_idx": 26294, "sentence_idx": 326, "label": "unlabeled"}, {"type": "text", "expr": "2  Sharing Features for RPN and Fast R-CNN", "word_idx": 26297, "sentence_idx": 327, "label": "unlabeled"}, {"type": "text", "expr": "Thus far we have described how to train a network for region proposal generation, without considering the region-based object detection CNN that will utilize these proposals", "word_idx": 26339, "sentence_idx": 328, "label": "unlabeled"}, {"type": "text", "expr": "\nFor the detection network, we adopt Fast R-CNN  ", "word_idx": 26512, "sentence_idx": 329, "label": "unlabeled"}, {"type": "text", "expr": "\nNext we describe algorithms that learn a unified network composed of RPN and Fast R-CNN with shared convolutional layers (Figure\u00a0 2 )", "word_idx": 26561, "sentence_idx": 330, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:  the learned average proposal size for each anchor using the ZF net (numbers for  $s=600$ )", "word_idx": 26695, "sentence_idx": 331, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:", "word_idx": 26795, "sentence_idx": 332, "label": "unlabeled"}, {"type": "math", "expr": "$$s=600$$", "word_idx": 26803, "sentence_idx": 333, "label": "unlabeled"}, {"type": "text", "expr": "anchor", "word_idx": 26808, "sentence_idx": 334, "label": "unlabeled"}, {"type": "text", "expr": "anchor", "word_idx": 26814, "sentence_idx": 335, "label": "unlabeled"}, {"type": "text", "expr": "anchor", "word_idx": 26820, "sentence_idx": 336, "label": "unlabeled"}, {"type": "text", "expr": "$128^{2}$ , 2:1", "word_idx": 26826, "sentence_idx": 337, "label": "unlabeled"}, {"type": "math", "expr": "$$128^{2}$$", "word_idx": 26841, "sentence_idx": 338, "label": "unlabeled"}, {"type": "text", "expr": ", 2:1", "word_idx": 26848, "sentence_idx": 339, "label": "unlabeled"}, {"type": "text", "expr": "$128^{2}$ , 1:1", "word_idx": 26853, "sentence_idx": 340, "label": "unlabeled"}, {"type": "math", "expr": "$$128^{2}$$", "word_idx": 26868, "sentence_idx": 341, "label": "unlabeled"}, {"type": "text", "expr": ", 1:1", "word_idx": 26875, "sentence_idx": 342, "label": "unlabeled"}, {"type": "text", "expr": "$128^{2}$ , 1:2", "word_idx": 26880, "sentence_idx": 343, "label": "unlabeled"}, {"type": "math", "expr": "$$128^{2}$$", "word_idx": 26895, "sentence_idx": 344, "label": "unlabeled"}, {"type": "text", "expr": ", 1:2", "word_idx": 26902, "sentence_idx": 345, "label": "unlabeled"}, {"type": "text", "expr": "$256^{2}$ , 2:1", "word_idx": 26907, "sentence_idx": 346, "label": "unlabeled"}, {"type": "math", "expr": "$$256^{2}$$", "word_idx": 26922, "sentence_idx": 347, "label": "unlabeled"}, {"type": "text", "expr": ", 2:1", "word_idx": 26929, "sentence_idx": 348, "label": "unlabeled"}, {"type": "text", "expr": "$256^{2}$ , 1:1", "word_idx": 26934, "sentence_idx": 349, "label": "unlabeled"}, {"type": "math", "expr": "$$256^{2}$$", "word_idx": 26949, "sentence_idx": 350, "label": "unlabeled"}, {"type": "text", "expr": ", 1:1", "word_idx": 26956, "sentence_idx": 351, "label": "unlabeled"}, {"type": "text", "expr": "$256^{2}$ , 1:2", "word_idx": 26961, "sentence_idx": 352, "label": "unlabeled"}, {"type": "math", "expr": "$$256^{2}$$", "word_idx": 26976, "sentence_idx": 353, "label": "unlabeled"}, {"type": "text", "expr": ", 1:2", "word_idx": 26983, "sentence_idx": 354, "label": "unlabeled"}, {"type": "text", "expr": "$512^{2}$ , 2:1", "word_idx": 26988, "sentence_idx": 355, "label": "unlabeled"}, {"type": "math", "expr": "$$512^{2}$$", "word_idx": 27003, "sentence_idx": 356, "label": "unlabeled"}, {"type": "text", "expr": ", 2:1", "word_idx": 27010, "sentence_idx": 357, "label": "unlabeled"}, {"type": "text", "expr": "$512^{2}$ , 1:1", "word_idx": 27015, "sentence_idx": 358, "label": "unlabeled"}, {"type": "math", "expr": "$$512^{2}$$", "word_idx": 27030, "sentence_idx": 359, "label": "unlabeled"}, {"type": "text", "expr": ", 1:1", "word_idx": 27037, "sentence_idx": 360, "label": "unlabeled"}, {"type": "text", "expr": "$512^{2}$ , 1:2", "word_idx": 27042, "sentence_idx": 361, "label": "unlabeled"}, {"type": "math", "expr": "$$512^{2}$$", "word_idx": 27057, "sentence_idx": 362, "label": "unlabeled"}, {"type": "text", "expr": ", 1:2", "word_idx": 27064, "sentence_idx": 363, "label": "unlabeled"}, {"type": "text", "expr": "proposal", "word_idx": 27069, "sentence_idx": 364, "label": "unlabeled"}, {"type": "text", "expr": "proposal", "word_idx": 27077, "sentence_idx": 365, "label": "unlabeled"}, {"type": "text", "expr": "proposal", "word_idx": 27085, "sentence_idx": 366, "label": "unlabeled"}, {"type": "text", "expr": "188 $\\times$ 111", "word_idx": 27093, "sentence_idx": 367, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 27109, "sentence_idx": 368, "label": "unlabeled"}, {"type": "text", "expr": "113 $\\times$ 114", "word_idx": 27115, "sentence_idx": 369, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 27131, "sentence_idx": 370, "label": "unlabeled"}, {"type": "text", "expr": "70 $\\times$ 92", "word_idx": 27137, "sentence_idx": 371, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 27151, "sentence_idx": 372, "label": "unlabeled"}, {"type": "text", "expr": "416 $\\times$ 229", "word_idx": 27157, "sentence_idx": 373, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 27173, "sentence_idx": 374, "label": "unlabeled"}, {"type": "text", "expr": "261 $\\times$ 284", "word_idx": 27179, "sentence_idx": 375, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 27195, "sentence_idx": 376, "label": "unlabeled"}, {"type": "text", "expr": "174 $\\times$ 332", "word_idx": 27201, "sentence_idx": 377, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 27217, "sentence_idx": 378, "label": "unlabeled"}, {"type": "text", "expr": "768 $\\times$ 437", "word_idx": 27223, "sentence_idx": 379, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 27239, "sentence_idx": 380, "label": "unlabeled"}, {"type": "text", "expr": "499 $\\times$ 501", "word_idx": 27245, "sentence_idx": 381, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 27261, "sentence_idx": 382, "label": "unlabeled"}, {"type": "text", "expr": "355 $\\times$ 715", "word_idx": 27267, "sentence_idx": 383, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 27283, "sentence_idx": 384, "label": "unlabeled"}, {"type": "text", "expr": "Both RPN and Fast R-CNN, trained independently, will modify their convolutional layers in different ways", "word_idx": 27289, "sentence_idx": 385, "label": "unlabeled"}, {"type": "text", "expr": "\nWe therefore need to develop a technique that allows for sharing convolutional layers between the two networks, rather than learning two separate networks", "word_idx": 27393, "sentence_idx": 386, "label": "unlabeled"}, {"type": "text", "expr": " We discuss three ways for training networks with features shared:", "word_idx": 27548, "sentence_idx": 387, "label": "unlabeled"}, {"type": "text", "expr": "Both RPN and Fast R-CNN, trained independently, will modify their convolutional layers in different ways", "word_idx": 27614, "sentence_idx": 388, "label": "unlabeled"}, {"type": "text", "expr": "\nWe therefore need to develop a technique that allows for sharing convolutional layers between the two networks, rather than learning two separate networks", "word_idx": 27718, "sentence_idx": 389, "label": "unlabeled"}, {"type": "text", "expr": " We discuss three ways for training networks with features shared:", "word_idx": 27873, "sentence_idx": 390, "label": "unlabeled"}, {"type": "text", "expr": "(i)  Alternating training ", "word_idx": 27939, "sentence_idx": 391, "label": "unlabeled"}, {"type": "text", "expr": " In this solution, we first train RPN, and use the proposals to train Fast R-CNN", "word_idx": 27965, "sentence_idx": 392, "label": "unlabeled"}, {"type": "text", "expr": " The network tuned by Fast R-CNN is then used to initialize RPN, and this process is iterated", "word_idx": 28045, "sentence_idx": 393, "label": "unlabeled"}, {"type": "text", "expr": " This is the solution that is used in all experiments in this paper", "word_idx": 28138, "sentence_idx": 394, "label": "unlabeled"}, {"type": "text", "expr": "Alternating training", "word_idx": 28205, "sentence_idx": 395, "label": "unlabeled"}, {"type": "text", "expr": "(ii)  Approximate joint training ", "word_idx": 28225, "sentence_idx": 396, "label": "unlabeled"}, {"type": "text", "expr": " In this solution, the RPN and Fast R-CNN networks are merged into one network during training as in Figure\u00a0 2 ", "word_idx": 28258, "sentence_idx": 397, "label": "unlabeled"}, {"type": "text", "expr": " In each SGD iteration, the forward pass generates region proposals which are treated just like fixed, pre-computed proposals when training a Fast R-CNN detector", "word_idx": 28369, "sentence_idx": 398, "label": "unlabeled"}, {"type": "text", "expr": "\nThe backward propagation takes place as usual, where for the shared layers the backward propagated signals from both the RPN loss and the Fast R-CNN loss are combined", "word_idx": 28530, "sentence_idx": 399, "label": "unlabeled"}, {"type": "text", "expr": " This solution is easy to implement", "word_idx": 28697, "sentence_idx": 400, "label": "unlabeled"}, {"type": "text", "expr": " But this solution ignores the derivative  \\wrt the proposal boxes\u2019 coordinates that are also network responses, so is approximate", "word_idx": 28732, "sentence_idx": 401, "label": "unlabeled"}, {"type": "text", "expr": "\nIn our experiments, we have empirically found this solver produces close results, yet reduces the training time by about 25-50% comparing with alternating training", "word_idx": 28862, "sentence_idx": 402, "label": "unlabeled"}, {"type": "text", "expr": " This solver is included in our released Python code", "word_idx": 29026, "sentence_idx": 403, "label": "unlabeled"}, {"type": "text", "expr": "Approximate joint training", "word_idx": 29078, "sentence_idx": 404, "label": "unlabeled"}, {"type": "text", "expr": "(iii)  Non-approximate joint training ", "word_idx": 29104, "sentence_idx": 405, "label": "unlabeled"}, {"type": "text", "expr": " As discussed above, the bounding boxes predicted by RPN are also functions of the input", "word_idx": 29142, "sentence_idx": 406, "label": "unlabeled"}, {"type": "text", "expr": " The RoI pooling layer   in Fast R-CNN accepts the convolutional features and also the predicted bounding boxes as input, so a theoretically valid backpropagation solver should also involve gradients  \\wrt the box coordinates", "word_idx": 29230, "sentence_idx": 407, "label": "unlabeled"}, {"type": "text", "expr": " These gradients are ignored in the above approximate joint training", "word_idx": 29455, "sentence_idx": 408, "label": "unlabeled"}, {"type": "text", "expr": "\nIn a non-approximate joint training solution, we need an RoI pooling layer that is differentiable  \\wrt the box coordinates", "word_idx": 29523, "sentence_idx": 409, "label": "unlabeled"}, {"type": "text", "expr": "\nThis is a nontrivial problem and a solution can be given by an \u201cRoI warping\u201d layer as developed in  , which is beyond the scope of this paper", "word_idx": 29647, "sentence_idx": 410, "label": "unlabeled"}, {"type": "text", "expr": "Non-approximate joint training", "word_idx": 29789, "sentence_idx": 411, "label": "unlabeled"}, {"type": "text", "expr": "4-Step Alternating Training ", "word_idx": 29819, "sentence_idx": 412, "label": "unlabeled"}, {"type": "text", "expr": " In this paper, we adopt a pragmatic 4-step training algorithm to learn shared features via alternating optimization", "word_idx": 29847, "sentence_idx": 413, "label": "unlabeled"}, {"type": "text", "expr": "\nIn the first step, we train the RPN as described in Section\u00a0 3", "word_idx": 29963, "sentence_idx": 414, "label": "unlabeled"}, {"type": "text", "expr": " This network is initialized with an ImageNet-pre-trained model and fine-tuned end-to-end for the region proposal task", "word_idx": 30026, "sentence_idx": 415, "label": "unlabeled"}, {"type": "text", "expr": "\nIn the second step, we train a separate detection network by Fast R-CNN using the proposals generated by the step-1 RPN", "word_idx": 30144, "sentence_idx": 416, "label": "unlabeled"}, {"type": "text", "expr": " This detection network is also initialized by the ImageNet-pre-trained model", "word_idx": 30264, "sentence_idx": 417, "label": "unlabeled"}, {"type": "text", "expr": " At this point the two networks do not share convolutional layers", "word_idx": 30341, "sentence_idx": 418, "label": "unlabeled"}, {"type": "text", "expr": " In the third step, we use the detector network to initialize RPN training, but we fix the shared convolutional layers and only fine-tune the layers unique to RPN", "word_idx": 30406, "sentence_idx": 419, "label": "unlabeled"}, {"type": "text", "expr": " Now the two networks share convolutional layers", "word_idx": 30568, "sentence_idx": 420, "label": "unlabeled"}, {"type": "text", "expr": " Finally, keeping the shared convolutional layers fixed, we fine-tune the unique layers of Fast R-CNN", "word_idx": 30616, "sentence_idx": 421, "label": "unlabeled"}, {"type": "text", "expr": " As such, both networks share the same convolutional layers and form a unified network", "word_idx": 30717, "sentence_idx": 422, "label": "unlabeled"}, {"type": "text", "expr": " A similar alternating training can be run for more iterations, but we have observed negligible improvements", "word_idx": 30803, "sentence_idx": 423, "label": "unlabeled"}, {"type": "text", "expr": "4-Step Alternating Training", "word_idx": 30911, "sentence_idx": 424, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:  Detection results on  PASCAL VOC 2007 test set  (trained on VOC 2007 trainval)", "word_idx": 30938, "sentence_idx": 425, "label": "unlabeled"}, {"type": "text", "expr": " The detectors are Fast R-CNN with ZF, but using various proposal methods for training and testing", "word_idx": 31026, "sentence_idx": 426, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:", "word_idx": 31124, "sentence_idx": 427, "label": "unlabeled"}, {"type": "text", "expr": "PASCAL VOC 2007 test set", "word_idx": 31132, "sentence_idx": 428, "label": "unlabeled"}, {"type": "text", "expr": "train-time region proposals test-time region proposals", "word_idx": 31156, "sentence_idx": 429, "label": "unlabeled"}, {"type": "text", "expr": "train-time region proposals", "word_idx": 31210, "sentence_idx": 430, "label": "unlabeled"}, {"type": "text", "expr": "test-time region proposals", "word_idx": 31237, "sentence_idx": 431, "label": "unlabeled"}, {"type": "text", "expr": "test-time region proposals", "word_idx": 31263, "sentence_idx": 432, "label": "unlabeled"}, {"type": "text", "expr": "method # boxes method # proposals mAP (%)", "word_idx": 31289, "sentence_idx": 433, "label": "unlabeled"}, {"type": "text", "expr": "method", "word_idx": 31330, "sentence_idx": 434, "label": "unlabeled"}, {"type": "text", "expr": "# boxes", "word_idx": 31336, "sentence_idx": 435, "label": "unlabeled"}, {"type": "text", "expr": "method", "word_idx": 31343, "sentence_idx": 436, "label": "unlabeled"}, {"type": "text", "expr": "method", "word_idx": 31349, "sentence_idx": 437, "label": "unlabeled"}, {"type": "text", "expr": "# proposals", "word_idx": 31355, "sentence_idx": 438, "label": "unlabeled"}, {"type": "text", "expr": "# proposals", "word_idx": 31366, "sentence_idx": 439, "label": "unlabeled"}, {"type": "text", "expr": "mAP (%)", "word_idx": 31377, "sentence_idx": 440, "label": "unlabeled"}, {"type": "text", "expr": "mAP (%)", "word_idx": 31384, "sentence_idx": 441, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 SS 2000 58", "word_idx": 31391, "sentence_idx": 442, "label": "unlabeled"}, {"type": "text", "expr": "EB 2000 EB 2000 58", "word_idx": 31409, "sentence_idx": 443, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF, shared 2000 RPN+ZF, shared 300 59", "word_idx": 31427, "sentence_idx": 444, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF, shared", "word_idx": 31468, "sentence_idx": 445, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF, shared", "word_idx": 31482, "sentence_idx": 446, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF, shared", "word_idx": 31496, "sentence_idx": 447, "label": "unlabeled"}, {"type": "text", "expr": "ablation experiments follow below", "word_idx": 31510, "sentence_idx": 448, "label": "unlabeled"}, {"type": "text", "expr": "ablation experiments follow below", "word_idx": 31543, "sentence_idx": 449, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF, unshared 2000 RPN+ZF, unshared 300 58", "word_idx": 31576, "sentence_idx": 450, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF, unshared", "word_idx": 31621, "sentence_idx": 451, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF, unshared", "word_idx": 31637, "sentence_idx": 452, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF, unshared", "word_idx": 31653, "sentence_idx": 453, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 RPN+ZF 100 55", "word_idx": 31669, "sentence_idx": 454, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF", "word_idx": 31690, "sentence_idx": 455, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF", "word_idx": 31696, "sentence_idx": 456, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 RPN+ZF 300 56", "word_idx": 31702, "sentence_idx": 457, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF", "word_idx": 31723, "sentence_idx": 458, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF", "word_idx": 31729, "sentence_idx": 459, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 RPN+ZF 1000 56", "word_idx": 31735, "sentence_idx": 460, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF", "word_idx": 31757, "sentence_idx": 461, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF", "word_idx": 31763, "sentence_idx": 462, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 RPN+ZF (no NMS) 6000 55", "word_idx": 31769, "sentence_idx": 463, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF (no NMS)", "word_idx": 31800, "sentence_idx": 464, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF (no NMS)", "word_idx": 31815, "sentence_idx": 465, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 100 44", "word_idx": 31830, "sentence_idx": 466, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF (no  cls )", "word_idx": 31844, "sentence_idx": 467, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF (no", "word_idx": 31861, "sentence_idx": 468, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 300 51", "word_idx": 31871, "sentence_idx": 469, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF (no  cls )", "word_idx": 31885, "sentence_idx": 470, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF (no", "word_idx": 31902, "sentence_idx": 471, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 1000 55", "word_idx": 31912, "sentence_idx": 472, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF (no  cls )", "word_idx": 31927, "sentence_idx": 473, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF (no", "word_idx": 31944, "sentence_idx": 474, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 300 52", "word_idx": 31954, "sentence_idx": 475, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF (no  reg )", "word_idx": 31968, "sentence_idx": 476, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF (no", "word_idx": 31985, "sentence_idx": 477, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 1000 51", "word_idx": 31995, "sentence_idx": 478, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF (no  reg )", "word_idx": 32010, "sentence_idx": 479, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF (no", "word_idx": 32027, "sentence_idx": 480, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 RPN+VGG 300 59", "word_idx": 32037, "sentence_idx": 481, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG", "word_idx": 32059, "sentence_idx": 482, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG", "word_idx": 32066, "sentence_idx": 483, "label": "unlabeled"}, {"type": "text", "expr": "3  Implementation Details", "word_idx": 32073, "sentence_idx": 484, "label": "unlabeled"}, {"type": "text", "expr": "We train and test both region proposal and object detection networks on images of a single scale  ", "word_idx": 32098, "sentence_idx": 485, "label": "unlabeled"}, {"type": "text", "expr": " We re-scale the images such that their shorter side is  $s=600$  pixels  ", "word_idx": 32196, "sentence_idx": 486, "label": "unlabeled"}, {"type": "text", "expr": " Multi-scale feature extraction (using an image pyramid) may improve accuracy but does not exhibit a good speed-accuracy trade-off  ", "word_idx": 32270, "sentence_idx": 487, "label": "unlabeled"}, {"type": "text", "expr": " On the re-scaled images, the total stride for both ZF and VGG nets on the last convolutional layer is 16 pixels, and thus is  $\\sim$ 10 pixels on a typical PASCAL image before resizing ( $\\sim$ 500 $\\times$ 375)", "word_idx": 32402, "sentence_idx": 488, "label": "unlabeled"}, {"type": "text", "expr": " Even such a large stride provides good results, though accuracy may be further improved with a smaller stride", "word_idx": 32614, "sentence_idx": 489, "label": "unlabeled"}, {"type": "math", "expr": "$$s=600$$", "word_idx": 32724, "sentence_idx": 490, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sim$$", "word_idx": 32729, "sentence_idx": 491, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sim$$", "word_idx": 32733, "sentence_idx": 492, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 32737, "sentence_idx": 493, "label": "unlabeled"}, {"type": "text", "expr": "For anchors, we use 3 scales with box areas of  $128^{2}$ ,  $256^{2}$ , and  $512^{2}$  pixels, and 3 aspect ratios of 1:1, 1:2, and 2:1", "word_idx": 32743, "sentence_idx": 494, "label": "unlabeled"}, {"type": "text", "expr": " These hyper-parameters are  not  carefully chosen for a particular dataset, and we provide ablation experiments on their effects in the next section", "word_idx": 32880, "sentence_idx": 495, "label": "unlabeled"}, {"type": "text", "expr": "\nAs discussed, our solution does not need an image pyramid or filter pyramid to predict regions of multiple scales, saving considerable running time", "word_idx": 33029, "sentence_idx": 496, "label": "unlabeled"}, {"type": "text", "expr": "\nFigure\u00a0 3  (right) shows the capability of our method for a wide range of scales and aspect ratios", "word_idx": 33177, "sentence_idx": 497, "label": "unlabeled"}, {"type": "text", "expr": " Table\u00a0 1  shows the learned average proposal size for each anchor using the ZF net", "word_idx": 33276, "sentence_idx": 498, "label": "unlabeled"}, {"type": "text", "expr": "\nWe note that our algorithm allows predictions that are larger than the underlying receptive field", "word_idx": 33359, "sentence_idx": 499, "label": "unlabeled"}, {"type": "text", "expr": " Such predictions are not impossible\u2014one may still roughly infer the extent of an object if only the middle of the object is visible", "word_idx": 33457, "sentence_idx": 500, "label": "unlabeled"}, {"type": "math", "expr": "$$128^{2}$$", "word_idx": 33589, "sentence_idx": 501, "label": "unlabeled"}, {"type": "math", "expr": "$$256^{2}$$", "word_idx": 33596, "sentence_idx": 502, "label": "unlabeled"}, {"type": "math", "expr": "$$512^{2}$$", "word_idx": 33603, "sentence_idx": 503, "label": "unlabeled"}, {"type": "text", "expr": "The anchor boxes that cross image boundaries need to be handled with care", "word_idx": 33610, "sentence_idx": 504, "label": "unlabeled"}, {"type": "text", "expr": " During training, we ignore all cross-boundary anchors so they do not contribute to the loss", "word_idx": 33683, "sentence_idx": 505, "label": "unlabeled"}, {"type": "text", "expr": " For a typical  $1000\\times 600$  image, there will be roughly 20000 ( $\\approx 60\\times 40\\times 9$ ) anchors in total", "word_idx": 33775, "sentence_idx": 506, "label": "unlabeled"}, {"type": "text", "expr": " With the cross-boundary anchors ignored, there are about 6000 anchors per image for training", "word_idx": 33894, "sentence_idx": 507, "label": "unlabeled"}, {"type": "text", "expr": " If the boundary-crossing outliers are not ignored in training, they introduce large, difficult to correct error terms in the objective, and training does not converge", "word_idx": 33987, "sentence_idx": 508, "label": "unlabeled"}, {"type": "text", "expr": " During testing, however, we still apply the fully convolutional RPN to the entire image", "word_idx": 34154, "sentence_idx": 509, "label": "unlabeled"}, {"type": "text", "expr": " This may generate cross-boundary proposal boxes, which we clip to the image boundary", "word_idx": 34242, "sentence_idx": 510, "label": "unlabeled"}, {"type": "math", "expr": "$$1000\\times 600$$", "word_idx": 34327, "sentence_idx": 511, "label": "unlabeled"}, {"type": "math", "expr": "$$\\approx 60\\times 40\\times 9$$", "word_idx": 34341, "sentence_idx": 512, "label": "unlabeled"}, {"type": "text", "expr": "Some RPN proposals highly overlap with each other", "word_idx": 34368, "sentence_idx": 513, "label": "unlabeled"}, {"type": "text", "expr": " To reduce redundancy, we adopt non-maximum suppression (NMS) on the proposal regions based on their  cls  scores", "word_idx": 34417, "sentence_idx": 514, "label": "unlabeled"}, {"type": "text", "expr": " We fix the IoU threshold for NMS at 0", "word_idx": 34530, "sentence_idx": 515, "label": "unlabeled"}, {"type": "text", "expr": "7, which leaves us about 2000 proposal regions per image", "word_idx": 34568, "sentence_idx": 516, "label": "unlabeled"}, {"type": "text", "expr": " As we will show, NMS does not harm the ultimate detection accuracy, but substantially reduces the number of proposals", "word_idx": 34624, "sentence_idx": 517, "label": "unlabeled"}, {"type": "text", "expr": "\nAfter NMS, we use the top- $N$  ranked proposal regions for detection", "word_idx": 34742, "sentence_idx": 518, "label": "unlabeled"}, {"type": "text", "expr": " In the following, we train Fast R-CNN using 2000 RPN proposals, but evaluate different numbers of proposals at test-time", "word_idx": 34812, "sentence_idx": 519, "label": "unlabeled"}, {"type": "text", "expr": "4  Experiments", "word_idx": 34933, "sentence_idx": 520, "label": "unlabeled"}, {"type": "text", "expr": "1  Experiments on PASCAL VOC", "word_idx": 34947, "sentence_idx": 521, "label": "unlabeled"}, {"type": "text", "expr": "We comprehensively evaluate our method on the PASCAL VOC 2007 detection benchmark  ", "word_idx": 34975, "sentence_idx": 522, "label": "unlabeled"}, {"type": "text", "expr": " This dataset consists of about 5k trainval images and 5k test images over 20 object categories", "word_idx": 35058, "sentence_idx": 523, "label": "unlabeled"}, {"type": "text", "expr": "\nWe also provide results on the PASCAL VOC 2012 benchmark for a few models", "word_idx": 35153, "sentence_idx": 524, "label": "unlabeled"}, {"type": "text", "expr": "\nFor the ImageNet pre-trained network, we use the \u201cfast\u201d version of ZF net   that has 5 convolutional layers and 3 fully-connected layers, and the public VGG-16 model    that has 13 convolutional layers and 3 fully-connected layers", "word_idx": 35227, "sentence_idx": 525, "label": "unlabeled"}, {"type": "text", "expr": "\nWe primarily evaluate detection mean Average Precision (mAP), because this is the actual metric for object detection (rather than focusing on object proposal proxy metrics)", "word_idx": 35458, "sentence_idx": 526, "label": "unlabeled"}, {"type": "text", "expr": "7 www", "word_idx": 35631, "sentence_idx": 527, "label": "unlabeled"}, {"type": "text", "expr": "robots", "word_idx": 35636, "sentence_idx": 528, "label": "unlabeled"}, {"type": "text", "expr": "uk/~vgg/research/very_deep/", "word_idx": 35642, "sentence_idx": 529, "label": "unlabeled"}, {"type": "text", "expr": "robots", "word_idx": 35669, "sentence_idx": 530, "label": "unlabeled"}, {"type": "text", "expr": "uk/~vgg/research/very_deep/", "word_idx": 35675, "sentence_idx": 531, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a0 2  (top) shows Fast R-CNN results when trained and tested using various region proposal methods", "word_idx": 35702, "sentence_idx": 532, "label": "unlabeled"}, {"type": "text", "expr": " These results use the ZF net", "word_idx": 35804, "sentence_idx": 533, "label": "unlabeled"}, {"type": "text", "expr": "\nFor Selective Search (SS)  , we generate about 2000 proposals by the \u201cfast\u201d mode", "word_idx": 35833, "sentence_idx": 534, "label": "unlabeled"}, {"type": "text", "expr": " For EdgeBoxes (EB)  , we generate the proposals by the default EB setting tuned for 0", "word_idx": 35914, "sentence_idx": 535, "label": "unlabeled"}, {"type": "text", "expr": "7 IoU", "word_idx": 36000, "sentence_idx": 536, "label": "unlabeled"}, {"type": "text", "expr": "\nSS has an mAP of 58", "word_idx": 36005, "sentence_idx": 537, "label": "unlabeled"}, {"type": "text", "expr": "7% and EB has an mAP of 58", "word_idx": 36025, "sentence_idx": 538, "label": "unlabeled"}, {"type": "text", "expr": "6% under the Fast R-CNN framework", "word_idx": 36051, "sentence_idx": 539, "label": "unlabeled"}, {"type": "text", "expr": "\nRPN with Fast R-CNN achieves competitive results, with an mAP of 59", "word_idx": 36084, "sentence_idx": 540, "label": "unlabeled"}, {"type": "text", "expr": "9% while using  up to  300 proposals ", "word_idx": 36152, "sentence_idx": 541, "label": "unlabeled"}, {"type": "text", "expr": "\nUsing RPN yields a much faster detection system than using either SS or EB because of shared convolutional computations; the fewer proposals also reduce the region-wise fully-connected layers\u2019 cost (Table\u00a0 5 )", "word_idx": 36189, "sentence_idx": 542, "label": "unlabeled"}, {"type": "text", "expr": "up to", "word_idx": 36399, "sentence_idx": 543, "label": "unlabeled"}, {"type": "text", "expr": "8 For RPN, the number of proposals ( \\eg , 300) is the maximum number for an image", "word_idx": 36404, "sentence_idx": 544, "label": "unlabeled"}, {"type": "text", "expr": " RPN may produce fewer proposals after NMS, and thus the average number of proposals is smaller", "word_idx": 36486, "sentence_idx": 545, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a03:  Detection results on  PASCAL VOC 2007 test set ", "word_idx": 36581, "sentence_idx": 546, "label": "unlabeled"}, {"type": "text", "expr": " The detector is Fast R-CNN and VGG-16", "word_idx": 36638, "sentence_idx": 547, "label": "unlabeled"}, {"type": "text", "expr": " Training data: \u201c07\u201d: VOC 2007 trainval, \u201c07+12\u201d: union set of VOC 2007 trainval and VOC 2012 trainval", "word_idx": 36676, "sentence_idx": 548, "label": "unlabeled"}, {"type": "text", "expr": " For RPN, the train-time proposals for Fast R-CNN are 2000", "word_idx": 36778, "sentence_idx": 549, "label": "unlabeled"}, {"type": "text", "expr": "  ${}^{\\dagger}$ : this number was reported in  ; using the repository provided by this paper, this result is higher (68", "word_idx": 36836, "sentence_idx": 550, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a03:", "word_idx": 36956, "sentence_idx": 551, "label": "unlabeled"}, {"type": "text", "expr": "PASCAL VOC 2007 test set", "word_idx": 36964, "sentence_idx": 552, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\dagger}$$", "word_idx": 36988, "sentence_idx": 553, "label": "unlabeled"}, {"type": "text", "expr": "method # proposals data mAP (%)", "word_idx": 37000, "sentence_idx": 554, "label": "unlabeled"}, {"type": "text", "expr": "method", "word_idx": 37031, "sentence_idx": 555, "label": "unlabeled"}, {"type": "text", "expr": "# proposals", "word_idx": 37037, "sentence_idx": 556, "label": "unlabeled"}, {"type": "text", "expr": "mAP (%)", "word_idx": 37048, "sentence_idx": 557, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 07", "word_idx": 37055, "sentence_idx": 558, "label": "unlabeled"}, {"type": "text", "expr": "9 ${}^{\\dagger}$", "word_idx": 37065, "sentence_idx": 559, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\dagger}$$", "word_idx": 37081, "sentence_idx": 560, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 07+12 70", "word_idx": 37093, "sentence_idx": 561, "label": "unlabeled"}, {"type": "text", "expr": "07+12", "word_idx": 37109, "sentence_idx": 562, "label": "unlabeled"}, {"type": "text", "expr": "07+12", "word_idx": 37114, "sentence_idx": 563, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG, unshared 300 07 68", "word_idx": 37119, "sentence_idx": 564, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG, unshared", "word_idx": 37146, "sentence_idx": 565, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG, shared 300 07 69", "word_idx": 37163, "sentence_idx": 566, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG, shared", "word_idx": 37188, "sentence_idx": 567, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG, shared 300 07+12 73", "word_idx": 37203, "sentence_idx": 568, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG, shared", "word_idx": 37231, "sentence_idx": 569, "label": "unlabeled"}, {"type": "text", "expr": "07+12", "word_idx": 37246, "sentence_idx": 570, "label": "unlabeled"}, {"type": "text", "expr": "07+12", "word_idx": 37251, "sentence_idx": 571, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG, shared 300 COCO+07+12 78", "word_idx": 37256, "sentence_idx": 572, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG, shared", "word_idx": 37289, "sentence_idx": 573, "label": "unlabeled"}, {"type": "text", "expr": "COCO+07+12", "word_idx": 37304, "sentence_idx": 574, "label": "unlabeled"}, {"type": "text", "expr": "COCO+07+12", "word_idx": 37314, "sentence_idx": 575, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a04:  Detection results on  PASCAL VOC 2012 test set ", "word_idx": 37324, "sentence_idx": 576, "label": "unlabeled"}, {"type": "text", "expr": " The detector is Fast R-CNN and VGG-16", "word_idx": 37381, "sentence_idx": 577, "label": "unlabeled"}, {"type": "text", "expr": " Training data: \u201c07\u201d: VOC 2007 trainval, \u201c07++12\u201d: union set of VOC 2007 trainval+test and VOC 2012 trainval", "word_idx": 37419, "sentence_idx": 578, "label": "unlabeled"}, {"type": "text", "expr": " For RPN, the train-time proposals for Fast R-CNN are 2000", "word_idx": 37527, "sentence_idx": 579, "label": "unlabeled"}, {"type": "text", "expr": "  ${}^{\\dagger}$ :  http://host", "word_idx": 37585, "sentence_idx": 580, "label": "unlabeled"}, {"type": "text", "expr": "robots", "word_idx": 37616, "sentence_idx": 581, "label": "unlabeled"}, {"type": "text", "expr": "uk:8080/anonymous/HZJTQA", "word_idx": 37622, "sentence_idx": 582, "label": "unlabeled"}, {"type": "text", "expr": "html ", "word_idx": 37646, "sentence_idx": 583, "label": "unlabeled"}, {"type": "text", "expr": "  ${}^{\\ddagger}$ :  http://host", "word_idx": 37651, "sentence_idx": 584, "label": "unlabeled"}, {"type": "text", "expr": "robots", "word_idx": 37683, "sentence_idx": 585, "label": "unlabeled"}, {"type": "text", "expr": "uk:8080/anonymous/YNPLXB", "word_idx": 37689, "sentence_idx": 586, "label": "unlabeled"}, {"type": "text", "expr": "html ", "word_idx": 37713, "sentence_idx": 587, "label": "unlabeled"}, {"type": "text", "expr": "  ${}^{\\S}$ :  http://host", "word_idx": 37718, "sentence_idx": 588, "label": "unlabeled"}, {"type": "text", "expr": "robots", "word_idx": 37744, "sentence_idx": 589, "label": "unlabeled"}, {"type": "text", "expr": "uk:8080/anonymous/XEDH10", "word_idx": 37750, "sentence_idx": 590, "label": "unlabeled"}, {"type": "text", "expr": "html ", "word_idx": 37774, "sentence_idx": 591, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a04:", "word_idx": 37779, "sentence_idx": 592, "label": "unlabeled"}, {"type": "text", "expr": "PASCAL VOC 2012 test set", "word_idx": 37787, "sentence_idx": 593, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\dagger}$$", "word_idx": 37811, "sentence_idx": 594, "label": "unlabeled"}, {"type": "text", "expr": "http://host", "word_idx": 37823, "sentence_idx": 595, "label": "unlabeled"}, {"type": "text", "expr": "robots", "word_idx": 37834, "sentence_idx": 596, "label": "unlabeled"}, {"type": "text", "expr": "uk:8080/anonymous/HZJTQA", "word_idx": 37840, "sentence_idx": 597, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\ddagger}$$", "word_idx": 37864, "sentence_idx": 598, "label": "unlabeled"}, {"type": "text", "expr": "http://host", "word_idx": 37877, "sentence_idx": 599, "label": "unlabeled"}, {"type": "text", "expr": "robots", "word_idx": 37888, "sentence_idx": 600, "label": "unlabeled"}, {"type": "text", "expr": "uk:8080/anonymous/YNPLXB", "word_idx": 37894, "sentence_idx": 601, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\S}$$", "word_idx": 37918, "sentence_idx": 602, "label": "unlabeled"}, {"type": "text", "expr": "http://host", "word_idx": 37925, "sentence_idx": 603, "label": "unlabeled"}, {"type": "text", "expr": "robots", "word_idx": 37936, "sentence_idx": 604, "label": "unlabeled"}, {"type": "text", "expr": "uk:8080/anonymous/XEDH10", "word_idx": 37942, "sentence_idx": 605, "label": "unlabeled"}, {"type": "text", "expr": "method # proposals data mAP (%)", "word_idx": 37966, "sentence_idx": 606, "label": "unlabeled"}, {"type": "text", "expr": "method", "word_idx": 37997, "sentence_idx": 607, "label": "unlabeled"}, {"type": "text", "expr": "# proposals", "word_idx": 38003, "sentence_idx": 608, "label": "unlabeled"}, {"type": "text", "expr": "mAP (%)", "word_idx": 38014, "sentence_idx": 609, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 12 65", "word_idx": 38021, "sentence_idx": 610, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 07++12 68", "word_idx": 38034, "sentence_idx": 611, "label": "unlabeled"}, {"type": "text", "expr": "07++12", "word_idx": 38051, "sentence_idx": 612, "label": "unlabeled"}, {"type": "text", "expr": "07++12", "word_idx": 38057, "sentence_idx": 613, "label": "unlabeled"}, {"type": "text", "expr": "300 12 67", "word_idx": 38063, "sentence_idx": 614, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG, shared ${}^{\\dagger}$", "word_idx": 38072, "sentence_idx": 615, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG, shared", "word_idx": 38102, "sentence_idx": 616, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\dagger}$$", "word_idx": 38117, "sentence_idx": 617, "label": "unlabeled"}, {"type": "text", "expr": "300 07++12 70", "word_idx": 38129, "sentence_idx": 618, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG, shared ${}^{\\ddagger}$", "word_idx": 38142, "sentence_idx": 619, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG, shared", "word_idx": 38173, "sentence_idx": 620, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\ddagger}$$", "word_idx": 38188, "sentence_idx": 621, "label": "unlabeled"}, {"type": "text", "expr": "07++12", "word_idx": 38201, "sentence_idx": 622, "label": "unlabeled"}, {"type": "text", "expr": "07++12", "word_idx": 38207, "sentence_idx": 623, "label": "unlabeled"}, {"type": "text", "expr": "300 COCO+07++12 75", "word_idx": 38213, "sentence_idx": 624, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG, shared ${}^{\\S}$", "word_idx": 38231, "sentence_idx": 625, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG, shared", "word_idx": 38256, "sentence_idx": 626, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\S}$$", "word_idx": 38271, "sentence_idx": 627, "label": "unlabeled"}, {"type": "text", "expr": "COCO+07++12", "word_idx": 38278, "sentence_idx": 628, "label": "unlabeled"}, {"type": "text", "expr": "COCO+07++12", "word_idx": 38289, "sentence_idx": 629, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a05:  Timing  (ms) on a K40 GPU, except SS proposal is evaluated in a CPU", "word_idx": 38300, "sentence_idx": 630, "label": "unlabeled"}, {"type": "text", "expr": " \u201cRegion-wise\u201d includes NMS, pooling, fully-connected, and softmax layers", "word_idx": 38377, "sentence_idx": 631, "label": "unlabeled"}, {"type": "text", "expr": " See our released code for the profiling of running time", "word_idx": 38450, "sentence_idx": 632, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a05:", "word_idx": 38506, "sentence_idx": 633, "label": "unlabeled"}, {"type": "text", "expr": "Timing", "word_idx": 38514, "sentence_idx": 634, "label": "unlabeled"}, {"type": "text", "expr": "model system conv proposal region-wise total rate", "word_idx": 38520, "sentence_idx": 635, "label": "unlabeled"}, {"type": "text", "expr": "model", "word_idx": 38569, "sentence_idx": 636, "label": "unlabeled"}, {"type": "text", "expr": "system", "word_idx": 38574, "sentence_idx": 637, "label": "unlabeled"}, {"type": "text", "expr": "proposal", "word_idx": 38580, "sentence_idx": 638, "label": "unlabeled"}, {"type": "text", "expr": "region-wise", "word_idx": 38588, "sentence_idx": 639, "label": "unlabeled"}, {"type": "text", "expr": "total", "word_idx": 38599, "sentence_idx": 640, "label": "unlabeled"}, {"type": "text", "expr": "VGG SS + Fast R-CNN 146 1510 174 1830 0", "word_idx": 38604, "sentence_idx": 641, "label": "unlabeled"}, {"type": "text", "expr": "5 fps", "word_idx": 38643, "sentence_idx": 642, "label": "unlabeled"}, {"type": "text", "expr": "SS + Fast R-CNN", "word_idx": 38648, "sentence_idx": 643, "label": "unlabeled"}, {"type": "text", "expr": "SS + Fast R-CNN", "word_idx": 38663, "sentence_idx": 644, "label": "unlabeled"}, {"type": "text", "expr": "5 fps", "word_idx": 38678, "sentence_idx": 645, "label": "unlabeled"}, {"type": "text", "expr": "5 fps", "word_idx": 38683, "sentence_idx": 646, "label": "unlabeled"}, {"type": "text", "expr": "VGG RPN + Fast R-CNN 141 10 47 198 5 fps", "word_idx": 38688, "sentence_idx": 647, "label": "unlabeled"}, {"type": "text", "expr": "RPN + Fast R-CNN", "word_idx": 38728, "sentence_idx": 648, "label": "unlabeled"}, {"type": "text", "expr": "RPN + Fast R-CNN", "word_idx": 38744, "sentence_idx": 649, "label": "unlabeled"}, {"type": "text", "expr": "5 fps", "word_idx": 38760, "sentence_idx": 650, "label": "unlabeled"}, {"type": "text", "expr": "5 fps", "word_idx": 38765, "sentence_idx": 651, "label": "unlabeled"}, {"type": "text", "expr": "ZF RPN + Fast R-CNN 31 3 25 59 17 fps", "word_idx": 38770, "sentence_idx": 652, "label": "unlabeled"}, {"type": "text", "expr": "RPN + Fast R-CNN", "word_idx": 38807, "sentence_idx": 653, "label": "unlabeled"}, {"type": "text", "expr": "RPN + Fast R-CNN", "word_idx": 38823, "sentence_idx": 654, "label": "unlabeled"}, {"type": "text", "expr": "17 fps", "word_idx": 38839, "sentence_idx": 655, "label": "unlabeled"}, {"type": "text", "expr": "17 fps", "word_idx": 38845, "sentence_idx": 656, "label": "unlabeled"}, {"type": "text", "expr": "Ablation Experiments on RPN", "word_idx": 38851, "sentence_idx": 657, "label": "unlabeled"}, {"type": "text", "expr": " \nTo investigate the behavior of RPNs as a proposal method, we conducted several ablation studies", "word_idx": 38878, "sentence_idx": 658, "label": "unlabeled"}, {"type": "text", "expr": "\nFirst, we show the effect of sharing convolutional layers between the RPN and Fast R-CNN detection network", "word_idx": 38975, "sentence_idx": 659, "label": "unlabeled"}, {"type": "text", "expr": "\nTo do this, we stop after the second step in the 4-step training process", "word_idx": 39082, "sentence_idx": 660, "label": "unlabeled"}, {"type": "text", "expr": "\nUsing separate networks reduces the result slightly to 58", "word_idx": 39155, "sentence_idx": 661, "label": "unlabeled"}, {"type": "text", "expr": "7% (RPN+ZF, unshared, Table\u00a0 2 )", "word_idx": 39213, "sentence_idx": 662, "label": "unlabeled"}, {"type": "text", "expr": "\nWe observe that this is because in the third step when the detector-tuned features are used to fine-tune the RPN, the proposal quality is improved", "word_idx": 39245, "sentence_idx": 663, "label": "unlabeled"}, {"type": "text", "expr": "Ablation Experiments on RPN", "word_idx": 39392, "sentence_idx": 664, "label": "unlabeled"}, {"type": "text", "expr": "Next, we disentangle the RPN\u2019s influence on training the Fast R-CNN detection network", "word_idx": 39419, "sentence_idx": 665, "label": "unlabeled"}, {"type": "text", "expr": "\nFor this purpose, we train a Fast R-CNN model by using the 2000 SS proposals and ZF net", "word_idx": 39504, "sentence_idx": 666, "label": "unlabeled"}, {"type": "text", "expr": "\nWe fix this detector and evaluate the detection mAP by changing the proposal regions used at test-time", "word_idx": 39592, "sentence_idx": 667, "label": "unlabeled"}, {"type": "text", "expr": "\nIn these ablation experiments, the RPN does not share features with the detector", "word_idx": 39695, "sentence_idx": 668, "label": "unlabeled"}, {"type": "text", "expr": "Next, we disentangle the RPN\u2019s influence on training the Fast R-CNN detection network", "word_idx": 39776, "sentence_idx": 669, "label": "unlabeled"}, {"type": "text", "expr": "\nFor this purpose, we train a Fast R-CNN model by using the 2000 SS proposals and ZF net", "word_idx": 39861, "sentence_idx": 670, "label": "unlabeled"}, {"type": "text", "expr": "\nWe fix this detector and evaluate the detection mAP by changing the proposal regions used at test-time", "word_idx": 39949, "sentence_idx": 671, "label": "unlabeled"}, {"type": "text", "expr": "\nIn these ablation experiments, the RPN does not share features with the detector", "word_idx": 40052, "sentence_idx": 672, "label": "unlabeled"}, {"type": "text", "expr": "Replacing SS with 300 RPN proposals at test-time leads to an mAP of 56", "word_idx": 40133, "sentence_idx": 673, "label": "unlabeled"}, {"type": "text", "expr": " The loss in mAP is because of the inconsistency between the training/testing proposals", "word_idx": 40203, "sentence_idx": 674, "label": "unlabeled"}, {"type": "text", "expr": " This result serves as the baseline for the following comparisons", "word_idx": 40290, "sentence_idx": 675, "label": "unlabeled"}, {"type": "text", "expr": "Replacing SS with 300 RPN proposals at test-time leads to an mAP of 56", "word_idx": 40355, "sentence_idx": 676, "label": "unlabeled"}, {"type": "text", "expr": " The loss in mAP is because of the inconsistency between the training/testing proposals", "word_idx": 40425, "sentence_idx": 677, "label": "unlabeled"}, {"type": "text", "expr": " This result serves as the baseline for the following comparisons", "word_idx": 40512, "sentence_idx": 678, "label": "unlabeled"}, {"type": "text", "expr": "Somewhat surprisingly, the RPN still leads to a competitive result (55", "word_idx": 40577, "sentence_idx": 679, "label": "unlabeled"}, {"type": "text", "expr": "1%) when using the top-ranked 100 proposals at test-time, indicating that the top-ranked RPN proposals are accurate", "word_idx": 40647, "sentence_idx": 680, "label": "unlabeled"}, {"type": "text", "expr": " On the other extreme, using the top-ranked 6000 RPN proposals (without NMS) has a comparable mAP (55", "word_idx": 40762, "sentence_idx": 681, "label": "unlabeled"}, {"type": "text", "expr": "2%), suggesting NMS does not harm the detection mAP and may reduce false alarms", "word_idx": 40863, "sentence_idx": 682, "label": "unlabeled"}, {"type": "text", "expr": "Somewhat surprisingly, the RPN still leads to a competitive result (55", "word_idx": 40942, "sentence_idx": 683, "label": "unlabeled"}, {"type": "text", "expr": "1%) when using the top-ranked 100 proposals at test-time, indicating that the top-ranked RPN proposals are accurate", "word_idx": 41012, "sentence_idx": 684, "label": "unlabeled"}, {"type": "text", "expr": " On the other extreme, using the top-ranked 6000 RPN proposals (without NMS) has a comparable mAP (55", "word_idx": 41127, "sentence_idx": 685, "label": "unlabeled"}, {"type": "text", "expr": "2%), suggesting NMS does not harm the detection mAP and may reduce false alarms", "word_idx": 41228, "sentence_idx": 686, "label": "unlabeled"}, {"type": "text", "expr": "Next, we separately investigate the roles of RPN\u2019s  cls  and  reg  outputs by turning off either of them at test-time", "word_idx": 41307, "sentence_idx": 687, "label": "unlabeled"}, {"type": "text", "expr": "\nWhen the  cls  layer is removed at test-time (thus no NMS/ranking is used), we randomly sample  $N$  proposals from the unscored regions", "word_idx": 41424, "sentence_idx": 688, "label": "unlabeled"}, {"type": "text", "expr": " The mAP is nearly unchanged with  $N=1000$  (55", "word_idx": 41561, "sentence_idx": 689, "label": "unlabeled"}, {"type": "text", "expr": "8%), but degrades considerably to 44", "word_idx": 41609, "sentence_idx": 690, "label": "unlabeled"}, {"type": "text", "expr": "6% when  $N=100$ ", "word_idx": 41645, "sentence_idx": 691, "label": "unlabeled"}, {"type": "text", "expr": " This shows that the  cls  scores account for the accuracy of the highest ranked proposals", "word_idx": 41662, "sentence_idx": 692, "label": "unlabeled"}, {"type": "math", "expr": "$$N=1000$$", "word_idx": 41752, "sentence_idx": 693, "label": "unlabeled"}, {"type": "math", "expr": "$$N=100$$", "word_idx": 41758, "sentence_idx": 694, "label": "unlabeled"}, {"type": "text", "expr": "On the other hand, when the  reg  layer is removed at test-time (so the proposals become anchor boxes), the mAP drops to 52", "word_idx": 41763, "sentence_idx": 695, "label": "unlabeled"}, {"type": "text", "expr": " This suggests that the high-quality proposals are mainly due to the regressed box bounds", "word_idx": 41886, "sentence_idx": 696, "label": "unlabeled"}, {"type": "text", "expr": " The anchor boxes, though having multiple scales and aspect ratios, are not sufficient for accurate detection", "word_idx": 41975, "sentence_idx": 697, "label": "unlabeled"}, {"type": "text", "expr": "We also evaluate the effects of more powerful networks on the proposal quality of RPN alone", "word_idx": 42084, "sentence_idx": 698, "label": "unlabeled"}, {"type": "text", "expr": " We use VGG-16 to train the RPN, and still use the above detector of SS+ZF", "word_idx": 42175, "sentence_idx": 699, "label": "unlabeled"}, {"type": "text", "expr": " The mAP improves from 56", "word_idx": 42249, "sentence_idx": 700, "label": "unlabeled"}, {"type": "text", "expr": "8% (using RPN+ZF) to 59", "word_idx": 42274, "sentence_idx": 701, "label": "unlabeled"}, {"type": "text", "expr": "2% (using RPN+VGG)", "word_idx": 42297, "sentence_idx": 702, "label": "unlabeled"}, {"type": "text", "expr": "\nThis is a promising result, because it suggests that the proposal quality of RPN+VGG is better than that of RPN+ZF", "word_idx": 42315, "sentence_idx": 703, "label": "unlabeled"}, {"type": "text", "expr": " Because proposals of RPN+ZF are competitive with SS (both are 58", "word_idx": 42430, "sentence_idx": 704, "label": "unlabeled"}, {"type": "text", "expr": "7% when consistently used for training and testing), we may expect RPN+VGG to be better than SS", "word_idx": 42495, "sentence_idx": 705, "label": "unlabeled"}, {"type": "text", "expr": " The following experiments justify this hypothesis", "word_idx": 42590, "sentence_idx": 706, "label": "unlabeled"}, {"type": "text", "expr": "We also evaluate the effects of more powerful networks on the proposal quality of RPN alone", "word_idx": 42640, "sentence_idx": 707, "label": "unlabeled"}, {"type": "text", "expr": " We use VGG-16 to train the RPN, and still use the above detector of SS+ZF", "word_idx": 42731, "sentence_idx": 708, "label": "unlabeled"}, {"type": "text", "expr": " The mAP improves from 56", "word_idx": 42805, "sentence_idx": 709, "label": "unlabeled"}, {"type": "text", "expr": "8% (using RPN+ZF) to 59", "word_idx": 42830, "sentence_idx": 710, "label": "unlabeled"}, {"type": "text", "expr": "2% (using RPN+VGG)", "word_idx": 42853, "sentence_idx": 711, "label": "unlabeled"}, {"type": "text", "expr": "\nThis is a promising result, because it suggests that the proposal quality of RPN+VGG is better than that of RPN+ZF", "word_idx": 42871, "sentence_idx": 712, "label": "unlabeled"}, {"type": "text", "expr": " Because proposals of RPN+ZF are competitive with SS (both are 58", "word_idx": 42986, "sentence_idx": 713, "label": "unlabeled"}, {"type": "text", "expr": "7% when consistently used for training and testing), we may expect RPN+VGG to be better than SS", "word_idx": 43051, "sentence_idx": 714, "label": "unlabeled"}, {"type": "text", "expr": " The following experiments justify this hypothesis", "word_idx": 43146, "sentence_idx": 715, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a06:  Results on PASCAL VOC 2007 test set with Fast R-CNN detectors and VGG-16", "word_idx": 43196, "sentence_idx": 716, "label": "unlabeled"}, {"type": "text", "expr": "\nFor RPN, the train-time proposals for Fast R-CNN are 2000", "word_idx": 43278, "sentence_idx": 717, "label": "unlabeled"}, {"type": "text", "expr": " RPN ${}^{*}$  denotes the unsharing feature version", "word_idx": 43336, "sentence_idx": 718, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a06:", "word_idx": 43388, "sentence_idx": 719, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{*}$$", "word_idx": 43396, "sentence_idx": 720, "label": "unlabeled"}, {"type": "text", "expr": "method # box data mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv", "word_idx": 43402, "sentence_idx": 721, "label": "unlabeled"}, {"type": "text", "expr": "method", "word_idx": 43527, "sentence_idx": 722, "label": "unlabeled"}, {"type": "text", "expr": "method", "word_idx": 43533, "sentence_idx": 723, "label": "unlabeled"}, {"type": "text", "expr": "# box", "word_idx": 43539, "sentence_idx": 724, "label": "unlabeled"}, {"type": "text", "expr": "# box", "word_idx": 43544, "sentence_idx": 725, "label": "unlabeled"}, {"type": "text", "expr": "bottle", "word_idx": 43549, "sentence_idx": 726, "label": "unlabeled"}, {"type": "text", "expr": "bottle", "word_idx": 43555, "sentence_idx": 727, "label": "unlabeled"}, {"type": "text", "expr": "chair", "word_idx": 43561, "sentence_idx": 728, "label": "unlabeled"}, {"type": "text", "expr": "chair", "word_idx": 43566, "sentence_idx": 729, "label": "unlabeled"}, {"type": "text", "expr": "table", "word_idx": 43571, "sentence_idx": 730, "label": "unlabeled"}, {"type": "text", "expr": "table", "word_idx": 43576, "sentence_idx": 731, "label": "unlabeled"}, {"type": "text", "expr": "horse", "word_idx": 43581, "sentence_idx": 732, "label": "unlabeled"}, {"type": "text", "expr": "horse", "word_idx": 43586, "sentence_idx": 733, "label": "unlabeled"}, {"type": "text", "expr": "mbike", "word_idx": 43591, "sentence_idx": 734, "label": "unlabeled"}, {"type": "text", "expr": "mbike", "word_idx": 43596, "sentence_idx": 735, "label": "unlabeled"}, {"type": "text", "expr": "person", "word_idx": 43601, "sentence_idx": 736, "label": "unlabeled"}, {"type": "text", "expr": "person", "word_idx": 43607, "sentence_idx": 737, "label": "unlabeled"}, {"type": "text", "expr": "plant", "word_idx": 43613, "sentence_idx": 738, "label": "unlabeled"}, {"type": "text", "expr": "plant", "word_idx": 43618, "sentence_idx": 739, "label": "unlabeled"}, {"type": "text", "expr": "sheep", "word_idx": 43623, "sentence_idx": 740, "label": "unlabeled"}, {"type": "text", "expr": "sheep", "word_idx": 43628, "sentence_idx": 741, "label": "unlabeled"}, {"type": "text", "expr": "train", "word_idx": 43633, "sentence_idx": 742, "label": "unlabeled"}, {"type": "text", "expr": "train", "word_idx": 43638, "sentence_idx": 743, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 07 66", "word_idx": 43643, "sentence_idx": 744, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 07+12 70", "word_idx": 43656, "sentence_idx": 745, "label": "unlabeled"}, {"type": "text", "expr": "07+12", "word_idx": 43672, "sentence_idx": 746, "label": "unlabeled"}, {"type": "text", "expr": "07+12", "word_idx": 43677, "sentence_idx": 747, "label": "unlabeled"}, {"type": "text", "expr": "300 07 68", "word_idx": 43682, "sentence_idx": 748, "label": "unlabeled"}, {"type": "text", "expr": "RPN ${}^{*}$", "word_idx": 43691, "sentence_idx": 749, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{*}$$", "word_idx": 43703, "sentence_idx": 750, "label": "unlabeled"}, {"type": "text", "expr": "RPN 300 07 69", "word_idx": 43709, "sentence_idx": 751, "label": "unlabeled"}, {"type": "text", "expr": "RPN 300 07+12 73", "word_idx": 43722, "sentence_idx": 752, "label": "unlabeled"}, {"type": "text", "expr": "07+12", "word_idx": 43738, "sentence_idx": 753, "label": "unlabeled"}, {"type": "text", "expr": "07+12", "word_idx": 43743, "sentence_idx": 754, "label": "unlabeled"}, {"type": "text", "expr": "RPN 300 COCO+07+12 78", "word_idx": 43748, "sentence_idx": 755, "label": "unlabeled"}, {"type": "text", "expr": "COCO+07+12", "word_idx": 43769, "sentence_idx": 756, "label": "unlabeled"}, {"type": "text", "expr": "COCO+07+12", "word_idx": 43779, "sentence_idx": 757, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a07:  Results on PASCAL VOC 2012 test set with Fast R-CNN detectors and VGG-16", "word_idx": 43789, "sentence_idx": 758, "label": "unlabeled"}, {"type": "text", "expr": "\nFor RPN, the train-time proposals for Fast R-CNN are 2000", "word_idx": 43871, "sentence_idx": 759, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a07:", "word_idx": 43929, "sentence_idx": 760, "label": "unlabeled"}, {"type": "text", "expr": "method # box data mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv", "word_idx": 43937, "sentence_idx": 761, "label": "unlabeled"}, {"type": "text", "expr": "method", "word_idx": 44062, "sentence_idx": 762, "label": "unlabeled"}, {"type": "text", "expr": "method", "word_idx": 44068, "sentence_idx": 763, "label": "unlabeled"}, {"type": "text", "expr": "# box", "word_idx": 44074, "sentence_idx": 764, "label": "unlabeled"}, {"type": "text", "expr": "# box", "word_idx": 44079, "sentence_idx": 765, "label": "unlabeled"}, {"type": "text", "expr": "bottle", "word_idx": 44084, "sentence_idx": 766, "label": "unlabeled"}, {"type": "text", "expr": "bottle", "word_idx": 44090, "sentence_idx": 767, "label": "unlabeled"}, {"type": "text", "expr": "chair", "word_idx": 44096, "sentence_idx": 768, "label": "unlabeled"}, {"type": "text", "expr": "chair", "word_idx": 44101, "sentence_idx": 769, "label": "unlabeled"}, {"type": "text", "expr": "table", "word_idx": 44106, "sentence_idx": 770, "label": "unlabeled"}, {"type": "text", "expr": "table", "word_idx": 44111, "sentence_idx": 771, "label": "unlabeled"}, {"type": "text", "expr": "horse", "word_idx": 44116, "sentence_idx": 772, "label": "unlabeled"}, {"type": "text", "expr": "horse", "word_idx": 44121, "sentence_idx": 773, "label": "unlabeled"}, {"type": "text", "expr": "mbike", "word_idx": 44126, "sentence_idx": 774, "label": "unlabeled"}, {"type": "text", "expr": "mbike", "word_idx": 44131, "sentence_idx": 775, "label": "unlabeled"}, {"type": "text", "expr": "person", "word_idx": 44136, "sentence_idx": 776, "label": "unlabeled"}, {"type": "text", "expr": "person", "word_idx": 44142, "sentence_idx": 777, "label": "unlabeled"}, {"type": "text", "expr": "plant", "word_idx": 44148, "sentence_idx": 778, "label": "unlabeled"}, {"type": "text", "expr": "plant", "word_idx": 44153, "sentence_idx": 779, "label": "unlabeled"}, {"type": "text", "expr": "sheep", "word_idx": 44158, "sentence_idx": 780, "label": "unlabeled"}, {"type": "text", "expr": "sheep", "word_idx": 44163, "sentence_idx": 781, "label": "unlabeled"}, {"type": "text", "expr": "train", "word_idx": 44168, "sentence_idx": 782, "label": "unlabeled"}, {"type": "text", "expr": "train", "word_idx": 44173, "sentence_idx": 783, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 12 65", "word_idx": 44178, "sentence_idx": 784, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 07++12 68", "word_idx": 44191, "sentence_idx": 785, "label": "unlabeled"}, {"type": "text", "expr": "07++12", "word_idx": 44208, "sentence_idx": 786, "label": "unlabeled"}, {"type": "text", "expr": "07++12", "word_idx": 44214, "sentence_idx": 787, "label": "unlabeled"}, {"type": "text", "expr": "RPN 300 12 67", "word_idx": 44220, "sentence_idx": 788, "label": "unlabeled"}, {"type": "text", "expr": "RPN 300 07++12 70", "word_idx": 44233, "sentence_idx": 789, "label": "unlabeled"}, {"type": "text", "expr": "07++12", "word_idx": 44250, "sentence_idx": 790, "label": "unlabeled"}, {"type": "text", "expr": "07++12", "word_idx": 44256, "sentence_idx": 791, "label": "unlabeled"}, {"type": "text", "expr": "RPN 300 COCO+07++12 75", "word_idx": 44262, "sentence_idx": 792, "label": "unlabeled"}, {"type": "text", "expr": "COCO+07++12", "word_idx": 44284, "sentence_idx": 793, "label": "unlabeled"}, {"type": "text", "expr": "COCO+07++12", "word_idx": 44295, "sentence_idx": 794, "label": "unlabeled"}, {"type": "text", "expr": "Performance of VGG-16", "word_idx": 44306, "sentence_idx": 795, "label": "unlabeled"}, {"type": "text", "expr": " \nTable\u00a0 3  shows the results of VGG-16 for both proposal and detection", "word_idx": 44327, "sentence_idx": 796, "label": "unlabeled"}, {"type": "text", "expr": " Using RPN+VGG, the result is 68", "word_idx": 44398, "sentence_idx": 797, "label": "unlabeled"}, {"type": "text", "expr": "5% for  unshared  features, slightly higher than the SS baseline", "word_idx": 44430, "sentence_idx": 798, "label": "unlabeled"}, {"type": "text", "expr": " As shown above, this is because the proposals generated by RPN+VGG are more accurate than SS", "word_idx": 44494, "sentence_idx": 799, "label": "unlabeled"}, {"type": "text", "expr": " Unlike SS that is pre-defined, the RPN is actively trained and benefits from better networks", "word_idx": 44587, "sentence_idx": 800, "label": "unlabeled"}, {"type": "text", "expr": " For the feature- shared  variant, the result is 69", "word_idx": 44680, "sentence_idx": 801, "label": "unlabeled"}, {"type": "text", "expr": "9%\u2014better than the strong SS baseline, yet with nearly cost-free proposals", "word_idx": 44731, "sentence_idx": 802, "label": "unlabeled"}, {"type": "text", "expr": " We further train the RPN and detection network on the union set of PASCAL VOC 2007 trainval and 2012 trainval", "word_idx": 44805, "sentence_idx": 803, "label": "unlabeled"}, {"type": "text", "expr": " The mAP is  73", "word_idx": 44915, "sentence_idx": 804, "label": "unlabeled"}, {"type": "text", "expr": "\nFigure\u00a0 5  shows some results on the PASCAL VOC 2007 test set", "word_idx": 44930, "sentence_idx": 805, "label": "unlabeled"}, {"type": "text", "expr": "\nOn the PASCAL VOC 2012 test set (Table\u00a0 4 ), our method has an mAP of  70", "word_idx": 44992, "sentence_idx": 806, "label": "unlabeled"}, {"type": "text", "expr": "4%  trained on the union set of VOC 2007 trainval+test and VOC 2012 trainval", "word_idx": 45066, "sentence_idx": 807, "label": "unlabeled"}, {"type": "text", "expr": " Table\u00a0 6  and Table\u00a0 7  show the detailed numbers", "word_idx": 45142, "sentence_idx": 808, "label": "unlabeled"}, {"type": "text", "expr": "Performance of VGG-16", "word_idx": 45192, "sentence_idx": 809, "label": "unlabeled"}, {"type": "text", "expr": "unshared", "word_idx": 45213, "sentence_idx": 810, "label": "unlabeled"}, {"type": "text", "expr": "shared", "word_idx": 45221, "sentence_idx": 811, "label": "unlabeled"}, {"type": "text", "expr": "In Table\u00a0 5  we summarize the running time of the entire object detection system", "word_idx": 45227, "sentence_idx": 812, "label": "unlabeled"}, {"type": "text", "expr": " SS takes 1-2 seconds depending on content (on average about 1", "word_idx": 45307, "sentence_idx": 813, "label": "unlabeled"}, {"type": "text", "expr": "5s), and Fast R-CNN with VGG-16 takes 320ms on 2000 SS proposals (or 223ms if using SVD on fully-connected layers  )", "word_idx": 45369, "sentence_idx": 814, "label": "unlabeled"}, {"type": "text", "expr": " Our system with VGG-16 takes in total  198ms  for both proposal and detection", "word_idx": 45485, "sentence_idx": 815, "label": "unlabeled"}, {"type": "text", "expr": " With the convolutional features shared, the RPN alone only takes 10ms computing the additional layers", "word_idx": 45563, "sentence_idx": 816, "label": "unlabeled"}, {"type": "text", "expr": "\nOur region-wise computation is also lower, thanks to fewer proposals (300 per image)", "word_idx": 45665, "sentence_idx": 817, "label": "unlabeled"}, {"type": "text", "expr": " Our system has a frame-rate of 17 fps with the ZF net", "word_idx": 45750, "sentence_idx": 818, "label": "unlabeled"}, {"type": "text", "expr": "198ms", "word_idx": 45804, "sentence_idx": 819, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a08:  Detection results of Faster R-CNN on PASCAL VOC 2007 test set using  different settings of anchors ", "word_idx": 45809, "sentence_idx": 820, "label": "unlabeled"}, {"type": "text", "expr": " The network is VGG-16", "word_idx": 45918, "sentence_idx": 821, "label": "unlabeled"}, {"type": "text", "expr": " The training data is VOC 2007 trainval", "word_idx": 45940, "sentence_idx": 822, "label": "unlabeled"}, {"type": "text", "expr": " The default setting of using 3 scales and 3 aspect ratios (69", "word_idx": 45979, "sentence_idx": 823, "label": "unlabeled"}, {"type": "text", "expr": "9%) is the same as that in Table\u00a0 3 ", "word_idx": 46041, "sentence_idx": 824, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a08:", "word_idx": 46077, "sentence_idx": 825, "label": "unlabeled"}, {"type": "text", "expr": "different settings of anchors", "word_idx": 46085, "sentence_idx": 826, "label": "unlabeled"}, {"type": "text", "expr": "settings anchor scales aspect ratios mAP (%)", "word_idx": 46114, "sentence_idx": 827, "label": "unlabeled"}, {"type": "text", "expr": "settings", "word_idx": 46158, "sentence_idx": 828, "label": "unlabeled"}, {"type": "text", "expr": "anchor scales", "word_idx": 46166, "sentence_idx": 829, "label": "unlabeled"}, {"type": "text", "expr": "aspect ratios", "word_idx": 46179, "sentence_idx": 830, "label": "unlabeled"}, {"type": "text", "expr": "mAP (%)", "word_idx": 46192, "sentence_idx": 831, "label": "unlabeled"}, {"type": "text", "expr": "1 scale, 1 ratio 1:1 65", "word_idx": 46199, "sentence_idx": 832, "label": "unlabeled"}, {"type": "text", "expr": "1 scale, 1 ratio", "word_idx": 46222, "sentence_idx": 833, "label": "unlabeled"}, {"type": "math", "expr": "$$128^{2}$$", "word_idx": 46238, "sentence_idx": 834, "label": "unlabeled"}, {"type": "text", "expr": "1:1 66", "word_idx": 46245, "sentence_idx": 835, "label": "unlabeled"}, {"type": "math", "expr": "$$256^{2}$$", "word_idx": 46251, "sentence_idx": 836, "label": "unlabeled"}, {"type": "text", "expr": "1 scale, 3 ratios {2:1, 1:1, 1:2} 68", "word_idx": 46258, "sentence_idx": 837, "label": "unlabeled"}, {"type": "text", "expr": "1 scale, 3 ratios", "word_idx": 46294, "sentence_idx": 838, "label": "unlabeled"}, {"type": "math", "expr": "$$128^{2}$$", "word_idx": 46311, "sentence_idx": 839, "label": "unlabeled"}, {"type": "text", "expr": "{2:1, 1:1, 1:2}", "word_idx": 46318, "sentence_idx": 840, "label": "unlabeled"}, {"type": "text", "expr": "{2:1, 1:1, 1:2}", "word_idx": 46333, "sentence_idx": 841, "label": "unlabeled"}, {"type": "text", "expr": "{2:1, 1:1, 1:2} 67", "word_idx": 46348, "sentence_idx": 842, "label": "unlabeled"}, {"type": "math", "expr": "$$256^{2}$$", "word_idx": 46366, "sentence_idx": 843, "label": "unlabeled"}, {"type": "text", "expr": "{2:1, 1:1, 1:2}", "word_idx": 46373, "sentence_idx": 844, "label": "unlabeled"}, {"type": "text", "expr": "{2:1, 1:1, 1:2}", "word_idx": 46388, "sentence_idx": 845, "label": "unlabeled"}, {"type": "text", "expr": "3 scales, 1 ratio 1:1 69", "word_idx": 46403, "sentence_idx": 846, "label": "unlabeled"}, {"type": "text", "expr": "3 scales, 1 ratio", "word_idx": 46427, "sentence_idx": 847, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{128^{2},256^{2},512^{2}\\}$$", "word_idx": 46444, "sentence_idx": 848, "label": "unlabeled"}, {"type": "text", "expr": "3 scales, 3 ratios {2:1, 1:1, 1:2} 69", "word_idx": 46471, "sentence_idx": 849, "label": "unlabeled"}, {"type": "text", "expr": "3 scales, 3 ratios", "word_idx": 46508, "sentence_idx": 850, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{128^{2},256^{2},512^{2}\\}$$", "word_idx": 46526, "sentence_idx": 851, "label": "unlabeled"}, {"type": "text", "expr": "{2:1, 1:1, 1:2}", "word_idx": 46553, "sentence_idx": 852, "label": "unlabeled"}, {"type": "text", "expr": "{2:1, 1:1, 1:2}", "word_idx": 46568, "sentence_idx": 853, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a09:  Detection results of Faster R-CNN on PASCAL VOC 2007 test set using   in Equation\u00a0( 1 )", "word_idx": 46583, "sentence_idx": 854, "label": "unlabeled"}, {"type": "text", "expr": " The network is VGG-16", "word_idx": 46680, "sentence_idx": 855, "label": "unlabeled"}, {"type": "text", "expr": " The training data is VOC 2007 trainval", "word_idx": 46702, "sentence_idx": 856, "label": "unlabeled"}, {"type": "text", "expr": " The default setting of using  $\\lambda=10$  (69", "word_idx": 46741, "sentence_idx": 857, "label": "usecase"}, {"type": "text", "expr": "9%) is the same as that in Table\u00a0 3 ", "word_idx": 46789, "sentence_idx": 858, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a09:", "word_idx": 46825, "sentence_idx": 859, "label": "unlabeled"}, {"type": "text", "expr": "different values of  $\\lambda$", "word_idx": 46833, "sentence_idx": 860, "label": "usecase"}, {"type": "math", "expr": "$$\\lambda$$", "word_idx": 46863, "sentence_idx": 861, "label": "none"}, {"type": "math", "expr": "$$\\lambda=10$$", "word_idx": 46870, "sentence_idx": 862, "label": "none"}, {"type": "text", "expr": "1 1 10 100", "word_idx": 46880, "sentence_idx": 863, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda$$", "word_idx": 46890, "sentence_idx": 864, "label": "none"}, {"type": "text", "expr": "mAP (%) 67", "word_idx": 46897, "sentence_idx": 865, "label": "unlabeled"}, {"type": "text", "expr": "mAP (%)", "word_idx": 46907, "sentence_idx": 866, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:  Recall  \\vs \u00a0IoU overlap ratio on the PASCAL VOC 2007 test set", "word_idx": 46914, "sentence_idx": 867, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:", "word_idx": 46987, "sentence_idx": 868, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a010:  ", "word_idx": 46996, "sentence_idx": 869, "label": "unlabeled"}, {"type": "text", "expr": " Detection results are on the PASCAL VOC 2007 test set using the ZF model and Fast R-CNN", "word_idx": 47007, "sentence_idx": 870, "label": "unlabeled"}, {"type": "text", "expr": " RPN uses unshared features", "word_idx": 47095, "sentence_idx": 871, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a010:", "word_idx": 47122, "sentence_idx": 872, "label": "unlabeled"}, {"type": "text", "expr": "One-Stage Detection  \\vs \u00a0Two-Stage Proposal + Detection", "word_idx": 47131, "sentence_idx": 873, "label": "unlabeled"}, {"type": "text", "expr": "proposals detector mAP (%)", "word_idx": 47187, "sentence_idx": 874, "label": "unlabeled"}, {"type": "text", "expr": "proposals", "word_idx": 47213, "sentence_idx": 875, "label": "unlabeled"}, {"type": "text", "expr": "detector", "word_idx": 47222, "sentence_idx": 876, "label": "unlabeled"}, {"type": "text", "expr": "mAP (%)", "word_idx": 47230, "sentence_idx": 877, "label": "unlabeled"}, {"type": "text", "expr": "Two-Stage RPN + ZF, unshared 300 Fast R-CNN + ZF, 1 scale 58", "word_idx": 47237, "sentence_idx": 878, "label": "unlabeled"}, {"type": "text", "expr": "Two-Stage", "word_idx": 47297, "sentence_idx": 879, "label": "unlabeled"}, {"type": "text", "expr": "Two-Stage", "word_idx": 47306, "sentence_idx": 880, "label": "unlabeled"}, {"type": "text", "expr": "RPN + ZF, unshared", "word_idx": 47315, "sentence_idx": 881, "label": "unlabeled"}, {"type": "text", "expr": "RPN + ZF, unshared", "word_idx": 47333, "sentence_idx": 882, "label": "unlabeled"}, {"type": "text", "expr": "Fast R-CNN + ZF, 1 scale", "word_idx": 47351, "sentence_idx": 883, "label": "unlabeled"}, {"type": "text", "expr": "Fast R-CNN + ZF, 1 scale", "word_idx": 47375, "sentence_idx": 884, "label": "unlabeled"}, {"type": "text", "expr": "One-Stage dense, 3 scales, 3 aspect ratios 20000 Fast R-CNN + ZF, 1 scale 53", "word_idx": 47399, "sentence_idx": 885, "label": "unlabeled"}, {"type": "text", "expr": "One-Stage", "word_idx": 47475, "sentence_idx": 886, "label": "unlabeled"}, {"type": "text", "expr": "dense, 3 scales, 3 aspect ratios", "word_idx": 47484, "sentence_idx": 887, "label": "unlabeled"}, {"type": "text", "expr": "20000", "word_idx": 47516, "sentence_idx": 888, "label": "unlabeled"}, {"type": "text", "expr": "Fast R-CNN + ZF, 1 scale", "word_idx": 47521, "sentence_idx": 889, "label": "unlabeled"}, {"type": "text", "expr": "One-Stage dense, 3 scales, 3 aspect ratios 20000 Fast R-CNN + ZF, 5 scales 53", "word_idx": 47545, "sentence_idx": 890, "label": "unlabeled"}, {"type": "text", "expr": "One-Stage", "word_idx": 47622, "sentence_idx": 891, "label": "unlabeled"}, {"type": "text", "expr": "One-Stage", "word_idx": 47631, "sentence_idx": 892, "label": "unlabeled"}, {"type": "text", "expr": "dense, 3 scales, 3 aspect ratios", "word_idx": 47640, "sentence_idx": 893, "label": "unlabeled"}, {"type": "text", "expr": "dense, 3 scales, 3 aspect ratios", "word_idx": 47672, "sentence_idx": 894, "label": "unlabeled"}, {"type": "text", "expr": "20000", "word_idx": 47704, "sentence_idx": 895, "label": "unlabeled"}, {"type": "text", "expr": "20000", "word_idx": 47709, "sentence_idx": 896, "label": "unlabeled"}, {"type": "text", "expr": "Fast R-CNN + ZF, 5 scales", "word_idx": 47714, "sentence_idx": 897, "label": "unlabeled"}, {"type": "text", "expr": "Fast R-CNN + ZF, 5 scales", "word_idx": 47739, "sentence_idx": 898, "label": "unlabeled"}, {"type": "text", "expr": "Sensitivities to Hyper-parameters", "word_idx": 47764, "sentence_idx": 899, "label": "unlabeled"}, {"type": "text", "expr": "  In Table\u00a0 8  we investigate the settings of anchors", "word_idx": 47797, "sentence_idx": 900, "label": "unlabeled"}, {"type": "text", "expr": " By default we use 3 scales and 3 aspect ratios (69", "word_idx": 47850, "sentence_idx": 901, "label": "unlabeled"}, {"type": "text", "expr": "9% mAP in Table\u00a0 8 )", "word_idx": 47901, "sentence_idx": 902, "label": "unlabeled"}, {"type": "text", "expr": "\nIf using just one anchor at each position, the mAP drops by a considerable margin of 3-4%", "word_idx": 47921, "sentence_idx": 903, "label": "unlabeled"}, {"type": "text", "expr": " The mAP is higher if using 3 scales (with 1 aspect ratio) or 3 aspect ratios (with 1 scale), demonstrating that using anchors of multiple sizes as the regression references is an effective solution", "word_idx": 48011, "sentence_idx": 904, "label": "unlabeled"}, {"type": "text", "expr": " Using just 3 scales with 1 aspect ratio (69", "word_idx": 48209, "sentence_idx": 905, "label": "unlabeled"}, {"type": "text", "expr": "8%) is as good as using 3 scales with 3 aspect ratios on this dataset, suggesting that scales and aspect ratios are not disentangled dimensions for the detection accuracy", "word_idx": 48253, "sentence_idx": 906, "label": "unlabeled"}, {"type": "text", "expr": " But we still adopt these two dimensions in our designs to keep our system flexible", "word_idx": 48423, "sentence_idx": 907, "label": "unlabeled"}, {"type": "text", "expr": "Sensitivities to Hyper-parameters", "word_idx": 48506, "sentence_idx": 908, "label": "unlabeled"}, {"type": "text", "expr": "In Table\u00a0 9  we compare different values of  $\\lambda$  in Equation\u00a0( 1 )", "word_idx": 48539, "sentence_idx": 909, "label": "usecase"}, {"type": "text", "expr": " By default we use  $\\lambda=10$  which makes the two terms in Equation\u00a0( 1 ) roughly equally weighted after normalization", "word_idx": 48612, "sentence_idx": 910, "label": "usecase"}, {"type": "text", "expr": " Table\u00a0 9  shows that our result is impacted just marginally (by  $\\sim 1\\%$ ) when  $\\lambda$  is within a scale of about two orders of magnitude (1 to 100)", "word_idx": 48734, "sentence_idx": 911, "label": "none"}, {"type": "text", "expr": " This demonstrates that the result is insensitive to  $\\lambda$  in a wide range", "word_idx": 48891, "sentence_idx": 912, "label": "none"}, {"type": "math", "expr": "$$\\lambda$$", "word_idx": 48971, "sentence_idx": 913, "label": "none"}, {"type": "math", "expr": "$$\\lambda=10$$", "word_idx": 48978, "sentence_idx": 914, "label": "none"}, {"type": "math", "expr": "$$\\sim 1\\%$$", "word_idx": 48988, "sentence_idx": 915, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda$$", "word_idx": 48996, "sentence_idx": 916, "label": "none"}, {"type": "math", "expr": "$$\\lambda$$", "word_idx": 49003, "sentence_idx": 917, "label": "none"}, {"type": "text", "expr": "Analysis of Recall-to-IoU", "word_idx": 49010, "sentence_idx": 918, "label": "unlabeled"}, {"type": "text", "expr": " \nNext we compute the recall of proposals at different IoU ratios with ground-truth boxes", "word_idx": 49035, "sentence_idx": 919, "label": "unlabeled"}, {"type": "text", "expr": " It is noteworthy that the Recall-to-IoU metric is just  loosely    related to the ultimate detection accuracy", "word_idx": 49124, "sentence_idx": 920, "label": "unlabeled"}, {"type": "text", "expr": " It is more appropriate to use this metric to  diagnose  the proposal method than to evaluate it", "word_idx": 49234, "sentence_idx": 921, "label": "unlabeled"}, {"type": "text", "expr": "Analysis of Recall-to-IoU", "word_idx": 49330, "sentence_idx": 922, "label": "unlabeled"}, {"type": "text", "expr": "loosely", "word_idx": 49355, "sentence_idx": 923, "label": "unlabeled"}, {"type": "text", "expr": "diagnose", "word_idx": 49362, "sentence_idx": 924, "label": "unlabeled"}, {"type": "text", "expr": "In Figure\u00a0 4 , we show the results of using 300, 1000, and 2000 proposals", "word_idx": 49370, "sentence_idx": 925, "label": "unlabeled"}, {"type": "text", "expr": " We compare with SS and EB, and the  $N$  proposals are the top- $N$  ranked ones based on the confidence generated by these methods", "word_idx": 49443, "sentence_idx": 926, "label": "unlabeled"}, {"type": "text", "expr": "\nThe plots show that the RPN method behaves gracefully when the number of proposals drops from 2000 to 300", "word_idx": 49575, "sentence_idx": 927, "label": "unlabeled"}, {"type": "text", "expr": " This explains why the RPN has a good ultimate detection mAP when using as few as 300 proposals", "word_idx": 49681, "sentence_idx": 928, "label": "unlabeled"}, {"type": "text", "expr": " As we analyzed before, this property is mainly attributed to the  cls  term of the RPN", "word_idx": 49776, "sentence_idx": 929, "label": "unlabeled"}, {"type": "text", "expr": " The recall of SS and EB drops more quickly than RPN when the proposals are fewer", "word_idx": 49863, "sentence_idx": 930, "label": "unlabeled"}, {"type": "text", "expr": "The OverFeat paper   proposes a detection method that uses regressors and classifiers on sliding windows over convolutional feature maps", "word_idx": 49944, "sentence_idx": 931, "label": "unlabeled"}, {"type": "text", "expr": " OverFeat is a  one-stage ,  class-specific  detection pipeline, and ours is a  two-stage cascade  consisting of class-agnostic proposals and class-specific detections", "word_idx": 50080, "sentence_idx": 932, "label": "unlabeled"}, {"type": "text", "expr": " In OverFeat, the region-wise features come from a sliding window of one aspect ratio over a scale pyramid", "word_idx": 50247, "sentence_idx": 933, "label": "unlabeled"}, {"type": "text", "expr": " These features are used to simultaneously determine the location and category of objects", "word_idx": 50353, "sentence_idx": 934, "label": "unlabeled"}, {"type": "text", "expr": " In RPN, the features are from square (3 $\\times$ 3) sliding windows and predict proposals relative to anchors with different scales and aspect ratios", "word_idx": 50442, "sentence_idx": 935, "label": "unlabeled"}, {"type": "text", "expr": " Though both methods use sliding windows, the region proposal task is only the first stage of Faster R-CNN\u2014the downstream Fast R-CNN detector  attends  to the proposals to refine them", "word_idx": 50592, "sentence_idx": 936, "label": "unlabeled"}, {"type": "text", "expr": " In the second stage of our cascade, the region-wise features are adaptively pooled   from proposal boxes that more faithfully cover the features of the regions", "word_idx": 50775, "sentence_idx": 937, "label": "unlabeled"}, {"type": "text", "expr": " We believe these features lead to more accurate detections", "word_idx": 50935, "sentence_idx": 938, "label": "unlabeled"}, {"type": "text", "expr": "One-Stage Detection  \\vs \u00a0Two-Stage Proposal + Detection", "word_idx": 50994, "sentence_idx": 939, "label": "unlabeled"}, {"type": "text", "expr": "one-stage", "word_idx": 51050, "sentence_idx": 940, "label": "unlabeled"}, {"type": "text", "expr": "class-specific", "word_idx": 51059, "sentence_idx": 941, "label": "unlabeled"}, {"type": "text", "expr": "two-stage cascade", "word_idx": 51073, "sentence_idx": 942, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 51090, "sentence_idx": 943, "label": "unlabeled"}, {"type": "text", "expr": "attends", "word_idx": 51096, "sentence_idx": 944, "label": "unlabeled"}, {"type": "text", "expr": "To compare the one-stage and two-stage systems, we  emulate  the OverFeat system (and thus also circumvent other differences of implementation details) by  one-stage  Fast R-CNN", "word_idx": 51103, "sentence_idx": 945, "label": "unlabeled"}, {"type": "text", "expr": " In this system, the \u201cproposals\u201d are dense sliding windows of 3 scales (128, 256, 512) and 3 aspect ratios (1:1, 1:2, 2:1)", "word_idx": 51280, "sentence_idx": 946, "label": "unlabeled"}, {"type": "text", "expr": "\nFast R-CNN is trained to predict class-specific scores and regress box locations from these sliding windows", "word_idx": 51402, "sentence_idx": 947, "label": "unlabeled"}, {"type": "text", "expr": " Because the OverFeat system adopts an image pyramid, we also evaluate using convolutional features extracted from 5 scales", "word_idx": 51510, "sentence_idx": 948, "label": "unlabeled"}, {"type": "text", "expr": " We use those 5 scales as in  ", "word_idx": 51633, "sentence_idx": 949, "label": "unlabeled"}, {"type": "text", "expr": "emulate", "word_idx": 51663, "sentence_idx": 950, "label": "unlabeled"}, {"type": "text", "expr": "one-stage", "word_idx": 51670, "sentence_idx": 951, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a0 10  compares the two-stage system and two variants of the one-stage system", "word_idx": 51679, "sentence_idx": 952, "label": "unlabeled"}, {"type": "text", "expr": " Using the ZF model, the one-stage system has an mAP of 53", "word_idx": 51760, "sentence_idx": 953, "label": "unlabeled"}, {"type": "text", "expr": " This is lower than the two-stage system (58", "word_idx": 51818, "sentence_idx": 954, "label": "unlabeled"}, {"type": "text", "expr": "7%) by 4", "word_idx": 51862, "sentence_idx": 955, "label": "unlabeled"}, {"type": "text", "expr": " This experiment justifies the effectiveness of cascaded region proposals and object detection", "word_idx": 51870, "sentence_idx": 956, "label": "unlabeled"}, {"type": "text", "expr": " Similar observations are reported in  , where replacing SS region proposals with sliding windows leads to  $\\sim$ 6% degradation in both papers", "word_idx": 51964, "sentence_idx": 957, "label": "unlabeled"}, {"type": "text", "expr": "\nWe also note that the one-stage system is slower as it has considerably more proposals to process", "word_idx": 52108, "sentence_idx": 958, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sim$$", "word_idx": 52206, "sentence_idx": 959, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a011:  Object detection results (%) on the  MS COCO  dataset", "word_idx": 52210, "sentence_idx": 960, "label": "unlabeled"}, {"type": "text", "expr": " The model is VGG-16", "word_idx": 52274, "sentence_idx": 961, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a011:", "word_idx": 52294, "sentence_idx": 962, "label": "unlabeled"}, {"type": "text", "expr": "MS COCO", "word_idx": 52303, "sentence_idx": 963, "label": "unlabeled"}, {"type": "text", "expr": "COCO val COCO test-dev", "word_idx": 52310, "sentence_idx": 964, "label": "unlabeled"}, {"type": "text", "expr": "COCO val", "word_idx": 52332, "sentence_idx": 965, "label": "unlabeled"}, {"type": "text", "expr": "COCO test-dev", "word_idx": 52340, "sentence_idx": 966, "label": "unlabeled"}, {"type": "text", "expr": "method proposals training data \u00a0\u00a0\u00a0mAP@", "word_idx": 52353, "sentence_idx": 967, "label": "unlabeled"}, {"type": "text", "expr": "5 mAP@[", "word_idx": 52391, "sentence_idx": 968, "label": "unlabeled"}, {"type": "text", "expr": "95] \u00a0\u00a0\u00a0mAP@", "word_idx": 52398, "sentence_idx": 969, "label": "unlabeled"}, {"type": "text", "expr": "5 mAP@[", "word_idx": 52409, "sentence_idx": 970, "label": "unlabeled"}, {"type": "text", "expr": "method", "word_idx": 52416, "sentence_idx": 971, "label": "unlabeled"}, {"type": "text", "expr": "proposals", "word_idx": 52422, "sentence_idx": 972, "label": "unlabeled"}, {"type": "text", "expr": "training data", "word_idx": 52431, "sentence_idx": 973, "label": "unlabeled"}, {"type": "text", "expr": "mAP@[", "word_idx": 52444, "sentence_idx": 974, "label": "unlabeled"}, {"type": "text", "expr": "mAP@[", "word_idx": 52449, "sentence_idx": 975, "label": "unlabeled"}, {"type": "text", "expr": "SS, 2000 COCO train - - 35", "word_idx": 52454, "sentence_idx": 976, "label": "unlabeled"}, {"type": "text", "expr": "Fast R-CNN", "word_idx": 52480, "sentence_idx": 977, "label": "unlabeled"}, {"type": "text", "expr": "Fast R-CNN", "word_idx": 52490, "sentence_idx": 978, "label": "unlabeled"}, {"type": "text", "expr": "SS, 2000", "word_idx": 52500, "sentence_idx": 979, "label": "unlabeled"}, {"type": "text", "expr": "SS, 2000", "word_idx": 52508, "sentence_idx": 980, "label": "unlabeled"}, {"type": "text", "expr": "COCO train", "word_idx": 52516, "sentence_idx": 981, "label": "unlabeled"}, {"type": "text", "expr": "COCO train", "word_idx": 52526, "sentence_idx": 982, "label": "unlabeled"}, {"type": "text", "expr": "SS, 2000 COCO train 38", "word_idx": 52536, "sentence_idx": 983, "label": "unlabeled"}, {"type": "text", "expr": "Fast R-CNN  [impl", "word_idx": 52558, "sentence_idx": 984, "label": "unlabeled"}, {"type": "text", "expr": " in this paper]", "word_idx": 52575, "sentence_idx": 985, "label": "unlabeled"}, {"type": "text", "expr": "Fast R-CNN", "word_idx": 52590, "sentence_idx": 986, "label": "unlabeled"}, {"type": "text", "expr": "[impl", "word_idx": 52600, "sentence_idx": 987, "label": "unlabeled"}, {"type": "text", "expr": " in this paper]", "word_idx": 52605, "sentence_idx": 988, "label": "unlabeled"}, {"type": "text", "expr": "SS, 2000", "word_idx": 52620, "sentence_idx": 989, "label": "unlabeled"}, {"type": "text", "expr": "SS, 2000", "word_idx": 52628, "sentence_idx": 990, "label": "unlabeled"}, {"type": "text", "expr": "COCO train", "word_idx": 52636, "sentence_idx": 991, "label": "unlabeled"}, {"type": "text", "expr": "COCO train", "word_idx": 52646, "sentence_idx": 992, "label": "unlabeled"}, {"type": "text", "expr": "Faster R-CNN RPN, 300 COCO train 41", "word_idx": 52656, "sentence_idx": 993, "label": "unlabeled"}, {"type": "text", "expr": "Faster R-CNN", "word_idx": 52691, "sentence_idx": 994, "label": "unlabeled"}, {"type": "text", "expr": "Faster R-CNN", "word_idx": 52703, "sentence_idx": 995, "label": "unlabeled"}, {"type": "text", "expr": "RPN, 300", "word_idx": 52715, "sentence_idx": 996, "label": "unlabeled"}, {"type": "text", "expr": "RPN, 300", "word_idx": 52723, "sentence_idx": 997, "label": "unlabeled"}, {"type": "text", "expr": "COCO train", "word_idx": 52731, "sentence_idx": 998, "label": "unlabeled"}, {"type": "text", "expr": "COCO train", "word_idx": 52741, "sentence_idx": 999, "label": "unlabeled"}, {"type": "text", "expr": "Faster R-CNN RPN, 300 COCO trainval - - 42", "word_idx": 52751, "sentence_idx": 1000, "label": "unlabeled"}, {"type": "text", "expr": "Faster R-CNN", "word_idx": 52793, "sentence_idx": 1001, "label": "unlabeled"}, {"type": "text", "expr": "Faster R-CNN", "word_idx": 52805, "sentence_idx": 1002, "label": "unlabeled"}, {"type": "text", "expr": "RPN, 300", "word_idx": 52817, "sentence_idx": 1003, "label": "unlabeled"}, {"type": "text", "expr": "RPN, 300", "word_idx": 52825, "sentence_idx": 1004, "label": "unlabeled"}, {"type": "text", "expr": "COCO trainval", "word_idx": 52833, "sentence_idx": 1005, "label": "unlabeled"}, {"type": "text", "expr": "COCO trainval", "word_idx": 52846, "sentence_idx": 1006, "label": "unlabeled"}, {"type": "text", "expr": "2  Experiments on MS COCO", "word_idx": 52859, "sentence_idx": 1007, "label": "unlabeled"}, {"type": "text", "expr": "We present more results on the Microsoft COCO object detection dataset  ", "word_idx": 52884, "sentence_idx": 1008, "label": "unlabeled"}, {"type": "text", "expr": " This dataset involves 80 object categories", "word_idx": 52956, "sentence_idx": 1009, "label": "unlabeled"}, {"type": "text", "expr": " We experiment with the 80k images on the training set, 40k images on the validation set, and 20k images on the test-dev set", "word_idx": 52999, "sentence_idx": 1010, "label": "unlabeled"}, {"type": "text", "expr": "\nWe evaluate the mAP averaged for IoU  $\\in[05:005:095]$  (COCO\u2019s standard metric, simply denoted as mAP@[", "word_idx": 53123, "sentence_idx": 1011, "label": "unlabeled"}, {"type": "text", "expr": "95]) and mAP@0", "word_idx": 53229, "sentence_idx": 1012, "label": "unlabeled"}, {"type": "text", "expr": "5 (PASCAL VOC\u2019s metric)", "word_idx": 53243, "sentence_idx": 1013, "label": "unlabeled"}, {"type": "math", "expr": "$$\\in[0.5:0.05:0.95]$$", "word_idx": 53266, "sentence_idx": 1014, "label": "unlabeled"}, {"type": "text", "expr": "There are a few minor changes of our system made for this dataset", "word_idx": 53284, "sentence_idx": 1015, "label": "unlabeled"}, {"type": "text", "expr": " We train our models on an 8-GPU implementation, and the effective mini-batch size becomes 8 for RPN (1 per GPU) and 16 for Fast R-CNN (2 per GPU)", "word_idx": 53349, "sentence_idx": 1016, "label": "unlabeled"}, {"type": "text", "expr": " The RPN step and Fast R-CNN step are both trained for 240k iterations with a learning rate of 0", "word_idx": 53495, "sentence_idx": 1017, "label": "unlabeled"}, {"type": "text", "expr": "003 and then for 80k iterations with 0", "word_idx": 53591, "sentence_idx": 1018, "label": "unlabeled"}, {"type": "text", "expr": " We modify the learning rates (starting with 0", "word_idx": 53629, "sentence_idx": 1019, "label": "unlabeled"}, {"type": "text", "expr": "003 instead of 0", "word_idx": 53675, "sentence_idx": 1020, "label": "unlabeled"}, {"type": "text", "expr": "001) because the mini-batch size is changed", "word_idx": 53691, "sentence_idx": 1021, "label": "unlabeled"}, {"type": "text", "expr": "\nFor the anchors, we use 3 aspect ratios and 4 scales (adding  $64^{2}$ ), mainly motivated by handling small objects on this dataset", "word_idx": 53734, "sentence_idx": 1022, "label": "unlabeled"}, {"type": "text", "expr": " In addition, in our Fast R-CNN step, the negative samples are defined as those with a maximum IoU with ground truth in the interval of  $[0,05)$ , instead of  $[01,05)$  used in  ", "word_idx": 53867, "sentence_idx": 1023, "label": "unlabeled"}, {"type": "text", "expr": " We note that in the SPPnet system  , the negative samples in  $[01,05)$  are used for network fine-tuning, but the negative samples in  $[0,05)$  are still visited in the SVM step with hard-negative mining", "word_idx": 54047, "sentence_idx": 1024, "label": "unlabeled"}, {"type": "text", "expr": " But the Fast R-CNN system   abandons the SVM step, so the negative samples in  $[0,01)$  are never visited", "word_idx": 54253, "sentence_idx": 1025, "label": "unlabeled"}, {"type": "text", "expr": " Including these  $[0,01)$  samples improves mAP@0", "word_idx": 54360, "sentence_idx": 1026, "label": "unlabeled"}, {"type": "text", "expr": "5 on the COCO dataset for both Fast R-CNN and Faster R-CNN systems (but the impact is negligible on PASCAL VOC)", "word_idx": 54410, "sentence_idx": 1027, "label": "unlabeled"}, {"type": "math", "expr": "$$64^{2}$$", "word_idx": 54521, "sentence_idx": 1028, "label": "unlabeled"}, {"type": "math", "expr": "$$[0,0.5)$$", "word_idx": 54527, "sentence_idx": 1029, "label": "unlabeled"}, {"type": "math", "expr": "$$[0.1,0.5)$$", "word_idx": 54534, "sentence_idx": 1030, "label": "unlabeled"}, {"type": "math", "expr": "$$[0.1,0.5)$$", "word_idx": 54543, "sentence_idx": 1031, "label": "unlabeled"}, {"type": "math", "expr": "$$[0,0.5)$$", "word_idx": 54552, "sentence_idx": 1032, "label": "unlabeled"}, {"type": "math", "expr": "$$[0,0.1)$$", "word_idx": 54559, "sentence_idx": 1033, "label": "unlabeled"}, {"type": "math", "expr": "$$[0,0.1)$$", "word_idx": 54566, "sentence_idx": 1034, "label": "unlabeled"}, {"type": "text", "expr": "The rest of the implementation details are the same as on PASCAL VOC", "word_idx": 54573, "sentence_idx": 1035, "label": "unlabeled"}, {"type": "text", "expr": " In particular, we keep using 300 proposals and single-scale ( $s=600$ ) testing", "word_idx": 54641, "sentence_idx": 1036, "label": "unlabeled"}, {"type": "text", "expr": " The testing time is still about 200ms per image on the COCO dataset", "word_idx": 54721, "sentence_idx": 1037, "label": "unlabeled"}, {"type": "math", "expr": "$$s=600$$", "word_idx": 54789, "sentence_idx": 1038, "label": "unlabeled"}, {"type": "text", "expr": "In Table\u00a0 11  we first report the results of the Fast R-CNN system   using the implementation in this paper", "word_idx": 54794, "sentence_idx": 1039, "label": "unlabeled"}, {"type": "text", "expr": " Our Fast R-CNN baseline has 39", "word_idx": 54901, "sentence_idx": 1040, "label": "unlabeled"}, {"type": "text", "expr": "3% mAP@0", "word_idx": 54932, "sentence_idx": 1041, "label": "unlabeled"}, {"type": "text", "expr": "5 on the test-dev set, higher than that reported in  ", "word_idx": 54940, "sentence_idx": 1042, "label": "unlabeled"}, {"type": "text", "expr": " We conjecture that the reason for this gap is mainly due to the definition of the negative samples and also the changes of the mini-batch sizes", "word_idx": 54993, "sentence_idx": 1043, "label": "unlabeled"}, {"type": "text", "expr": " We also note that the mAP@[", "word_idx": 55137, "sentence_idx": 1044, "label": "unlabeled"}, {"type": "text", "expr": "95] is just comparable", "word_idx": 55165, "sentence_idx": 1045, "label": "unlabeled"}, {"type": "text", "expr": "Next we evaluate our Faster R-CNN system", "word_idx": 55187, "sentence_idx": 1046, "label": "unlabeled"}, {"type": "text", "expr": "\nUsing the COCO training set to train, Faster R-CNN has 42", "word_idx": 55227, "sentence_idx": 1047, "label": "unlabeled"}, {"type": "text", "expr": "1% mAP@0", "word_idx": 55285, "sentence_idx": 1048, "label": "unlabeled"}, {"type": "text", "expr": "5 and 21", "word_idx": 55293, "sentence_idx": 1049, "label": "unlabeled"}, {"type": "text", "expr": "5% mAP@[", "word_idx": 55301, "sentence_idx": 1050, "label": "unlabeled"}, {"type": "text", "expr": "95] on the COCO test-dev set", "word_idx": 55309, "sentence_idx": 1051, "label": "unlabeled"}, {"type": "text", "expr": " This is 2", "word_idx": 55337, "sentence_idx": 1052, "label": "unlabeled"}, {"type": "text", "expr": "8% higher for mAP@0", "word_idx": 55347, "sentence_idx": 1053, "label": "unlabeled"}, {"type": "text", "expr": "5 and  2", "word_idx": 55366, "sentence_idx": 1054, "label": "unlabeled"}, {"type": "text", "expr": "2% higher for mAP@[", "word_idx": 55374, "sentence_idx": 1055, "label": "unlabeled"}, {"type": "text", "expr": "95]  than the Fast R-CNN counterpart under the same protocol (Table\u00a0 11 )", "word_idx": 55393, "sentence_idx": 1056, "label": "unlabeled"}, {"type": "text", "expr": " This indicates that RPN performs excellent for improving the localization accuracy at higher IoU thresholds", "word_idx": 55466, "sentence_idx": 1057, "label": "unlabeled"}, {"type": "text", "expr": "\nUsing the COCO trainval set to train, Faster R-CNN has 42", "word_idx": 55574, "sentence_idx": 1058, "label": "unlabeled"}, {"type": "text", "expr": "7% mAP@0", "word_idx": 55632, "sentence_idx": 1059, "label": "unlabeled"}, {"type": "text", "expr": "5 and 21", "word_idx": 55640, "sentence_idx": 1060, "label": "unlabeled"}, {"type": "text", "expr": "9% mAP@[", "word_idx": 55648, "sentence_idx": 1061, "label": "unlabeled"}, {"type": "text", "expr": "95] on the COCO test-dev set", "word_idx": 55656, "sentence_idx": 1062, "label": "unlabeled"}, {"type": "text", "expr": "\nFigure\u00a0 6  shows some results on the MS COCO test-dev set", "word_idx": 55684, "sentence_idx": 1063, "label": "unlabeled"}, {"type": "text", "expr": "2% higher for mAP@[", "word_idx": 55742, "sentence_idx": 1064, "label": "unlabeled"}, {"type": "text", "expr": "Faster R-CNN in ILSVRC & COCO 2015 competitions \nWe have demonstrated that Faster R-CNN benefits more from better features, thanks to the fact that the RPN completely learns to propose regions by neural networks", "word_idx": 55761, "sentence_idx": 1065, "label": "unlabeled"}, {"type": "text", "expr": " This observation is still valid even when one increases the depth substantially to over 100 layers  ", "word_idx": 55972, "sentence_idx": 1066, "label": "unlabeled"}, {"type": "text", "expr": " Only by replacing VGG-16 with a 101-layer residual net (ResNet-101)  , the Faster R-CNN system increases the mAP from 41", "word_idx": 56073, "sentence_idx": 1067, "label": "unlabeled"}, {"type": "text", "expr": "5%/21", "word_idx": 56194, "sentence_idx": 1068, "label": "unlabeled"}, {"type": "text", "expr": "2% (VGG-16) to 48", "word_idx": 56199, "sentence_idx": 1069, "label": "unlabeled"}, {"type": "text", "expr": "4%/27", "word_idx": 56216, "sentence_idx": 1070, "label": "unlabeled"}, {"type": "text", "expr": "2% (ResNet-101) on the COCO val set", "word_idx": 56221, "sentence_idx": 1071, "label": "unlabeled"}, {"type": "text", "expr": " With other improvements orthogonal to Faster R-CNN, He  \\etal  obtained a single-model result of 55", "word_idx": 56256, "sentence_idx": 1072, "label": "unlabeled"}, {"type": "text", "expr": "7%/34", "word_idx": 56356, "sentence_idx": 1073, "label": "unlabeled"}, {"type": "text", "expr": "9% and an ensemble result of 59", "word_idx": 56361, "sentence_idx": 1074, "label": "unlabeled"}, {"type": "text", "expr": "0%/37", "word_idx": 56392, "sentence_idx": 1075, "label": "unlabeled"}, {"type": "text", "expr": "4% on the COCO test-dev set, which won the 1st place in the COCO 2015 object detection competition", "word_idx": 56397, "sentence_idx": 1076, "label": "unlabeled"}, {"type": "text", "expr": " The same system   also won the 1st place in the ILSVRC 2015 object detection competition, surpassing the second place by absolute 8", "word_idx": 56495, "sentence_idx": 1077, "label": "unlabeled"}, {"type": "text", "expr": " RPN is also a building block of the 1st-place winning entries in ILSVRC 2015 localization and COCO 2015 segmentation competitions, for which the details are available in   and   respectively", "word_idx": 56627, "sentence_idx": 1078, "label": "unlabeled"}, {"type": "text", "expr": "Faster R-CNN in ILSVRC & COCO 2015 competitions", "word_idx": 56818, "sentence_idx": 1079, "label": "unlabeled"}, {"type": "text", "expr": "\\etal", "word_idx": 56865, "sentence_idx": 1080, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a05:  Selected examples of object detection results on the PASCAL VOC 2007 test set using the Faster R-CNN system", "word_idx": 56870, "sentence_idx": 1081, "label": "unlabeled"}, {"type": "text", "expr": " The model is VGG-16 and the training data is 07+12 trainval (73", "word_idx": 56988, "sentence_idx": 1082, "label": "unlabeled"}, {"type": "text", "expr": "2% mAP on the 2007 test set)", "word_idx": 57052, "sentence_idx": 1083, "label": "unlabeled"}, {"type": "text", "expr": " Our method detects objects of a wide range of scales and aspect ratios", "word_idx": 57080, "sentence_idx": 1084, "label": "unlabeled"}, {"type": "text", "expr": " Each output box is associated with a category label and a softmax score in  $[0,1]$ ", "word_idx": 57151, "sentence_idx": 1085, "label": "unlabeled"}, {"type": "text", "expr": " A score threshold of 0", "word_idx": 57236, "sentence_idx": 1086, "label": "unlabeled"}, {"type": "text", "expr": "6 is used to display these images", "word_idx": 57259, "sentence_idx": 1087, "label": "unlabeled"}, {"type": "text", "expr": " The running time for obtaining these results is  198ms  per image,  including all steps ", "word_idx": 57292, "sentence_idx": 1088, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a05:", "word_idx": 57381, "sentence_idx": 1089, "label": "unlabeled"}, {"type": "math", "expr": "$$[0,1]$$", "word_idx": 57390, "sentence_idx": 1090, "label": "unlabeled"}, {"type": "text", "expr": "198ms", "word_idx": 57395, "sentence_idx": 1091, "label": "unlabeled"}, {"type": "text", "expr": "including all steps", "word_idx": 57400, "sentence_idx": 1092, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a06:  Selected examples of object detection results on the MS COCO test-dev set using the Faster R-CNN system", "word_idx": 57419, "sentence_idx": 1093, "label": "unlabeled"}, {"type": "text", "expr": " The model is VGG-16 and the training data is COCO trainval (42", "word_idx": 57533, "sentence_idx": 1094, "label": "unlabeled"}, {"type": "text", "expr": "7% mAP@0", "word_idx": 57596, "sentence_idx": 1095, "label": "unlabeled"}, {"type": "text", "expr": "5 on the test-dev set)", "word_idx": 57604, "sentence_idx": 1096, "label": "unlabeled"}, {"type": "text", "expr": " Each output box is associated with a category label and a softmax score in  $[0,1]$ ", "word_idx": 57626, "sentence_idx": 1097, "label": "unlabeled"}, {"type": "text", "expr": " A score threshold of 0", "word_idx": 57711, "sentence_idx": 1098, "label": "unlabeled"}, {"type": "text", "expr": "6 is used to display these images", "word_idx": 57734, "sentence_idx": 1099, "label": "unlabeled"}, {"type": "text", "expr": " For each image, one color represents one object category in that image", "word_idx": 57767, "sentence_idx": 1100, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a06:", "word_idx": 57838, "sentence_idx": 1101, "label": "unlabeled"}, {"type": "math", "expr": "$$[0,1]$$", "word_idx": 57847, "sentence_idx": 1102, "label": "unlabeled"}, {"type": "text", "expr": "3  From MS COCO to PASCAL VOC", "word_idx": 57852, "sentence_idx": 1103, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a012:  Detection mAP (%) of Faster R-CNN on PASCAL VOC 2007 test set and 2012 test set using different training data", "word_idx": 57881, "sentence_idx": 1104, "label": "unlabeled"}, {"type": "text", "expr": " The model is VGG-16", "word_idx": 58001, "sentence_idx": 1105, "label": "unlabeled"}, {"type": "text", "expr": " \u201cCOCO\u201d denotes that the COCO trainval set is used for training", "word_idx": 58021, "sentence_idx": 1106, "label": "unlabeled"}, {"type": "text", "expr": " See also Table\u00a0 6  and Table\u00a0 7 ", "word_idx": 58084, "sentence_idx": 1107, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a012:", "word_idx": 58117, "sentence_idx": 1108, "label": "unlabeled"}, {"type": "text", "expr": "training data 2007 test 2012 test", "word_idx": 58126, "sentence_idx": 1109, "label": "unlabeled"}, {"type": "text", "expr": "training data", "word_idx": 58159, "sentence_idx": 1110, "label": "unlabeled"}, {"type": "text", "expr": "2007 test", "word_idx": 58172, "sentence_idx": 1111, "label": "unlabeled"}, {"type": "text", "expr": "2012 test", "word_idx": 58181, "sentence_idx": 1112, "label": "unlabeled"}, {"type": "text", "expr": "VOC07 69", "word_idx": 58190, "sentence_idx": 1113, "label": "unlabeled"}, {"type": "text", "expr": "VOC07", "word_idx": 58198, "sentence_idx": 1114, "label": "unlabeled"}, {"type": "text", "expr": "VOC07+12 73", "word_idx": 58203, "sentence_idx": 1115, "label": "unlabeled"}, {"type": "text", "expr": "VOC07+12", "word_idx": 58214, "sentence_idx": 1116, "label": "unlabeled"}, {"type": "text", "expr": "VOC07++12 - 70", "word_idx": 58222, "sentence_idx": 1117, "label": "unlabeled"}, {"type": "text", "expr": "VOC07++12", "word_idx": 58236, "sentence_idx": 1118, "label": "unlabeled"}, {"type": "text", "expr": "COCO (no VOC) 76", "word_idx": 58245, "sentence_idx": 1119, "label": "unlabeled"}, {"type": "text", "expr": "COCO (no VOC)", "word_idx": 58261, "sentence_idx": 1120, "label": "unlabeled"}, {"type": "text", "expr": "COCO+VOC07+12 78", "word_idx": 58274, "sentence_idx": 1121, "label": "unlabeled"}, {"type": "text", "expr": "COCO+VOC07+12", "word_idx": 58290, "sentence_idx": 1122, "label": "unlabeled"}, {"type": "text", "expr": "COCO+VOC07++12 - 75", "word_idx": 58303, "sentence_idx": 1123, "label": "unlabeled"}, {"type": "text", "expr": "COCO+VOC07++12", "word_idx": 58322, "sentence_idx": 1124, "label": "unlabeled"}, {"type": "text", "expr": "Large-scale data is of crucial importance for improving deep neural networks", "word_idx": 58336, "sentence_idx": 1125, "label": "unlabeled"}, {"type": "text", "expr": " Next, we investigate how the MS COCO dataset can help with the detection performance on PASCAL VOC", "word_idx": 58412, "sentence_idx": 1126, "label": "unlabeled"}, {"type": "text", "expr": "Large-scale data is of crucial importance for improving deep neural networks", "word_idx": 58511, "sentence_idx": 1127, "label": "unlabeled"}, {"type": "text", "expr": " Next, we investigate how the MS COCO dataset can help with the detection performance on PASCAL VOC", "word_idx": 58587, "sentence_idx": 1128, "label": "unlabeled"}, {"type": "text", "expr": "As a simple baseline, we directly evaluate the COCO detection model on the PASCAL VOC dataset,  without fine-tuning on any PASCAL VOC data ", "word_idx": 58686, "sentence_idx": 1129, "label": "unlabeled"}, {"type": "text", "expr": " This evaluation is possible because the categories on COCO are a superset of those on PASCAL VOC", "word_idx": 58825, "sentence_idx": 1130, "label": "unlabeled"}, {"type": "text", "expr": " The categories that are exclusive on COCO are ignored in this experiment, and the softmax layer is performed only on the 20 categories plus background", "word_idx": 58922, "sentence_idx": 1131, "label": "unlabeled"}, {"type": "text", "expr": "\nThe mAP under this setting is 76", "word_idx": 59073, "sentence_idx": 1132, "label": "unlabeled"}, {"type": "text", "expr": "1% on the PASCAL VOC 2007 test set (Table\u00a0 12 )", "word_idx": 59106, "sentence_idx": 1133, "label": "unlabeled"}, {"type": "text", "expr": " This result is better than that trained on VOC07+12 (73", "word_idx": 59153, "sentence_idx": 1134, "label": "unlabeled"}, {"type": "text", "expr": "2%) by a good margin, even though the PASCAL VOC data are not exploited", "word_idx": 59209, "sentence_idx": 1135, "label": "unlabeled"}, {"type": "text", "expr": "without fine-tuning on any PASCAL VOC data", "word_idx": 59280, "sentence_idx": 1136, "label": "unlabeled"}, {"type": "text", "expr": "Then we fine-tune the COCO detection model on the VOC dataset", "word_idx": 59322, "sentence_idx": 1137, "label": "unlabeled"}, {"type": "text", "expr": " In this experiment, the COCO model is in place of the ImageNet-pre-trained model (that is used to initialize the network weights), and the Faster R-CNN system is fine-tuned as described in Section\u00a0 3", "word_idx": 59383, "sentence_idx": 1138, "label": "unlabeled"}, {"type": "text", "expr": " Doing so leads to 78", "word_idx": 59583, "sentence_idx": 1139, "label": "unlabeled"}, {"type": "text", "expr": "8% mAP on the PASCAL VOC 2007 test set", "word_idx": 59604, "sentence_idx": 1140, "label": "unlabeled"}, {"type": "text", "expr": " The extra data from the COCO set increases the mAP by 5", "word_idx": 59642, "sentence_idx": 1141, "label": "unlabeled"}, {"type": "text", "expr": " Table\u00a0 6  shows that the model trained on COCO+VOC has the best AP for every individual category on PASCAL VOC 2007", "word_idx": 59698, "sentence_idx": 1142, "label": "unlabeled"}, {"type": "text", "expr": "\nSimilar improvements are observed on the PASCAL VOC 2012 test set (Table\u00a0 12  and Table\u00a0 7 )", "word_idx": 59814, "sentence_idx": 1143, "label": "unlabeled"}, {"type": "text", "expr": " We note that the test-time speed of obtaining these strong results is still about  200ms per image ", "word_idx": 59907, "sentence_idx": 1144, "label": "unlabeled"}, {"type": "text", "expr": "200ms per image", "word_idx": 60007, "sentence_idx": 1145, "label": "unlabeled"}, {"type": "text", "expr": "5  Conclusion", "word_idx": 60022, "sentence_idx": 1146, "label": "unlabeled"}, {"type": "text", "expr": "We have presented RPNs for efficient and accurate region proposal generation", "word_idx": 60035, "sentence_idx": 1147, "label": "unlabeled"}, {"type": "text", "expr": " By sharing convolutional features with the down-stream detection network, the region proposal step is nearly cost-free", "word_idx": 60111, "sentence_idx": 1148, "label": "unlabeled"}, {"type": "text", "expr": " Our method enables a unified, deep-learning-based object detection system to run at near real-time frame rates", "word_idx": 60230, "sentence_idx": 1149, "label": "unlabeled"}, {"type": "text", "expr": " The learned RPN also improves region proposal quality and thus the overall object detection accuracy", "word_idx": 60341, "sentence_idx": 1150, "label": "unlabeled"}, {"type": "text", "expr": "We have presented RPNs for efficient and accurate region proposal generation", "word_idx": 60442, "sentence_idx": 1151, "label": "unlabeled"}, {"type": "text", "expr": " By sharing convolutional features with the down-stream detection network, the region proposal step is nearly cost-free", "word_idx": 60518, "sentence_idx": 1152, "label": "unlabeled"}, {"type": "text", "expr": " Our method enables a unified, deep-learning-based object detection system to run at near real-time frame rates", "word_idx": 60637, "sentence_idx": 1153, "label": "unlabeled"}, {"type": "text", "expr": " The learned RPN also improves region proposal quality and thus the overall object detection accuracy", "word_idx": 60748, "sentence_idx": 1154, "label": "unlabeled"}, {"type": "text", "expr": "References", "word_idx": 60849, "sentence_idx": 1155, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, X", "word_idx": 60859, "sentence_idx": 1156, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang, S", "word_idx": 60865, "sentence_idx": 1157, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ren, and J", "word_idx": 60874, "sentence_idx": 1158, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sun, \u201cSpatial pyramid pooling in deep\nconvolutional networks for visual recognition,\u201d in  European Conference\non Computer Vision (ECCV) , 2014", "word_idx": 60885, "sentence_idx": 1159, "label": "unlabeled"}, {"type": "text", "expr": "European Conference\non Computer Vision (ECCV)", "word_idx": 61028, "sentence_idx": 1160, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Girshick, \u201cFast R-CNN,\u201d in  IEEE International Conference on\nComputer Vision (ICCV) , 2015", "word_idx": 61073, "sentence_idx": 1161, "label": "unlabeled"}, {"type": "text", "expr": "IEEE International Conference on\nComputer Vision (ICCV)", "word_idx": 61164, "sentence_idx": 1162, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Simonyan and A", "word_idx": 61219, "sentence_idx": 1163, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zisserman, \u201cVery deep convolutional networks for\nlarge-scale image recognition,\u201d in  International Conference on\nLearning Representations (ICLR) , 2015", "word_idx": 61234, "sentence_idx": 1164, "label": "unlabeled"}, {"type": "text", "expr": "International Conference on\nLearning Representations (ICLR)", "word_idx": 61386, "sentence_idx": 1165, "label": "unlabeled"}, {"type": "text", "expr": " Uijlings, K", "word_idx": 61445, "sentence_idx": 1166, "label": "unlabeled"}, {"type": "text", "expr": " van\u00a0de Sande, T", "word_idx": 61457, "sentence_idx": 1167, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gevers, and A", "word_idx": 61473, "sentence_idx": 1168, "label": "unlabeled"}, {"type": "text", "expr": " Smeulders, \u201cSelective\nsearch for object recognition,\u201d  International Journal of Computer\nVision (IJCV) , 2013", "word_idx": 61487, "sentence_idx": 1169, "label": "unlabeled"}, {"type": "text", "expr": "International Journal of Computer\nVision (IJCV)", "word_idx": 61597, "sentence_idx": 1170, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Girshick, J", "word_idx": 61644, "sentence_idx": 1171, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Donahue, T", "word_idx": 61656, "sentence_idx": 1172, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Darrell, and J", "word_idx": 61667, "sentence_idx": 1173, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Malik, \u201cRich feature hierarchies\nfor accurate object detection and semantic segmentation,\u201d in  IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) , 2014", "word_idx": 61682, "sentence_idx": 1174, "label": "unlabeled"}, {"type": "text", "expr": "IEEE\nConference on Computer Vision and Pattern Recognition (CVPR)", "word_idx": 61850, "sentence_idx": 1175, "label": "unlabeled"}, {"type": "text", "expr": " Zitnick and P", "word_idx": 61915, "sentence_idx": 1176, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Doll\u00e1r, \u201cEdge boxes: Locating object proposals from\nedges,\u201d in  European Conference on Computer Vision (ECCV) , 2014", "word_idx": 61929, "sentence_idx": 1177, "label": "unlabeled"}, {"type": "text", "expr": "European Conference on Computer Vision (ECCV)", "word_idx": 62046, "sentence_idx": 1178, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Long, E", "word_idx": 62091, "sentence_idx": 1179, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Shelhamer, and T", "word_idx": 62099, "sentence_idx": 1180, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Darrell, \u201cFully convolutional networks for\nsemantic segmentation,\u201d in  IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) , 2015", "word_idx": 62116, "sentence_idx": 1181, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Conference on Computer Vision and\nPattern Recognition (CVPR)", "word_idx": 62260, "sentence_idx": 1182, "label": "unlabeled"}, {"type": "text", "expr": " Felzenszwalb, R", "word_idx": 62325, "sentence_idx": 1183, "label": "unlabeled"}, {"type": "text", "expr": " Girshick, D", "word_idx": 62341, "sentence_idx": 1184, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0McAllester, and D", "word_idx": 62353, "sentence_idx": 1185, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ramanan, \u201cObject\ndetection with discriminatively trained part-based models,\u201d  IEEE\nTransactions on Pattern Analysis and Machine Intelligence (TPAMI) , 2010", "word_idx": 62371, "sentence_idx": 1186, "label": "unlabeled"}, {"type": "text", "expr": "IEEE\nTransactions on Pattern Analysis and Machine Intelligence (TPAMI)", "word_idx": 62527, "sentence_idx": 1187, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sermanet, D", "word_idx": 62597, "sentence_idx": 1188, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Eigen, X", "word_idx": 62609, "sentence_idx": 1189, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang, M", "word_idx": 62618, "sentence_idx": 1190, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mathieu, R", "word_idx": 62627, "sentence_idx": 1191, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fergus, and Y", "word_idx": 62638, "sentence_idx": 1192, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0LeCun,\n\u201cOverfeat: Integrated recognition, localization and detection using\nconvolutional networks,\u201d in  International Conference on Learning\nRepresentations (ICLR) , 2014", "word_idx": 62652, "sentence_idx": 1193, "label": "unlabeled"}, {"type": "text", "expr": "International Conference on Learning\nRepresentations (ICLR)", "word_idx": 62823, "sentence_idx": 1194, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ren, K", "word_idx": 62882, "sentence_idx": 1195, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, R", "word_idx": 62889, "sentence_idx": 1196, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Girshick, and J", "word_idx": 62895, "sentence_idx": 1197, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sun, \u201cFaster R-CNN: Towards real-time\nobject detection with region proposal networks,\u201d in  Neural\nInformation Processing Systems (NIPS) , 2015", "word_idx": 62911, "sentence_idx": 1198, "label": "unlabeled"}, {"type": "text", "expr": "Neural\nInformation Processing Systems (NIPS)", "word_idx": 63054, "sentence_idx": 1199, "label": "unlabeled"}, {"type": "text", "expr": "11 \nM", "word_idx": 63098, "sentence_idx": 1200, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Everingham, L", "word_idx": 63103, "sentence_idx": 1201, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Van\u00a0Gool, C", "word_idx": 63117, "sentence_idx": 1202, "label": "unlabeled"}, {"type": "text", "expr": " Williams, J", "word_idx": 63129, "sentence_idx": 1203, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Winn, and A", "word_idx": 63141, "sentence_idx": 1204, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zisserman, \u201cThe\nPASCAL Visual Object Classes Challenge 2007 (VOC2007)\nResults,\u201d 2007", "word_idx": 63153, "sentence_idx": 1205, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Everingham, L", "word_idx": 63238, "sentence_idx": 1206, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Van\u00a0Gool, C", "word_idx": 63252, "sentence_idx": 1207, "label": "unlabeled"}, {"type": "text", "expr": " Williams, J", "word_idx": 63264, "sentence_idx": 1208, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Winn, and A", "word_idx": 63276, "sentence_idx": 1209, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zisserman, \u201cThe\nPASCAL Visual Object Classes Challenge 2007 (VOC2007)\nResults,\u201d 2007", "word_idx": 63288, "sentence_idx": 1210, "label": "unlabeled"}, {"type": "text", "expr": " Lin, M", "word_idx": 63373, "sentence_idx": 1211, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Maire, S", "word_idx": 63380, "sentence_idx": 1212, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Belongie, J", "word_idx": 63389, "sentence_idx": 1213, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hays, P", "word_idx": 63401, "sentence_idx": 1214, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Perona, D", "word_idx": 63409, "sentence_idx": 1215, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ramanan,\nP", "word_idx": 63419, "sentence_idx": 1216, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Doll\u00e1r, and C", "word_idx": 63430, "sentence_idx": 1217, "label": "unlabeled"}, {"type": "text", "expr": " Zitnick, \u201cMicrosoft COCO: Common Objects in\nContext,\u201d in  European Conference on Computer Vision (ECCV) , 2014", "word_idx": 63444, "sentence_idx": 1218, "label": "unlabeled"}, {"type": "text", "expr": "European Conference on Computer Vision (ECCV)", "word_idx": 63555, "sentence_idx": 1219, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Song and J", "word_idx": 63600, "sentence_idx": 1220, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xiao, \u201cDeep sliding shapes for amodal 3d object detection in\nrgb-d images,\u201d  arXiv:1511", "word_idx": 63611, "sentence_idx": 1221, "label": "unlabeled"}, {"type": "text", "expr": "02300 , 2015", "word_idx": 63699, "sentence_idx": 1222, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1511", "word_idx": 63711, "sentence_idx": 1223, "label": "unlabeled"}, {"type": "text", "expr": "02300", "word_idx": 63721, "sentence_idx": 1224, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhu, X", "word_idx": 63726, "sentence_idx": 1225, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chen, and A", "word_idx": 63733, "sentence_idx": 1226, "label": "unlabeled"}, {"type": "text", "expr": " Yuille, \u201cDeePM: A deep part-based model for\nobject detection and semantic part localization,\u201d  arXiv:1511", "word_idx": 63745, "sentence_idx": 1227, "label": "unlabeled"}, {"type": "text", "expr": "07131 ,\n2015", "word_idx": 63851, "sentence_idx": 1228, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1511", "word_idx": 63863, "sentence_idx": 1229, "label": "unlabeled"}, {"type": "text", "expr": "07131", "word_idx": 63873, "sentence_idx": 1230, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dai, K", "word_idx": 63878, "sentence_idx": 1231, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, and J", "word_idx": 63885, "sentence_idx": 1232, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sun, \u201cInstance-aware semantic segmentation via\nmulti-task network cascades,\u201d  arXiv:1512", "word_idx": 63895, "sentence_idx": 1233, "label": "unlabeled"}, {"type": "text", "expr": "04412 , 2015", "word_idx": 63984, "sentence_idx": 1234, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1512", "word_idx": 63996, "sentence_idx": 1235, "label": "unlabeled"}, {"type": "text", "expr": "04412", "word_idx": 64006, "sentence_idx": 1236, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Johnson, A", "word_idx": 64011, "sentence_idx": 1237, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Karpathy, and L", "word_idx": 64022, "sentence_idx": 1238, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fei-Fei, \u201cDensecap: Fully convolutional\nlocalization networks for dense captioning,\u201d  arXiv:1511", "word_idx": 64038, "sentence_idx": 1239, "label": "unlabeled"}, {"type": "text", "expr": "07571 , 2015", "word_idx": 64135, "sentence_idx": 1240, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1511", "word_idx": 64147, "sentence_idx": 1241, "label": "unlabeled"}, {"type": "text", "expr": "07571", "word_idx": 64157, "sentence_idx": 1242, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kislyuk, Y", "word_idx": 64162, "sentence_idx": 1243, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Liu, D", "word_idx": 64173, "sentence_idx": 1244, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Liu, E", "word_idx": 64180, "sentence_idx": 1245, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Tzeng, and Y", "word_idx": 64187, "sentence_idx": 1246, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Jing, \u201cHuman curation and\nconvnets: Powering item-to-item recommendations on pinterest,\u201d\n arXiv:1511", "word_idx": 64200, "sentence_idx": 1247, "label": "unlabeled"}, {"type": "text", "expr": "04003 , 2015", "word_idx": 64301, "sentence_idx": 1248, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1511", "word_idx": 64313, "sentence_idx": 1249, "label": "unlabeled"}, {"type": "text", "expr": "04003", "word_idx": 64323, "sentence_idx": 1250, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, X", "word_idx": 64328, "sentence_idx": 1251, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang, S", "word_idx": 64334, "sentence_idx": 1252, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ren, and J", "word_idx": 64343, "sentence_idx": 1253, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sun, \u201cDeep residual learning for image\nrecognition,\u201d  arXiv:1512", "word_idx": 64354, "sentence_idx": 1254, "label": "unlabeled"}, {"type": "text", "expr": "03385 , 2015", "word_idx": 64419, "sentence_idx": 1255, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1512", "word_idx": 64431, "sentence_idx": 1256, "label": "unlabeled"}, {"type": "text", "expr": "03385", "word_idx": 64441, "sentence_idx": 1257, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hosang, R", "word_idx": 64446, "sentence_idx": 1258, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Benenson, and B", "word_idx": 64456, "sentence_idx": 1259, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Schiele, \u201cHow good are detection proposals,\nreally?\u201d in  British Machine Vision Conference (BMVC) , 2014", "word_idx": 64472, "sentence_idx": 1260, "label": "unlabeled"}, {"type": "text", "expr": "British Machine Vision Conference (BMVC)", "word_idx": 64577, "sentence_idx": 1261, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hosang, R", "word_idx": 64617, "sentence_idx": 1262, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Benenson, P", "word_idx": 64627, "sentence_idx": 1263, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Doll\u00e1r, and B", "word_idx": 64639, "sentence_idx": 1264, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Schiele, \u201cWhat makes for\neffective detection proposals?\u201d  IEEE Transactions on Pattern Analysis\nand Machine Intelligence (TPAMI) , 2015", "word_idx": 64653, "sentence_idx": 1265, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Transactions on Pattern Analysis\nand Machine Intelligence (TPAMI)", "word_idx": 64789, "sentence_idx": 1266, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chavali, H", "word_idx": 64859, "sentence_idx": 1267, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Agrawal, A", "word_idx": 64870, "sentence_idx": 1268, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mahendru, and D", "word_idx": 64881, "sentence_idx": 1269, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Batra, \u201cObject-Proposal\nEvaluation Protocol is \u2019Gameable\u2019,\u201d  arXiv: 1505", "word_idx": 64897, "sentence_idx": 1270, "label": "unlabeled"}, {"type": "text", "expr": "05836 , 2015", "word_idx": 64970, "sentence_idx": 1271, "label": "unlabeled"}, {"type": "text", "expr": "arXiv: 1505", "word_idx": 64982, "sentence_idx": 1272, "label": "unlabeled"}, {"type": "text", "expr": "05836", "word_idx": 64993, "sentence_idx": 1273, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Carreira and C", "word_idx": 64998, "sentence_idx": 1274, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sminchisescu, \u201cCPMC: Automatic object segmentation using\nconstrained parametric min-cuts,\u201d  IEEE Transactions on Pattern\nAnalysis and Machine Intelligence (TPAMI) , 2012", "word_idx": 65013, "sentence_idx": 1275, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Transactions on Pattern\nAnalysis and Machine Intelligence (TPAMI)", "word_idx": 65183, "sentence_idx": 1276, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Arbel\u00e1ez, J", "word_idx": 65253, "sentence_idx": 1277, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Pont-Tuset, J", "word_idx": 65265, "sentence_idx": 1278, "label": "unlabeled"}, {"type": "text", "expr": " Barron, F", "word_idx": 65279, "sentence_idx": 1279, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Marques, and J", "word_idx": 65289, "sentence_idx": 1280, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Malik,\n\u201cMultiscale combinatorial grouping,\u201d in  IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) , 2014", "word_idx": 65304, "sentence_idx": 1281, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Conference on Computer\nVision and Pattern Recognition (CVPR)", "word_idx": 65425, "sentence_idx": 1282, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Alexe, T", "word_idx": 65490, "sentence_idx": 1283, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Deselaers, and V", "word_idx": 65499, "sentence_idx": 1284, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ferrari, \u201cMeasuring the objectness of image\nwindows,\u201d  IEEE Transactions on Pattern Analysis and Machine\nIntelligence (TPAMI) , 2012", "word_idx": 65516, "sentence_idx": 1285, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Transactions on Pattern Analysis and Machine\nIntelligence (TPAMI)", "word_idx": 65649, "sentence_idx": 1286, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Szegedy, A", "word_idx": 65719, "sentence_idx": 1287, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Toshev, and D", "word_idx": 65730, "sentence_idx": 1288, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Erhan, \u201cDeep neural networks for object\ndetection,\u201d in  Neural Information Processing Systems (NIPS) , 2013", "word_idx": 65744, "sentence_idx": 1289, "label": "unlabeled"}, {"type": "text", "expr": "Neural Information Processing Systems (NIPS)", "word_idx": 65852, "sentence_idx": 1290, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Erhan, C", "word_idx": 65896, "sentence_idx": 1291, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Szegedy, A", "word_idx": 65905, "sentence_idx": 1292, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Toshev, and D", "word_idx": 65916, "sentence_idx": 1293, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Anguelov, \u201cScalable object detection\nusing deep neural networks,\u201d in  IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) , 2014", "word_idx": 65930, "sentence_idx": 1294, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Conference on Computer Vision and\nPattern Recognition (CVPR)", "word_idx": 66073, "sentence_idx": 1295, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Szegedy, S", "word_idx": 66138, "sentence_idx": 1296, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Reed, D", "word_idx": 66149, "sentence_idx": 1297, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Erhan, and D", "word_idx": 66157, "sentence_idx": 1298, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Anguelov, \u201cScalable, high-quality object\ndetection,\u201d  arXiv:1412", "word_idx": 66170, "sentence_idx": 1299, "label": "unlabeled"}, {"type": "text", "expr": "1441 (v1) , 2015", "word_idx": 66235, "sentence_idx": 1300, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1412", "word_idx": 66251, "sentence_idx": 1301, "label": "unlabeled"}, {"type": "text", "expr": "1441 (v1)", "word_idx": 66261, "sentence_idx": 1302, "label": "unlabeled"}, {"type": "text", "expr": " Pinheiro, R", "word_idx": 66270, "sentence_idx": 1303, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Collobert, and P", "word_idx": 66282, "sentence_idx": 1304, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dollar, \u201cLearning to segment object\ncandidates,\u201d in  Neural Information Processing Systems (NIPS) , 2015", "word_idx": 66299, "sentence_idx": 1305, "label": "unlabeled"}, {"type": "text", "expr": "Neural Information Processing Systems (NIPS)", "word_idx": 66404, "sentence_idx": 1306, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dai, K", "word_idx": 66448, "sentence_idx": 1307, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, and J", "word_idx": 66455, "sentence_idx": 1308, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sun, \u201cConvolutional feature masking for joint object and\nstuff segmentation,\u201d in  IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , 2015", "word_idx": 66465, "sentence_idx": 1309, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR)", "word_idx": 66620, "sentence_idx": 1310, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ren, K", "word_idx": 66685, "sentence_idx": 1311, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, R", "word_idx": 66692, "sentence_idx": 1312, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Girshick, X", "word_idx": 66698, "sentence_idx": 1313, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang, and J", "word_idx": 66710, "sentence_idx": 1314, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sun, \u201cObject detection networks\non convolutional feature maps,\u201d  arXiv:1504", "word_idx": 66723, "sentence_idx": 1315, "label": "unlabeled"}, {"type": "text", "expr": "06066 , 2015", "word_idx": 66799, "sentence_idx": 1316, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1504", "word_idx": 66811, "sentence_idx": 1317, "label": "unlabeled"}, {"type": "text", "expr": "06066", "word_idx": 66821, "sentence_idx": 1318, "label": "unlabeled"}, {"type": "text", "expr": " Chorowski, D", "word_idx": 66826, "sentence_idx": 1319, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bahdanau, D", "word_idx": 66839, "sentence_idx": 1320, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Serdyuk, K", "word_idx": 66851, "sentence_idx": 1321, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Cho, and Y", "word_idx": 66862, "sentence_idx": 1322, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bengio,\n\u201cAttention-based models for speech recognition,\u201d in  Neural\nInformation Processing Systems (NIPS) , 2015", "word_idx": 66873, "sentence_idx": 1323, "label": "unlabeled"}, {"type": "text", "expr": "Neural\nInformation Processing Systems (NIPS)", "word_idx": 66986, "sentence_idx": 1324, "label": "unlabeled"}, {"type": "text", "expr": " Zeiler and R", "word_idx": 67030, "sentence_idx": 1325, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fergus, \u201cVisualizing and understanding convolutional\nneural networks,\u201d in  European Conference on Computer Vision (ECCV) ,\n2014", "word_idx": 67043, "sentence_idx": 1326, "label": "unlabeled"}, {"type": "text", "expr": "European Conference on Computer Vision (ECCV)", "word_idx": 67171, "sentence_idx": 1327, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Nair and G", "word_idx": 67216, "sentence_idx": 1328, "label": "unlabeled"}, {"type": "text", "expr": " Hinton, \u201cRectified linear units improve restricted boltzmann\nmachines,\u201d in  International Conference on Machine Learning (ICML) ,\n2010", "word_idx": 67227, "sentence_idx": 1329, "label": "unlabeled"}, {"type": "text", "expr": "International Conference on Machine Learning (ICML)", "word_idx": 67362, "sentence_idx": 1330, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Szegedy, W", "word_idx": 67413, "sentence_idx": 1331, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Liu, Y", "word_idx": 67424, "sentence_idx": 1332, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Jia, P", "word_idx": 67431, "sentence_idx": 1333, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sermanet, S", "word_idx": 67438, "sentence_idx": 1334, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Reed, D", "word_idx": 67450, "sentence_idx": 1335, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Anguelov, D", "word_idx": 67458, "sentence_idx": 1336, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Erhan, and\nA", "word_idx": 67470, "sentence_idx": 1337, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rabinovich, \u201cGoing deeper with convolutions,\u201d in  IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) , 2015", "word_idx": 67483, "sentence_idx": 1338, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Conference\non Computer Vision and Pattern Recognition (CVPR)", "word_idx": 67606, "sentence_idx": 1339, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0LeCun, B", "word_idx": 67671, "sentence_idx": 1340, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Boser, J", "word_idx": 67680, "sentence_idx": 1341, "label": "unlabeled"}, {"type": "text", "expr": " Denker, D", "word_idx": 67689, "sentence_idx": 1342, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Henderson, R", "word_idx": 67699, "sentence_idx": 1343, "label": "unlabeled"}, {"type": "text", "expr": " Howard, W", "word_idx": 67712, "sentence_idx": 1344, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hubbard, and\nL", "word_idx": 67722, "sentence_idx": 1345, "label": "unlabeled"}, {"type": "text", "expr": " Jackel, \u201cBackpropagation applied to handwritten zip code\nrecognition,\u201d  Neural computation , 1989", "word_idx": 67737, "sentence_idx": 1346, "label": "unlabeled"}, {"type": "text", "expr": "Neural computation", "word_idx": 67835, "sentence_idx": 1347, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Russakovsky, J", "word_idx": 67853, "sentence_idx": 1348, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Deng, H", "word_idx": 67868, "sentence_idx": 1349, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Su, J", "word_idx": 67876, "sentence_idx": 1350, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Krause, S", "word_idx": 67882, "sentence_idx": 1351, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Satheesh, S", "word_idx": 67892, "sentence_idx": 1352, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ma, Z", "word_idx": 67904, "sentence_idx": 1353, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Huang,\nA", "word_idx": 67910, "sentence_idx": 1354, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Karpathy, A", "word_idx": 67919, "sentence_idx": 1355, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Khosla, M", "word_idx": 67931, "sentence_idx": 1356, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bernstein, A", "word_idx": 67941, "sentence_idx": 1357, "label": "unlabeled"}, {"type": "text", "expr": " Berg, and L", "word_idx": 67954, "sentence_idx": 1358, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fei-Fei, \u201cImageNet\nLarge Scale Visual Recognition Challenge,\u201d in  International Journal\nof Computer Vision (IJCV) , 2015", "word_idx": 67966, "sentence_idx": 1359, "label": "unlabeled"}, {"type": "text", "expr": "International Journal\nof Computer Vision (IJCV)", "word_idx": 68087, "sentence_idx": 1360, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Krizhevsky, I", "word_idx": 68134, "sentence_idx": 1361, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sutskever, and G", "word_idx": 68148, "sentence_idx": 1362, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hinton, \u201cImagenet classification with deep\nconvolutional neural networks,\u201d in  Neural Information Processing\nSystems (NIPS) , 2012", "word_idx": 68165, "sentence_idx": 1363, "label": "unlabeled"}, {"type": "text", "expr": "Neural Information Processing\nSystems (NIPS)", "word_idx": 68296, "sentence_idx": 1364, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Jia, E", "word_idx": 68340, "sentence_idx": 1365, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Shelhamer, J", "word_idx": 68347, "sentence_idx": 1366, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Donahue, S", "word_idx": 68360, "sentence_idx": 1367, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Karayev, J", "word_idx": 68371, "sentence_idx": 1368, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Long, R", "word_idx": 68382, "sentence_idx": 1369, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Girshick,\nS", "word_idx": 68390, "sentence_idx": 1370, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Guadarrama, and T", "word_idx": 68402, "sentence_idx": 1371, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Darrell, \u201cCaffe: Convolutional architecture for fast\nfeature embedding,\u201d  arXiv:1408", "word_idx": 68420, "sentence_idx": 1372, "label": "unlabeled"}, {"type": "text", "expr": "5093 , 2014", "word_idx": 68505, "sentence_idx": 1373, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1408", "word_idx": 68516, "sentence_idx": 1374, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lenc and A", "word_idx": 68526, "sentence_idx": 1375, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Vedaldi, \u201cR-CNN minus R,\u201d in  British Machine Vision\nConference (BMVC) , 2015", "word_idx": 68537, "sentence_idx": 1376, "label": "unlabeled"}, {"type": "text", "expr": "British Machine Vision\nConference (BMVC)", "word_idx": 68615, "sentence_idx": 1377, "label": "unlabeled"}, {"type": "text", "expr": "Generated  on Wed Mar  7 13:38:41 2018 by", "word_idx": 68655, "sentence_idx": 1378, "label": "unlabeled"}, {"type": "text", "expr": "LaTeXML", "word_idx": 68696, "sentence_idx": 1379, "label": "unlabeled"}], "face_detection": [{"type": "text", "expr": "Beyond Context: Exploring Semantic Similarity for Tiny Face Detection", "word_idx": 0, "sentence_idx": 0, "label": "unlabeled"}, {"type": "text", "expr": "Beyond Context: Exploring Semantic Similarity for Tiny Face Detection", "word_idx": 69, "sentence_idx": 1, "label": "unlabeled"}, {"type": "text", "expr": "Abstract Tiny face detection aims to find faces with high degrees of variability in scale, resolution and occlusion in cluttered scenes", "word_idx": 138, "sentence_idx": 2, "label": "unlabeled"}, {"type": "text", "expr": " Due to the very little information available on tiny faces, it is not sufficient to detect them merely based on the information presented inside the tiny bounding boxes or their context", "word_idx": 273, "sentence_idx": 3, "label": "unlabeled"}, {"type": "text", "expr": " In this paper, we propose to exploit the semantic similarity among all predicted targets in each image to boost current face detectors", "word_idx": 459, "sentence_idx": 4, "label": "unlabeled"}, {"type": "text", "expr": " To this end, we present a novel framework to model semantic similarity as pairwise constraints within the metric learning scheme, and then refine our predictions with the semantic similarity by utilizing the graph cut techniques", "word_idx": 594, "sentence_idx": 5, "label": "unlabeled"}, {"type": "text", "expr": " Experiments conducted on three widely-used benchmark datasets have demonstrated the improvement over the-state-of-the-arts gained by applying this idea", "word_idx": 823, "sentence_idx": 6, "label": "unlabeled"}, {"type": "text", "expr": "Abstract", "word_idx": 975, "sentence_idx": 7, "label": "unlabeled"}, {"type": "text", "expr": "Tiny face detection aims to find faces with high degrees of variability in scale, resolution and occlusion in cluttered scenes", "word_idx": 983, "sentence_idx": 8, "label": "unlabeled"}, {"type": "text", "expr": " Due to the very little information available on tiny faces, it is not sufficient to detect them merely based on the information presented inside the tiny bounding boxes or their context", "word_idx": 1109, "sentence_idx": 9, "label": "unlabeled"}, {"type": "text", "expr": " In this paper, we propose to exploit the semantic similarity among all predicted targets in each image to boost current face detectors", "word_idx": 1295, "sentence_idx": 10, "label": "unlabeled"}, {"type": "text", "expr": " To this end, we present a novel framework to model semantic similarity as pairwise constraints within the metric learning scheme, and then refine our predictions with the semantic similarity by utilizing the graph cut techniques", "word_idx": 1430, "sentence_idx": 11, "label": "unlabeled"}, {"type": "text", "expr": " Experiments conducted on three widely-used benchmark datasets have demonstrated the improvement over the-state-of-the-arts gained by applying this idea", "word_idx": 1659, "sentence_idx": 12, "label": "unlabeled"}, {"type": "text", "expr": "\\name", "word_idx": 1811, "sentence_idx": 13, "label": "unlabeled"}, {"type": "text", "expr": "YueXi ${}^{1,2}$ ,JiangbinZheng ${}^{1,}$ *,XiangjiaHe ${}^{2,}$ *,WenjingJia ${}^{2}$ ,HanhuiLi ${}^{2,3}$ \\address ${}^{1}$ SchoolofComputerScience,NorthwesternPolytechnicalUniversity,P", "word_idx": 1816, "sentence_idx": 14, "label": "unlabeled"}, {"type": "text", "expr": "China\n ${}^{2}$ GlobalBigDataTechnologiesCentre(GBDTC),UniversityofTechnologySydney,Australia\n ${}^{3}$ SchoolofDataandComputerScience,SunYat-senUniversity,P", "word_idx": 2003, "sentence_idx": 15, "label": "unlabeled"}, {"type": "text", "expr": "China", "word_idx": 2160, "sentence_idx": 16, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{1,2}$$", "word_idx": 2165, "sentence_idx": 17, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{1,}$$", "word_idx": 2173, "sentence_idx": 18, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{2,}$$", "word_idx": 2180, "sentence_idx": 19, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{2}$$", "word_idx": 2187, "sentence_idx": 20, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{2,3}$$", "word_idx": 2193, "sentence_idx": 21, "label": "unlabeled"}, {"type": "text", "expr": "\u2020 thanks:  *XiangjianHeandJiangbinZhengaretheco-correspondingauthorsforthispaper", "word_idx": 2201, "sentence_idx": 22, "label": "unlabeled"}, {"type": "text", "expr": "thanks:", "word_idx": 2281, "sentence_idx": 23, "label": "unlabeled"}, {"type": "text", "expr": "\\address", "word_idx": 2288, "sentence_idx": 24, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{1}$$", "word_idx": 2296, "sentence_idx": 25, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{2}$$", "word_idx": 2302, "sentence_idx": 26, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{3}$$", "word_idx": 2308, "sentence_idx": 27, "label": "unlabeled"}, {"type": "text", "expr": "{keywords}", "word_idx": 2314, "sentence_idx": 28, "label": "unlabeled"}, {"type": "text", "expr": "Tiny face detection, semantic information, metric learning, graph-cut", "word_idx": 2324, "sentence_idx": 29, "label": "unlabeled"}, {"type": "text", "expr": "Tiny face detection, semantic information, metric learning, graph-cut", "word_idx": 2393, "sentence_idx": 30, "label": "unlabeled"}, {"type": "text", "expr": "1  Introduction", "word_idx": 2462, "sentence_idx": 31, "label": "unlabeled"}, {"type": "text", "expr": "Robust face detection is one of the ultimate components to support various facial\nrelated problems, such as face alignment  , face recognition  , face verification  , and face tracking  , etc", "word_idx": 2477, "sentence_idx": 32, "label": "unlabeled"}, {"type": "text", "expr": " From the cornerstone by Viola-Jones   to the recent work by Hu et al", "word_idx": 2668, "sentence_idx": 33, "label": "unlabeled"}, {"type": "text", "expr": "  , the performance of face detection has been\nimproved dramatically", "word_idx": 2737, "sentence_idx": 34, "label": "unlabeled"}, {"type": "text", "expr": "\nThe recent introduction of the WIDER\nface dataset  , which contains a large number of small faces, exposes the performance gap between humans and\nthe current face detection techniques due to a number of challenges in practice", "word_idx": 2805, "sentence_idx": 35, "label": "unlabeled"}, {"type": "text", "expr": "\nDifferent from the classical face detection, tiny face detection mainly focuses on low-resolution, large scale variation and serious occlusion, as shown in Fig", "word_idx": 3031, "sentence_idx": 36, "label": "unlabeled"}, {"type": "text", "expr": "\nAll of these challenges suggest the information on small objects is far too limited", "word_idx": 3191, "sentence_idx": 37, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:  Tiny faces detected with our proposed approach (shown as yellow and green boxes) and the HR approach\u00a0  (shown as green boxes)", "word_idx": 3275, "sentence_idx": 38, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:", "word_idx": 3411, "sentence_idx": 39, "label": "unlabeled"}, {"type": "text", "expr": "The existing methods for finding small objects in imageries can be grouped into three categories", "word_idx": 3420, "sentence_idx": 40, "label": "unlabeled"}, {"type": "text", "expr": " The first group (e", "word_idx": 3516, "sentence_idx": 41, "label": "unlabeled"}, {"type": "text", "expr": ",  ) aims to extract scale-invariant features using pre-trained deep networks", "word_idx": 3535, "sentence_idx": 42, "label": "unlabeled"}, {"type": "text", "expr": " However, their performance drops dramatically as the target faces become too small", "word_idx": 3612, "sentence_idx": 43, "label": "unlabeled"}, {"type": "text", "expr": " Another group tries to generate additional information inside the objects by interpolation", "word_idx": 3695, "sentence_idx": 44, "label": "unlabeled"}, {"type": "text", "expr": " For example, the work in\u00a0  demonstrated that interpolating the lowest layer of image pyramid was significantly beneficial for capturing small objects", "word_idx": 3786, "sentence_idx": 45, "label": "unlabeled"}, {"type": "text", "expr": "\nThe last group (e", "word_idx": 3936, "sentence_idx": 46, "label": "unlabeled"}, {"type": "text", "expr": ",  ) seeks to incorporate information surrounding the objects (i", "word_idx": 3954, "sentence_idx": 47, "label": "unlabeled"}, {"type": "text", "expr": ", context) in order to improve the performance of tiny face detection", "word_idx": 4018, "sentence_idx": 48, "label": "unlabeled"}, {"type": "text", "expr": " It is clear that computer vision needs additional contextual information to accurately classify small faces", "word_idx": 4087, "sentence_idx": 49, "label": "unlabeled"}, {"type": "text", "expr": " Is there another way to improve the performance of small object detection?", "word_idx": 4195, "sentence_idx": 50, "label": "unlabeled"}, {"type": "text", "expr": "Note that, the existing classification-based tiny face detectors simply apply a\nthreshold on a classification score to determine whether the corresponding candidate is face or non-face, as shown in the first stage of Fig", "word_idx": 4270, "sentence_idx": 51, "label": "unlabeled"}, {"type": "text", "expr": "\nHowever, the optimal threshold is often difficult to obtain", "word_idx": 4490, "sentence_idx": 52, "label": "unlabeled"}, {"type": "text", "expr": "\nIn this paper,\nwe propose a novel idea to exploit the semantic information (consisting of spatial locations, scales and textures) of a candidate\u2019s neighbors to classify a target to face or background", "word_idx": 4550, "sentence_idx": 53, "label": "unlabeled"}, {"type": "text", "expr": "\nSpecifically, based on such semantic information, we try to group all of the faces into one cluster, while backgrounds are kept far away from the cluster", "word_idx": 4750, "sentence_idx": 54, "label": "unlabeled"}, {"type": "text", "expr": "\nFor this purpose, we propose a Metric Learning and Graph-Cut (MLGC) framework, which carries out further classification on the candidates produced by other object detectors", "word_idx": 4904, "sentence_idx": 55, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 2  illustrates the framework of this idea", "word_idx": 5077, "sentence_idx": 56, "label": "unlabeled"}, {"type": "text", "expr": "We first obtain a high-recall classifier which aims to retrieve all of the targets in an image, but may unavoidably introduce lots of false positives", "word_idx": 5120, "sentence_idx": 57, "label": "unlabeled"}, {"type": "text", "expr": "\nOur focus is to retrieve faces with low classification scores but remove these false positives", "word_idx": 5269, "sentence_idx": 58, "label": "unlabeled"}, {"type": "text", "expr": "\nIn order to do this, we design a metric learning method to learn a similarity matrix to evaluate the similarity of each pair of candidates", "word_idx": 5364, "sentence_idx": 59, "label": "unlabeled"}, {"type": "text", "expr": "\nA graph model is built to represent the similarity matrix of these candidates", "word_idx": 5503, "sentence_idx": 60, "label": "unlabeled"}, {"type": "text", "expr": "\nThe graph cut technique is utilized to divide the graph into several groups where candidates in the same group are similar and those in different groups are dissimilar to each other", "word_idx": 5581, "sentence_idx": 61, "label": "unlabeled"}, {"type": "text", "expr": "\nFinally, the candidates in each group are classified into faces or non-faces, correspondingly, by voting", "word_idx": 5763, "sentence_idx": 62, "label": "unlabeled"}, {"type": "text", "expr": "We first obtain a high-recall classifier which aims to retrieve all of the targets in an image, but may unavoidably introduce lots of false positives", "word_idx": 5868, "sentence_idx": 63, "label": "unlabeled"}, {"type": "text", "expr": "\nOur focus is to retrieve faces with low classification scores but remove these false positives", "word_idx": 6017, "sentence_idx": 64, "label": "unlabeled"}, {"type": "text", "expr": "\nIn order to do this, we design a metric learning method to learn a similarity matrix to evaluate the similarity of each pair of candidates", "word_idx": 6112, "sentence_idx": 65, "label": "unlabeled"}, {"type": "text", "expr": "\nA graph model is built to represent the similarity matrix of these candidates", "word_idx": 6251, "sentence_idx": 66, "label": "unlabeled"}, {"type": "text", "expr": "\nThe graph cut technique is utilized to divide the graph into several groups where candidates in the same group are similar and those in different groups are dissimilar to each other", "word_idx": 6329, "sentence_idx": 67, "label": "unlabeled"}, {"type": "text", "expr": "\nFinally, the candidates in each group are classified into faces or non-faces, correspondingly, by voting", "word_idx": 6511, "sentence_idx": 68, "label": "unlabeled"}, {"type": "text", "expr": "The main contributions of this paper can be highlighted as follows", "word_idx": 6616, "sentence_idx": 69, "label": "unlabeled"}, {"type": "text", "expr": "\nFirst, aiming to boost the detection performance, we propose a novel metric learning and graph-cut framework to exploit the semantic information between targeting objects\u2019 neighbors", "word_idx": 6682, "sentence_idx": 70, "label": "unlabeled"}, {"type": "text", "expr": "\nSecondly, to depict local neighborhood relationships, we introduce a pairwise constraint into the tiny face detector to improve the detection accuracy", "word_idx": 6864, "sentence_idx": 71, "label": "unlabeled"}, {"type": "text", "expr": "\nThirdly, to realize such a pairwise constraint, we convert the problem of regression that estimates the similarity between different candidates into a classification problem that produces the scores of classification for each pair of candidates", "word_idx": 7015, "sentence_idx": 72, "label": "unlabeled"}, {"type": "text", "expr": "The main contributions of this paper can be highlighted as follows", "word_idx": 7260, "sentence_idx": 73, "label": "unlabeled"}, {"type": "text", "expr": "\nFirst, aiming to boost the detection performance, we propose a novel metric learning and graph-cut framework to exploit the semantic information between targeting objects\u2019 neighbors", "word_idx": 7326, "sentence_idx": 74, "label": "unlabeled"}, {"type": "text", "expr": "\nSecondly, to depict local neighborhood relationships, we introduce a pairwise constraint into the tiny face detector to improve the detection accuracy", "word_idx": 7508, "sentence_idx": 75, "label": "unlabeled"}, {"type": "text", "expr": "\nThirdly, to realize such a pairwise constraint, we convert the problem of regression that estimates the similarity between different candidates into a classification problem that produces the scores of classification for each pair of candidates", "word_idx": 7659, "sentence_idx": 76, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:  The framework of our proposed MLGC for high-density tiny face detection", "word_idx": 7904, "sentence_idx": 77, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:", "word_idx": 7986, "sentence_idx": 78, "label": "unlabeled"}, {"type": "text", "expr": "2  Related Work", "word_idx": 7995, "sentence_idx": 79, "label": "unlabeled"}, {"type": "text", "expr": "Face detection is a classic topic in computer vision", "word_idx": 8010, "sentence_idx": 80, "label": "unlabeled"}, {"type": "text", "expr": "\nThe pioneer work on the topic was published by Viola and\nJones   who designed a cascade of weak classifiers using Haar features and AdaBoost for fast and robust face detection", "word_idx": 8062, "sentence_idx": 81, "label": "unlabeled"}, {"type": "text", "expr": " Similar in spirit, numerous approaches have been developed to\nimprove the performance with more sophisticated hand-crafted features   and more powerful classifiers  ", "word_idx": 8238, "sentence_idx": 82, "label": "unlabeled"}, {"type": "text", "expr": "\nHowever, these methods using non-robust hand-crafted features\nand optimized each components independently, and hence led to sub-optimal face detection results", "word_idx": 8404, "sentence_idx": 83, "label": "unlabeled"}, {"type": "text", "expr": "\nRecently, face detectors based on  CNNs    have greatly bridged the gap between human vision and artificial detectors", "word_idx": 8563, "sentence_idx": 84, "label": "unlabeled"}, {"type": "text", "expr": "Tiny face detection aims to detect a large number of small faces in crowded and cluttered scenes", "word_idx": 8681, "sentence_idx": 85, "label": "unlabeled"}, {"type": "text", "expr": "\nIt is totally different from detecting normal faces, because the cues for detecting a 3-pixel tall face are fundamentally different from those for detecting a 300-pixel tall face  ", "word_idx": 8777, "sentence_idx": 86, "label": "unlabeled"}, {"type": "text", "expr": " Bell\u00a0  presented the Inside-Outside Net (ION) to model the context outside a region of interest and showed improvements on small object detection", "word_idx": 8958, "sentence_idx": 87, "label": "unlabeled"}, {"type": "text", "expr": "\nVery recently, Hu and Ramanan   designed a foveal descriptor that captured both coarse context and high-resolution image features in order to effectively encode context information, which has achieved state-of-the-art performance on the WIDER FACE dataset", "word_idx": 9104, "sentence_idx": 88, "label": "unlabeled"}, {"type": "text", "expr": "\nAs we all know, it is not sufficient to detect small objects merely by extracting deep learning features from the texture inside an object region", "word_idx": 9360, "sentence_idx": 89, "label": "unlabeled"}, {"type": "text", "expr": " One main drawback is that, these approaches have neglected local semantic information", "word_idx": 9506, "sentence_idx": 90, "label": "unlabeled"}, {"type": "text", "expr": " We have observed that there exists local coherent relationships in terms of spatial location, scale, and texture in high-density tiny face detection, ignoring the influence of various viewpoints", "word_idx": 9592, "sentence_idx": 91, "label": "unlabeled"}, {"type": "text", "expr": " For example, as shown in Fig", "word_idx": 9787, "sentence_idx": 92, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 1 , face bounding boxes close to each other are similar in their scales and textures", "word_idx": 9816, "sentence_idx": 93, "label": "unlabeled"}, {"type": "text", "expr": "\nLocal semantic information helps tiny face detectors better eliminate false alarms", "word_idx": 9902, "sentence_idx": 94, "label": "unlabeled"}, {"type": "text", "expr": "\nTo introduce local coherent relationships, we learn a metric to represent this coherence and use the graph-cut algorithm to divide candidates into several groups, where candidates in the same group are similar, and dissimilar when they are in different groups", "word_idx": 9985, "sentence_idx": 95, "label": "unlabeled"}, {"type": "text", "expr": "3  The Proposed Method", "word_idx": 10245, "sentence_idx": 96, "label": "unlabeled"}, {"type": "text", "expr": "Our goal is to integrate local coherent relationships into tiny face detection", "word_idx": 10267, "sentence_idx": 97, "label": "unlabeled"}, {"type": "text", "expr": "\nIn order to represent local coherent relationships, we define pairwise constraints, which are an equivalence\nconstraint for pairs of data points belonging to same classes, and an inequivalence constraint for pairs of data points belonging to different classes", "word_idx": 10345, "sentence_idx": 98, "label": "unlabeled"}, {"type": "text", "expr": "Our goal is to integrate local coherent relationships into tiny face detection", "word_idx": 10605, "sentence_idx": 99, "label": "unlabeled"}, {"type": "text", "expr": "\nIn order to represent local coherent relationships, we define pairwise constraints, which are an equivalence\nconstraint for pairs of data points belonging to same classes, and an inequivalence constraint for pairs of data points belonging to different classes", "word_idx": 10683, "sentence_idx": 100, "label": "unlabeled"}, {"type": "text", "expr": "As shown in Fig", "word_idx": 10943, "sentence_idx": 101, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 2 , we present a metric learning and graph-cut (MLGC) approach for high-density tiny face detection", "word_idx": 10958, "sentence_idx": 102, "label": "unlabeled"}, {"type": "text", "expr": "\nWe first use a linear-SVM to estimate the similarity matrix among all candidates (Sect", "word_idx": 11059, "sentence_idx": 103, "label": "unlabeled"}, {"type": "text", "expr": "1 ) and then we construct a graph model and use the graph-cut algorithm to divide candidates into several groups (Sect", "word_idx": 11146, "sentence_idx": 104, "label": "unlabeled"}, {"type": "text", "expr": "\nFinally, we design a voting method to classify groups (Sect", "word_idx": 11264, "sentence_idx": 105, "label": "unlabeled"}, {"type": "text", "expr": "1  Metric learning based on linear-SVM", "word_idx": 11324, "sentence_idx": 106, "label": "unlabeled"}, {"type": "text", "expr": "Let  $X=\\{x_{1},x_{2},,x_{N}\\}$  denote the set of  $N$  candidates (i", "word_idx": 11362, "sentence_idx": 107, "label": "unlabeled"}, {"type": "text", "expr": ", face or non-face bounding boxes)", "word_idx": 11432, "sentence_idx": 108, "label": "unlabeled"}, {"type": "text", "expr": "\nTo introduce the pairwise constraint,\nwe first build a similarity matrix  $S=s(x_{i},x_{j}),x_{i},x_{j}\\in X,i,j=1,2,N$ , where  $s(x_{i},x_{j})$  represents the similarity between  $x_{i}$  and  $x_{j}$ ", "word_idx": 11466, "sentence_idx": 109, "label": "usecase"}, {"type": "text", "expr": "\n $s(x_{i},x_{j})=1$  means that  $x_{i}$  has a strong resemblance of  $x_{j}$ , and  $s(x_{i},x_{j})=0$  means that  $x_{i}$  is completely different from  $x_{j}$ ", "word_idx": 11671, "sentence_idx": 110, "label": "usecase"}, {"type": "math", "expr": "$$X=\\{x_{1},x_{2},...,x_{N}\\}$$", "word_idx": 11837, "sentence_idx": 111, "label": "unlabeled"}, {"type": "math", "expr": "$$S=s(x_{i},x_{j}),x_{i},x_{j}\\in X,i,j=1,2,...N$$", "word_idx": 11864, "sentence_idx": 112, "label": "usecase"}, {"type": "math", "expr": "$$s(x_{i},x_{j})$$", "word_idx": 11910, "sentence_idx": 113, "label": "none"}, {"type": "math", "expr": "$$x_{i}$$", "word_idx": 11924, "sentence_idx": 114, "label": "none"}, {"type": "math", "expr": "$$x_{j}$$", "word_idx": 11929, "sentence_idx": 115, "label": "unlabeled"}, {"type": "math", "expr": "$$s(x_{i},x_{j})=1$$", "word_idx": 11934, "sentence_idx": 116, "label": "none"}, {"type": "math", "expr": "$$x_{i}$$", "word_idx": 11950, "sentence_idx": 117, "label": "none"}, {"type": "math", "expr": "$$x_{j}$$", "word_idx": 11955, "sentence_idx": 118, "label": "unlabeled"}, {"type": "math", "expr": "$$s(x_{i},x_{j})=0$$", "word_idx": 11960, "sentence_idx": 119, "label": "none"}, {"type": "math", "expr": "$$x_{i}$$", "word_idx": 11976, "sentence_idx": 120, "label": "none"}, {"type": "math", "expr": "$$x_{j}$$", "word_idx": 11981, "sentence_idx": 121, "label": "unlabeled"}, {"type": "text", "expr": "In order to obtain the similarity score between two candidates  $x_{i}$  and  $x_{j}$ , we treat it as a classification problem and propose an unsupervised way to obtain the similarity score between two candidates", "word_idx": 11986, "sentence_idx": 122, "label": "usecase"}, {"type": "text", "expr": "\nWe use SVM to compute the similarity score between two candidates  $x_{i}$  and  $x_{j}$  based on multiple cues, i", "word_idx": 12199, "sentence_idx": 123, "label": "definition"}, {"type": "text", "expr": ", the position, scale, classification score and deep features of the candidates, which are concatenated together into a feature vector  $\\phi(x_{i})$ ", "word_idx": 12315, "sentence_idx": 124, "label": "none"}, {"type": "text", "expr": " Note that, classification scores and deep features of a candidate  $x_{i}$  are obtained from the tiny face detector  ", "word_idx": 12465, "sentence_idx": 125, "label": "none"}, {"type": "text", "expr": " During the training stage, we sort  $X$  by their scores in descending order", "word_idx": 12584, "sentence_idx": 126, "label": "unlabeled"}, {"type": "text", "expr": " We suppose that  $X_{Top}$  denotes the top  $10\\%$  of  $X$  which are face patches, while  $X_{Bottom}$  denotes the bottom  $10\\%$ \nof the non-face patches in  $X$ ", "word_idx": 12661, "sentence_idx": 127, "label": "unlabeled"}, {"type": "text", "expr": " As shown in Fig", "word_idx": 12829, "sentence_idx": 128, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 2 , in Stage 2 of our MLGC, we build a training set  $\\{(x_{11}^{{}^{\\prime}},y_{11}^{{}^{\\prime}}),(x_{12}^{{}^{\\prime}},y_{12}^{{}%\n^{\\prime}}),(x_{nn}^{{}^{\\prime}},y_{nn}^{{}^{\\prime}})\\}$ ,  $x_{ij}^{{}^{\\prime}}=\\phi(x_{i})-\\phi(x_{j})$ ,  $y_{ij}^{{}^{\\prime}}=\\{0,1\\}$ ", "word_idx": 12845, "sentence_idx": 129, "label": "none"}, {"type": "text", "expr": " If  $x_{i},x_{j}\\in X_{Top},y_{ij}^{{}^{\\prime}}=1$ ", "word_idx": 13124, "sentence_idx": 130, "label": "none"}, {"type": "text", "expr": " If  $x_{i}\\in X_{Top},x_{j}\\in X_{Bottom},y_{ij}^{{}^{\\prime}}=0$ ", "word_idx": 13177, "sentence_idx": 131, "label": "none"}, {"type": "text", "expr": "\nDuring the testing stage, we feed  $x_{ij}^{{}^{\\prime}}=\\phi(x_{i})-\\phi(x_{j})$  to the SVM classifier, and then use the output score as the similarity score  $s(x_{i},x_{j})$  between  $x_{i}$  and  $x_{j}$ ", "word_idx": 13244, "sentence_idx": 132, "label": "none"}, {"type": "text", "expr": " Thus, we build the similarity matrix  $S$ ", "word_idx": 13455, "sentence_idx": 133, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{i}$$", "word_idx": 13498, "sentence_idx": 134, "label": "none"}, {"type": "math", "expr": "$$x_{j}$$", "word_idx": 13503, "sentence_idx": 135, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{i}$$", "word_idx": 13508, "sentence_idx": 136, "label": "none"}, {"type": "math", "expr": "$$x_{j}$$", "word_idx": 13513, "sentence_idx": 137, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi(x_{i})$$", "word_idx": 13518, "sentence_idx": 138, "label": "none"}, {"type": "math", "expr": "$$x_{i}$$", "word_idx": 13529, "sentence_idx": 139, "label": "none"}, {"type": "math", "expr": "$$X_{Top}$$", "word_idx": 13534, "sentence_idx": 140, "label": "unlabeled"}, {"type": "math", "expr": "$$10\\%$$", "word_idx": 13541, "sentence_idx": 141, "label": "unlabeled"}, {"type": "math", "expr": "$$X_{Bottom}$$", "word_idx": 13545, "sentence_idx": 142, "label": "unlabeled"}, {"type": "math", "expr": "$$10\\%$$", "word_idx": 13555, "sentence_idx": 143, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{(x_{11}^{{}^{\\prime}},y_{11}^{{}^{\\prime}}),(x_{12}^{{}^{\\prime}},y_{12}^{{}%\n^{\\prime}}),...(x_{nn}^{{}^{\\prime}},y_{nn}^{{}^{\\prime}})\\}$$", "word_idx": 13559, "sentence_idx": 144, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{ij}^{{}^{\\prime}}=\\phi(x_{i})-\\phi(x_{j})$$", "word_idx": 13699, "sentence_idx": 145, "label": "none"}, {"type": "math", "expr": "$$y_{ij}^{{}^{\\prime}}=\\{0,1\\}$$", "word_idx": 13743, "sentence_idx": 146, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{i},x_{j}\\in X_{Top},y_{ij}^{{}^{\\prime}}=1$$", "word_idx": 13771, "sentence_idx": 147, "label": "none"}, {"type": "math", "expr": "$$x_{i}\\in X_{Top},x_{j}\\in X_{Bottom},y_{ij}^{{}^{\\prime}}=0$$", "word_idx": 13816, "sentence_idx": 148, "label": "none"}, {"type": "math", "expr": "$$x_{ij}^{{}^{\\prime}}=\\phi(x_{i})-\\phi(x_{j})$$", "word_idx": 13875, "sentence_idx": 149, "label": "none"}, {"type": "math", "expr": "$$s(x_{i},x_{j})$$", "word_idx": 13919, "sentence_idx": 150, "label": "none"}, {"type": "math", "expr": "$$x_{i}$$", "word_idx": 13933, "sentence_idx": 151, "label": "none"}, {"type": "math", "expr": "$$x_{j}$$", "word_idx": 13938, "sentence_idx": 152, "label": "unlabeled"}, {"type": "text", "expr": "2  Graph-cut based on spectral clustering", "word_idx": 13943, "sentence_idx": 153, "label": "unlabeled"}, {"type": "text", "expr": "Given a set of candidates  $X=\\{x_{1},x_{2},,x_{N}\\}$  and a similarity matrix  $S$ , our goal is to cluster  $X$  into different groups", "word_idx": 13984, "sentence_idx": 154, "label": "unlabeled"}, {"type": "text", "expr": " Candidates are similar when they are in the same group, and are dissimilar when they are in different groups", "word_idx": 14120, "sentence_idx": 155, "label": "unlabeled"}, {"type": "text", "expr": " In this work, we adopt the graph-cut algorithm for this purpose", "word_idx": 14229, "sentence_idx": 156, "label": "unlabeled"}, {"type": "text", "expr": " First, we build a graph model  $G=(V,E)$  to represent  $X$ , where each vertex  $v_{i}\\in V$  represents a candidate  $x_{i}$ , and  $e_{ij}\\in E$  represents the similarity  $s(x_{i},x_{j})$  between the corresponding candidates  $x_{i}$  and  $x_{j}$ ", "word_idx": 14293, "sentence_idx": 157, "label": "none"}, {"type": "text", "expr": " Then, clustering  $X$  into groups can be reformulated with the graph model represented in Eq", "word_idx": 14548, "sentence_idx": 158, "label": "unlabeled"}, {"type": "text", "expr": "\nWe want to find a partition of the graph so that the weights of edges between different subgraphs are very low\n(indicating that points in different clusters are dissimilar from each other) and the weights of edges in the same group are very high (meaning that points within the same cluster are similar to each other)", "word_idx": 14642, "sentence_idx": 159, "label": "unlabeled"}, {"type": "text", "expr": "\nFormally,", "word_idx": 14960, "sentence_idx": 160, "label": "unlabeled"}, {"type": "math", "expr": "$$X=\\{x_{1},x_{2},...,x_{N}\\}$$", "word_idx": 14970, "sentence_idx": 161, "label": "unlabeled"}, {"type": "math", "expr": "$$G=(V,E)$$", "word_idx": 14997, "sentence_idx": 162, "label": "unlabeled"}, {"type": "math", "expr": "$$v_{i}\\in V$$", "word_idx": 15004, "sentence_idx": 163, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{i}$$", "word_idx": 15014, "sentence_idx": 164, "label": "none"}, {"type": "math", "expr": "$$e_{ij}\\in E$$", "word_idx": 15019, "sentence_idx": 165, "label": "unlabeled"}, {"type": "math", "expr": "$$s(x_{i},x_{j})$$", "word_idx": 15030, "sentence_idx": 166, "label": "none"}, {"type": "math", "expr": "$$x_{i}$$", "word_idx": 15044, "sentence_idx": 167, "label": "none"}, {"type": "math", "expr": "$$x_{j}$$", "word_idx": 15049, "sentence_idx": 168, "label": "unlabeled"}, {"type": "text", "expr": "$cut(A_{1},A_{2},,A_{k})=\\frac{1}{2}\\sum_{i=1}^{k}W(A_{i},\\bar{A_{i}})$", "word_idx": 15054, "sentence_idx": 169, "label": "unlabeled"}, {"type": "math", "expr": "$$cut(A_{1},A_{2},...,A_{k})=\\frac{1}{2}\\sum_{i=1}^{k}W(A_{i},\\bar{A_{i}})$$", "word_idx": 15125, "sentence_idx": 170, "label": "unlabeled"}, {"type": "text", "expr": "where  $A_{i}\\subset V,A_{i}\\cap A_{j}=\\emptyset$  and  $A_{1}\\cup A_{2}\\cup\\cup A_{k}=V$ ,  $W(A_{i},\\bar{A_{i}})=\\sum_{m\\in{A_{i}},n\\in\\bar{A_{i}}}w_{mn}$ ,  $w_{mn}=exp({-S_{mn}}/{2\\delta^{2}})$  used to boost local neighborhood relationships", "word_idx": 15197, "sentence_idx": 171, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{i}\\subset V,A_{i}\\cap A_{j}=\\emptyset$$", "word_idx": 15442, "sentence_idx": 172, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{1}\\cup A_{2}\\cup...\\cup A_{k}=V$$", "word_idx": 15482, "sentence_idx": 173, "label": "unlabeled"}, {"type": "math", "expr": "$$W(A_{i},\\bar{A_{i}})=\\sum_{m\\in{A_{i}},n\\in\\bar{A_{i}}}w_{mn}$$", "word_idx": 15516, "sentence_idx": 174, "label": "unlabeled"}, {"type": "math", "expr": "$$w_{mn}=exp({-S_{mn}}/{2\\delta^{2}})$$", "word_idx": 15577, "sentence_idx": 175, "label": "unlabeled"}, {"type": "text", "expr": "However, the solution simply separates one individual vertex from the rest of the graph", "word_idx": 15612, "sentence_idx": 176, "label": "unlabeled"}, {"type": "text", "expr": "\nTo avoid unbalanced graph-cut situation that there is a large difference in sizes of subgraphs, we introduce the size of\nsubgraph  $|A|$  which is the number of vertexes in  $A$  to ensure the set of subgraph  $\\{A_{1},A_{2},,A_{k}\\}$  is reasonably large", "word_idx": 15699, "sentence_idx": 177, "label": "unlabeled"}, {"type": "text", "expr": " So, we can transform Eq", "word_idx": 15955, "sentence_idx": 178, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 1  as follows:", "word_idx": 15979, "sentence_idx": 179, "label": "unlabeled"}, {"type": "math", "expr": "$$|A|$$", "word_idx": 15995, "sentence_idx": 180, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{A_{1},A_{2},...,A_{k}\\}$$", "word_idx": 15998, "sentence_idx": 181, "label": "unlabeled"}, {"type": "text", "expr": "$cut(A_{1},A_{2},,A_{k})=\\frac{1}{2}\\sum_{i=1}^{k}\\frac{W(A_{i},\\bar{A_{i}})%\n}{|A_{i}|}$", "word_idx": 16023, "sentence_idx": 182, "label": "unlabeled"}, {"type": "math", "expr": "$$cut(A_{1},A_{2},...,A_{k})=\\frac{1}{2}\\sum_{i=1}^{k}\\frac{W(A_{i},\\bar{A_{i}})%\n}{|A_{i}|}$$", "word_idx": 16112, "sentence_idx": 183, "label": "unlabeled"}, {"type": "text", "expr": "According to  ,", "word_idx": 16202, "sentence_idx": 184, "label": "unlabeled"}, {"type": "text", "expr": "$\\begin{split}\\displaystyle\\arg\\min cut(A_{1},A_{2},,A_{k})=\\mathop{\\arg\\min%\n}_{H}Tr(H^{T}LH)\\end{split}$", "word_idx": 16217, "sentence_idx": 185, "label": "unlabeled"}, {"type": "math", "expr": "$$\\begin{split}\\displaystyle\\arg\\min cut(A_{1},A_{2},...,A_{k})=\\mathop{\\arg\\min%\n}_{H}Tr(H^{T}LH)\\end{split}$$", "word_idx": 16323, "sentence_idx": 186, "label": "unlabeled"}, {"type": "text", "expr": "where\n $L\\textrm{ is the Laplacian matrix},H^{T}H=I$ , and the indicator", "word_idx": 16430, "sentence_idx": 187, "label": "unlabeled"}, {"type": "math", "expr": "$$L\\textrm{ is the Laplacian matrix},H^{T}H=I$$", "word_idx": 16502, "sentence_idx": 188, "label": "unlabeled"}, {"type": "text", "expr": "$H=\\{h_{1},h_{2},,h_{k}\\}$", "word_idx": 16545, "sentence_idx": 189, "label": "unlabeled"}, {"type": "math", "expr": "$$H=\\{h_{1},h_{2},...,h_{k}\\}$$", "word_idx": 16571, "sentence_idx": 190, "label": "unlabeled"}, {"type": "text", "expr": "$h_{i,j}=\\left\\{\\begin{array}[]{ll}\\frac{1}{\\sqrt{A_{j}}}&\\textrm{if $v_{i}\\in A%\n_{j}$}\\\\\n0&\\text{otherwise}\\\\\n\\end{array}\\right$", "word_idx": 16598, "sentence_idx": 191, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{i,j}=\\left\\{\\begin{array}[]{ll}\\frac{1}{\\sqrt{A_{j}}}&\\textrm{if $v_{i}\\in A%\n_{j}$}\\\\\n0&\\text{otherwise}\\\\\n\\end{array}\\right.$$", "word_idx": 16728, "sentence_idx": 192, "label": "unlabeled"}, {"type": "text", "expr": "for  $i=1,2,,N;j=1,2,,k$ ", "word_idx": 16857, "sentence_idx": 193, "label": "unlabeled"}, {"type": "math", "expr": "$$i=1,2,...,N;j=1,2,...,k$$", "word_idx": 16882, "sentence_idx": 194, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 3  is the standard form of a trace minimization\nproblem", "word_idx": 16905, "sentence_idx": 195, "label": "unlabeled"}, {"type": "text", "expr": " According to the Rayleigh-Ritz theorem\u00a0 , the solution is given by choosing the matrix  $U$  which contains the first  $k$  eigenvectors of  $L$  and then uses the  $k$ -means algorithm on  $U$ ", "word_idx": 16962, "sentence_idx": 196, "label": "unlabeled"}, {"type": "text", "expr": " So, we manage to cluster  $X$  into\n $k$  groups  $\\{A_{1},A_{2},,A_{k}\\}$ ", "word_idx": 17157, "sentence_idx": 197, "label": "unlabeled"}, {"type": "text", "expr": "\nFinally, candidates in each group are classified to face or non-face class using voting", "word_idx": 17233, "sentence_idx": 198, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{A_{1},A_{2},...,A_{k}\\}$$", "word_idx": 17321, "sentence_idx": 199, "label": "unlabeled"}, {"type": "text", "expr": "4  Experiments", "word_idx": 17346, "sentence_idx": 200, "label": "unlabeled"}, {"type": "text", "expr": "In this section, we first demonstrate the effectiveness of our proposesd semantic similarity metric and then evaluate the whole model on three widely-used face detection benchmarks, including WIDER FACE\u00a0 , Annotated Faces in the Wild (AFW)\u00a0  and Pascal Faces\u00a0 ", "word_idx": 17360, "sentence_idx": 201, "label": "unlabeled"}, {"type": "text", "expr": "To demonstrate the effectiveness of our proposed semantic metric (see Subsection\u00a0 3", "word_idx": 17620, "sentence_idx": 202, "label": "unlabeled"}, {"type": "text", "expr": "1 ) for similarity measurement, we create positive samples, i", "word_idx": 17703, "sentence_idx": 203, "label": "unlabeled"}, {"type": "text", "expr": ", the ground truth face regions, and negative samples which are patches randomly sampled from background, and evaluate the discriminative ability of the computed similarity matrix on the WIDER FACE validation set", "word_idx": 17764, "sentence_idx": 204, "label": "unlabeled"}, {"type": "text", "expr": "\nThe average precision in each image on the validation set is\n $7958\\%$  in the testing set composed of both positive and negative samples,\n $7225\\%$  in the set of positive samples only,\nand  $8675\\%$  in the set of negative samples only", "word_idx": 17976, "sentence_idx": 205, "label": "unlabeled"}, {"type": "math", "expr": "$$79.58\\%$$", "word_idx": 18214, "sentence_idx": 206, "label": "unlabeled"}, {"type": "math", "expr": "$$72.25\\%$$", "word_idx": 18221, "sentence_idx": 207, "label": "unlabeled"}, {"type": "math", "expr": "$$86.75\\%$$", "word_idx": 18228, "sentence_idx": 208, "label": "unlabeled"}, {"type": "text", "expr": "1  The AFW and PASCAL FACE Dataset Results", "word_idx": 18235, "sentence_idx": 209, "label": "unlabeled"}, {"type": "text", "expr": "The AFW dataset has 205 images containing in total 473 labelled faces", "word_idx": 18277, "sentence_idx": 210, "label": "unlabeled"}, {"type": "text", "expr": "\nWe evaluate our model against the HR\u00a0 , DPM  , Headhunter, SquaresChnFtrs\u00a0 , Structured Models  , Shen et al", "word_idx": 18346, "sentence_idx": 211, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 , TSM\u00a0  and commercial detectors (e", "word_idx": 18455, "sentence_idx": 212, "label": "unlabeled"}, {"type": "text", "expr": ", Face", "word_idx": 18492, "sentence_idx": 213, "label": "unlabeled"}, {"type": "text", "expr": "com, Face++ and Picasa)", "word_idx": 18498, "sentence_idx": 214, "label": "unlabeled"}, {"type": "text", "expr": "\nAs illustrated in Fig", "word_idx": 18521, "sentence_idx": 215, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 LABEL:exp:allb , our MLGC\noutperforms all other detectors on precision-recall (PR) curves", "word_idx": 18543, "sentence_idx": 216, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:exp:allb", "word_idx": 18634, "sentence_idx": 217, "label": "unlabeled"}, {"type": "text", "expr": "The PASCAL FACE dataset contains 1,335 labeled faces in 851 images, which are collected from PASCAL person layout subset", "word_idx": 18648, "sentence_idx": 218, "label": "unlabeled"}, {"type": "text", "expr": " Because this paper focuses on face detection, we ignore images without persons from the original dataset, similar like DPM\u00a0 ", "word_idx": 18768, "sentence_idx": 219, "label": "unlabeled"}, {"type": "text", "expr": "\nWe also evaluate our model against the HR\u00a0 , DPM\u00a0 , Headhunter, SquaresChnFtrs\u00a0 , Structured Models\u00a0 , Shen et al", "word_idx": 18893, "sentence_idx": 220, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 , TSM\u00a0  and commercial detectors (e", "word_idx": 19007, "sentence_idx": 221, "label": "unlabeled"}, {"type": "text", "expr": ", Face++ and Picasa)", "word_idx": 19044, "sentence_idx": 222, "label": "unlabeled"}, {"type": "text", "expr": "\nAs shown in Fig", "word_idx": 19064, "sentence_idx": 223, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 LABEL:exp:allc , our MLGC\noutperforms all other detectors on PR curves", "word_idx": 19080, "sentence_idx": 224, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:exp:allc", "word_idx": 19152, "sentence_idx": 225, "label": "unlabeled"}, {"type": "text", "expr": "2  Results Obtained on the WIDER FACE Dataset", "word_idx": 19166, "sentence_idx": 226, "label": "unlabeled"}, {"type": "text", "expr": "The WIDER FACE Dataset is one of the most challenging public face datasets due to the variety of face scales and occlusion", "word_idx": 19211, "sentence_idx": 227, "label": "unlabeled"}, {"type": "text", "expr": "\nIt contains 32,203 images split into training (40%), validation (10%) and testing (50%) set", "word_idx": 19333, "sentence_idx": 228, "label": "unlabeled"}, {"type": "text", "expr": "\nThe validation set and testing set are divided into \u201ceasy\u201d, \u201cmedium\u201d, and \u201chard\u201d subsets according to the difficulties of the detection", "word_idx": 19425, "sentence_idx": 229, "label": "unlabeled"}, {"type": "text", "expr": "The WIDER FACE Dataset is one of the most challenging public face datasets due to the variety of face scales and occlusion", "word_idx": 19561, "sentence_idx": 230, "label": "unlabeled"}, {"type": "text", "expr": "\nIt contains 32,203 images split into training (40%), validation (10%) and testing (50%) set", "word_idx": 19683, "sentence_idx": 231, "label": "unlabeled"}, {"type": "text", "expr": "\nThe validation set and testing set are divided into \u201ceasy\u201d, \u201cmedium\u201d, and \u201chard\u201d subsets according to the difficulties of the detection", "word_idx": 19775, "sentence_idx": 232, "label": "unlabeled"}, {"type": "text", "expr": "We compare our MLGC with the HR\u00a0 , MSCNN\u00a0 , ScaleFace\u00a0 , CMS-RCNN\u00a0  and Multitask Cascade CNN\u00a0 ", "word_idx": 19911, "sentence_idx": 233, "label": "unlabeled"}, {"type": "text", "expr": "\nThe PR curves on the testing set is presented in Fig", "word_idx": 20006, "sentence_idx": 234, "label": "unlabeled"}, {"type": "text", "expr": " \u00a0 LABEL:exp:alla , and our method outperforms HR by 0", "word_idx": 20059, "sentence_idx": 235, "label": "unlabeled"}, {"type": "text", "expr": "2% in \u201ceasy\u201d subset", "word_idx": 20113, "sentence_idx": 236, "label": "unlabeled"}, {"type": "text", "expr": "\nThe PR curves on the validation set is presented in Fig", "word_idx": 20132, "sentence_idx": 237, "label": "unlabeled"}, {"type": "text", "expr": "  LABEL:fig:overall  and our method outperforms the HR by 0", "word_idx": 20188, "sentence_idx": 238, "label": "unlabeled"}, {"type": "text", "expr": "5%, 0", "word_idx": 20247, "sentence_idx": 239, "label": "unlabeled"}, {"type": "text", "expr": "2%, 0", "word_idx": 20252, "sentence_idx": 240, "label": "unlabeled"}, {"type": "text", "expr": "3%, in \u201ceasy\u201d, \u201cmedium\u201d and \u201chard\u201d subsets respectively", "word_idx": 20257, "sentence_idx": 241, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:exp:alla", "word_idx": 20312, "sentence_idx": 242, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:fig:overall", "word_idx": 20326, "sentence_idx": 243, "label": "unlabeled"}, {"type": "text", "expr": "5  Conclusion", "word_idx": 20343, "sentence_idx": 244, "label": "unlabeled"}, {"type": "text", "expr": "In this paper, aiming to improve the performance of tiny face detection, we have proposed a novel idea to exploit the semantic similarity between targeting objects\u2019 neighbors and created a pairwise constraint to depict such semantic similarity", "word_idx": 20356, "sentence_idx": 245, "label": "unlabeled"}, {"type": "text", "expr": "\nThen, a framework which adopts the metric learning and graph-cut techniques has been formulated to boost the accuracy of existing tiny object classifiers", "word_idx": 20599, "sentence_idx": 246, "label": "unlabeled"}, {"type": "text", "expr": "\nExperiments conducted on three widely-used benchmark datasets for face detection have demonstrated the improvement over the state-of-the-arts by applying this idea", "word_idx": 20753, "sentence_idx": 247, "label": "unlabeled"}, {"type": "text", "expr": "\nThe mechanism of our proposed framework is generic indicating that the framework has a great potential being applied on other small and generic object detectors", "word_idx": 20917, "sentence_idx": 248, "label": "unlabeled"}, {"type": "text", "expr": "\nThis forms our work for the next step", "word_idx": 21078, "sentence_idx": 249, "label": "unlabeled"}, {"type": "text", "expr": "In this paper, aiming to improve the performance of tiny face detection, we have proposed a novel idea to exploit the semantic similarity between targeting objects\u2019 neighbors and created a pairwise constraint to depict such semantic similarity", "word_idx": 21116, "sentence_idx": 250, "label": "unlabeled"}, {"type": "text", "expr": "\nThen, a framework which adopts the metric learning and graph-cut techniques has been formulated to boost the accuracy of existing tiny object classifiers", "word_idx": 21359, "sentence_idx": 251, "label": "unlabeled"}, {"type": "text", "expr": "\nExperiments conducted on three widely-used benchmark datasets for face detection have demonstrated the improvement over the state-of-the-arts by applying this idea", "word_idx": 21513, "sentence_idx": 252, "label": "unlabeled"}, {"type": "text", "expr": "\nThe mechanism of our proposed framework is generic indicating that the framework has a great potential being applied on other small and generic object detectors", "word_idx": 21677, "sentence_idx": 253, "label": "unlabeled"}, {"type": "text", "expr": "\nThis forms our work for the next step", "word_idx": 21838, "sentence_idx": 254, "label": "unlabeled"}, {"type": "text", "expr": "References", "word_idx": 21876, "sentence_idx": 255, "label": "unlabeled"}, {"type": "text", "expr": "1 \nXuehan Xiong and Fernando De\u00a0la Torre,\n\n \u201cSupervised descent method and its applications to face alignment,\u201d", "word_idx": 21886, "sentence_idx": 256, "label": "unlabeled"}, {"type": "text", "expr": "Xuehan Xiong and Fernando De\u00a0la Torre,", "word_idx": 21997, "sentence_idx": 257, "label": "unlabeled"}, {"type": "text", "expr": "\u201cSupervised descent method and its applications to face alignment,\u201d", "word_idx": 22035, "sentence_idx": 258, "label": "unlabeled"}, {"type": "text", "expr": "in  CVPR , 2013", "word_idx": 22102, "sentence_idx": 259, "label": "unlabeled"}, {"type": "text", "expr": "2 \nXiangyu Zhu, Zhen Lei, Xiaoming Liu, Hailin Shi, and Stan\u00a0Z Li,\n\n \u201cFace alignment across large poses: A 3d solution,\u201d", "word_idx": 22117, "sentence_idx": 260, "label": "unlabeled"}, {"type": "text", "expr": "Xiangyu Zhu, Zhen Lei, Xiaoming Liu, Hailin Shi, and Stan\u00a0Z Li,", "word_idx": 22237, "sentence_idx": 261, "label": "unlabeled"}, {"type": "text", "expr": "\u201cFace alignment across large poses: A 3d solution,\u201d", "word_idx": 22300, "sentence_idx": 262, "label": "unlabeled"}, {"type": "text", "expr": "in  CVPR , 2016, pp", "word_idx": 22351, "sentence_idx": 263, "label": "unlabeled"}, {"type": "text", "expr": " 146\u2013155", "word_idx": 22370, "sentence_idx": 264, "label": "unlabeled"}, {"type": "text", "expr": "3 \nOmkar\u00a0M Parkhi, Andrea Vedaldi, Andrew Zisserman, et\u00a0al", "word_idx": 22378, "sentence_idx": 265, "label": "unlabeled"}, {"type": "text", "expr": ",\n\n \u201cDeep face recognition", "word_idx": 22436, "sentence_idx": 266, "label": "unlabeled"}, {"type": "text", "expr": "Omkar\u00a0M Parkhi, Andrea Vedaldi, Andrew Zisserman, et\u00a0al", "word_idx": 22462, "sentence_idx": 267, "label": "unlabeled"}, {"type": "text", "expr": "\u201cDeep face recognition", "word_idx": 22517, "sentence_idx": 268, "label": "unlabeled"}, {"type": "text", "expr": "in  BMVC , 2015", "word_idx": 22539, "sentence_idx": 269, "label": "unlabeled"}, {"type": "text", "expr": "4 \nFlorian Schroff, Dmitry Kalenichenko, and James Philbin,\n\n \u201cFacenet: A unified embedding for face recognition and clustering,\u201d", "word_idx": 22554, "sentence_idx": 270, "label": "unlabeled"}, {"type": "text", "expr": "Florian Schroff, Dmitry Kalenichenko, and James Philbin,", "word_idx": 22683, "sentence_idx": 271, "label": "unlabeled"}, {"type": "text", "expr": "\u201cFacenet: A unified embedding for face recognition and clustering,\u201d", "word_idx": 22739, "sentence_idx": 272, "label": "unlabeled"}, {"type": "text", "expr": "in  CVPR , 2015, pp", "word_idx": 22806, "sentence_idx": 273, "label": "unlabeled"}, {"type": "text", "expr": " 815\u2013823", "word_idx": 22825, "sentence_idx": 274, "label": "unlabeled"}, {"type": "text", "expr": "5 \nXiangyu Zhu, Zhen Lei, Junjie Yan, Dong Yi, and Stan\u00a0Z Li,\n\n \u201cHigh-fidelity pose and expression normalization for face\nrecognition in the wild,\u201d", "word_idx": 22833, "sentence_idx": 275, "label": "unlabeled"}, {"type": "text", "expr": "Xiangyu Zhu, Zhen Lei, Junjie Yan, Dong Yi, and Stan\u00a0Z Li,", "word_idx": 22980, "sentence_idx": 276, "label": "unlabeled"}, {"type": "text", "expr": "\u201cHigh-fidelity pose and expression normalization for face\nrecognition in the wild,\u201d", "word_idx": 23038, "sentence_idx": 277, "label": "unlabeled"}, {"type": "text", "expr": "in  CVPR , 2015", "word_idx": 23121, "sentence_idx": 278, "label": "unlabeled"}, {"type": "text", "expr": "6 \nYi\u00a0Sun, Yuheng Chen, Xiaogang Wang, and Xiaoou Tang,\n\n \u201cDeep learning face representation by joint\nidentification-verification,\u201d", "word_idx": 23136, "sentence_idx": 279, "label": "unlabeled"}, {"type": "text", "expr": "Yi\u00a0Sun, Yuheng Chen, Xiaogang Wang, and Xiaoou Tang,", "word_idx": 23267, "sentence_idx": 280, "label": "unlabeled"}, {"type": "text", "expr": "\u201cDeep learning face representation by joint\nidentification-verification,\u201d", "word_idx": 23319, "sentence_idx": 281, "label": "unlabeled"}, {"type": "text", "expr": "in  NIPS , 2014", "word_idx": 23392, "sentence_idx": 282, "label": "unlabeled"}, {"type": "text", "expr": "7 \nMinyoung Kim, Sanjiv Kumar, Vladimir Pavlovic, and Henry Rowley,\n\n \u201cFace tracking and recognition with visual constraints in real-world\nvideos,\u201d", "word_idx": 23407, "sentence_idx": 283, "label": "unlabeled"}, {"type": "text", "expr": "Minyoung Kim, Sanjiv Kumar, Vladimir Pavlovic, and Henry Rowley,", "word_idx": 23554, "sentence_idx": 284, "label": "unlabeled"}, {"type": "text", "expr": "\u201cFace tracking and recognition with visual constraints in real-world\nvideos,\u201d", "word_idx": 23618, "sentence_idx": 285, "label": "unlabeled"}, {"type": "text", "expr": "in  CVPR ", "word_idx": 23695, "sentence_idx": 286, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 2008, pp", "word_idx": 23704, "sentence_idx": 287, "label": "unlabeled"}, {"type": "text", "expr": "8 \nPaul Viola and Michael Jones,\n\n \u201cRobust real-time face detection,\u201d", "word_idx": 23719, "sentence_idx": 288, "label": "unlabeled"}, {"type": "text", "expr": "Paul Viola and Michael Jones,", "word_idx": 23788, "sentence_idx": 289, "label": "unlabeled"}, {"type": "text", "expr": "\u201cRobust real-time face detection,\u201d", "word_idx": 23817, "sentence_idx": 290, "label": "unlabeled"}, {"type": "text", "expr": "IJCV , vol", "word_idx": 23851, "sentence_idx": 291, "label": "unlabeled"}, {"type": "text", "expr": " 57, no", "word_idx": 23861, "sentence_idx": 292, "label": "unlabeled"}, {"type": "text", "expr": " 2, pp", "word_idx": 23868, "sentence_idx": 293, "label": "unlabeled"}, {"type": "text", "expr": " 137\u2013154, 2004", "word_idx": 23874, "sentence_idx": 294, "label": "unlabeled"}, {"type": "text", "expr": "9 \nPeiyun Hu and Deva Ramanan,\n\n \u201cFinding tiny faces,\u201d", "word_idx": 23888, "sentence_idx": 295, "label": "unlabeled"}, {"type": "text", "expr": "Peiyun Hu and Deva Ramanan,", "word_idx": 23942, "sentence_idx": 296, "label": "unlabeled"}, {"type": "text", "expr": "\u201cFinding tiny faces,\u201d", "word_idx": 23969, "sentence_idx": 297, "label": "unlabeled"}, {"type": "text", "expr": "in  CVPR ", "word_idx": 23990, "sentence_idx": 298, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 2017, pp", "word_idx": 23999, "sentence_idx": 299, "label": "unlabeled"}, {"type": "text", "expr": " 1522\u20131530", "word_idx": 24014, "sentence_idx": 300, "label": "unlabeled"}, {"type": "text", "expr": "10 \nShuo Yang, Ping Luo, Chen-Change Loy, and Xiaoou Tang,\n\n \u201cWider face: A face detection benchmark,\u201d", "word_idx": 24024, "sentence_idx": 301, "label": "unlabeled"}, {"type": "text", "expr": "Shuo Yang, Ping Luo, Chen-Change Loy, and Xiaoou Tang,", "word_idx": 24126, "sentence_idx": 302, "label": "unlabeled"}, {"type": "text", "expr": "\u201cWider face: A face detection benchmark,\u201d", "word_idx": 24180, "sentence_idx": 303, "label": "unlabeled"}, {"type": "text", "expr": "in  CVPR , 2016, pp", "word_idx": 24221, "sentence_idx": 304, "label": "unlabeled"}, {"type": "text", "expr": " 5525\u20135533", "word_idx": 24240, "sentence_idx": 305, "label": "unlabeled"}, {"type": "text", "expr": "11 \nWei Liu, Dragomir Anguelov, Dumitru Erhan, Cheng-Yang Szegedy, and Alexander\u00a0C\nBerg,\n\n \u201cSsd: Single shot multibox detector,\u201d", "word_idx": 24250, "sentence_idx": 306, "label": "unlabeled"}, {"type": "text", "expr": "Wei Liu, Dragomir Anguelov, Dumitru Erhan, Cheng-Yang Szegedy, and Alexander\u00a0C\nBerg,", "word_idx": 24378, "sentence_idx": 307, "label": "unlabeled"}, {"type": "text", "expr": "\u201cSsd: Single shot multibox detector,\u201d", "word_idx": 24462, "sentence_idx": 308, "label": "unlabeled"}, {"type": "text", "expr": "in  ECCV , 2016", "word_idx": 24499, "sentence_idx": 309, "label": "unlabeled"}, {"type": "text", "expr": "12 \nQuoc\u00a0V", "word_idx": 24514, "sentence_idx": 310, "label": "unlabeled"}, {"type": "text", "expr": " Le, Navdeep Jaitly, and Geoffrey\u00a0E", "word_idx": 24524, "sentence_idx": 311, "label": "unlabeled"}, {"type": "text", "expr": " Hinton,\n\n \u201cA simple way to initialize recurrent networks of rectified linear\nunits,\u201d", "word_idx": 24559, "sentence_idx": 312, "label": "unlabeled"}, {"type": "text", "expr": "Quoc\u00a0V", "word_idx": 24644, "sentence_idx": 313, "label": "unlabeled"}, {"type": "text", "expr": " Le, Navdeep Jaitly, and Geoffrey\u00a0E", "word_idx": 24650, "sentence_idx": 314, "label": "unlabeled"}, {"type": "text", "expr": " Hinton,", "word_idx": 24685, "sentence_idx": 315, "label": "unlabeled"}, {"type": "text", "expr": "\u201cA simple way to initialize recurrent networks of rectified linear\nunits,\u201d", "word_idx": 24693, "sentence_idx": 316, "label": "unlabeled"}, {"type": "text", "expr": "CoRR , vol", "word_idx": 24767, "sentence_idx": 317, "label": "unlabeled"}, {"type": "text", "expr": " abs/1504", "word_idx": 24777, "sentence_idx": 318, "label": "unlabeled"}, {"type": "text", "expr": "00941, 2015", "word_idx": 24786, "sentence_idx": 319, "label": "unlabeled"}, {"type": "text", "expr": "13 \nBin Yang, Junjie Yan, Zhen Lei, and Stan\u00a0Z Li,\n\n \u201cAggregate channel features for multi-view face detection,\u201d", "word_idx": 24797, "sentence_idx": 320, "label": "unlabeled"}, {"type": "text", "expr": "Bin Yang, Junjie Yan, Zhen Lei, and Stan\u00a0Z Li,", "word_idx": 24909, "sentence_idx": 321, "label": "unlabeled"}, {"type": "text", "expr": "\u201cAggregate channel features for multi-view face detection,\u201d", "word_idx": 24955, "sentence_idx": 322, "label": "unlabeled"}, {"type": "text", "expr": "in  IJCB ", "word_idx": 25014, "sentence_idx": 323, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 2014", "word_idx": 25023, "sentence_idx": 324, "label": "unlabeled"}, {"type": "text", "expr": "14 \nMinh\u00a0Tri Pham and Tat\u00a0Jen Chain,\n\n \u201cFast training and selection of haar features using statistics in\nboosting-based face detection,\u201d", "word_idx": 25034, "sentence_idx": 325, "label": "unlabeled"}, {"type": "text", "expr": "Minh\u00a0Tri Pham and Tat\u00a0Jen Chain,", "word_idx": 25170, "sentence_idx": 326, "label": "unlabeled"}, {"type": "text", "expr": "\u201cFast training and selection of haar features using statistics in\nboosting-based face detection,\u201d", "word_idx": 25202, "sentence_idx": 327, "label": "unlabeled"}, {"type": "text", "expr": "in  ICCV , 2007, pp", "word_idx": 25299, "sentence_idx": 328, "label": "unlabeled"}, {"type": "text", "expr": "15 \nHaoxiang Li, Zhe Lin, Xiaohui Shen, Jonathan Brandt, and Gang Hua,\n\n \u201cA convolutional neural network cascade for face detection,\u201d", "word_idx": 25318, "sentence_idx": 329, "label": "unlabeled"}, {"type": "text", "expr": "Haoxiang Li, Zhe Lin, Xiaohui Shen, Jonathan Brandt, and Gang Hua,", "word_idx": 25451, "sentence_idx": 330, "label": "unlabeled"}, {"type": "text", "expr": "\u201cA convolutional neural network cascade for face detection,\u201d", "word_idx": 25517, "sentence_idx": 331, "label": "unlabeled"}, {"type": "text", "expr": "in  CVPR , 2015", "word_idx": 25577, "sentence_idx": 332, "label": "unlabeled"}, {"type": "text", "expr": "16 \nBin Yang, Junjie Yan, Zhen Lei, and Stan\u00a0Z Li,\n\n \u201cConvolutional channel features,\u201d", "word_idx": 25592, "sentence_idx": 333, "label": "unlabeled"}, {"type": "text", "expr": "Bin Yang, Junjie Yan, Zhen Lei, and Stan\u00a0Z Li,", "word_idx": 25678, "sentence_idx": 334, "label": "unlabeled"}, {"type": "text", "expr": "\u201cConvolutional channel features,\u201d", "word_idx": 25724, "sentence_idx": 335, "label": "unlabeled"}, {"type": "text", "expr": "in  CVPR , 2015, pp", "word_idx": 25757, "sentence_idx": 336, "label": "unlabeled"}, {"type": "text", "expr": " 82\u201390", "word_idx": 25776, "sentence_idx": 337, "label": "unlabeled"}, {"type": "text", "expr": "17 \nShuo Yang, Ping Luo, Chen-Change Loy, and Xiaoou Tang,\n\n \u201cFrom facial parts responses to face detection: A deep learning\napproach,\u201d", "word_idx": 25782, "sentence_idx": 338, "label": "unlabeled"}, {"type": "text", "expr": "Shuo Yang, Ping Luo, Chen-Change Loy, and Xiaoou Tang,", "word_idx": 25917, "sentence_idx": 339, "label": "unlabeled"}, {"type": "text", "expr": "\u201cFrom facial parts responses to face detection: A deep learning\napproach,\u201d", "word_idx": 25971, "sentence_idx": 340, "label": "unlabeled"}, {"type": "text", "expr": "in  ICCV , 2015", "word_idx": 26045, "sentence_idx": 341, "label": "unlabeled"}, {"type": "text", "expr": "18 \nSean Bell, C\u00a0Lawrence\u00a0Zitnick, Kavita Bala, and Ross Girshick,\n\n \u201cInside-outside net: Detecting objects in context with skip pooling\nand recurrent neural networks,\u201d", "word_idx": 26060, "sentence_idx": 342, "label": "unlabeled"}, {"type": "text", "expr": "Sean Bell, C\u00a0Lawrence\u00a0Zitnick, Kavita Bala, and Ross Girshick,", "word_idx": 26228, "sentence_idx": 343, "label": "unlabeled"}, {"type": "text", "expr": "\u201cInside-outside net: Detecting objects in context with skip pooling\nand recurrent neural networks,\u201d", "word_idx": 26290, "sentence_idx": 344, "label": "unlabeled"}, {"type": "text", "expr": "in  CVPR , 2016, pp", "word_idx": 26389, "sentence_idx": 345, "label": "unlabeled"}, {"type": "text", "expr": " 2874\u20132883", "word_idx": 26408, "sentence_idx": 346, "label": "unlabeled"}, {"type": "text", "expr": "19 \nUlrike Von\u00a0Luxburg,\n\n \u201cA tutorial on spectral clustering,\u201d", "word_idx": 26418, "sentence_idx": 347, "label": "unlabeled"}, {"type": "text", "expr": "Ulrike Von\u00a0Luxburg,", "word_idx": 26480, "sentence_idx": 348, "label": "unlabeled"}, {"type": "text", "expr": "\u201cA tutorial on spectral clustering,\u201d", "word_idx": 26499, "sentence_idx": 349, "label": "unlabeled"}, {"type": "text", "expr": "Statistics and computing , 2007", "word_idx": 26535, "sentence_idx": 350, "label": "unlabeled"}, {"type": "text", "expr": "Statistics and computing", "word_idx": 26566, "sentence_idx": 351, "label": "unlabeled"}, {"type": "text", "expr": "20 \nHelmut Lutkepohl,\n\n \u201cHandbook of matrices", "word_idx": 26590, "sentence_idx": 352, "label": "unlabeled"}, {"type": "text", "expr": "Helmut Lutkepohl,", "word_idx": 26635, "sentence_idx": 353, "label": "unlabeled"}, {"type": "text", "expr": "\u201cHandbook of matrices", "word_idx": 26652, "sentence_idx": 354, "label": "unlabeled"}, {"type": "text", "expr": "Computational Statistics and Data Analysis , 1997", "word_idx": 26673, "sentence_idx": 355, "label": "unlabeled"}, {"type": "text", "expr": "Computational Statistics and Data Analysis", "word_idx": 26722, "sentence_idx": 356, "label": "unlabeled"}, {"type": "text", "expr": "21 \nXiangxin Zhu and Deva Ramanan,\n\n \u201cFace detection, pose estimation, and landmark localization in the\nwild,\u201d", "word_idx": 26764, "sentence_idx": 357, "label": "unlabeled"}, {"type": "text", "expr": "Xiangxin Zhu and Deva Ramanan,", "word_idx": 26874, "sentence_idx": 358, "label": "unlabeled"}, {"type": "text", "expr": "\u201cFace detection, pose estimation, and landmark localization in the\nwild,\u201d", "word_idx": 26904, "sentence_idx": 359, "label": "unlabeled"}, {"type": "text", "expr": "in  CVPR ", "word_idx": 26977, "sentence_idx": 360, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 2012, pp", "word_idx": 26986, "sentence_idx": 361, "label": "unlabeled"}, {"type": "text", "expr": " 2879\u20132886", "word_idx": 27001, "sentence_idx": 362, "label": "unlabeled"}, {"type": "text", "expr": "22 \nJunjie Yan, Xuzong Zhang, Zhen Lei, and Stan\u00a0Z Li,\n\n \u201cFace detection by structural models,\u201d", "word_idx": 27011, "sentence_idx": 363, "label": "unlabeled"}, {"type": "text", "expr": "Junjie Yan, Xuzong Zhang, Zhen Lei, and Stan\u00a0Z Li,", "word_idx": 27106, "sentence_idx": 364, "label": "unlabeled"}, {"type": "text", "expr": "\u201cFace detection by structural models,\u201d", "word_idx": 27156, "sentence_idx": 365, "label": "unlabeled"}, {"type": "text", "expr": "Image and Vision Computing , vol", "word_idx": 27194, "sentence_idx": 366, "label": "unlabeled"}, {"type": "text", "expr": " 32, no", "word_idx": 27226, "sentence_idx": 367, "label": "unlabeled"}, {"type": "text", "expr": " 10, pp", "word_idx": 27233, "sentence_idx": 368, "label": "unlabeled"}, {"type": "text", "expr": " 790\u2013799,\n2014", "word_idx": 27240, "sentence_idx": 369, "label": "unlabeled"}, {"type": "text", "expr": "Image and Vision Computing", "word_idx": 27254, "sentence_idx": 370, "label": "unlabeled"}, {"type": "text", "expr": "23 \nPedro\u00a0F Felzenszwalb, Ross\u00a0B Girshick, David McAllester, and Deva Ramanan,\n\n \u201cObject detection with discriminatively trained part-based models,\u201d", "word_idx": 27280, "sentence_idx": 371, "label": "unlabeled"}, {"type": "text", "expr": "Pedro\u00a0F Felzenszwalb, Ross\u00a0B Girshick, David McAllester, and Deva Ramanan,", "word_idx": 27428, "sentence_idx": 372, "label": "unlabeled"}, {"type": "text", "expr": "\u201cObject detection with discriminatively trained part-based models,\u201d", "word_idx": 27502, "sentence_idx": 373, "label": "unlabeled"}, {"type": "text", "expr": "TPAMI , vol", "word_idx": 27569, "sentence_idx": 374, "label": "unlabeled"}, {"type": "text", "expr": " 32, no", "word_idx": 27580, "sentence_idx": 375, "label": "unlabeled"}, {"type": "text", "expr": " 9, pp", "word_idx": 27587, "sentence_idx": 376, "label": "unlabeled"}, {"type": "text", "expr": " 1627\u20131645, 2010", "word_idx": 27593, "sentence_idx": 377, "label": "unlabeled"}, {"type": "text", "expr": "TPAMI", "word_idx": 27609, "sentence_idx": 378, "label": "unlabeled"}, {"type": "text", "expr": "24 \nMarkus Mathias, Rodrigo Benenson, Marco Pedersoli, and Luc Van\u00a0Gool,\n\n \u201cFace detection without bells and whistles,\u201d", "word_idx": 27614, "sentence_idx": 379, "label": "unlabeled"}, {"type": "text", "expr": "Markus Mathias, Rodrigo Benenson, Marco Pedersoli, and Luc Van\u00a0Gool,", "word_idx": 27733, "sentence_idx": 380, "label": "unlabeled"}, {"type": "text", "expr": "\u201cFace detection without bells and whistles,\u201d", "word_idx": 27801, "sentence_idx": 381, "label": "unlabeled"}, {"type": "text", "expr": "in  ECCV ", "word_idx": 27845, "sentence_idx": 382, "label": "unlabeled"}, {"type": "text", "expr": " Springer, 2014, pp", "word_idx": 27854, "sentence_idx": 383, "label": "unlabeled"}, {"type": "text", "expr": " 720\u2013735", "word_idx": 27873, "sentence_idx": 384, "label": "unlabeled"}, {"type": "text", "expr": "25 \nXiaohui Shen, Zhe Lin, Jonathan Brandt, and Ying Wu,\n\n \u201cDetecting and aligning faces by image retrieval,\u201d", "word_idx": 27881, "sentence_idx": 385, "label": "unlabeled"}, {"type": "text", "expr": "Xiaohui Shen, Zhe Lin, Jonathan Brandt, and Ying Wu,", "word_idx": 27990, "sentence_idx": 386, "label": "unlabeled"}, {"type": "text", "expr": "\u201cDetecting and aligning faces by image retrieval,\u201d", "word_idx": 28042, "sentence_idx": 387, "label": "unlabeled"}, {"type": "text", "expr": "in  CVPR ", "word_idx": 28092, "sentence_idx": 388, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 2013, pp", "word_idx": 28101, "sentence_idx": 389, "label": "unlabeled"}, {"type": "text", "expr": " 3460\u20133467", "word_idx": 28116, "sentence_idx": 390, "label": "unlabeled"}, {"type": "text", "expr": "26 \nZhaowei Cai, Quanfu Fan, Rogerio\u00a0S Feris, and Nuno Vasconcelos,\n\n \u201cA unified multi-scale deep convolutional neural network for fast\nobject detection,\u201d", "word_idx": 28126, "sentence_idx": 391, "label": "unlabeled"}, {"type": "text", "expr": "Zhaowei Cai, Quanfu Fan, Rogerio\u00a0S Feris, and Nuno Vasconcelos,", "word_idx": 28280, "sentence_idx": 392, "label": "unlabeled"}, {"type": "text", "expr": "\u201cA unified multi-scale deep convolutional neural network for fast\nobject detection,\u201d", "word_idx": 28343, "sentence_idx": 393, "label": "unlabeled"}, {"type": "text", "expr": "in  ECCV ", "word_idx": 28427, "sentence_idx": 394, "label": "unlabeled"}, {"type": "text", "expr": " Springer, 2016, pp", "word_idx": 28436, "sentence_idx": 395, "label": "unlabeled"}, {"type": "text", "expr": " 354\u2013370", "word_idx": 28455, "sentence_idx": 396, "label": "unlabeled"}, {"type": "text", "expr": "27 \nShuo Yang, Yuanjun Xiong, Chen\u00a0Change Loy, and Xiaoou Tang,\n\n \u201cFace detection through scale-friendly deep convolutional\nnetworks,\u201d", "word_idx": 28463, "sentence_idx": 397, "label": "unlabeled"}, {"type": "text", "expr": "Shuo Yang, Yuanjun Xiong, Chen\u00a0Change Loy, and Xiaoou Tang,", "word_idx": 28597, "sentence_idx": 398, "label": "unlabeled"}, {"type": "text", "expr": "\u201cFace detection through scale-friendly deep convolutional\nnetworks,\u201d", "word_idx": 28656, "sentence_idx": 399, "label": "unlabeled"}, {"type": "text", "expr": "arXiv , 2017", "word_idx": 28724, "sentence_idx": 400, "label": "unlabeled"}, {"type": "text", "expr": "arXiv", "word_idx": 28736, "sentence_idx": 401, "label": "unlabeled"}, {"type": "text", "expr": "28 \nChenchen Zhu, Yutong Zheng, Khoa Luu, and Marios Savvides,\n\n \u201cCms-rcnn: contextual multi-scale region-based cnn for unconstrained\nface detection,\u201d", "word_idx": 28741, "sentence_idx": 402, "label": "unlabeled"}, {"type": "text", "expr": "Chenchen Zhu, Yutong Zheng, Khoa Luu, and Marios Savvides,", "word_idx": 28891, "sentence_idx": 403, "label": "unlabeled"}, {"type": "text", "expr": "\u201cCms-rcnn: contextual multi-scale region-based cnn for unconstrained\nface detection,\u201d", "word_idx": 28949, "sentence_idx": 404, "label": "unlabeled"}, {"type": "text", "expr": "in  Deep Learning for Biometrics , pp", "word_idx": 29034, "sentence_idx": 405, "label": "unlabeled"}, {"type": "text", "expr": " 57\u201379", "word_idx": 29071, "sentence_idx": 406, "label": "unlabeled"}, {"type": "text", "expr": " Springer, 2017", "word_idx": 29077, "sentence_idx": 407, "label": "unlabeled"}, {"type": "text", "expr": "Deep Learning for Biometrics", "word_idx": 29092, "sentence_idx": 408, "label": "unlabeled"}, {"type": "text", "expr": "29 \nKaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu\u00a0Qiao,\n\n \u201cJoint face detection and alignment using multitask cascaded\nconvolutional networks,\u201d", "word_idx": 29120, "sentence_idx": 409, "label": "unlabeled"}, {"type": "text", "expr": "Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu\u00a0Qiao,", "word_idx": 29267, "sentence_idx": 410, "label": "unlabeled"}, {"type": "text", "expr": "\u201cJoint face detection and alignment using multitask cascaded\nconvolutional networks,\u201d", "word_idx": 29322, "sentence_idx": 411, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Signal Processing Letters , 2016", "word_idx": 29407, "sentence_idx": 412, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Signal Processing Letters", "word_idx": 29444, "sentence_idx": 413, "label": "unlabeled"}, {"type": "text", "expr": "Generated  on Wed Mar  7 13:38:49 2018 by", "word_idx": 29474, "sentence_idx": 414, "label": "unlabeled"}, {"type": "text", "expr": "LaTeXML", "word_idx": 29515, "sentence_idx": 415, "label": "unlabeled"}], "transfer_learning": [{"type": "text", "expr": "Untitled Document", "word_idx": 0, "sentence_idx": 0, "label": "unlabeled"}, {"type": "text", "expr": "Abstract Classical distillation methods transfer representations from a \u201cteacher\u201d neural network to a \u201cstudent\u201d network by matching their output activations", "word_idx": 17, "sentence_idx": 1, "label": "unlabeled"}, {"type": "text", "expr": " Recent methods also match the Jacobians, or the gradient of output activations with the input", "word_idx": 173, "sentence_idx": 2, "label": "unlabeled"}, {"type": "text", "expr": " However, this involves making some ad hoc decisions, in particular, the choice of the loss function", "word_idx": 267, "sentence_idx": 3, "label": "unlabeled"}, {"type": "text", "expr": " In this paper, we first establish an equivalence between Jacobian matching and distillation with input noise, from which we derive appropriate loss functions for Jacobian matching", "word_idx": 367, "sentence_idx": 4, "label": "unlabeled"}, {"type": "text", "expr": " We then rely on this analysis to apply Jacobian matching to transfer learning by establishing equivalence of a recent transfer learning procedure to distillation", "word_idx": 547, "sentence_idx": 5, "label": "unlabeled"}, {"type": "text", "expr": " We then show experimentally on standard image datasets that Jacobian-based penalties improve distillation, robustness to noisy inputs, and transfer learning", "word_idx": 709, "sentence_idx": 6, "label": "unlabeled"}, {"type": "text", "expr": "Abstract", "word_idx": 866, "sentence_idx": 7, "label": "unlabeled"}, {"type": "text", "expr": "Classical distillation methods transfer representations from a \u201cteacher\u201d neural network to a \u201cstudent\u201d network by matching their output activations", "word_idx": 874, "sentence_idx": 8, "label": "unlabeled"}, {"type": "text", "expr": " Recent methods also match the Jacobians, or the gradient of output activations with the input", "word_idx": 1021, "sentence_idx": 9, "label": "unlabeled"}, {"type": "text", "expr": " However, this involves making some ad hoc decisions, in particular, the choice of the loss function", "word_idx": 1115, "sentence_idx": 10, "label": "unlabeled"}, {"type": "text", "expr": "In this paper, we first establish an equivalence between Jacobian matching and distillation with input noise, from which we derive appropriate loss functions for Jacobian matching", "word_idx": 1215, "sentence_idx": 11, "label": "unlabeled"}, {"type": "text", "expr": " We then rely on this analysis to apply Jacobian matching to transfer learning by establishing equivalence of a recent transfer learning procedure to distillation", "word_idx": 1394, "sentence_idx": 12, "label": "unlabeled"}, {"type": "text", "expr": "We then show experimentally on standard image datasets that Jacobian-based penalties improve distillation, robustness to noisy inputs, and transfer learning", "word_idx": 1556, "sentence_idx": 13, "label": "unlabeled"}, {"type": "text", "expr": "\\icmltitlerunning", "word_idx": 1712, "sentence_idx": 14, "label": "unlabeled"}, {"type": "text", "expr": "KnowledgeTransferwithJacobianMatching", "word_idx": 1729, "sentence_idx": 15, "label": "unlabeled"}, {"type": "text", "expr": "KnowledgeTransferwithJacobianMatching", "word_idx": 1766, "sentence_idx": 16, "label": "unlabeled"}, {"type": "text", "expr": "\\printAffiliationsAndNotice", "word_idx": 1803, "sentence_idx": 17, "label": "unlabeled"}, {"type": "text", "expr": "1  Introduction", "word_idx": 1830, "sentence_idx": 18, "label": "unlabeled"}, {"type": "text", "expr": "Consider that we are given a neural network  $\\mathcal{A}$  trained on a particular dataset, and want to train another neural network  $\\mathcal{B}$  on a similar (or related) dataset", "word_idx": 1845, "sentence_idx": 19, "label": "unlabeled"}, {"type": "text", "expr": " Is it possible to leverage  $\\mathcal{A}$  to train  $\\mathcal{B}$  more efficiently? We call this the problem of  knowledge transfer ", "word_idx": 2028, "sentence_idx": 20, "label": "unlabeled"}, {"type": "text", "expr": " Distillation\u00a0 \\citep hinton2015distilling is a form of knowledge transfer where  $\\mathcal{A}$  and  $\\mathcal{B}$  are trained on the same dataset, but have different architectures", "word_idx": 2163, "sentence_idx": 21, "label": "unlabeled"}, {"type": "text", "expr": " Transfer Learning\u00a0 \\citep pan2010survey is another form of knowledge transfer where  $\\mathcal{A}$  and  $\\mathcal{B}$  are trained on different (but related) datasets", "word_idx": 2345, "sentence_idx": 22, "label": "unlabeled"}, {"type": "text", "expr": " If the architectures are the same, we can in both cases simply copy the weights from  $\\mathcal{A}$  to  $\\mathcal{B}$ ", "word_idx": 2513, "sentence_idx": 23, "label": "unlabeled"}, {"type": "text", "expr": " The problem becomes more challenging when  $\\mathcal{A}$  and  $\\mathcal{B}$  have different architectures", "word_idx": 2633, "sentence_idx": 24, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{A}$$", "word_idx": 2740, "sentence_idx": 25, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{B}$$", "word_idx": 2751, "sentence_idx": 26, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{A}$$", "word_idx": 2762, "sentence_idx": 27, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{B}$$", "word_idx": 2773, "sentence_idx": 28, "label": "unlabeled"}, {"type": "text", "expr": "knowledge transfer", "word_idx": 2784, "sentence_idx": 29, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 2802, "sentence_idx": 30, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{A}$$", "word_idx": 2808, "sentence_idx": 31, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{B}$$", "word_idx": 2819, "sentence_idx": 32, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 2830, "sentence_idx": 33, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{A}$$", "word_idx": 2836, "sentence_idx": 34, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{B}$$", "word_idx": 2847, "sentence_idx": 35, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{A}$$", "word_idx": 2858, "sentence_idx": 36, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{B}$$", "word_idx": 2869, "sentence_idx": 37, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{A}$$", "word_idx": 2880, "sentence_idx": 38, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{B}$$", "word_idx": 2891, "sentence_idx": 39, "label": "unlabeled"}, {"type": "text", "expr": "A perfect distillation method would enable us to transform one neural network architecture into another, while preserving generalization", "word_idx": 2902, "sentence_idx": 40, "label": "unlabeled"}, {"type": "text", "expr": " This would allow us to easily explore the space of neural network architectures, which can be used for neural network architecture search, model compression, or creating diverse ensembles", "word_idx": 3038, "sentence_idx": 41, "label": "unlabeled"}, {"type": "text", "expr": " A perfect transfer learning method, on the other hand, would use little data to train  $\\mathcal{B}$ , optimally using the limited samples at it\u2019s disposal", "word_idx": 3226, "sentence_idx": 42, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{B}$$", "word_idx": 3382, "sentence_idx": 43, "label": "unlabeled"}, {"type": "text", "expr": "This paper deals with the problem of knowledge transfer based on matching the Jacobian of the network\u2019s output which, like the output itself, has a dimension independent of the network\u2019s architecture", "word_idx": 3393, "sentence_idx": 44, "label": "unlabeled"}, {"type": "text", "expr": " This approach has also been recently explored for the case of distillation by\u00a0 \\citet czarnecki2017sobolev, who considered the general idea of matching Jacobians, and by\u00a0 \\citet zagoruyko2016paying who view Jacobians as attention maps", "word_idx": 3592, "sentence_idx": 45, "label": "unlabeled"}, {"type": "text", "expr": " However, in both of these cases, it was not clear what loss function must be used to match Jacobians", "word_idx": 3827, "sentence_idx": 46, "label": "unlabeled"}, {"type": "text", "expr": " It was also unclear how these methods relate to classical distillation approaches\u00a0 \\citep ba2014deep, hinton2015distilling", "word_idx": 3928, "sentence_idx": 47, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 4051, "sentence_idx": 48, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 4057, "sentence_idx": 49, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 4063, "sentence_idx": 50, "label": "unlabeled"}, {"type": "text", "expr": "Recently\u00a0 \\citet li2016learning proposed a distillation-like approach to perform transfer learning", "word_idx": 4069, "sentence_idx": 51, "label": "unlabeled"}, {"type": "text", "expr": " While this approach works well in practice, it was not clear how this exactly relates to regular distillation", "word_idx": 4167, "sentence_idx": 52, "label": "unlabeled"}, {"type": "text", "expr": " It is also not clear how this applies to the challenging setting of transfer learning where the architectures of both networks  $\\mathcal{A}$  and  $\\mathcal{B}$  can be arbitrary", "word_idx": 4277, "sentence_idx": 53, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 4457, "sentence_idx": 54, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{A}$$", "word_idx": 4463, "sentence_idx": 55, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{B}$$", "word_idx": 4474, "sentence_idx": 56, "label": "unlabeled"}, {"type": "text", "expr": "The overall contributions of our paper are:", "word_idx": 4485, "sentence_idx": 57, "label": "unlabeled"}, {"type": "text", "expr": "The overall contributions of our paper are:", "word_idx": 4528, "sentence_idx": 58, "label": "unlabeled"}, {"type": "text", "expr": "We show that matching Jacobians is a special case of classical distillation, where noise is added to the inputs", "word_idx": 4571, "sentence_idx": 59, "label": "unlabeled"}, {"type": "text", "expr": "We show that matching Jacobians is a special case of classical distillation, where noise is added to the inputs", "word_idx": 4682, "sentence_idx": 60, "label": "unlabeled"}, {"type": "text", "expr": "We show that a recent transfer learning method (LwF by  \\citealp li2016learning) can be viewed as distillation, which allows us to match Jacobians for this case", "word_idx": 4793, "sentence_idx": 61, "label": "unlabeled"}, {"type": "text", "expr": "\\citealp", "word_idx": 4953, "sentence_idx": 62, "label": "unlabeled"}, {"type": "text", "expr": "We provide methods to match Jacobians of practical deep networks, where architecture of both networks are arbitrary", "word_idx": 4961, "sentence_idx": 63, "label": "unlabeled"}, {"type": "text", "expr": "We provide methods to match Jacobians of practical deep networks, where architecture of both networks are arbitrary", "word_idx": 5076, "sentence_idx": 64, "label": "unlabeled"}, {"type": "text", "expr": "We experimentally validate these results by providing evidence that Jacobian matching helps both regular distillation and transfer learning, and that Jacobian-norm penalties learn models robust to noise", "word_idx": 5191, "sentence_idx": 65, "label": "unlabeled"}, {"type": "text", "expr": "We experimentally validate these results by providing evidence that Jacobian matching helps both regular distillation and transfer learning, and that Jacobian-norm penalties learn models robust to noise", "word_idx": 5393, "sentence_idx": 66, "label": "unlabeled"}, {"type": "text", "expr": "2  Related Work", "word_idx": 5595, "sentence_idx": 67, "label": "unlabeled"}, {"type": "text", "expr": "Several Jacobian-based regularizers have been proposed in recent times", "word_idx": 5610, "sentence_idx": 68, "label": "unlabeled"}, {"type": "text", "expr": " Sobolev training\u00a0 \\citep czarnecki2017sobolev, showed that using higher order derivatives along with the targets can help in training with less data", "word_idx": 5680, "sentence_idx": 69, "label": "unlabeled"}, {"type": "text", "expr": " This work is similar to ours", "word_idx": 5829, "sentence_idx": 70, "label": "unlabeled"}, {"type": "text", "expr": " While we also make similar claims, we clarify the relationship of this method with regular distillation based on matching activations, and show how it can help", "word_idx": 5858, "sentence_idx": 71, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, we show how the loss function used for activation matching also decides the loss function we use for Jacobian matching", "word_idx": 6018, "sentence_idx": 72, "label": "unlabeled"}, {"type": "text", "expr": " Similarly,  \\citet wang2016analysis use the Jacobian for distillation and show that it helps improve performance", "word_idx": 6151, "sentence_idx": 73, "label": "unlabeled"}, {"type": "text", "expr": "  \\citet zagoruyko2016paying introduce the idea of matching attention maps", "word_idx": 6264, "sentence_idx": 74, "label": "unlabeled"}, {"type": "text", "expr": " The Jacobian was also considered as one such attention map", "word_idx": 6338, "sentence_idx": 75, "label": "unlabeled"}, {"type": "text", "expr": " This work also finds that combining both activation matching and Jacobian matching is helpful", "word_idx": 6397, "sentence_idx": 76, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 6491, "sentence_idx": 77, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 6497, "sentence_idx": 78, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 6503, "sentence_idx": 79, "label": "unlabeled"}, {"type": "text", "expr": "Jacobian-norm regularizers were used in early works by\u00a0 \\citet drucker1992improving, where they looked at penalizing the Jacobian norm", "word_idx": 6509, "sentence_idx": 80, "label": "unlabeled"}, {"type": "text", "expr": " The intuition was to make the model more robust to small changes in the input", "word_idx": 6643, "sentence_idx": 81, "label": "unlabeled"}, {"type": "text", "expr": " We find that this conforms to our analysis as well", "word_idx": 6721, "sentence_idx": 82, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 6772, "sentence_idx": 83, "label": "unlabeled"}, {"type": "text", "expr": "Knowledge Distillation\u00a0 \\citep hinton2015distilling first showed that one can use softmax with temperature to perform knowledge transfer with neural nets", "word_idx": 6778, "sentence_idx": 84, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 \\citet ba2014deep found that squared error between logits worked better than the softmax method, and they used this method to train shallow nets with equivalent performance to deep nets", "word_idx": 6931, "sentence_idx": 85, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 \\citet romero2014fitnets and\u00a0 \\citet zagoruyko2016paying showed how to enhance distillation by matching intermediate features along with the outputs, but use different methods to do so", "word_idx": 7118, "sentence_idx": 86, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 \\citet sau2016deep found that adding noise to logits helps during teacher-student training", "word_idx": 7304, "sentence_idx": 87, "label": "unlabeled"}, {"type": "text", "expr": " We show that the use of the Jacobian can be interpreted as adding such noise to the inputs analytically", "word_idx": 7396, "sentence_idx": 88, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 7500, "sentence_idx": 89, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 7506, "sentence_idx": 90, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 7512, "sentence_idx": 91, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 7518, "sentence_idx": 92, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 7524, "sentence_idx": 93, "label": "unlabeled"}, {"type": "text", "expr": "3  Jacobians of Neural Networks", "word_idx": 7530, "sentence_idx": 94, "label": "unlabeled"}, {"type": "text", "expr": "Let us consider the first order Taylor series expansion of a function  $f:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}$  around a small neighborhood  $\\{\\mathbf{x}+\\Delta\\mathbf{x}:\\|\\Delta\\mathbf{x}\\|\\leq\\epsilon\\}$ ", "word_idx": 7561, "sentence_idx": 95, "label": "definition"}, {"type": "text", "expr": " It can be written as", "word_idx": 7769, "sentence_idx": 96, "label": "unlabeled"}, {"type": "math", "expr": "$$f:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}$$", "word_idx": 7790, "sentence_idx": 97, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{\\mathbf{x}+\\Delta\\mathbf{x}:\\|\\Delta\\mathbf{x}\\|\\leq\\epsilon\\}$$", "word_idx": 7827, "sentence_idx": 98, "label": "definition"}, {"type": "text", "expr": "$\\displaystyle f(\\mathbf{x}+\\Delta\\mathbf{x})$", "word_idx": 7891, "sentence_idx": 99, "label": "none"}, {"type": "math", "expr": "$$\\displaystyle f(\\mathbf{x}+\\Delta\\mathbf{x})$$", "word_idx": 7937, "sentence_idx": 100, "label": "none"}, {"type": "text", "expr": "$\\displaystyle=f(\\mathbf{x})+\\nabla_{x}f(\\mathbf{x})^{T}(\\Delta\\mathbf{x})+%\n\\mathcal{O}(\\epsilon^{2})$", "word_idx": 7981, "sentence_idx": 101, "label": "none"}, {"type": "math", "expr": "$$\\displaystyle=f(\\mathbf{x})+\\nabla_{x}f(\\mathbf{x})^{T}(\\Delta\\mathbf{x})+%\n\\mathcal{O}(\\epsilon^{2})$$", "word_idx": 8084, "sentence_idx": 102, "label": "none"}, {"type": "text", "expr": "We can apply this linearization to neural nets", "word_idx": 8185, "sentence_idx": 103, "label": "unlabeled"}, {"type": "text", "expr": " The source of non-linearity for neural nets lie in the elementwise non-linear activations (like ReLU, sigmoid) and pooling operators", "word_idx": 8231, "sentence_idx": 104, "label": "unlabeled"}, {"type": "text", "expr": "  It is easy to see that to linearize the entire neural network, one must linearize all such non-linearities in the network", "word_idx": 8364, "sentence_idx": 105, "label": "unlabeled"}, {"type": "text", "expr": "It is easy to see that to linearize the entire neural network, one must linearize all such non-linearities in the network", "word_idx": 8487, "sentence_idx": 106, "label": "unlabeled"}, {"type": "text", "expr": "1  Special case: ReLU and MaxPool", "word_idx": 8608, "sentence_idx": 107, "label": "unlabeled"}, {"type": "text", "expr": "For the ReLU nonlinearity, the Taylor approximation is locally exact and simple to compute, as the derivative  $\\frac{\\mathrm{d}\\sigma(z)}{\\mathrm{d}z}$  is either 0 or 1 (except at  $z=0$ , where it is undefined)", "word_idx": 8641, "sentence_idx": 108, "label": "unlabeled"}, {"type": "text", "expr": " A similar statement holds for max-pooling", "word_idx": 8854, "sentence_idx": 109, "label": "unlabeled"}, {"type": "text", "expr": " Going back to the definition in Equation  1 , for piecewise linear nets there exist  $\\epsilon>0$  such that the super-linear terms are exactly zero,  i", "word_idx": 8896, "sentence_idx": 110, "label": "unlabeled"}, {"type": "text", "expr": " ;  $f(\\mathbf{x}+\\Delta\\mathbf{x})=f(\\mathbf{x})+\\nabla_{x}f(\\mathbf{x})^{T}(%\n\\Delta\\mathbf{x})$ ,  i", "word_idx": 9049, "sentence_idx": 111, "label": "none"}, {"type": "text", "expr": " ;  $\\mathbf{x}$  and  $\\Delta\\mathbf{x}$  lie on the same linear surface", "word_idx": 9152, "sentence_idx": 112, "label": "none"}, {"type": "math", "expr": "$$\\frac{\\mathrm{d}\\sigma(z)}{\\mathrm{d}z}$$", "word_idx": 9225, "sentence_idx": 113, "label": "unlabeled"}, {"type": "math", "expr": "$$z=0$$", "word_idx": 9264, "sentence_idx": 114, "label": "unlabeled"}, {"type": "math", "expr": "$$\\epsilon>0$$", "word_idx": 9267, "sentence_idx": 115, "label": "unlabeled"}, {"type": "math", "expr": "$$f(\\mathbf{x}+\\Delta\\mathbf{x})=f(\\mathbf{x})+\\nabla_{x}f(\\mathbf{x})^{T}(%\n\\Delta\\mathbf{x})$$", "word_idx": 9277, "sentence_idx": 116, "label": "none"}, {"type": "math", "expr": "$$\\mathbf{x}$$", "word_idx": 9369, "sentence_idx": 117, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Delta\\mathbf{x}$$", "word_idx": 9379, "sentence_idx": 118, "label": "unknown"}, {"type": "text", "expr": "2  Invariance to weight and architecture specification", "word_idx": 9395, "sentence_idx": 119, "label": "unlabeled"}, {"type": "text", "expr": "One useful property of the Jacobian is that its size does not depend on the network architecture", "word_idx": 9449, "sentence_idx": 120, "label": "unlabeled"}, {"type": "text", "expr": " For  $k$  output classes, and input dimension  $D$  , the Jacobian of a neural network is of dimension  $D\\times k$ ", "word_idx": 9545, "sentence_idx": 121, "label": "unlabeled"}, {"type": "text", "expr": " This means that one can compare Jacobians of different architectures", "word_idx": 9662, "sentence_idx": 122, "label": "unlabeled"}, {"type": "math", "expr": "$$D\\times k$$", "word_idx": 9731, "sentence_idx": 123, "label": "unlabeled"}, {"type": "text", "expr": "Another useful property is that for a given neural network architecture, different weight configurations can lead to the same Jacobian", "word_idx": 9740, "sentence_idx": 124, "label": "unlabeled"}, {"type": "text", "expr": " One simple example of this is permutation symmetry of neurons in intermediate hidden layers", "word_idx": 9874, "sentence_idx": 125, "label": "unlabeled"}, {"type": "text", "expr": " It is easy to see different permutations of neurons leave the Jacobian unchanged (as they have the same underlying function)", "word_idx": 9966, "sentence_idx": 126, "label": "unlabeled"}, {"type": "text", "expr": " In general, because of redundancy of neural network models and non-convexity of the loss surface, several different weight configurations can end up having similar Jacobians", "word_idx": 10091, "sentence_idx": 127, "label": "unlabeled"}, {"type": "text", "expr": "Another useful property is that for a given neural network architecture, different weight configurations can lead to the same Jacobian", "word_idx": 10265, "sentence_idx": 128, "label": "unlabeled"}, {"type": "text", "expr": " One simple example of this is permutation symmetry of neurons in intermediate hidden layers", "word_idx": 10399, "sentence_idx": 129, "label": "unlabeled"}, {"type": "text", "expr": " It is easy to see different permutations of neurons leave the Jacobian unchanged (as they have the same underlying function)", "word_idx": 10491, "sentence_idx": 130, "label": "unlabeled"}, {"type": "text", "expr": " In general, because of redundancy of neural network models and non-convexity of the loss surface, several different weight configurations can end up having similar Jacobians", "word_idx": 10616, "sentence_idx": 131, "label": "unlabeled"}, {"type": "text", "expr": "Given these desirable properties, we consider using the Jacobian to perform knowledge transfer between neural networks of different architectures", "word_idx": 10790, "sentence_idx": 132, "label": "unlabeled"}, {"type": "text", "expr": " Note that these properties hold trivially for output activations also", "word_idx": 10935, "sentence_idx": 133, "label": "unlabeled"}, {"type": "text", "expr": " Thus it seems sensible that both these quantities must be used for knowledge transfer", "word_idx": 11005, "sentence_idx": 134, "label": "unlabeled"}, {"type": "text", "expr": " However, the important practical question remains: how exactly should this be done?", "word_idx": 11091, "sentence_idx": 135, "label": "unlabeled"}, {"type": "text", "expr": "Given these desirable properties, we consider using the Jacobian to perform knowledge transfer between neural networks of different architectures", "word_idx": 11175, "sentence_idx": 136, "label": "unlabeled"}, {"type": "text", "expr": " Note that these properties hold trivially for output activations also", "word_idx": 11320, "sentence_idx": 137, "label": "unlabeled"}, {"type": "text", "expr": " Thus it seems sensible that both these quantities must be used for knowledge transfer", "word_idx": 11390, "sentence_idx": 138, "label": "unlabeled"}, {"type": "text", "expr": " However, the important practical question remains: how exactly should this be done?", "word_idx": 11476, "sentence_idx": 139, "label": "unlabeled"}, {"type": "text", "expr": "4  Distillation", "word_idx": 11560, "sentence_idx": 140, "label": "unlabeled"}, {"type": "text", "expr": "We consider the problem of improving distillation using Jacobians", "word_idx": 11575, "sentence_idx": 141, "label": "unlabeled"}, {"type": "text", "expr": " This problem of distillation may be posed as follows: given a  teacher  network  $\\mathcal{T}$  which is trained on a dataset  $\\mathcal{D}$ , we wish to enhance the training of a student network  $\\mathcal{S}$  on  $\\mathcal{D}$  using \u201chints\u201d from  $\\mathcal{T}$ ", "word_idx": 11640, "sentence_idx": 142, "label": "unlabeled"}, {"type": "text", "expr": " Classically, such \u201chints\u201d involve activations of the output layer or some intermediate layers", "word_idx": 11906, "sentence_idx": 143, "label": "unlabeled"}, {"type": "text", "expr": " Recent works\u00a0 \\citep czarnecki2017sobolev, zagoruyko2016paying sought to match the Jacobians of  $\\mathcal{S}$  and  $\\mathcal{T}$ ", "word_idx": 12000, "sentence_idx": 144, "label": "unlabeled"}, {"type": "text", "expr": " However, two aspects are not clear in these formalisms: (i) what penalty term must be used between Jacobians, and (ii) how this idea of matching Jacobians relates to simpler methods such as activation matching\u00a0 \\citep ba2014deep, hinton2015distilling", "word_idx": 12132, "sentence_idx": 145, "label": "unlabeled"}, {"type": "text", "expr": " To resolve these issues, we make the following claim", "word_idx": 12383, "sentence_idx": 146, "label": "unlabeled"}, {"type": "text", "expr": "teacher", "word_idx": 12436, "sentence_idx": 147, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{T}$$", "word_idx": 12443, "sentence_idx": 148, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{D}$$", "word_idx": 12454, "sentence_idx": 149, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{S}$$", "word_idx": 12465, "sentence_idx": 150, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{D}$$", "word_idx": 12476, "sentence_idx": 151, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{T}$$", "word_idx": 12487, "sentence_idx": 152, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 12498, "sentence_idx": 153, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{S}$$", "word_idx": 12504, "sentence_idx": 154, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{T}$$", "word_idx": 12515, "sentence_idx": 155, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 12526, "sentence_idx": 156, "label": "unlabeled"}, {"type": "text", "expr": "Claim ", "word_idx": 12532, "sentence_idx": 157, "label": "unlabeled"}, {"type": "text", "expr": "Claim", "word_idx": 12538, "sentence_idx": 158, "label": "unlabeled"}, {"type": "text", "expr": "Matching Jacobians of two networks is equivalent to matching soft targets with noise added to the inputs during training", "word_idx": 12543, "sentence_idx": 159, "label": "unlabeled"}, {"type": "text", "expr": "Matching Jacobians of two networks is equivalent to matching soft targets with noise added to the inputs during training", "word_idx": 12663, "sentence_idx": 160, "label": "unlabeled"}, {"type": "text", "expr": "Matching Jacobians of two networks is equivalent to matching soft targets with noise added to the inputs during training", "word_idx": 12783, "sentence_idx": 161, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:  Illustration of teacher-student learning in a simple 1D case", "word_idx": 12903, "sentence_idx": 162, "label": "unlabeled"}, {"type": "text", "expr": " Here, x-axis is the input data, and y-axis denotes function outputs", "word_idx": 12974, "sentence_idx": 163, "label": "unlabeled"}, {"type": "text", "expr": " Given a limited number of data points, there exist multiple student functions consistent with the data", "word_idx": 13042, "sentence_idx": 164, "label": "unlabeled"}, {"type": "text", "expr": " How do we select the hypothesis closest to the teacher\u2019s? There are two equivalent solutions: either by augmenting the data set by adding noise to the inputs or by directly matching slopes (Jacobians) of the function at the data points", "word_idx": 13145, "sentence_idx": 165, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:", "word_idx": 13381, "sentence_idx": 166, "label": "unlabeled"}, {"type": "text", "expr": "More concretely, we make the following proposition", "word_idx": 13390, "sentence_idx": 167, "label": "unlabeled"}, {"type": "text", "expr": "More concretely, we make the following proposition", "word_idx": 13440, "sentence_idx": 168, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 1 ", "word_idx": 13490, "sentence_idx": 169, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 1", "word_idx": 13504, "sentence_idx": 170, "label": "unlabeled"}, {"type": "text", "expr": "Consider the squared error cost function for matching soft targets of two neural networks with  $k$ -length targets ( $\\in\\mathbb{R}^{k}$ ), given by  $\\ell(\\mathcal{T}(\\mathbf{x}),\\mathcal{S}(\\mathbf{x}))=\\sum_{i=1}^{k}(\\mathcal{%\nT}^{i}(\\mathbf{x})-\\mathcal{S}^{i}(\\mathbf{x}))^{2}$ , where  $\\mathbf{x}\\in\\mathbb{R}^{D}$  is an input data point", "word_idx": 13517, "sentence_idx": 171, "label": "unlabeled"}, {"type": "text", "expr": " Let  $\\bm{\\xi}~{}(\\in\\mathbb{R}^{D})=\\sigma~{}\\mathbf{z}$  be a scaled version of a unit normal random variable  $\\mathbf{z}~{}\\in\\mathbb{R}^{D}$  with scaling factor  $\\sigma\\in\\mathbb{R}$ ", "word_idx": 13864, "sentence_idx": 172, "label": "unlabeled"}, {"type": "text", "expr": " Then the following is true", "word_idx": 14055, "sentence_idx": 173, "label": "unlabeled"}, {"type": "math", "expr": "$$\\in\\mathbb{R}^{k}$$", "word_idx": 14082, "sentence_idx": 174, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ell(\\mathcal{T}(\\mathbf{x}),\\mathcal{S}(\\mathbf{x}))=\\sum_{i=1}^{k}(\\mathcal{%\nT}^{i}(\\mathbf{x})-\\mathcal{S}^{i}(\\mathbf{x}))^{2}$$", "word_idx": 14099, "sentence_idx": 175, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}\\in\\mathbb{R}^{D}$$", "word_idx": 14230, "sentence_idx": 176, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{\\xi}~{}(\\in\\mathbb{R}^{D})=\\sigma~{}\\mathbf{z}$$", "word_idx": 14257, "sentence_idx": 177, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{z}~{}\\in\\mathbb{R}^{D}$$", "word_idx": 14307, "sentence_idx": 178, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma\\in\\mathbb{R}$$", "word_idx": 14337, "sentence_idx": 179, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\mathbb{E}_{\\bm{\\xi}}\\left[\\sum_{i=1}^{k}\\left(\\mathcal{T}^{i}(%\n\\mathbf{x}+\\bm{\\xi})-\\mathcal{S}^{i}(\\mathbf{x}+\\bm{\\xi})\\right)^{2}\\right]$", "word_idx": 14356, "sentence_idx": 180, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\mathbb{E}_{\\bm{\\xi}}\\left[\\sum_{i=1}^{k}\\left(\\mathcal{T}^{i}(%\n\\mathbf{x}+\\bm{\\xi})-\\mathcal{S}^{i}(\\mathbf{x}+\\bm{\\xi})\\right)^{2}\\right]$$", "word_idx": 14511, "sentence_idx": 181, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\sum_{i=1}^{k}\\left(\\mathcal{T}^{i}(\\mathbf{x})-\\mathcal{S}^{i}(%\n\\mathbf{x})\\right)^{2}+\\sigma^{2}\\sum_{i=1}^{k}\\|\\nabla_{x}\\mathcal{T}^{i}(%\n\\mathbf{x})-\\nabla_{x}\\mathcal{S}^{i}(\\mathbf{x})\\|_{2}^{2}$", "word_idx": 14664, "sentence_idx": 182, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\sum_{i=1}^{k}\\left(\\mathcal{T}^{i}(\\mathbf{x})-\\mathcal{S}^{i}(%\n\\mathbf{x})\\right)^{2}+\\sigma^{2}\\sum_{i=1}^{k}\\|\\nabla_{x}\\mathcal{T}^{i}(%\n\\mathbf{x})-\\nabla_{x}\\mathcal{S}^{i}(\\mathbf{x})\\|_{2}^{2}$$", "word_idx": 14882, "sentence_idx": 183, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle+\\mathcal{O}(\\sigma^{4})$", "word_idx": 15098, "sentence_idx": 184, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle+\\mathcal{O}(\\sigma^{4})$$", "word_idx": 15137, "sentence_idx": 185, "label": "unlabeled"}, {"type": "text", "expr": "Notice that in this expression, we have decomposed the loss function into two components: one representing the usual distillation loss on the samples, and the second regularizer term representing the Jacobian matching loss", "word_idx": 15174, "sentence_idx": 186, "label": "unlabeled"}, {"type": "text", "expr": " The higher order error terms are small for small  $\\sigma$  and can be ignored", "word_idx": 15396, "sentence_idx": 187, "label": "unlabeled"}, {"type": "text", "expr": " The above proposition is a simple consequence of using the first-order Taylor series expansion around  $x$ ", "word_idx": 15475, "sentence_idx": 188, "label": "unlabeled"}, {"type": "text", "expr": " Note that the error term is exactly zero for piecewise-linear nets", "word_idx": 15583, "sentence_idx": 189, "label": "unlabeled"}, {"type": "text", "expr": " An analogous statement is true for the case of cross entropy error between soft targets, leading to:", "word_idx": 15650, "sentence_idx": 190, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma$$", "word_idx": 15751, "sentence_idx": 191, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\mathbb{E}_{\\bm{\\xi}}\\left[-\\sum_{i=1}^{k}\\mathcal{T}^{i}_{s}(%\n\\mathbf{x}+\\bm{\\xi})\\log\\left(\\mathcal{S}^{i}_{s}(\\mathbf{x}+\\bm{\\xi})\\right)\\right]$", "word_idx": 15757, "sentence_idx": 192, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\mathbb{E}_{\\bm{\\xi}}\\left[-\\sum_{i=1}^{k}\\mathcal{T}^{i}_{s}(%\n\\mathbf{x}+\\bm{\\xi})\\log\\left(\\mathcal{S}^{i}_{s}(\\mathbf{x}+\\bm{\\xi})\\right)\\right]$$", "word_idx": 15920, "sentence_idx": 193, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\approx-\\sum_{i=1}^{k}\\mathcal{T}^{i}_{s}(\\mathbf{x})\\log(%\n\\mathcal{S}^{i}_{s}(\\mathbf{x}))~{}-~{}\\sigma^{2}\\sum_{i=1}^{k}\\frac{\\nabla_{x%\n}\\mathcal{T}^{i}_{s}(\\mathbf{x})^{T}\\nabla_{x}\\mathcal{S}^{i}_{s}(\\mathbf{x})}%\n{\\mathcal{S}^{i}_{s}(\\mathbf{x})}$", "word_idx": 16081, "sentence_idx": 194, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\approx-\\sum_{i=1}^{k}\\mathcal{T}^{i}_{s}(\\mathbf{x})\\log(%\n\\mathcal{S}^{i}_{s}(\\mathbf{x}))~{}-~{}\\sigma^{2}\\sum_{i=1}^{k}\\frac{\\nabla_{x%\n}\\mathcal{T}^{i}_{s}(\\mathbf{x})^{T}\\nabla_{x}\\mathcal{S}^{i}_{s}(\\mathbf{x})}%\n{\\mathcal{S}^{i}_{s}(\\mathbf{x})}$$", "word_idx": 16349, "sentence_idx": 195, "label": "unlabeled"}, {"type": "text", "expr": "where  $\\mathcal{T}^{i}_{s}(\\mathbf{x})$  denotes the same network  $\\mathcal{T}^{i}(\\mathbf{x})$  but with a softmax or sigmoid (with temperature parameter  $T$  if needed) added at the end", "word_idx": 16615, "sentence_idx": 196, "label": "unlabeled"}, {"type": "text", "expr": " We do not write the super-linear error terms for convenience", "word_idx": 16805, "sentence_idx": 197, "label": "unlabeled"}, {"type": "text", "expr": " This shows that the Jacobian matching loss depends crucially on the loss used to match activations", "word_idx": 16866, "sentence_idx": 198, "label": "unlabeled"}, {"type": "text", "expr": " This observation can be used in practice to pick appropriate loss function by choosing a specific noise model of interest", "word_idx": 16965, "sentence_idx": 199, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{T}^{i}_{s}(\\mathbf{x})$$", "word_idx": 17087, "sentence_idx": 200, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{T}^{i}(\\mathbf{x})$$", "word_idx": 17118, "sentence_idx": 201, "label": "unlabeled"}, {"type": "text", "expr": "To summarize, these statements show that matching Jacobians is a natural consequence of matching not only the raw CNN outputs at the given data points, but also at the infinitely many data points nearby", "word_idx": 17145, "sentence_idx": 202, "label": "unlabeled"}, {"type": "text", "expr": " This is illustrated in Figure  1 , which shows that by matching on a noise-augmented dataset, the student is able to mimic the teacher better", "word_idx": 17347, "sentence_idx": 203, "label": "unlabeled"}, {"type": "text", "expr": "We can use a similar idea to derive regularizers for the case of regular neural network training as well", "word_idx": 17489, "sentence_idx": 204, "label": "unlabeled"}, {"type": "text", "expr": " These regularizers seek to make the underlying model  robust  to small amounts of noise added to the inputs", "word_idx": 17593, "sentence_idx": 205, "label": "unlabeled"}, {"type": "text", "expr": "robust", "word_idx": 17701, "sentence_idx": 206, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 2 ", "word_idx": 17707, "sentence_idx": 207, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 2", "word_idx": 17721, "sentence_idx": 208, "label": "unlabeled"}, {"type": "text", "expr": "Consider the squared error cost function for training a neural network with  $k$  targets, given by  $\\ell(y(\\mathbf{x}),\\mathcal{S}(\\mathbf{x}))=\\sum_{i=1}^{k}(y^{i}(\\mathbf{x})-%\n\\mathcal{S}^{i}(\\mathbf{x}))^{2}$ , where  $\\mathbf{x}\\in\\mathbb{R}^{D}$  is an input data point, and  $y^{i}(\\mathbf{x})$  is the  $i^{th}$  target output", "word_idx": 17734, "sentence_idx": 209, "label": "unlabeled"}, {"type": "text", "expr": " Let  $\\bm{\\xi}~{}(\\in\\mathbb{R}^{D})=\\sigma~{}\\mathbf{z}$  be a scaled version of a unit normal random variable  $\\mathbf{z}~{}\\in\\mathbb{R}^{D}$  with scaling factor  $\\sigma\\in\\mathbb{R}$ ", "word_idx": 18070, "sentence_idx": 210, "label": "unlabeled"}, {"type": "text", "expr": " Then the following is true", "word_idx": 18261, "sentence_idx": 211, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ell(y(\\mathbf{x}),\\mathcal{S}(\\mathbf{x}))=\\sum_{i=1}^{k}(y^{i}(\\mathbf{x})-%\n\\mathcal{S}^{i}(\\mathbf{x}))^{2}$$", "word_idx": 18288, "sentence_idx": 212, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}\\in\\mathbb{R}^{D}$$", "word_idx": 18399, "sentence_idx": 213, "label": "unlabeled"}, {"type": "math", "expr": "$$y^{i}(\\mathbf{x})$$", "word_idx": 18426, "sentence_idx": 214, "label": "unlabeled"}, {"type": "math", "expr": "$$i^{th}$$", "word_idx": 18443, "sentence_idx": 215, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{\\xi}~{}(\\in\\mathbb{R}^{D})=\\sigma~{}\\mathbf{z}$$", "word_idx": 18449, "sentence_idx": 216, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{z}~{}\\in\\mathbb{R}^{D}$$", "word_idx": 18499, "sentence_idx": 217, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma\\in\\mathbb{R}$$", "word_idx": 18529, "sentence_idx": 218, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\mathbb{E}_{\\bm{\\xi}}\\left[\\sum_{i=1}^{k}\\left(y^{i}(\\mathbf{x})-%\n\\mathcal{S}^{i}(\\mathbf{x}+\\bm{\\xi})\\right)^{2}\\right]$", "word_idx": 18548, "sentence_idx": 219, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\mathbb{E}_{\\bm{\\xi}}\\left[\\sum_{i=1}^{k}\\left(y^{i}(\\mathbf{x})-%\n\\mathcal{S}^{i}(\\mathbf{x}+\\bm{\\xi})\\right)^{2}\\right]$$", "word_idx": 18684, "sentence_idx": 220, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\sum_{i=1}^{k}\\left(y^{i}(\\mathbf{x})-\\mathcal{S}^{i}(\\mathbf{x}%\n)\\right)^{2}+\\sigma^{2}\\sum_{i=1}^{k}\\|\\nabla_{x}\\mathcal{S}^{i}(\\mathbf{x})\\|%\n_{2}^{2}+\\mathcal{O}(\\sigma^{4})$", "word_idx": 18818, "sentence_idx": 221, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\sum_{i=1}^{k}\\left(y^{i}(\\mathbf{x})-\\mathcal{S}^{i}(\\mathbf{x}%\n)\\right)^{2}+\\sigma^{2}\\sum_{i=1}^{k}\\|\\nabla_{x}\\mathcal{S}^{i}(\\mathbf{x})\\|%\n_{2}^{2}+\\mathcal{O}(\\sigma^{4})$$", "word_idx": 19012, "sentence_idx": 222, "label": "unlabeled"}, {"type": "text", "expr": "A statement similar to Proposition  2  has been previously derived by\u00a0 \\citet bishop1995training, who observed that the regularizer term for linear models corresponds exactly to the well-known Tikhonov regularizer", "word_idx": 19204, "sentence_idx": 223, "label": "unlabeled"}, {"type": "text", "expr": " This regularizer was also proposed by\u00a0 \\citet drucker1992improving", "word_idx": 19417, "sentence_idx": 224, "label": "unlabeled"}, {"type": "text", "expr": " The  $\\ell_{2}$  weight decay regularizer for neural networks can be derived by applying this regularizer layer-wise separately", "word_idx": 19484, "sentence_idx": 225, "label": "unlabeled"}, {"type": "text", "expr": " However, we see here that a more appropriate way to ensure noise robustness is to penalize the norm of the Jacobian rather than weights", "word_idx": 19612, "sentence_idx": 226, "label": "unlabeled"}, {"type": "text", "expr": " We can derive a similar result for the case of cross-entropy error as well, which is given by -", "word_idx": 19748, "sentence_idx": 227, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 19844, "sentence_idx": 228, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 19850, "sentence_idx": 229, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ell_{2}$$", "word_idx": 19856, "sentence_idx": 230, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\mathbb{E}_{\\bm{\\xi}}\\left[-\\sum_{i=1}^{k}y^{i}(\\mathbf{x})\\log(%\n\\mathcal{S}^{i}_{s}(\\mathbf{x}+\\bm{\\xi}))\\right]$", "word_idx": 19864, "sentence_idx": 231, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\mathbb{E}_{\\bm{\\xi}}\\left[-\\sum_{i=1}^{k}y^{i}(\\mathbf{x})\\log(%\n\\mathcal{S}^{i}_{s}(\\mathbf{x}+\\bm{\\xi}))\\right]$$", "word_idx": 19993, "sentence_idx": 232, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\approx-\\sum_{i=1}^{k}y^{i}(\\mathbf{x})\\log(\\mathcal{S}^{i}_{s}(%\n\\mathbf{x}))~{}+~{}\\sigma^{2}\\sum_{i=1}^{k}y^{i}(\\mathbf{x})\\frac{\\|\\nabla_{x}%\n\\mathcal{S}^{i}_{s}(\\mathbf{x})\\|_{2}^{2}}{\\mathcal{S}^{i}_{s}(\\mathbf{x})^{2}}$", "word_idx": 20120, "sentence_idx": 233, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\approx-\\sum_{i=1}^{k}y^{i}(\\mathbf{x})\\log(\\mathcal{S}^{i}_{s}(%\n\\mathbf{x}))~{}+~{}\\sigma^{2}\\sum_{i=1}^{k}y^{i}(\\mathbf{x})\\frac{\\|\\nabla_{x}%\n\\mathcal{S}^{i}_{s}(\\mathbf{x})\\|_{2}^{2}}{\\mathcal{S}^{i}_{s}(\\mathbf{x})^{2}}$$", "word_idx": 20360, "sentence_idx": 234, "label": "unlabeled"}, {"type": "text", "expr": "We notice here again that the regularizer involves  $\\mathcal{S}^{i}_{s}(\\mathbf{x})$ , which has the sigmoid / softmax nonlinearity applied on top of the final layer of  $\\mathcal{S}^{i}(\\mathbf{x})$ ", "word_idx": 20598, "sentence_idx": 235, "label": "unlabeled"}, {"type": "text", "expr": " Deriving all the above results is a simple matter of using first-order Taylor series expansions, and a second-order expansion in the case of Equation  3 ", "word_idx": 20799, "sentence_idx": 236, "label": "unlabeled"}, {"type": "text", "expr": " Proof is provided in the supplementary material", "word_idx": 20953, "sentence_idx": 237, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{S}^{i}_{s}(\\mathbf{x})$$", "word_idx": 21001, "sentence_idx": 238, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{S}^{i}(\\mathbf{x})$$", "word_idx": 21032, "sentence_idx": 239, "label": "unlabeled"}, {"type": "text", "expr": "In all cases above we see that the regularizers for cross-entropy error term seem more unstable when compared to those for squared error", "word_idx": 21059, "sentence_idx": 240, "label": "unlabeled"}, {"type": "text", "expr": " We find that this is true experimentally as well", "word_idx": 21195, "sentence_idx": 241, "label": "unlabeled"}, {"type": "text", "expr": " As a result, we use squared error loss for distillation", "word_idx": 21244, "sentence_idx": 242, "label": "unlabeled"}, {"type": "text", "expr": "In all cases above we see that the regularizers for cross-entropy error term seem more unstable when compared to those for squared error", "word_idx": 21300, "sentence_idx": 243, "label": "unlabeled"}, {"type": "text", "expr": " We find that this is true experimentally as well", "word_idx": 21436, "sentence_idx": 244, "label": "unlabeled"}, {"type": "text", "expr": " As a result, we use squared error loss for distillation", "word_idx": 21485, "sentence_idx": 245, "label": "unlabeled"}, {"type": "text", "expr": "1  Approximating the Full Jacobian", "word_idx": 21541, "sentence_idx": 246, "label": "unlabeled"}, {"type": "text", "expr": "One can see that both in the case of Proposition  1  and  2 , we are required to compute the full Jacobian", "word_idx": 21575, "sentence_idx": 247, "label": "unlabeled"}, {"type": "text", "expr": " This is computationally expensive, and sometimes unnecessary", "word_idx": 21681, "sentence_idx": 248, "label": "unlabeled"}, {"type": "text", "expr": " For example, Equation  3  requires only the terms where  $y^{i}(\\mathbf{x})$  is non-zero", "word_idx": 21742, "sentence_idx": 249, "label": "unlabeled"}, {"type": "math", "expr": "$$y^{i}(\\mathbf{x})$$", "word_idx": 21832, "sentence_idx": 250, "label": "unlabeled"}, {"type": "text", "expr": "In general, we can approximate the summation of Jacobian terms with the one with largest magnitude", "word_idx": 21849, "sentence_idx": 251, "label": "unlabeled"}, {"type": "text", "expr": " However, we cannot estimate this without computing the Jacobians themselves", "word_idx": 21947, "sentence_idx": 252, "label": "unlabeled"}, {"type": "text", "expr": " As a result, we use a heuristic where the only output variable involving the correct answer  $c\\in[1,k]$  is used for computing the Jacobian", "word_idx": 22023, "sentence_idx": 253, "label": "unlabeled"}, {"type": "text", "expr": " This corresponds to the case of Equation  3 ", "word_idx": 22164, "sentence_idx": 254, "label": "unlabeled"}, {"type": "text", "expr": " Alternately, if we do not want to use the labels, we may instead use the output variable with the largest magnitude, as it often corresponds to the right label (for good models)", "word_idx": 22209, "sentence_idx": 255, "label": "unlabeled"}, {"type": "math", "expr": "$$c\\in[1,k]$$", "word_idx": 22387, "sentence_idx": 256, "label": "unlabeled"}, {"type": "text", "expr": "5  Transfer Learning", "word_idx": 22396, "sentence_idx": 257, "label": "unlabeled"}, {"type": "text", "expr": "We now use our Jacobian matching machinery to transfer learning problems", "word_idx": 22416, "sentence_idx": 258, "label": "unlabeled"}, {"type": "text", "expr": " In computer vision, transfer learning is often done by fine-tuning\u00a0 \\citep yosinski2014transferable, where models pre-trained on a large dataset  $\\mathcal{D}_{l}$ , such as Imagenet\u00a0 \\citep russakovsky2015imagenet, are used as initialization for training on another smaller dataset  $\\mathcal{D}_{s}$ ", "word_idx": 22488, "sentence_idx": 259, "label": "unlabeled"}, {"type": "text", "expr": " We can also refer to these as the source dataset and the target dataset respectively", "word_idx": 22791, "sentence_idx": 260, "label": "unlabeled"}, {"type": "text", "expr": " Practically, this means that the architecture used for fine-tuning must be the same as that of the pre-trained network, which is restrictive", "word_idx": 22876, "sentence_idx": 261, "label": "unlabeled"}, {"type": "text", "expr": " We would like to develop transfer learning methods where the architectures of the pre-trained network and target \u201cfine-tuned\u201d network can be arbitrarily different", "word_idx": 23017, "sentence_idx": 262, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 23180, "sentence_idx": 263, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{D}_{l}$$", "word_idx": 23186, "sentence_idx": 264, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 23201, "sentence_idx": 265, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{D}_{s}$$", "word_idx": 23207, "sentence_idx": 266, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:  Illustration of our proposed method for transfer learning", "word_idx": 23222, "sentence_idx": 267, "label": "unlabeled"}, {"type": "text", "expr": " We match the output activations of a pre-trained Imagenet network similar to LwF\u00a0 \\citep li2016learning", "word_idx": 23290, "sentence_idx": 268, "label": "unlabeled"}, {"type": "text", "expr": " We also match aggregated activations or \u201cattention\u201d maps between networks, similar to the work of\u00a0 \\citet zagoruyko2016paying", "word_idx": 23394, "sentence_idx": 269, "label": "unlabeled"}, {"type": "text", "expr": " We propose to match Jacobians of (aggregated) attention maps w", "word_idx": 23520, "sentence_idx": 270, "label": "unlabeled"}, {"type": "text", "expr": " inputs", "word_idx": 23583, "sentence_idx": 271, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:", "word_idx": 23590, "sentence_idx": 272, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 23599, "sentence_idx": 273, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 23605, "sentence_idx": 274, "label": "unlabeled"}, {"type": "text", "expr": "One way to achieve this is by distillation: we match output activations of a pre-trained teacher network and an untrained student network", "word_idx": 23611, "sentence_idx": 275, "label": "unlabeled"}, {"type": "text", "expr": " However, this procedure is not general as the target dataset may not share the same label space as the large source dataset", "word_idx": 23748, "sentence_idx": 276, "label": "unlabeled"}, {"type": "text", "expr": " This leads to size mismatch between output activations of the teacher and the student", "word_idx": 23872, "sentence_idx": 277, "label": "unlabeled"}, {"type": "text", "expr": " To overcome this, we can design the student network to have two sets of outputs (or two output \u201cbranches\u201d), one with the label space of the smaller target dataset, while the other with that of the larger source dataset", "word_idx": 23958, "sentence_idx": 278, "label": "unlabeled"}, {"type": "text", "expr": " This leads to the method proposed by\u00a0 \\citet li2016learning, called \u201cLearning without Forgetting\u201d ( LwF )", "word_idx": 24177, "sentence_idx": 279, "label": "unlabeled"}, {"type": "text", "expr": " Note that similar methods were concurrently developed by\u00a0 \\citet jung2016less and\u00a0 \\citet furlanello2016active", "word_idx": 24283, "sentence_idx": 280, "label": "unlabeled"}, {"type": "text", "expr": " In this method, the student network is trained with a composite loss function involving two terms, one in each output branch", "word_idx": 24394, "sentence_idx": 281, "label": "unlabeled"}, {"type": "text", "expr": " The two objectives are  (1)  matching ground truth labels on the target dataset, and  (2)  matching the activations of the student network and a pre-trained teacher network on the target dataset", "word_idx": 24519, "sentence_idx": 282, "label": "unlabeled"}, {"type": "text", "expr": " This is illustrated in Figure  2 ", "word_idx": 24714, "sentence_idx": 283, "label": "unlabeled"}, {"type": "text", "expr": " Crucially, these losses are matched only on the target dataset, and the source data is untouched", "word_idx": 24748, "sentence_idx": 284, "label": "unlabeled"}, {"type": "text", "expr": " This is conceptually different from distillation, where the teacher network is trained on the dataset being used", "word_idx": 24845, "sentence_idx": 285, "label": "unlabeled"}, {"type": "text", "expr": " In LwF, the pre-trained teacher is not trained on the target dataset", "word_idx": 24958, "sentence_idx": 286, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 25027, "sentence_idx": 287, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 25033, "sentence_idx": 288, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 25039, "sentence_idx": 289, "label": "unlabeled"}, {"type": "text", "expr": "This makes it problematic to apply our Jacobian matching framework to LwF", "word_idx": 25045, "sentence_idx": 290, "label": "unlabeled"}, {"type": "text", "expr": " For distillation, it is clear that adding input noise (or Jacobian matching) can improve overall matching as shown in Figure  1 ", "word_idx": 25118, "sentence_idx": 291, "label": "unlabeled"}, {"type": "text", "expr": " For the case of LwF, it is not clear whether improving matching between teacher and student will necessarily improve transfer learning performance", "word_idx": 25247, "sentence_idx": 292, "label": "unlabeled"}, {"type": "text", "expr": " This is especially because the teacher is not trained on the target dataset, and can potentially produce noisy or incorrect results on this data", "word_idx": 25394, "sentence_idx": 293, "label": "unlabeled"}, {"type": "text", "expr": " To resolve this ambiguity, we shall now connect LwF with distillation", "word_idx": 25539, "sentence_idx": 294, "label": "unlabeled"}, {"type": "text", "expr": "1  LwF as Distillation", "word_idx": 25609, "sentence_idx": 295, "label": "unlabeled"}, {"type": "text", "expr": "In the discussion below we shall only consider the distillation-like loss of LwF, and ignore the branch which matches ground truth labels", "word_idx": 25631, "sentence_idx": 296, "label": "unlabeled"}, {"type": "text", "expr": " For LwF to work well, it must be the case that the activations of the pre-trained teacher network on the target dataset must contain information about the source dataset ( i", "word_idx": 25768, "sentence_idx": 297, "label": "unlabeled"}, {"type": "text", "expr": " ; Imagenet)", "word_idx": 25942, "sentence_idx": 298, "label": "unlabeled"}, {"type": "text", "expr": " The attractiveness of LwF lies in the fact that this is done without explicitly using Imagenet", "word_idx": 25954, "sentence_idx": 299, "label": "unlabeled"}, {"type": "text", "expr": " Here, we make the claim that  LwF approximates distillation on (a part of) Imagenet", "word_idx": 26049, "sentence_idx": 300, "label": "unlabeled"}, {"type": "text", "expr": "LwF approximates distillation on (a part of) Imagenet", "word_idx": 26133, "sentence_idx": 301, "label": "unlabeled"}, {"type": "text", "expr": "Note that this is no longer a valid distance metric unlike the Hausdorff", "word_idx": 26186, "sentence_idx": 302, "label": "unlabeled"}, {"type": "text", "expr": " Given these assumptions, we are now ready to state our result", "word_idx": 26258, "sentence_idx": 303, "label": "unlabeled"}, {"type": "text", "expr": "Let  $f(\\cdot)$  be an untrained neural network,  $g(\\cdot)$  be a pre-trained network,  $\\mathbf{x},\\mathbf{y}$  be the input image and corresponding ground truth label respectively", "word_idx": 26320, "sentence_idx": 304, "label": "unlabeled"}, {"type": "text", "expr": " Let  $|\\mathcal{D}|$  be the size of the dataset  $\\mathcal{D}$ ", "word_idx": 26502, "sentence_idx": 305, "label": "unlabeled"}, {"type": "text", "expr": " Let us denote  $\\rho(\\mathbf{x})=\\ell(f(\\mathbf{x}),g(\\mathbf{x}))$  for convenience", "word_idx": 26567, "sentence_idx": 306, "label": "unlabeled"}, {"type": "text", "expr": " Assume Lipschitz continuity for  $\\rho(\\mathbf{x})$  with Lipschitz constant  $\\mathrm{K}$ , and distance metric  $\\psi_{\\mathbf{x}}$  in the input space", "word_idx": 26652, "sentence_idx": 307, "label": "unlabeled"}, {"type": "math", "expr": "$$f(\\cdot)$$", "word_idx": 26806, "sentence_idx": 308, "label": "unlabeled"}, {"type": "math", "expr": "$$g(\\cdot)$$", "word_idx": 26814, "sentence_idx": 309, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x},\\mathbf{y}$$", "word_idx": 26822, "sentence_idx": 310, "label": "unlabeled"}, {"type": "math", "expr": "$$|\\mathcal{D}|$$", "word_idx": 26843, "sentence_idx": 311, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{D}$$", "word_idx": 26856, "sentence_idx": 312, "label": "unlabeled"}, {"type": "math", "expr": "$$\\rho(\\mathbf{x})=\\ell(f(\\mathbf{x}),g(\\mathbf{x}))$$", "word_idx": 26867, "sentence_idx": 313, "label": "unlabeled"}, {"type": "math", "expr": "$$\\rho(\\mathbf{x})$$", "word_idx": 26917, "sentence_idx": 314, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{K}$$", "word_idx": 26933, "sentence_idx": 315, "label": "unlabeled"}, {"type": "math", "expr": "$$\\psi_{\\mathbf{x}}$$", "word_idx": 26943, "sentence_idx": 316, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\|\\rho(\\mathbf{x}_{1})-\\rho(\\mathbf{x}_{2})\\|\\leq\\mathrm{K}\\psi_{%\n\\mathbf{x}}(\\mathbf{x}_{1},\\mathbf{x}_{2})$", "word_idx": 26960, "sentence_idx": 317, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\|\\rho(\\mathbf{x}_{1})-\\rho(\\mathbf{x}_{2})\\|\\leq\\mathrm{K}\\psi_{%\n\\mathbf{x}}(\\mathbf{x}_{1},\\mathbf{x}_{2})$$", "word_idx": 27084, "sentence_idx": 318, "label": "unlabeled"}, {"type": "text", "expr": "Note here that the distance in the input space need not be in terms of pixelwise distances, but can also be neural net feature distance, for example", "word_idx": 27206, "sentence_idx": 319, "label": "unlabeled"}, {"type": "text", "expr": " Let us also define an assymetric version of the Hausdorff distance between two sets  $A,B$ :", "word_idx": 27354, "sentence_idx": 320, "label": "unlabeled"}, {"type": "math", "expr": "$$A,B$$", "word_idx": 27447, "sentence_idx": 321, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\mathcal{H}_{a}(A,B)=\\sup_{a\\in A}\\inf_{b\\in B}\\psi_{\\mathbf{x}}(%\na,b)$", "word_idx": 27450, "sentence_idx": 322, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\mathcal{H}_{a}(A,B)=\\sup_{a\\in A}\\inf_{b\\in B}\\psi_{\\mathbf{x}}(%\na,b).$$", "word_idx": 27536, "sentence_idx": 323, "label": "unlabeled"}, {"type": "text", "expr": "Note that this is no longer a valid distance metric unlike the Hausdorff", "word_idx": 27621, "sentence_idx": 324, "label": "unlabeled"}, {"type": "text", "expr": " Given these assumptions, we are now ready to state our result", "word_idx": 27693, "sentence_idx": 325, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 3 ", "word_idx": 27755, "sentence_idx": 326, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 3", "word_idx": 27769, "sentence_idx": 327, "label": "unlabeled"}, {"type": "text", "expr": "Given the assumptions and notations described above, we have", "word_idx": 27782, "sentence_idx": 328, "label": "unlabeled"}, {"type": "text", "expr": "Given the assumptions and notations described above, we have", "word_idx": 27842, "sentence_idx": 329, "label": "unlabeled"}, {"type": "text", "expr": "Given the assumptions and notations described above, we have", "word_idx": 27902, "sentence_idx": 330, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\frac{1}{|\\mathcal{D}_{l}|}\\sum_{\\mathbf{x}\\sim\\mathcal{D}_{l}}%\n\\ell(f(\\mathbf{x}),g(\\mathbf{x}))$", "word_idx": 27962, "sentence_idx": 331, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\frac{1}{|\\mathcal{D}_{l}|}\\sum_{\\mathbf{x}\\sim\\mathcal{D}_{l}}%\n\\ell(f(\\mathbf{x}),g(\\mathbf{x}))$$", "word_idx": 28075, "sentence_idx": 332, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\leq\\max_{\\mathbf{x}\\sim\\mathcal{D}_{s}}\\ell(f(\\mathbf{x}),g(%\n\\mathbf{x}))$", "word_idx": 28186, "sentence_idx": 333, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\leq\\max_{\\mathbf{x}\\sim\\mathcal{D}_{s}}\\ell(f(\\mathbf{x}),g(%\n\\mathbf{x}))$$", "word_idx": 28276, "sentence_idx": 334, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle+\\mathrm{K}\\mathcal{H}_{a}(\\mathcal{D}_{l},\\mathcal{D}_{s})$", "word_idx": 28364, "sentence_idx": 335, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle+\\mathrm{K}\\mathcal{H}_{a}(\\mathcal{D}_{l},\\mathcal{D}_{s})$$", "word_idx": 28438, "sentence_idx": 336, "label": "unlabeled"}, {"type": "text", "expr": "On the left side of  6  we have the distillation loss on the source dataset, and on the right we have a max-loss term on the target dataset", "word_idx": 28510, "sentence_idx": 337, "label": "unlabeled"}, {"type": "text", "expr": " Note that the LwF loss is an average loss on the target dataset", "word_idx": 28649, "sentence_idx": 338, "label": "unlabeled"}, {"type": "text", "expr": " As expected, the slack terms in the inequality depends on the distance between the source and target datasets ( 7 )", "word_idx": 28713, "sentence_idx": 339, "label": "unlabeled"}, {"type": "text", "expr": " This bounds a loss related to the LwF loss ( i", "word_idx": 28829, "sentence_idx": 340, "label": "unlabeled"}, {"type": "text", "expr": "  max-loss instead of average) with the distillation loss", "word_idx": 28876, "sentence_idx": 341, "label": "unlabeled"}, {"type": "text", "expr": " If the Hausdorff distance is small, then reducing the max-loss would reduce the distillation loss as well", "word_idx": 28933, "sentence_idx": 342, "label": "unlabeled"}, {"type": "text", "expr": " A similar theory was previously presented by  \\citet ben2010theory, but with different formalisms", "word_idx": 29039, "sentence_idx": 343, "label": "unlabeled"}, {"type": "text", "expr": " Our formalism allows us to connect with Jacobian matching, which is our primary objective", "word_idx": 29137, "sentence_idx": 344, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 29227, "sentence_idx": 345, "label": "unlabeled"}, {"type": "text", "expr": "In practice, the target dataset is often much smaller than the Imagenet and has different overall statistics", "word_idx": 29233, "sentence_idx": 346, "label": "unlabeled"}, {"type": "text", "expr": " For example, the target dataset could be a restricted dataset of flower images", "word_idx": 29341, "sentence_idx": 347, "label": "unlabeled"}, {"type": "text", "expr": " In such a case, we can restrict the source dataset to its \u201cbest\u201d subset, in particular with all the irrelevant samples (those far from target dataset) removed", "word_idx": 29420, "sentence_idx": 348, "label": "unlabeled"}, {"type": "text", "expr": " This would make the Hausdorff distance smaller, and provide a tighter bound", "word_idx": 29579, "sentence_idx": 349, "label": "unlabeled"}, {"type": "text", "expr": " In our example, this involves keeping only flowers from Imagenet", "word_idx": 29655, "sentence_idx": 350, "label": "unlabeled"}, {"type": "text", "expr": "In practice, the target dataset is often much smaller than the Imagenet and has different overall statistics", "word_idx": 29720, "sentence_idx": 351, "label": "unlabeled"}, {"type": "text", "expr": " For example, the target dataset could be a restricted dataset of flower images", "word_idx": 29828, "sentence_idx": 352, "label": "unlabeled"}, {"type": "text", "expr": " In such a case, we can restrict the source dataset to its \u201cbest\u201d subset, in particular with all the irrelevant samples (those far from target dataset) removed", "word_idx": 29907, "sentence_idx": 353, "label": "unlabeled"}, {"type": "text", "expr": " This would make the Hausdorff distance smaller, and provide a tighter bound", "word_idx": 30066, "sentence_idx": 354, "label": "unlabeled"}, {"type": "text", "expr": " In our example, this involves keeping only flowers from Imagenet", "word_idx": 30142, "sentence_idx": 355, "label": "unlabeled"}, {"type": "text", "expr": "This makes intuitive sense as well: if the source and target datasets are completely different, we do not expect transfer learning (and thus LwF) to help", "word_idx": 30207, "sentence_idx": 356, "label": "unlabeled"}, {"type": "text", "expr": " The greater the overlap between source and target datasets, the smaller is the Hausdorff distance, the tighter is the bound, and the more we expect knowledge transfer to help", "word_idx": 30360, "sentence_idx": 357, "label": "unlabeled"}, {"type": "text", "expr": " Our results capture this intuition in a rigorous manner", "word_idx": 30535, "sentence_idx": 358, "label": "unlabeled"}, {"type": "text", "expr": " In addition, this predicts that Lipschitz-smooth teacher neural nets that produce small feature distance between source and target images are expected to do well in transfer learning", "word_idx": 30591, "sentence_idx": 359, "label": "unlabeled"}, {"type": "text", "expr": " Lipschitz-smoothness of models has been previously related to adversarial noise robustness\u00a0 \\citep cisse2017parseval, and to learning theory as a sufficient condition for generalization\u00a0 \\citep xu2012robustness", "word_idx": 30774, "sentence_idx": 360, "label": "unlabeled"}, {"type": "text", "expr": " It is interesting that this relates to transfer learning as well", "word_idx": 30985, "sentence_idx": 361, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 31050, "sentence_idx": 362, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 31056, "sentence_idx": 363, "label": "unlabeled"}, {"type": "text", "expr": "More importantly, this establishes LwF as a distillation method", "word_idx": 31062, "sentence_idx": 364, "label": "unlabeled"}, {"type": "text", "expr": " The following result motivates input noise added to the target dataset", "word_idx": 31125, "sentence_idx": 365, "label": "unlabeled"}, {"type": "text", "expr": "More importantly, this establishes LwF as a distillation method", "word_idx": 31196, "sentence_idx": 366, "label": "unlabeled"}, {"type": "text", "expr": " The following result motivates input noise added to the target dataset", "word_idx": 31259, "sentence_idx": 367, "label": "unlabeled"}, {"type": "text", "expr": "Corollary ", "word_idx": 31330, "sentence_idx": 368, "label": "unlabeled"}, {"type": "text", "expr": "Corollary", "word_idx": 31340, "sentence_idx": 369, "label": "unlabeled"}, {"type": "text", "expr": "For any superset  $\\mathrm{D^{\\prime}}_{s}\\supseteq\\mathcal{D}_{s}$  of the target dataset,  $\\mathcal{H}_{a}(\\mathcal{D}_{l},\\mathcal{D^{\\prime}}_{s})\\leq\\mathcal{H}_{a}(%\n\\mathcal{D}_{l},\\mathcal{D}_{s})$", "word_idx": 31349, "sentence_idx": 370, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{D^{\\prime}}_{s}\\supseteq\\mathcal{D}_{s}$$", "word_idx": 31555, "sentence_idx": 371, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{H}_{a}(\\mathcal{D}_{l},\\mathcal{D^{\\prime}}_{s})\\leq\\mathcal{H}_{a}(%\n\\mathcal{D}_{l},\\mathcal{D}_{s})$$", "word_idx": 31602, "sentence_idx": 372, "label": "unlabeled"}, {"type": "text", "expr": "Thus if we augment the target dataset  $\\mathcal{D}_{s}$  by adding noise, we expect the bound in Proposition  3  to get tighter", "word_idx": 31713, "sentence_idx": 373, "label": "unlabeled"}, {"type": "text", "expr": " This is because when we add noise to points in  $\\mathcal{D}_{s}$ , the minimum distance between points from  $\\mathcal{D}_{l}$  to  $\\mathcal{D}_{s}$  decreases", "word_idx": 31841, "sentence_idx": 374, "label": "unlabeled"}, {"type": "text", "expr": " Proofs are provided in the supplementary material", "word_idx": 32003, "sentence_idx": 375, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{D}_{s}$$", "word_idx": 32053, "sentence_idx": 376, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{D}_{s}$$", "word_idx": 32068, "sentence_idx": 377, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{D}_{l}$$", "word_idx": 32083, "sentence_idx": 378, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{D}_{s}$$", "word_idx": 32098, "sentence_idx": 379, "label": "unlabeled"}, {"type": "text", "expr": "To summarize, we have showed that a loss related to the LwF loss (max-loss) is an upper bound on the true distillation loss", "word_idx": 32113, "sentence_idx": 380, "label": "unlabeled"}, {"type": "text", "expr": " Thus by minimizing the upper bound, we can expect to reduce the distillation loss also", "word_idx": 32236, "sentence_idx": 381, "label": "unlabeled"}, {"type": "text", "expr": "To summarize, we have showed that a loss related to the LwF loss (max-loss) is an upper bound on the true distillation loss", "word_idx": 32323, "sentence_idx": 382, "label": "unlabeled"}, {"type": "text", "expr": " Thus by minimizing the upper bound, we can expect to reduce the distillation loss also", "word_idx": 32446, "sentence_idx": 383, "label": "unlabeled"}, {"type": "text", "expr": "1  Incorporating Jacobian matching", "word_idx": 32533, "sentence_idx": 384, "label": "unlabeled"}, {"type": "text", "expr": "Now that input noise and thus Jacobian matching is well motivated, we can incorporate these losses into LwF", "word_idx": 32567, "sentence_idx": 385, "label": "unlabeled"}, {"type": "text", "expr": " When we implemented this for practical deep networks we found that the optimizer wasn\u2019t able to reduce the Jacobian loss at all", "word_idx": 32674, "sentence_idx": 386, "label": "unlabeled"}, {"type": "text", "expr": " We conjecture that it might be because of a vanishing gradient effect / network degeneracy on propagation of second order gradients through the network (and not the first)", "word_idx": 32802, "sentence_idx": 387, "label": "unlabeled"}, {"type": "text", "expr": " As a result, we need an alternative way to match Jacobians", "word_idx": 32974, "sentence_idx": 388, "label": "unlabeled"}, {"type": "text", "expr": "Now that input noise and thus Jacobian matching is well motivated, we can incorporate these losses into LwF", "word_idx": 33033, "sentence_idx": 389, "label": "unlabeled"}, {"type": "text", "expr": " When we implemented this for practical deep networks we found that the optimizer wasn\u2019t able to reduce the Jacobian loss at all", "word_idx": 33140, "sentence_idx": 390, "label": "unlabeled"}, {"type": "text", "expr": " We conjecture that it might be because of a vanishing gradient effect / network degeneracy on propagation of second order gradients through the network (and not the first)", "word_idx": 33268, "sentence_idx": 391, "label": "unlabeled"}, {"type": "text", "expr": " As a result, we need an alternative way to match Jacobians", "word_idx": 33440, "sentence_idx": 392, "label": "unlabeled"}, {"type": "text", "expr": "2  Matching attention maps", "word_idx": 33499, "sentence_idx": 393, "label": "unlabeled"}, {"type": "text", "expr": "It is often insufficient to match only output activations between a teacher and a student network, especially when both networks are deep", "word_idx": 33525, "sentence_idx": 394, "label": "unlabeled"}, {"type": "text", "expr": " In such cases we can consider matching intermediate feature maps as well", "word_idx": 33662, "sentence_idx": 395, "label": "unlabeled"}, {"type": "text", "expr": " In general it is not possible to match the full feature maps between an arbitrary teacher and student network as they may have different architectures, and features sizes may never match at any layer", "word_idx": 33735, "sentence_idx": 396, "label": "unlabeled"}, {"type": "text", "expr": " However, for modern convolutional architectures, spatial sizes of certain features often match across architectures even when the number of channels doesn\u2019t", "word_idx": 33935, "sentence_idx": 397, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 \\citet zagoruyko2016paying noticed that it in such cases it helps to match channelwise aggregated activations, which they call  attention  maps", "word_idx": 34092, "sentence_idx": 398, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, this aggregation is performed by summing over squared absolute value of channels of a feature activation  $Z$ , and is given by -", "word_idx": 34237, "sentence_idx": 399, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 34381, "sentence_idx": 400, "label": "unlabeled"}, {"type": "text", "expr": "attention", "word_idx": 34387, "sentence_idx": 401, "label": "unlabeled"}, {"type": "text", "expr": "$A=AttMap(Z)=\\sum_{\\mathclap{{i\\in channels}}}|Z_{i}|^{2}$", "word_idx": 34396, "sentence_idx": 402, "label": "unlabeled"}, {"type": "math", "expr": "$$A=AttMap(Z)=\\sum_{\\mathclap{{i\\in channels}}}|Z_{i}|^{2}$$", "word_idx": 34454, "sentence_idx": 403, "label": "unlabeled"}, {"type": "text", "expr": "Further, the loss function used to match these activations is", "word_idx": 34510, "sentence_idx": 404, "label": "unlabeled"}, {"type": "text", "expr": "Further, the loss function used to match these activations is", "word_idx": 34571, "sentence_idx": 405, "label": "unlabeled"}, {"type": "text", "expr": "$\\mathrm{Match~{}Activations}=\\left|\\left|\\frac{A_{t}}{\\|A_{t}\\|_{2}}-\\frac{A_{%\ns}}{\\|A_{s}\\|_{2}}\\right|\\right|_{2}$", "word_idx": 34632, "sentence_idx": 406, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{Match~{}Activations}=\\left|\\left|\\frac{A_{t}}{\\|A_{t}\\|_{2}}-\\frac{A_{%\ns}}{\\|A_{s}\\|_{2}}\\right|\\right|_{2}$$", "word_idx": 34750, "sentence_idx": 407, "label": "unlabeled"}, {"type": "text", "expr": "Here,  $A_{t},A_{s}$  are the attention maps of the teacher and student respectively", "word_idx": 34866, "sentence_idx": 408, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 \\citet zagoruyko2016paying note that this choice of loss function is especially crucial", "word_idx": 34950, "sentence_idx": 409, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{t},A_{s}$$", "word_idx": 35039, "sentence_idx": 410, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 35050, "sentence_idx": 411, "label": "unlabeled"}, {"type": "text", "expr": "1  Incorporating Jacobian loss", "word_idx": 35056, "sentence_idx": 412, "label": "unlabeled"}, {"type": "text", "expr": "As the activation maps have large spatial dimensions, it is computationally costly to compute the full Jacobians", "word_idx": 35086, "sentence_idx": 413, "label": "unlabeled"}, {"type": "text", "expr": " We hence resort to computing approximating Jacobians in the same manner as previously discussed", "word_idx": 35198, "sentence_idx": 414, "label": "unlabeled"}, {"type": "text", "expr": " In this case, this leads to picking the pixel in the attention map with the largest magnitude, and computing the Jacobian of this quantity w", "word_idx": 35294, "sentence_idx": 415, "label": "unlabeled"}, {"type": "text", "expr": " the input", "word_idx": 35435, "sentence_idx": 416, "label": "unlabeled"}, {"type": "text", "expr": " We compute the index  $(i,j)$  of this maximum-valued pixel for the teacher network and use the same index to compute the student\u2019s Jacobian", "word_idx": 35445, "sentence_idx": 417, "label": "unlabeled"}, {"type": "text", "expr": " We then use a loss function similar to Equation\u00a0 9 , given by", "word_idx": 35586, "sentence_idx": 418, "label": "unlabeled"}, {"type": "math", "expr": "$$(i,j)$$", "word_idx": 35648, "sentence_idx": 419, "label": "unlabeled"}, {"type": "text", "expr": "$\\mathrm{Match~{}Jacobians}=\\left|\\left|\\frac{\\nabla_{x}f(\\mathbf{x})}{\\|\\nabla%\n_{x}f(\\mathbf{x})\\|_{2}}-\\frac{\\nabla_{x}g(\\mathbf{x})}{\\|\\nabla_{x}g(\\mathbf{%\nx})\\|_{2}}\\right|\\right|_{2}^{2}$", "word_idx": 35653, "sentence_idx": 420, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{Match~{}Jacobians}=\\left|\\left|\\frac{\\nabla_{x}f(\\mathbf{x})}{\\|\\nabla%\n_{x}f(\\mathbf{x})\\|_{2}}-\\frac{\\nabla_{x}g(\\mathbf{x})}{\\|\\nabla_{x}g(\\mathbf{%\nx})\\|_{2}}\\right|\\right|_{2}^{2}$$", "word_idx": 35847, "sentence_idx": 421, "label": "unlabeled"}, {"type": "text", "expr": "2  Justification for Jacobian loss", "word_idx": 36039, "sentence_idx": 422, "label": "unlabeled"}, {"type": "text", "expr": "We can show that the above loss term corresponds to adding a noise term  $\\bm{\\xi}_{f}\\propto\\|\\nabla_{x}f(\\mathbf{x})\\|^{-1}_{2}$  for  $f(\\mathbf{x})$  and  $\\bm{\\xi}_{g}\\propto\\|\\nabla_{x}g(\\mathbf{x})\\|^{-1}_{2}$  for  $g(\\mathbf{x})$  for the distillation loss", "word_idx": 36073, "sentence_idx": 423, "label": "unlabeled"}, {"type": "text", "expr": " From the first order Taylor series expansion, we see that  $g(x+\\bm{\\xi})=g(x)+\\bm{\\xi}_{g}\\nabla_{x}g(\\mathbf{x})$ ", "word_idx": 36338, "sentence_idx": 424, "label": "unlabeled"}, {"type": "text", "expr": " Thus for networks  $f(\\cdot)$  and  $g(\\cdot)$  with different Jacobian magnitudes, we expect different responses for the same noisy inputs", "word_idx": 36455, "sentence_idx": 425, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, we see that  $\\mathbb{E}_{\\bm{\\xi}_{g}}\\|g(x+\\bm{\\xi}_{g})-g(x)\\|^{2}_{2}=\\sigma_{g}^{2}\\|%\n\\nabla_{x}g(\\mathbf{x})\\|^{2}_{2}=\\sigma^{2}\\frac{\\|\\nabla_{x}g(\\mathbf{x})\\|^%\n{2}_{2}}{\\|\\nabla_{x}g(\\mathbf{x})\\|^{2}_{2}}=\\sigma^{2}$  for a gaussian model with covariance matrix being  $\\sigma$  times the identity", "word_idx": 36595, "sentence_idx": 426, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{\\xi}_{f}\\propto\\|\\nabla_{x}f(\\mathbf{x})\\|^{-1}_{2}$$", "word_idx": 36920, "sentence_idx": 427, "label": "unlabeled"}, {"type": "math", "expr": "$$f(\\mathbf{x})$$", "word_idx": 36975, "sentence_idx": 428, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{\\xi}_{g}\\propto\\|\\nabla_{x}g(\\mathbf{x})\\|^{-1}_{2}$$", "word_idx": 36988, "sentence_idx": 429, "label": "unlabeled"}, {"type": "math", "expr": "$$g(\\mathbf{x})$$", "word_idx": 37043, "sentence_idx": 430, "label": "unlabeled"}, {"type": "math", "expr": "$$g(x+\\bm{\\xi})=g(x)+\\bm{\\xi}_{g}\\nabla_{x}g(\\mathbf{x})$$", "word_idx": 37056, "sentence_idx": 431, "label": "unlabeled"}, {"type": "math", "expr": "$$f(\\cdot)$$", "word_idx": 37110, "sentence_idx": 432, "label": "unlabeled"}, {"type": "math", "expr": "$$g(\\cdot)$$", "word_idx": 37118, "sentence_idx": 433, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbb{E}_{\\bm{\\xi}_{g}}\\|g(x+\\bm{\\xi}_{g})-g(x)\\|^{2}_{2}=\\sigma_{g}^{2}\\|%\n\\nabla_{x}g(\\mathbf{x})\\|^{2}_{2}=\\sigma^{2}\\frac{\\|\\nabla_{x}g(\\mathbf{x})\\|^%\n{2}_{2}}{\\|\\nabla_{x}g(\\mathbf{x})\\|^{2}_{2}}=\\sigma^{2}$$", "word_idx": 37126, "sentence_idx": 434, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma$$", "word_idx": 37340, "sentence_idx": 435, "label": "unlabeled"}, {"type": "text", "expr": "6  Experiments", "word_idx": 37346, "sentence_idx": 436, "label": "unlabeled"}, {"type": "text", "expr": "We perform three experiments to show the effectiveness of using Jacobians", "word_idx": 37360, "sentence_idx": 437, "label": "unlabeled"}, {"type": "text", "expr": " First, we perform distillation in a limited data setting on the CIFAR100 dataset\u00a0 \\citep krizhevsky2009learning", "word_idx": 37433, "sentence_idx": 438, "label": "unlabeled"}, {"type": "text", "expr": " Second, we show on that same dataset that penalizing Jacobian norm increases stability of networks to random noise", "word_idx": 37545, "sentence_idx": 439, "label": "unlabeled"}, {"type": "text", "expr": " Finally, we perform transfer learning experiments on the MIT Scenes dataset\u00a0 \\citep quattoni2009recognizing", "word_idx": 37660, "sentence_idx": 440, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 37768, "sentence_idx": 441, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 37774, "sentence_idx": 442, "label": "unlabeled"}, {"type": "text", "expr": "1  Distillation", "word_idx": 37780, "sentence_idx": 443, "label": "unlabeled"}, {"type": "text", "expr": "For the distillation experiments, we use VGG-like\u00a0 \\citep simonyan2014very architectures with batch normalization", "word_idx": 37795, "sentence_idx": 444, "label": "unlabeled"}, {"type": "text", "expr": " The main difference is that we keep only the convolutional layers and have only one fully connected layer rather than three", "word_idx": 37908, "sentence_idx": 445, "label": "unlabeled"}, {"type": "text", "expr": " Our workflow is as follows", "word_idx": 38032, "sentence_idx": 446, "label": "unlabeled"}, {"type": "text", "expr": " First, a 9-layer \u201cteacher\u201d network is trained on the full CIFAR100 dataset", "word_idx": 38059, "sentence_idx": 447, "label": "unlabeled"}, {"type": "text", "expr": " Then, a smaller 4-layer \u201cstudent\u201d network is trained, but this time on small subsets rather than the full dataset", "word_idx": 38134, "sentence_idx": 448, "label": "unlabeled"}, {"type": "text", "expr": " As the teacher is trained on much more data than the student, we expect distillation to improve the student\u2019s performance", "word_idx": 38248, "sentence_idx": 449, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 38370, "sentence_idx": 450, "label": "unlabeled"}, {"type": "text", "expr": "A practical scenario where this would be useful is the case of architecture search and ensemble training, where we require to train many candidate neural network architectures on the same task", "word_idx": 38376, "sentence_idx": 451, "label": "unlabeled"}, {"type": "text", "expr": " Distillation methods can help speed up such methods by using already trained networks to accelerate training of newer models", "word_idx": 38568, "sentence_idx": 452, "label": "unlabeled"}, {"type": "text", "expr": " One way to achieve acceleration is by using less data to train the student dataset", "word_idx": 38693, "sentence_idx": 453, "label": "unlabeled"}, {"type": "text", "expr": "A practical scenario where this would be useful is the case of architecture search and ensemble training, where we require to train many candidate neural network architectures on the same task", "word_idx": 38776, "sentence_idx": 454, "label": "unlabeled"}, {"type": "text", "expr": " Distillation methods can help speed up such methods by using already trained networks to accelerate training of newer models", "word_idx": 38968, "sentence_idx": 455, "label": "unlabeled"}, {"type": "text", "expr": " One way to achieve acceleration is by using less data to train the student dataset", "word_idx": 39093, "sentence_idx": 456, "label": "unlabeled"}, {"type": "text", "expr": "We compare our methods against the following baselines", "word_idx": 39176, "sentence_idx": 457, "label": "unlabeled"}, {"type": "text", "expr": "  (1): Cross-Entropy (CE) training  \u2013 Here we train the student using only the ground truth (hard labels) available with the dataset without invoking the teacher network", "word_idx": 39230, "sentence_idx": 458, "label": "unlabeled"}, {"type": "text", "expr": "  (2): CE + match activations  \u2013 This is the classical form of distillation, where the activations of the teacher network are matched with that of the student", "word_idx": 39399, "sentence_idx": 459, "label": "unlabeled"}, {"type": "text", "expr": " This is weighted with the cross-entropy term which uses ground truth targets", "word_idx": 39557, "sentence_idx": 460, "label": "unlabeled"}, {"type": "text", "expr": "  (3): Match activations only  \u2013 Same as above, except that the cross-entropy term is not used in the loss function", "word_idx": 39634, "sentence_idx": 461, "label": "unlabeled"}, {"type": "text", "expr": "(1): Cross-Entropy (CE) training", "word_idx": 39749, "sentence_idx": 462, "label": "unlabeled"}, {"type": "text", "expr": "(2): CE + match activations", "word_idx": 39781, "sentence_idx": 463, "label": "unlabeled"}, {"type": "text", "expr": "(3): Match activations only", "word_idx": 39808, "sentence_idx": 464, "label": "unlabeled"}, {"type": "text", "expr": "We compare these methods by appending the Jacobian matching term in each case", "word_idx": 39835, "sentence_idx": 465, "label": "unlabeled"}, {"type": "text", "expr": " In all cases, we use the squared-error distillation loss shown in Proposition  1 ", "word_idx": 39912, "sentence_idx": 466, "label": "unlabeled"}, {"type": "text", "expr": " We found that squared loss worked much better than the cross-entropy loss for distillation and it was much easier to tune", "word_idx": 39994, "sentence_idx": 467, "label": "unlabeled"}, {"type": "text", "expr": "From Table  1  we can conclude that (1) it is generally beneficial to do any form of distillation to improve performance, (2) matching Jacobians along with activations outperforms matching only activations in low-data settings, (3) not matching Jacobians is often beneficial for large data", "word_idx": 40116, "sentence_idx": 468, "label": "unlabeled"}, {"type": "text", "expr": "One interesting phenomenon we observe is that having a cross-entropy (CE) error term is often not crucial to maintain good performance", "word_idx": 40405, "sentence_idx": 469, "label": "unlabeled"}, {"type": "text", "expr": " It is only slightly worse than using ground truth labels", "word_idx": 40539, "sentence_idx": 470, "label": "unlabeled"}, {"type": "text", "expr": "One interesting phenomenon we observe is that having a cross-entropy (CE) error term is often not crucial to maintain good performance", "word_idx": 40596, "sentence_idx": 471, "label": "unlabeled"}, {"type": "text", "expr": " It is only slightly worse than using ground truth labels", "word_idx": 40730, "sentence_idx": 472, "label": "unlabeled"}, {"type": "text", "expr": "For Table  1 , we see that when training with activations, Jacobians and regular cross-entropy training (fourth row), we reach an accuracy of  $5243\\%$  when training with 100 data points per class", "word_idx": 40787, "sentence_idx": 473, "label": "unlabeled"}, {"type": "text", "expr": " We observe that the overall accuracy of raw training using the full dataset is  $5428\\%$ ", "word_idx": 40984, "sentence_idx": 474, "label": "unlabeled"}, {"type": "text", "expr": " Thus we are able to reach close to the full training accuracy using  $1/5^{th}$  of the data requirement", "word_idx": 41074, "sentence_idx": 475, "label": "unlabeled"}, {"type": "math", "expr": "$$52.43\\%$$", "word_idx": 41179, "sentence_idx": 476, "label": "unlabeled"}, {"type": "math", "expr": "$$54.28\\%$$", "word_idx": 41186, "sentence_idx": 477, "label": "unlabeled"}, {"type": "math", "expr": "$$1/5^{th}$$", "word_idx": 41193, "sentence_idx": 478, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:  Distillation performance on the CIFAR100 dataset", "word_idx": 41201, "sentence_idx": 479, "label": "unlabeled"}, {"type": "text", "expr": " Table shows test accuracy (%)", "word_idx": 41259, "sentence_idx": 480, "label": "unlabeled"}, {"type": "text", "expr": " We find that matching both activations and Jacobians along with cross-entropy error performs the best for limited-data settings", "word_idx": 41289, "sentence_idx": 481, "label": "unlabeled"}, {"type": "text", "expr": " The student network is VGG-4 while the teacher is a VGG-9 network which achieves  $6478\\%$  accuracy", "word_idx": 41417, "sentence_idx": 482, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:", "word_idx": 41518, "sentence_idx": 483, "label": "unlabeled"}, {"type": "math", "expr": "$$64.78\\%$$", "word_idx": 41526, "sentence_idx": 484, "label": "unlabeled"}, {"type": "text", "expr": "1 5 10 50 100 500 (full)", "word_idx": 41533, "sentence_idx": 485, "label": "unlabeled"}, {"type": "text", "expr": "# of Data points per class  $\\rightarrow$", "word_idx": 41557, "sentence_idx": 486, "label": "unlabeled"}, {"type": "math", "expr": "$$\\rightarrow$$", "word_idx": 41598, "sentence_idx": 487, "label": "unlabeled"}, {"type": "text", "expr": "500 (full)", "word_idx": 41609, "sentence_idx": 488, "label": "unlabeled"}, {"type": "text", "expr": "69 13", "word_idx": 41619, "sentence_idx": 489, "label": "unlabeled"}, {"type": "text", "expr": "03 37", "word_idx": 41624, "sentence_idx": 490, "label": "unlabeled"}, {"type": "text", "expr": "92 54", "word_idx": 41629, "sentence_idx": 491, "label": "unlabeled"}, {"type": "text", "expr": "Cross-Entropy (CE) training", "word_idx": 41634, "sentence_idx": 492, "label": "unlabeled"}, {"type": "text", "expr": "Cross-Entropy (CE) training", "word_idx": 41661, "sentence_idx": 493, "label": "unlabeled"}, {"type": "text", "expr": "13 26", "word_idx": 41688, "sentence_idx": 494, "label": "unlabeled"}, {"type": "text", "expr": "97 33", "word_idx": 41693, "sentence_idx": 495, "label": "unlabeled"}, {"type": "text", "expr": "92 46", "word_idx": 41698, "sentence_idx": 496, "label": "unlabeled"}, {"type": "text", "expr": "47 50", "word_idx": 41703, "sentence_idx": 497, "label": "unlabeled"}, {"type": "text", "expr": "92 56", "word_idx": 41708, "sentence_idx": 498, "label": "unlabeled"}, {"type": "text", "expr": "CE + match activations", "word_idx": 41713, "sentence_idx": 499, "label": "unlabeled"}, {"type": "text", "expr": "CE + match activations", "word_idx": 41735, "sentence_idx": 500, "label": "unlabeled"}, {"type": "text", "expr": "CE + match Jacobians 6", "word_idx": 41757, "sentence_idx": 501, "label": "unlabeled"}, {"type": "text", "expr": "78 23", "word_idx": 41779, "sentence_idx": 502, "label": "unlabeled"}, {"type": "text", "expr": "94 32", "word_idx": 41784, "sentence_idx": 503, "label": "unlabeled"}, {"type": "text", "expr": "03 45", "word_idx": 41789, "sentence_idx": 504, "label": "unlabeled"}, {"type": "text", "expr": "71 51", "word_idx": 41794, "sentence_idx": 505, "label": "unlabeled"}, {"type": "text", "expr": "47 53", "word_idx": 41799, "sentence_idx": 506, "label": "unlabeled"}, {"type": "text", "expr": "CE + match Jacobians", "word_idx": 41804, "sentence_idx": 507, "label": "unlabeled"}, {"type": "text", "expr": "CE + match Jacobians", "word_idx": 41824, "sentence_idx": 508, "label": "unlabeled"}, {"type": "text", "expr": "CE + match {activations + Jacobians} 13", "word_idx": 41844, "sentence_idx": 509, "label": "unlabeled"}, {"type": "text", "expr": "78 33", "word_idx": 41883, "sentence_idx": 510, "label": "unlabeled"}, {"type": "text", "expr": "39 39", "word_idx": 41888, "sentence_idx": 511, "label": "unlabeled"}, {"type": "text", "expr": "55 49", "word_idx": 41893, "sentence_idx": 512, "label": "unlabeled"}, {"type": "text", "expr": "49 52", "word_idx": 41898, "sentence_idx": 513, "label": "unlabeled"}, {"type": "text", "expr": "43 54", "word_idx": 41903, "sentence_idx": 514, "label": "unlabeled"}, {"type": "text", "expr": "CE + match {activations + Jacobians}", "word_idx": 41908, "sentence_idx": 515, "label": "unlabeled"}, {"type": "text", "expr": "CE + match {activations + Jacobians}", "word_idx": 41944, "sentence_idx": 516, "label": "unlabeled"}, {"type": "text", "expr": "73 28", "word_idx": 41980, "sentence_idx": 517, "label": "unlabeled"}, {"type": "text", "expr": "56 33", "word_idx": 41985, "sentence_idx": 518, "label": "unlabeled"}, {"type": "text", "expr": "73 50", "word_idx": 41990, "sentence_idx": 519, "label": "unlabeled"}, {"type": "text", "expr": "15 56", "word_idx": 41995, "sentence_idx": 520, "label": "unlabeled"}, {"type": "text", "expr": "Match activations only", "word_idx": 42000, "sentence_idx": 521, "label": "unlabeled"}, {"type": "text", "expr": "Match activations only", "word_idx": 42022, "sentence_idx": 522, "label": "unlabeled"}, {"type": "text", "expr": "Match {activations + Jacobians} 13", "word_idx": 42044, "sentence_idx": 523, "label": "unlabeled"}, {"type": "text", "expr": "09 33", "word_idx": 42078, "sentence_idx": 524, "label": "unlabeled"}, {"type": "text", "expr": "31 38", "word_idx": 42083, "sentence_idx": 525, "label": "unlabeled"}, {"type": "text", "expr": "16 47", "word_idx": 42088, "sentence_idx": 526, "label": "unlabeled"}, {"type": "text", "expr": "79 50", "word_idx": 42093, "sentence_idx": 527, "label": "unlabeled"}, {"type": "text", "expr": "06 51", "word_idx": 42098, "sentence_idx": 528, "label": "unlabeled"}, {"type": "text", "expr": "Match {activations + Jacobians}", "word_idx": 42103, "sentence_idx": 529, "label": "unlabeled"}, {"type": "text", "expr": "Match {activations + Jacobians}", "word_idx": 42134, "sentence_idx": 530, "label": "unlabeled"}, {"type": "text", "expr": "2  Noise robustness", "word_idx": 42165, "sentence_idx": 531, "label": "unlabeled"}, {"type": "text", "expr": "We perform experiments where we penalize the Jacobian norm to improve robustness of models to random noise", "word_idx": 42184, "sentence_idx": 532, "label": "unlabeled"}, {"type": "text", "expr": " We train 9-layer VGG networks on CIFAR100 with varying amount of the regularization strength ( $\\lambda$ ), and measure their classification accuracy in presence of noise added to the normalized images", "word_idx": 42290, "sentence_idx": 533, "label": "unlabeled"}, {"type": "text", "expr": " From Table  2  we find that using higher regularization strengths is better for robustness, even when the initial accuracy at the zero-noise case is lower", "word_idx": 42492, "sentence_idx": 534, "label": "unlabeled"}, {"type": "text", "expr": " This aligns remarkably well with theory", "word_idx": 42647, "sentence_idx": 535, "label": "unlabeled"}, {"type": "text", "expr": " Surprisingly, we find that popular regularizers such as  $\\ell_{2}$  regularization and dropout\u00a0 \\citep srivastava2014dropout are not robust", "word_idx": 42687, "sentence_idx": 536, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda$$", "word_idx": 42828, "sentence_idx": 537, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ell_{2}$$", "word_idx": 42835, "sentence_idx": 538, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 42843, "sentence_idx": 539, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:  Robustness of various VGG-9 models to gaussian noise added to CIFAR100 images at test time", "word_idx": 42849, "sentence_idx": 540, "label": "unlabeled"}, {"type": "text", "expr": " Table shows test accuracy (%)", "word_idx": 42949, "sentence_idx": 541, "label": "unlabeled"}, {"type": "text", "expr": "  $\\lambda$  is the regularization strength of the Jacobian-norm penalty regularizer", "word_idx": 42979, "sentence_idx": 542, "label": "unlabeled"}, {"type": "text", "expr": "  $\\gamma$  is the  $\\ell_{2}$  regularization strength and  $p$  is the dropout value", "word_idx": 43063, "sentence_idx": 543, "label": "unlabeled"}, {"type": "text", "expr": " We see that the Jacobian-norm penalty can be remarkably robust to noise, unlike  $\\ell_{2}$  regularization and dropout", "word_idx": 43149, "sentence_idx": 544, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:", "word_idx": 43269, "sentence_idx": 545, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda$$", "word_idx": 43277, "sentence_idx": 546, "label": "unlabeled"}, {"type": "math", "expr": "$$\\gamma$$", "word_idx": 43284, "sentence_idx": 547, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ell_{2}$$", "word_idx": 43290, "sentence_idx": 548, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ell_{2}$$", "word_idx": 43298, "sentence_idx": 549, "label": "unlabeled"}, {"type": "text", "expr": "Noise std", "word_idx": 43306, "sentence_idx": 550, "label": "unlabeled"}, {"type": "text", "expr": "  $\\rightarrow$", "word_idx": 43315, "sentence_idx": 551, "label": "unlabeled"}, {"type": "math", "expr": "$$\\rightarrow$$", "word_idx": 43330, "sentence_idx": 552, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda=0$$", "word_idx": 43341, "sentence_idx": 553, "label": "unlabeled"}, {"type": "text", "expr": "$619\\pm 007$", "word_idx": 43350, "sentence_idx": 554, "label": "unlabeled"}, {"type": "math", "expr": "$$61.9\\pm 0.07$$", "word_idx": 43362, "sentence_idx": 555, "label": "unlabeled"}, {"type": "text", "expr": "$4753\\pm 023$", "word_idx": 43374, "sentence_idx": 556, "label": "unlabeled"}, {"type": "math", "expr": "$$47.53\\pm 0.23$$", "word_idx": 43387, "sentence_idx": 557, "label": "unlabeled"}, {"type": "text", "expr": "$2963\\pm 016$", "word_idx": 43400, "sentence_idx": 558, "label": "unlabeled"}, {"type": "math", "expr": "$$29.63\\pm 0.16$$", "word_idx": 43413, "sentence_idx": 559, "label": "unlabeled"}, {"type": "text", "expr": "$1769\\pm 017$", "word_idx": 43426, "sentence_idx": 560, "label": "unlabeled"}, {"type": "math", "expr": "$$17.69\\pm 0.17$$", "word_idx": 43439, "sentence_idx": 561, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda=0.1$$", "word_idx": 43452, "sentence_idx": 562, "label": "unlabeled"}, {"type": "text", "expr": "$\\bm{6336\\pm 018}$", "word_idx": 43463, "sentence_idx": 563, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{63.36\\pm 0.18}$$", "word_idx": 43481, "sentence_idx": 564, "label": "unlabeled"}, {"type": "text", "expr": "$5357\\pm 023$", "word_idx": 43499, "sentence_idx": 565, "label": "unlabeled"}, {"type": "math", "expr": "$$53.57\\pm 0.23$$", "word_idx": 43512, "sentence_idx": 566, "label": "unlabeled"}, {"type": "text", "expr": "$3738\\pm 018$", "word_idx": 43525, "sentence_idx": 567, "label": "unlabeled"}, {"type": "math", "expr": "$$37.38\\pm 0.18$$", "word_idx": 43538, "sentence_idx": 568, "label": "unlabeled"}, {"type": "text", "expr": "$2399\\pm 019$", "word_idx": 43551, "sentence_idx": 569, "label": "unlabeled"}, {"type": "math", "expr": "$$23.99\\pm 0.19$$", "word_idx": 43564, "sentence_idx": 570, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda=1.0$$", "word_idx": 43577, "sentence_idx": 571, "label": "unlabeled"}, {"type": "text", "expr": "$6266\\pm 013$", "word_idx": 43588, "sentence_idx": 572, "label": "unlabeled"}, {"type": "math", "expr": "$$62.66\\pm 0.13$$", "word_idx": 43601, "sentence_idx": 573, "label": "unlabeled"}, {"type": "text", "expr": "${5753\\pm 017}$", "word_idx": 43614, "sentence_idx": 574, "label": "unlabeled"}, {"type": "math", "expr": "$${57.53\\pm 0.17}$$", "word_idx": 43629, "sentence_idx": 575, "label": "unlabeled"}, {"type": "text", "expr": "${4748\\pm 014}$", "word_idx": 43644, "sentence_idx": 576, "label": "unlabeled"}, {"type": "math", "expr": "$${47.48\\pm 0.14}$$", "word_idx": 43659, "sentence_idx": 577, "label": "unlabeled"}, {"type": "text", "expr": "${3543\\pm 011}$", "word_idx": 43674, "sentence_idx": 578, "label": "unlabeled"}, {"type": "math", "expr": "$${35.43\\pm 0.11}$$", "word_idx": 43689, "sentence_idx": 579, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda=10.0$$", "word_idx": 43704, "sentence_idx": 580, "label": "unlabeled"}, {"type": "text", "expr": "$6078\\pm 005$", "word_idx": 43716, "sentence_idx": 581, "label": "unlabeled"}, {"type": "math", "expr": "$$60.78\\pm 0.05$$", "word_idx": 43729, "sentence_idx": 582, "label": "unlabeled"}, {"type": "text", "expr": "$\\bm{5828\\pm 013}$", "word_idx": 43742, "sentence_idx": 583, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{58.28\\pm 0.13}$$", "word_idx": 43760, "sentence_idx": 584, "label": "unlabeled"}, {"type": "text", "expr": "$\\bm{5282\\pm 010}$", "word_idx": 43778, "sentence_idx": 585, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{52.82\\pm 0.10}$$", "word_idx": 43796, "sentence_idx": 586, "label": "unlabeled"}, {"type": "text", "expr": "$\\bm{4496\\pm 019}$", "word_idx": 43814, "sentence_idx": 587, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{44.96\\pm 0.19}$$", "word_idx": 43832, "sentence_idx": 588, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ell_{2}$$", "word_idx": 43850, "sentence_idx": 589, "label": "unlabeled"}, {"type": "math", "expr": "$$\\gamma=1e-3$$", "word_idx": 43858, "sentence_idx": 590, "label": "unlabeled"}, {"type": "text", "expr": "$6041\\pm 027$", "word_idx": 43869, "sentence_idx": 591, "label": "unlabeled"}, {"type": "math", "expr": "$$60.41\\pm 0.27$$", "word_idx": 43882, "sentence_idx": 592, "label": "unlabeled"}, {"type": "text", "expr": "$3993\\pm 028$", "word_idx": 43895, "sentence_idx": 593, "label": "unlabeled"}, {"type": "math", "expr": "$$39.93\\pm 0.28$$", "word_idx": 43908, "sentence_idx": 594, "label": "unlabeled"}, {"type": "text", "expr": "$2332\\pm 025$", "word_idx": 43921, "sentence_idx": 595, "label": "unlabeled"}, {"type": "math", "expr": "$$23.32\\pm 0.25$$", "word_idx": 43934, "sentence_idx": 596, "label": "unlabeled"}, {"type": "text", "expr": "$1376\\pm 016$", "word_idx": 43947, "sentence_idx": 597, "label": "unlabeled"}, {"type": "math", "expr": "$$13.76\\pm 0.16$$", "word_idx": 43960, "sentence_idx": 598, "label": "unlabeled"}, {"type": "math", "expr": "$$(p=0.25)$$", "word_idx": 43973, "sentence_idx": 599, "label": "unlabeled"}, {"type": "text", "expr": "$6153\\pm 010$", "word_idx": 43981, "sentence_idx": 600, "label": "unlabeled"}, {"type": "math", "expr": "$$61.53\\pm 0.10$$", "word_idx": 43994, "sentence_idx": 601, "label": "unlabeled"}, {"type": "text", "expr": "$4434\\pm 019$", "word_idx": 44007, "sentence_idx": 602, "label": "unlabeled"}, {"type": "math", "expr": "$$44.34\\pm 0.19$$", "word_idx": 44020, "sentence_idx": 603, "label": "unlabeled"}, {"type": "text", "expr": "$2670\\pm 024$", "word_idx": 44033, "sentence_idx": 604, "label": "unlabeled"}, {"type": "math", "expr": "$$26.70\\pm 0.24$$", "word_idx": 44046, "sentence_idx": 605, "label": "unlabeled"}, {"type": "text", "expr": "$1577\\pm 011$", "word_idx": 44059, "sentence_idx": 606, "label": "unlabeled"}, {"type": "math", "expr": "$$15.77\\pm 0.11$$", "word_idx": 44072, "sentence_idx": 607, "label": "unlabeled"}, {"type": "text", "expr": "3  Transfer Learning", "word_idx": 44085, "sentence_idx": 608, "label": "unlabeled"}, {"type": "text", "expr": "For transfer learning, our objective is to improve training on the target dataset (MIT Scenes) by using Imagenet pre-trained models", "word_idx": 44105, "sentence_idx": 609, "label": "unlabeled"}, {"type": "text", "expr": " Crucially, we want our MIT Scenes model to have a different architecture than the Imagenet model", "word_idx": 44236, "sentence_idx": 610, "label": "unlabeled"}, {"type": "text", "expr": " The teacher model we use is a ResNet-34\u00a0 \\citep he2016deep pre-trained on Imagenet, while the student model is an untrained VGG-9 model with batch normalization", "word_idx": 44333, "sentence_idx": 611, "label": "unlabeled"}, {"type": "text", "expr": " We choose VGG-9 because its architecture is fundamentally different from a ResNet", "word_idx": 44494, "sentence_idx": 612, "label": "unlabeled"}, {"type": "text", "expr": " In principle we could use any architecture for the student", "word_idx": 44576, "sentence_idx": 613, "label": "unlabeled"}, {"type": "text", "expr": " We modify this VGG-9 architecture such that it has two sets of outputs, one sharing the label space with Imagenet (1000 classes), and another with MIT Scenes (67 classes,  $\\sim$  6k images)", "word_idx": 44635, "sentence_idx": 614, "label": "unlabeled"}, {"type": "text", "expr": " The pre-final layer is common to both outputs", "word_idx": 44826, "sentence_idx": 615, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 44872, "sentence_idx": 616, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sim$$", "word_idx": 44878, "sentence_idx": 617, "label": "unlabeled"}, {"type": "text", "expr": "Our baselines are the following", "word_idx": 44882, "sentence_idx": 618, "label": "unlabeled"}, {"type": "text", "expr": "  (1): Cross-Entropy (CE) training of student with ground truth  \u2013 Here we ignore the VGG-9 branch with 1000 classes and optimize the cross-entropy error on MIT Scenes data", "word_idx": 44913, "sentence_idx": 619, "label": "unlabeled"}, {"type": "text", "expr": " The loss function on this branch is always the same for all methods", "word_idx": 45085, "sentence_idx": 620, "label": "unlabeled"}, {"type": "text", "expr": "  (2): CE on pre-trained network  \u2013 This is exactly the same as the first baseline, except that the VGG-9 model is initialized from Imagenet pre-trained weights", "word_idx": 45153, "sentence_idx": 621, "label": "unlabeled"}, {"type": "text", "expr": " We consider this as our \u201coracle\u201d method and strive to match it\u2019s performance", "word_idx": 45313, "sentence_idx": 622, "label": "unlabeled"}, {"type": "text", "expr": "  (3): CE + match activations (LwF)  \u2013 This corresponds to the method of\u00a0 \\citet li2016learning, where the Imagenet branch output activations of the student are matched with those of the teacher", "word_idx": 45390, "sentence_idx": 623, "label": "unlabeled"}, {"type": "text", "expr": "  (4): CE + match { activations + attention}   \u2013 This corresponds to the method of\u00a0 \\citet zagoruyko2016paying, where attention maps are matched between some intermediate layers", "word_idx": 45584, "sentence_idx": 624, "label": "unlabeled"}, {"type": "text", "expr": "(1): Cross-Entropy (CE) training of student with ground truth", "word_idx": 45761, "sentence_idx": 625, "label": "unlabeled"}, {"type": "text", "expr": "(2): CE on pre-trained network", "word_idx": 45822, "sentence_idx": 626, "label": "unlabeled"}, {"type": "text", "expr": "(3): CE + match activations (LwF)", "word_idx": 45852, "sentence_idx": 627, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 45885, "sentence_idx": 628, "label": "unlabeled"}, {"type": "text", "expr": "(4): CE + match { activations + attention}", "word_idx": 45891, "sentence_idx": 629, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 45933, "sentence_idx": 630, "label": "unlabeled"}, {"type": "text", "expr": "We add our Jacobian matching terms to the baselines 3 and 4", "word_idx": 45939, "sentence_idx": 631, "label": "unlabeled"}, {"type": "text", "expr": " We provide our results in Table  3 ", "word_idx": 45998, "sentence_idx": 632, "label": "unlabeled"}, {"type": "text", "expr": " In all cases, we vary the number of images per class on MIT Scenes to observe the performance on low-data settings as well", "word_idx": 46034, "sentence_idx": 633, "label": "unlabeled"}, {"type": "text", "expr": " In this case we average our results over two runs by choosing different random subsets", "word_idx": 46157, "sentence_idx": 634, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a03:  Transfer Learning from Imagenet to MIT Scenes dataset", "word_idx": 46244, "sentence_idx": 635, "label": "unlabeled"}, {"type": "text", "expr": " Table shows test accuracy (%)", "word_idx": 46307, "sentence_idx": 636, "label": "unlabeled"}, {"type": "text", "expr": " The student network (VGG9) is trained from scratch unless otherwise mentioned", "word_idx": 46337, "sentence_idx": 637, "label": "unlabeled"}, {"type": "text", "expr": " The teacher network used is a pre-trained ResNet34", "word_idx": 46415, "sentence_idx": 638, "label": "unlabeled"}, {"type": "text", "expr": " Results are averaged over two runs", "word_idx": 46466, "sentence_idx": 639, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a03:", "word_idx": 46501, "sentence_idx": 640, "label": "unlabeled"}, {"type": "text", "expr": "5 10 25 50 Full", "word_idx": 46509, "sentence_idx": 641, "label": "unlabeled"}, {"type": "text", "expr": "# of Data points per class  $\\rightarrow$", "word_idx": 46524, "sentence_idx": 642, "label": "unlabeled"}, {"type": "math", "expr": "$$\\rightarrow$$", "word_idx": 46565, "sentence_idx": 643, "label": "unlabeled"}, {"type": "text", "expr": "64 20", "word_idx": 46576, "sentence_idx": 644, "label": "unlabeled"}, {"type": "text", "expr": "30 35", "word_idx": 46581, "sentence_idx": 645, "label": "unlabeled"}, {"type": "text", "expr": "19 46", "word_idx": 46586, "sentence_idx": 646, "label": "unlabeled"}, {"type": "text", "expr": "38 59", "word_idx": 46591, "sentence_idx": 647, "label": "unlabeled"}, {"type": "text", "expr": "Cross-Entropy (CE) training on untrained student network", "word_idx": 46596, "sentence_idx": 648, "label": "unlabeled"}, {"type": "text", "expr": "Cross-Entropy (CE) training on untrained student network", "word_idx": 46652, "sentence_idx": 649, "label": "unlabeled"}, {"type": "text", "expr": "CE on pre-trained student network (Oracle) 25", "word_idx": 46708, "sentence_idx": 650, "label": "unlabeled"}, {"type": "text", "expr": "93 43", "word_idx": 46753, "sentence_idx": 651, "label": "unlabeled"}, {"type": "text", "expr": "81 57", "word_idx": 46758, "sentence_idx": 652, "label": "unlabeled"}, {"type": "text", "expr": "65 64", "word_idx": 46763, "sentence_idx": 653, "label": "unlabeled"}, {"type": "text", "expr": "18 71", "word_idx": 46768, "sentence_idx": 654, "label": "unlabeled"}, {"type": "text", "expr": "CE on pre-trained student network (Oracle)", "word_idx": 46773, "sentence_idx": 655, "label": "unlabeled"}, {"type": "text", "expr": "CE on pre-trained student network (Oracle)", "word_idx": 46815, "sentence_idx": 656, "label": "unlabeled"}, {"type": "text", "expr": "08 27", "word_idx": 46857, "sentence_idx": 657, "label": "unlabeled"}, {"type": "text", "expr": "13 45", "word_idx": 46862, "sentence_idx": 658, "label": "unlabeled"}, {"type": "text", "expr": "08 55", "word_idx": 46867, "sentence_idx": 659, "label": "unlabeled"}, {"type": "text", "expr": "22 65", "word_idx": 46872, "sentence_idx": 660, "label": "unlabeled"}, {"type": "text", "expr": "CE + match activations\u00a0 \\citep li2016learning", "word_idx": 46877, "sentence_idx": 661, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 46922, "sentence_idx": 662, "label": "unlabeled"}, {"type": "text", "expr": "CE + match {activations + Jacobians} 17", "word_idx": 46928, "sentence_idx": 663, "label": "unlabeled"}, {"type": "text", "expr": "88 28", "word_idx": 46967, "sentence_idx": 664, "label": "unlabeled"}, {"type": "text", "expr": "25 45", "word_idx": 46972, "sentence_idx": 665, "label": "unlabeled"}, {"type": "text", "expr": "26 56", "word_idx": 46977, "sentence_idx": 666, "label": "unlabeled"}, {"type": "text", "expr": "49 66", "word_idx": 46982, "sentence_idx": 667, "label": "unlabeled"}, {"type": "text", "expr": "CE + match {activations + Jacobians}", "word_idx": 46987, "sentence_idx": 668, "label": "unlabeled"}, {"type": "text", "expr": "CE + match {activations + Jacobians}", "word_idx": 47023, "sentence_idx": 669, "label": "unlabeled"}, {"type": "text", "expr": "53 28", "word_idx": 47059, "sentence_idx": 670, "label": "unlabeled"}, {"type": "text", "expr": "35 46", "word_idx": 47064, "sentence_idx": 671, "label": "unlabeled"}, {"type": "text", "expr": "01 57", "word_idx": 47069, "sentence_idx": 672, "label": "unlabeled"}, {"type": "text", "expr": "80 67", "word_idx": 47074, "sentence_idx": 673, "label": "unlabeled"}, {"type": "text", "expr": "CE + match {activations + attention}\u00a0 \\citep zagoruyko2016paying", "word_idx": 47079, "sentence_idx": 674, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 47143, "sentence_idx": 675, "label": "unlabeled"}, {"type": "text", "expr": "CE + match {activations + attention + Jacobians} 18", "word_idx": 47149, "sentence_idx": 676, "label": "unlabeled"}, {"type": "text", "expr": "02 29", "word_idx": 47200, "sentence_idx": 677, "label": "unlabeled"}, {"type": "text", "expr": "25 47", "word_idx": 47205, "sentence_idx": 678, "label": "unlabeled"}, {"type": "text", "expr": "31 58", "word_idx": 47210, "sentence_idx": 679, "label": "unlabeled"}, {"type": "text", "expr": "35 67", "word_idx": 47215, "sentence_idx": 680, "label": "unlabeled"}, {"type": "text", "expr": "CE + match {activations + attention + Jacobians}", "word_idx": 47220, "sentence_idx": 681, "label": "unlabeled"}, {"type": "text", "expr": "CE + match {activations + attention + Jacobians}", "word_idx": 47268, "sentence_idx": 682, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a04:  Ablation experiments over choice of feature matching depth", "word_idx": 47316, "sentence_idx": 683, "label": "unlabeled"}, {"type": "text", "expr": " ( $\\mathcal{T}$ ,  $\\mathcal{S}$ ) denotes teacher (ResNet34) and student (VGG9) feature depths", "word_idx": 47384, "sentence_idx": 684, "label": "unlabeled"}, {"type": "text", "expr": " These pairs are chosen such that resulting features have same spatial dimensions", "word_idx": 47480, "sentence_idx": 685, "label": "unlabeled"}, {"type": "text", "expr": " We see that matching the shallowest feature works the best", "word_idx": 47561, "sentence_idx": 686, "label": "unlabeled"}, {"type": "text", "expr": " Results are averaged over two runs", "word_idx": 47620, "sentence_idx": 687, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a04:", "word_idx": 47655, "sentence_idx": 688, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{T}$$", "word_idx": 47663, "sentence_idx": 689, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{S}$$", "word_idx": 47674, "sentence_idx": 690, "label": "unlabeled"}, {"type": "text", "expr": "(7, 2) (15, 4) (27, 6) (33, 8)", "word_idx": 47685, "sentence_idx": 691, "label": "unlabeled"}, {"type": "text", "expr": "Feature matching", "word_idx": 47715, "sentence_idx": 692, "label": "unlabeled"}, {"type": "text", "expr": "Feature matching", "word_idx": 47731, "sentence_idx": 693, "label": "unlabeled"}, {"type": "text", "expr": "Feature matching", "word_idx": 47747, "sentence_idx": 694, "label": "unlabeled"}, {"type": "text", "expr": "depth ( $\\mathcal{T}$ ,  $\\mathcal{S}$ )", "word_idx": 47763, "sentence_idx": 695, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{T}$$", "word_idx": 47803, "sentence_idx": 696, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{S}$$", "word_idx": 47814, "sentence_idx": 697, "label": "unlabeled"}, {"type": "text", "expr": "39 21", "word_idx": 47825, "sentence_idx": 698, "label": "unlabeled"}, {"type": "text", "expr": "98 20", "word_idx": 47830, "sentence_idx": 699, "label": "unlabeled"}, {"type": "text", "expr": "45 20", "word_idx": 47835, "sentence_idx": 700, "label": "unlabeled"}, {"type": "text", "expr": "Accuracy (%)", "word_idx": 47840, "sentence_idx": 701, "label": "unlabeled"}, {"type": "text", "expr": "Accuracy (%)", "word_idx": 47852, "sentence_idx": 702, "label": "unlabeled"}, {"type": "text", "expr": "88 15", "word_idx": 47864, "sentence_idx": 703, "label": "unlabeled"}, {"type": "text", "expr": "59 11", "word_idx": 47869, "sentence_idx": 704, "label": "unlabeled"}, {"type": "text", "expr": "Jacobian loss", "word_idx": 47874, "sentence_idx": 705, "label": "unlabeled"}, {"type": "text", "expr": "Jacobian loss", "word_idx": 47887, "sentence_idx": 706, "label": "unlabeled"}, {"type": "text", "expr": "Jacobian loss", "word_idx": 47900, "sentence_idx": 707, "label": "unlabeled"}, {"type": "text", "expr": "reduction (%)", "word_idx": 47913, "sentence_idx": 708, "label": "unlabeled"}, {"type": "text", "expr": "reduction (%)", "word_idx": 47926, "sentence_idx": 709, "label": "unlabeled"}, {"type": "text", "expr": "reduction (%)", "word_idx": 47939, "sentence_idx": 710, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a05:  Ablation experiments over the computation of Jacobian", "word_idx": 47952, "sentence_idx": 711, "label": "unlabeled"}, {"type": "text", "expr": " Here,  $s$  is the size of the attention map", "word_idx": 48015, "sentence_idx": 712, "label": "unlabeled"}, {"type": "text", "expr": " \u201cFull\u201d is global average pooling, and \u201cNone\u201d is no average pooling", "word_idx": 48060, "sentence_idx": 713, "label": "unlabeled"}, {"type": "text", "expr": " We see that using average pooling while computing Jacobians helps performance", "word_idx": 48127, "sentence_idx": 714, "label": "unlabeled"}, {"type": "text", "expr": " Results are averaged over two runs", "word_idx": 48205, "sentence_idx": 715, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a05:", "word_idx": 48240, "sentence_idx": 716, "label": "unlabeled"}, {"type": "text", "expr": "Average Pool", "word_idx": 48248, "sentence_idx": 717, "label": "unlabeled"}, {"type": "text", "expr": "Average Pool", "word_idx": 48260, "sentence_idx": 718, "label": "unlabeled"}, {"type": "text", "expr": "Average Pool", "word_idx": 48272, "sentence_idx": 719, "label": "unlabeled"}, {"type": "text", "expr": "Window size", "word_idx": 48284, "sentence_idx": 720, "label": "unlabeled"}, {"type": "text", "expr": "Window size", "word_idx": 48295, "sentence_idx": 721, "label": "unlabeled"}, {"type": "text", "expr": "Window size", "word_idx": 48306, "sentence_idx": 722, "label": "unlabeled"}, {"type": "math", "expr": "$$s/3$$", "word_idx": 48317, "sentence_idx": 723, "label": "unlabeled"}, {"type": "math", "expr": "$$s/5$$", "word_idx": 48320, "sentence_idx": 724, "label": "unlabeled"}, {"type": "math", "expr": "$$s/7$$", "word_idx": 48323, "sentence_idx": 725, "label": "unlabeled"}, {"type": "text", "expr": "00 21", "word_idx": 48326, "sentence_idx": 726, "label": "unlabeled"}, {"type": "text", "expr": "20 21", "word_idx": 48331, "sentence_idx": 727, "label": "unlabeled"}, {"type": "text", "expr": "87 21", "word_idx": 48336, "sentence_idx": 728, "label": "unlabeled"}, {"type": "text", "expr": "09 19", "word_idx": 48341, "sentence_idx": 729, "label": "unlabeled"}, {"type": "text", "expr": "Accuracy (%)", "word_idx": 48346, "sentence_idx": 730, "label": "unlabeled"}, {"type": "text", "expr": "Accuracy (%)", "word_idx": 48358, "sentence_idx": 731, "label": "unlabeled"}, {"type": "text", "expr": "Experiments show that matching activations and attention maps increases performance at all levels of data size", "word_idx": 48370, "sentence_idx": 732, "label": "unlabeled"}, {"type": "text", "expr": " It also shows that Jacobians improve performance of all these methods", "word_idx": 48480, "sentence_idx": 733, "label": "unlabeled"}, {"type": "text", "expr": " However, we observe that none of the methods match the oracle performance of using a pre-trained model", "word_idx": 48550, "sentence_idx": 734, "label": "unlabeled"}, {"type": "text", "expr": " The gap in performance is especially large at intermediate data sizes of  $10$  and  $25$  images per class", "word_idx": 48653, "sentence_idx": 735, "label": "unlabeled"}, {"type": "text", "expr": "1  Which features to match?", "word_idx": 48761, "sentence_idx": 736, "label": "unlabeled"}, {"type": "text", "expr": "An important practical question is the choice of intermediate features to compute attention maps for matching", "word_idx": 48788, "sentence_idx": 737, "label": "unlabeled"}, {"type": "text", "expr": " The recipe followed by\u00a0 \\citet zagoruyko2016paying for ResNets is to consider features at the end of a residual block", "word_idx": 48897, "sentence_idx": 738, "label": "unlabeled"}, {"type": "text", "expr": "   As there are typically 3-5 max-pooling layers in most modern networks, we have 3-5 intermediate features to match between any typical teacher and student network", "word_idx": 49015, "sentence_idx": 739, "label": "unlabeled"}, {"type": "text", "expr": " Note that we require the attention maps (channelwise aggregated features) to be of similar spatial size to match", "word_idx": 49179, "sentence_idx": 740, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 \\citet zagoruyko2016paying match at all such possible locations, and we use the same approach", "word_idx": 49292, "sentence_idx": 741, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 49387, "sentence_idx": 742, "label": "unlabeled"}, {"type": "text", "expr": "1 A residual block is the set of all layers in between two consecutive max-pooling layers in a ResNet", "word_idx": 49393, "sentence_idx": 743, "label": "unlabeled"}, {"type": "text", "expr": " All features in a block have the same dimensions", "word_idx": 49494, "sentence_idx": 744, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 49543, "sentence_idx": 745, "label": "unlabeled"}, {"type": "text", "expr": "However, computing Jacobians at all such locations is computationally demanding and perhaps unnecessary", "word_idx": 49549, "sentence_idx": 746, "label": "unlabeled"}, {"type": "text", "expr": " We observe that if we compute Jacobians at later layers, we are still not able to reduce training Jacobian loss, possibly due to a \u201csecond-order\u201d vanishing gradient effect", "word_idx": 49652, "sentence_idx": 747, "label": "unlabeled"}, {"type": "text", "expr": " At suitable intermediate layers, we see that loss reduction occurs", "word_idx": 49824, "sentence_idx": 748, "label": "unlabeled"}, {"type": "text", "expr": " This is reflected in Table  4 , where we vary the feature matching depth and observe performance", "word_idx": 49891, "sentence_idx": 749, "label": "unlabeled"}, {"type": "text", "expr": " We observe that the Jacobian loss reduction (during training) is highest for the shallowest layers, and this corresponds to good test performance as well", "word_idx": 49988, "sentence_idx": 750, "label": "unlabeled"}, {"type": "text", "expr": " These ablation experiments are done on the MIT Scenes dataset picking only  $10$  points per class", "word_idx": 50142, "sentence_idx": 751, "label": "unlabeled"}, {"type": "text", "expr": "2  Feature Pooling to compute Jacobians", "word_idx": 50241, "sentence_idx": 752, "label": "unlabeled"}, {"type": "text", "expr": "Instead of considering a single pixel per attention map to compute Jacobians, we can aggregate a large number of large-magnitude pixels", "word_idx": 50280, "sentence_idx": 753, "label": "unlabeled"}, {"type": "text", "expr": " One way to do this is by average pooling over the attention map, and then picking the maximum pixel over the average pooled map", "word_idx": 50415, "sentence_idx": 754, "label": "unlabeled"}, {"type": "text", "expr": " In Table  5  we vary the window size for average pooling and observe performance", "word_idx": 50543, "sentence_idx": 755, "label": "unlabeled"}, {"type": "text", "expr": " We observe that it is beneficial to do average pooling, we find that using a window size of  $\\mathrm{(feature~{}size)}/5$  works the best", "word_idx": 50624, "sentence_idx": 756, "label": "unlabeled"}, {"type": "text", "expr": " These ablation experiments are done on the MIT Scenes dataset picking only  $10$  points per class", "word_idx": 50763, "sentence_idx": 757, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{(feature~{}size)}/5$$", "word_idx": 50862, "sentence_idx": 758, "label": "unlabeled"}, {"type": "text", "expr": "7  Conclusion", "word_idx": 50889, "sentence_idx": 759, "label": "unlabeled"}, {"type": "text", "expr": "In this paper we considered matching Jacobians of deep neural networks for knowledge transfer", "word_idx": 50902, "sentence_idx": 760, "label": "unlabeled"}, {"type": "text", "expr": " Viewing Jacobian matching as a form of data augmentation with gaussian noise motivates their usage, and also informs us about the loss functions to use", "word_idx": 50995, "sentence_idx": 761, "label": "unlabeled"}, {"type": "text", "expr": " We also connected a recent transfer learning method (LwF) to distillation, enabling us to incorporate Jacobian matching", "word_idx": 51147, "sentence_idx": 762, "label": "unlabeled"}, {"type": "text", "expr": "In this paper we considered matching Jacobians of deep neural networks for knowledge transfer", "word_idx": 51267, "sentence_idx": 763, "label": "unlabeled"}, {"type": "text", "expr": " Viewing Jacobian matching as a form of data augmentation with gaussian noise motivates their usage, and also informs us about the loss functions to use", "word_idx": 51360, "sentence_idx": 764, "label": "unlabeled"}, {"type": "text", "expr": " We also connected a recent transfer learning method (LwF) to distillation, enabling us to incorporate Jacobian matching", "word_idx": 51512, "sentence_idx": 765, "label": "unlabeled"}, {"type": "text", "expr": "Despite our advances, there is still a large gap between distillation-based methods and the oracle method of using pre-trained nets for transfer learning", "word_idx": 51632, "sentence_idx": 766, "label": "unlabeled"}, {"type": "text", "expr": " Future work can focus on closing this gap by considering more structured forms of data augmentation than simple noise addition", "word_idx": 51785, "sentence_idx": 767, "label": "unlabeled"}, {"type": "text", "expr": "Despite our advances, there is still a large gap between distillation-based methods and the oracle method of using pre-trained nets for transfer learning", "word_idx": 51912, "sentence_idx": 768, "label": "unlabeled"}, {"type": "text", "expr": " Future work can focus on closing this gap by considering more structured forms of data augmentation than simple noise addition", "word_idx": 52065, "sentence_idx": 769, "label": "unlabeled"}, {"type": "text", "expr": "References", "word_idx": 52192, "sentence_idx": 770, "label": "unlabeled"}, {"type": "text", "expr": "Ba & Caruana(2014)Ba and Caruana \nBa, LJ and Caruana, R", "word_idx": 52202, "sentence_idx": 771, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Do deep networks really need to be deep", "word_idx": 52257, "sentence_idx": 772, "label": "unlabeled"}, {"type": "text", "expr": "Ba & Caruana(2014)Ba and Caruana", "word_idx": 52299, "sentence_idx": 773, "label": "unlabeled"}, {"type": "text", "expr": "Ba, LJ and Caruana, R", "word_idx": 52331, "sentence_idx": 774, "label": "unlabeled"}, {"type": "text", "expr": "Do deep networks really need to be deep", "word_idx": 52352, "sentence_idx": 775, "label": "unlabeled"}, {"type": "text", "expr": "Advances in neural information processing systems ,\n27:1\u20139, 2014", "word_idx": 52391, "sentence_idx": 776, "label": "unlabeled"}, {"type": "text", "expr": "Advances in neural information processing systems", "word_idx": 52455, "sentence_idx": 777, "label": "unlabeled"}, {"type": "text", "expr": "Ben-David et\u00a0al", "word_idx": 52504, "sentence_idx": 778, "label": "unlabeled"}, {"type": "text", "expr": "(2010)Ben-David, Blitzer, Crammer, Kulesza, Pereira,\nand Vaughan \nBen-David, Shai, Blitzer, John, Crammer, Koby, Kulesza, Alex, Pereira,\nFernando, and Vaughan, Jennifer\u00a0Wortman", "word_idx": 52519, "sentence_idx": 779, "label": "unlabeled"}, {"type": "text", "expr": "\n\n A theory of learning from different domains", "word_idx": 52695, "sentence_idx": 780, "label": "unlabeled"}, {"type": "text", "expr": "Ben-David et\u00a0al", "word_idx": 52741, "sentence_idx": 781, "label": "unlabeled"}, {"type": "text", "expr": "(2010)Ben-David, Blitzer, Crammer, Kulesza, Pereira,\nand Vaughan", "word_idx": 52756, "sentence_idx": 782, "label": "unlabeled"}, {"type": "text", "expr": "Ben-David, Shai, Blitzer, John, Crammer, Koby, Kulesza, Alex, Pereira,\nFernando, and Vaughan, Jennifer\u00a0Wortman", "word_idx": 52820, "sentence_idx": 783, "label": "unlabeled"}, {"type": "text", "expr": "A theory of learning from different domains", "word_idx": 52930, "sentence_idx": 784, "label": "unlabeled"}, {"type": "text", "expr": "Machine learning , 79(1-2):151\u2013175, 2010", "word_idx": 52973, "sentence_idx": 785, "label": "unlabeled"}, {"type": "text", "expr": "Machine learning", "word_idx": 53013, "sentence_idx": 786, "label": "unlabeled"}, {"type": "text", "expr": "Bishop(1995) \nBishop, Chris\u00a0M", "word_idx": 53029, "sentence_idx": 787, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Training with noise is equivalent to tikhonov regularization", "word_idx": 53058, "sentence_idx": 788, "label": "unlabeled"}, {"type": "text", "expr": "Bishop(1995)", "word_idx": 53121, "sentence_idx": 789, "label": "unlabeled"}, {"type": "text", "expr": "Bishop, Chris\u00a0M", "word_idx": 53133, "sentence_idx": 790, "label": "unlabeled"}, {"type": "text", "expr": "Training with noise is equivalent to tikhonov regularization", "word_idx": 53148, "sentence_idx": 791, "label": "unlabeled"}, {"type": "text", "expr": "Neural Computation , 1995", "word_idx": 53208, "sentence_idx": 792, "label": "unlabeled"}, {"type": "text", "expr": "Neural Computation", "word_idx": 53233, "sentence_idx": 793, "label": "unlabeled"}, {"type": "text", "expr": "Cisse et\u00a0al", "word_idx": 53251, "sentence_idx": 794, "label": "unlabeled"}, {"type": "text", "expr": "(2017)Cisse, Bojanowski, Grave, Dauphin, and\nUsunier \nCisse, Moustapha, Bojanowski, Piotr, Grave, Edouard, Dauphin, Yann, and\nUsunier, Nicolas", "word_idx": 53262, "sentence_idx": 795, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Parseval networks: Improving robustness to adversarial examples", "word_idx": 53404, "sentence_idx": 796, "label": "unlabeled"}, {"type": "text", "expr": "Cisse et\u00a0al", "word_idx": 53470, "sentence_idx": 797, "label": "unlabeled"}, {"type": "text", "expr": "(2017)Cisse, Bojanowski, Grave, Dauphin, and\nUsunier", "word_idx": 53481, "sentence_idx": 798, "label": "unlabeled"}, {"type": "text", "expr": "Cisse, Moustapha, Bojanowski, Piotr, Grave, Edouard, Dauphin, Yann, and\nUsunier, Nicolas", "word_idx": 53533, "sentence_idx": 799, "label": "unlabeled"}, {"type": "text", "expr": "Parseval networks: Improving robustness to adversarial examples", "word_idx": 53621, "sentence_idx": 800, "label": "unlabeled"}, {"type": "text", "expr": "In  International Conference on Machine Learning , pp", "word_idx": 53684, "sentence_idx": 801, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0854\u2013863, 2017", "word_idx": 53737, "sentence_idx": 802, "label": "unlabeled"}, {"type": "text", "expr": "International Conference on Machine Learning", "word_idx": 53751, "sentence_idx": 803, "label": "unlabeled"}, {"type": "text", "expr": "Czarnecki et\u00a0al", "word_idx": 53795, "sentence_idx": 804, "label": "unlabeled"}, {"type": "text", "expr": "(2017)Czarnecki, Osindero, Jaderberg, \u015awirszcz,\nand Pascanu \nCzarnecki, Wojciech\u00a0Marian, Osindero, Simon, Jaderberg, Max, \u015awirszcz,\nGrzegorz, and Pascanu, Razvan", "word_idx": 53810, "sentence_idx": 805, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Sobolev training for neural networks", "word_idx": 53971, "sentence_idx": 806, "label": "unlabeled"}, {"type": "text", "expr": "Czarnecki et\u00a0al", "word_idx": 54010, "sentence_idx": 807, "label": "unlabeled"}, {"type": "text", "expr": "(2017)Czarnecki, Osindero, Jaderberg, \u015awirszcz,\nand Pascanu", "word_idx": 54025, "sentence_idx": 808, "label": "unlabeled"}, {"type": "text", "expr": "Czarnecki, Wojciech\u00a0Marian, Osindero, Simon, Jaderberg, Max, \u015awirszcz,\nGrzegorz, and Pascanu, Razvan", "word_idx": 54084, "sentence_idx": 809, "label": "unlabeled"}, {"type": "text", "expr": "Sobolev training for neural networks", "word_idx": 54184, "sentence_idx": 810, "label": "unlabeled"}, {"type": "text", "expr": "NIPS , 2017", "word_idx": 54220, "sentence_idx": 811, "label": "unlabeled"}, {"type": "text", "expr": "Drucker & Le\u00a0Cun(1992)Drucker and Le\u00a0Cun \nDrucker, Harris and Le\u00a0Cun, Yann", "word_idx": 54231, "sentence_idx": 812, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Improving generalization performance using double backpropagation", "word_idx": 54305, "sentence_idx": 813, "label": "unlabeled"}, {"type": "text", "expr": "Drucker & Le\u00a0Cun(1992)Drucker and Le\u00a0Cun", "word_idx": 54373, "sentence_idx": 814, "label": "unlabeled"}, {"type": "text", "expr": "Drucker, Harris and Le\u00a0Cun, Yann", "word_idx": 54413, "sentence_idx": 815, "label": "unlabeled"}, {"type": "text", "expr": "Improving generalization performance using double backpropagation", "word_idx": 54445, "sentence_idx": 816, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Transactions on Neural Networks , 1992", "word_idx": 54510, "sentence_idx": 817, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Transactions on Neural Networks", "word_idx": 54553, "sentence_idx": 818, "label": "unlabeled"}, {"type": "text", "expr": "Furlanello et\u00a0al", "word_idx": 54589, "sentence_idx": 819, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Furlanello, Zhao, Saxe, Itti, and\nTjan \nFurlanello, Tommaso, Zhao, Jiaping, Saxe, Andrew\u00a0M, Itti, Laurent, and Tjan,\nBosco\u00a0S", "word_idx": 54605, "sentence_idx": 820, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Active long term memory networks", "word_idx": 54735, "sentence_idx": 821, "label": "unlabeled"}, {"type": "text", "expr": "Furlanello et\u00a0al", "word_idx": 54770, "sentence_idx": 822, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Furlanello, Zhao, Saxe, Itti, and\nTjan", "word_idx": 54786, "sentence_idx": 823, "label": "unlabeled"}, {"type": "text", "expr": "Furlanello, Tommaso, Zhao, Jiaping, Saxe, Andrew\u00a0M, Itti, Laurent, and Tjan,\nBosco\u00a0S", "word_idx": 54830, "sentence_idx": 824, "label": "unlabeled"}, {"type": "text", "expr": "Active long term memory networks", "word_idx": 54914, "sentence_idx": 825, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1606", "word_idx": 54946, "sentence_idx": 826, "label": "unlabeled"}, {"type": "text", "expr": "02355 , 2016", "word_idx": 54971, "sentence_idx": 827, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1606", "word_idx": 54983, "sentence_idx": 828, "label": "unlabeled"}, {"type": "text", "expr": "02355", "word_idx": 55008, "sentence_idx": 829, "label": "unlabeled"}, {"type": "text", "expr": "He et\u00a0al", "word_idx": 55013, "sentence_idx": 830, "label": "unlabeled"}, {"type": "text", "expr": "(2016)He, Zhang, Ren, and Sun \nHe, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian", "word_idx": 55021, "sentence_idx": 831, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Deep residual learning for image recognition", "word_idx": 55109, "sentence_idx": 832, "label": "unlabeled"}, {"type": "text", "expr": "He et\u00a0al", "word_idx": 55156, "sentence_idx": 833, "label": "unlabeled"}, {"type": "text", "expr": "(2016)He, Zhang, Ren, and Sun", "word_idx": 55164, "sentence_idx": 834, "label": "unlabeled"}, {"type": "text", "expr": "He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian", "word_idx": 55193, "sentence_idx": 835, "label": "unlabeled"}, {"type": "text", "expr": "Deep residual learning for image recognition", "word_idx": 55250, "sentence_idx": 836, "label": "unlabeled"}, {"type": "text", "expr": "In  Proceedings of the IEEE conference on computer vision and\npattern recognition , pp", "word_idx": 55294, "sentence_idx": 837, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0770\u2013778, 2016", "word_idx": 55380, "sentence_idx": 838, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the IEEE conference on computer vision and\npattern recognition", "word_idx": 55394, "sentence_idx": 839, "label": "unlabeled"}, {"type": "text", "expr": "Hinton et\u00a0al", "word_idx": 55471, "sentence_idx": 840, "label": "unlabeled"}, {"type": "text", "expr": "(2015)Hinton, Vinyals, and Dean \nHinton, Geoffrey, Vinyals, Oriol, and Dean, Jeff", "word_idx": 55483, "sentence_idx": 841, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Distilling the knowledge in a neural network", "word_idx": 55564, "sentence_idx": 842, "label": "unlabeled"}, {"type": "text", "expr": "Hinton et\u00a0al", "word_idx": 55611, "sentence_idx": 843, "label": "unlabeled"}, {"type": "text", "expr": "(2015)Hinton, Vinyals, and Dean", "word_idx": 55623, "sentence_idx": 844, "label": "unlabeled"}, {"type": "text", "expr": "Hinton, Geoffrey, Vinyals, Oriol, and Dean, Jeff", "word_idx": 55654, "sentence_idx": 845, "label": "unlabeled"}, {"type": "text", "expr": "Distilling the knowledge in a neural network", "word_idx": 55702, "sentence_idx": 846, "label": "unlabeled"}, {"type": "text", "expr": "NIPS Deep Learning Workshop , 2015", "word_idx": 55746, "sentence_idx": 847, "label": "unlabeled"}, {"type": "text", "expr": "NIPS Deep Learning Workshop", "word_idx": 55780, "sentence_idx": 848, "label": "unlabeled"}, {"type": "text", "expr": "Jung et\u00a0al", "word_idx": 55807, "sentence_idx": 849, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Jung, Ju, Jung, and Kim \nJung, Heechul, Ju, Jeongwoo, Jung, Minju, and Kim, Junmo", "word_idx": 55817, "sentence_idx": 850, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Less-forgetting learning in deep neural networks", "word_idx": 55904, "sentence_idx": 851, "label": "unlabeled"}, {"type": "text", "expr": "Jung et\u00a0al", "word_idx": 55955, "sentence_idx": 852, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Jung, Ju, Jung, and Kim", "word_idx": 55965, "sentence_idx": 853, "label": "unlabeled"}, {"type": "text", "expr": "Jung, Heechul, Ju, Jeongwoo, Jung, Minju, and Kim, Junmo", "word_idx": 55994, "sentence_idx": 854, "label": "unlabeled"}, {"type": "text", "expr": "Less-forgetting learning in deep neural networks", "word_idx": 56050, "sentence_idx": 855, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1607", "word_idx": 56098, "sentence_idx": 856, "label": "unlabeled"}, {"type": "text", "expr": "00122 , 2016", "word_idx": 56123, "sentence_idx": 857, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1607", "word_idx": 56135, "sentence_idx": 858, "label": "unlabeled"}, {"type": "text", "expr": "00122", "word_idx": 56160, "sentence_idx": 859, "label": "unlabeled"}, {"type": "text", "expr": "Krizhevsky & Hinton(2009)Krizhevsky and\nHinton \nKrizhevsky, Alex and Hinton, Geoffrey", "word_idx": 56165, "sentence_idx": 860, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Learning multiple layers of features from tiny images", "word_idx": 56250, "sentence_idx": 861, "label": "unlabeled"}, {"type": "text", "expr": "\n\n 2009", "word_idx": 56306, "sentence_idx": 862, "label": "unlabeled"}, {"type": "text", "expr": "Krizhevsky & Hinton(2009)Krizhevsky and\nHinton", "word_idx": 56313, "sentence_idx": 863, "label": "unlabeled"}, {"type": "text", "expr": "Krizhevsky, Alex and Hinton, Geoffrey", "word_idx": 56359, "sentence_idx": 864, "label": "unlabeled"}, {"type": "text", "expr": "Learning multiple layers of features from tiny images", "word_idx": 56396, "sentence_idx": 865, "label": "unlabeled"}, {"type": "text", "expr": "Li & Hoiem(2016)Li and Hoiem \nLi, Zhizhong and Hoiem, Derek", "word_idx": 56449, "sentence_idx": 866, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Learning without forgetting", "word_idx": 56508, "sentence_idx": 867, "label": "unlabeled"}, {"type": "text", "expr": "Li & Hoiem(2016)Li and Hoiem", "word_idx": 56538, "sentence_idx": 868, "label": "unlabeled"}, {"type": "text", "expr": "Li, Zhizhong and Hoiem, Derek", "word_idx": 56566, "sentence_idx": 869, "label": "unlabeled"}, {"type": "text", "expr": "Learning without forgetting", "word_idx": 56595, "sentence_idx": 870, "label": "unlabeled"}, {"type": "text", "expr": "In  European Conference on Computer Vision , pp", "word_idx": 56622, "sentence_idx": 871, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0614\u2013629", "word_idx": 56669, "sentence_idx": 872, "label": "unlabeled"}, {"type": "text", "expr": "\nSpringer, 2016", "word_idx": 56677, "sentence_idx": 873, "label": "unlabeled"}, {"type": "text", "expr": "European Conference on Computer Vision", "word_idx": 56692, "sentence_idx": 874, "label": "unlabeled"}, {"type": "text", "expr": "Pan & Yang(2010)Pan and Yang \nPan, Sinno\u00a0Jialin and Yang, Qiang", "word_idx": 56730, "sentence_idx": 875, "label": "unlabeled"}, {"type": "text", "expr": "\n\n A survey on transfer learning", "word_idx": 56793, "sentence_idx": 876, "label": "unlabeled"}, {"type": "text", "expr": "Pan & Yang(2010)Pan and Yang", "word_idx": 56825, "sentence_idx": 877, "label": "unlabeled"}, {"type": "text", "expr": "Pan, Sinno\u00a0Jialin and Yang, Qiang", "word_idx": 56853, "sentence_idx": 878, "label": "unlabeled"}, {"type": "text", "expr": "A survey on transfer learning", "word_idx": 56886, "sentence_idx": 879, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Transactions on knowledge and data engineering ,\n22(10):1345\u20131359, 2010", "word_idx": 56915, "sentence_idx": 880, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Transactions on knowledge and data engineering", "word_idx": 56991, "sentence_idx": 881, "label": "unlabeled"}, {"type": "text", "expr": "Quattoni & Torralba(2009)Quattoni and\nTorralba \nQuattoni, A and Torralba, A", "word_idx": 57042, "sentence_idx": 882, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Recognizing indoor scenes", "word_idx": 57117, "sentence_idx": 883, "label": "unlabeled"}, {"type": "text", "expr": "Quattoni & Torralba(2009)Quattoni and\nTorralba", "word_idx": 57145, "sentence_idx": 884, "label": "unlabeled"}, {"type": "text", "expr": "Quattoni, A and Torralba, A", "word_idx": 57191, "sentence_idx": 885, "label": "unlabeled"}, {"type": "text", "expr": "Recognizing indoor scenes", "word_idx": 57218, "sentence_idx": 886, "label": "unlabeled"}, {"type": "text", "expr": "In  2009 IEEE Conference on Computer Vision and Pattern\nRecognition , 2009", "word_idx": 57243, "sentence_idx": 887, "label": "unlabeled"}, {"type": "text", "expr": "2009 IEEE Conference on Computer Vision and Pattern\nRecognition", "word_idx": 57317, "sentence_idx": 888, "label": "unlabeled"}, {"type": "text", "expr": "Romero et\u00a0al", "word_idx": 57380, "sentence_idx": 889, "label": "unlabeled"}, {"type": "text", "expr": "(2014)Romero, Ballas, Kahou, Chassang, Gatta, and\nBengio \nRomero, Adriana, Ballas, Nicolas, Kahou, Samira\u00a0Ebrahimi, Chassang, Antoine,\nGatta, Carlo, and Bengio, Yoshua", "word_idx": 57392, "sentence_idx": 890, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Fitnets: Hints for thin deep nets", "word_idx": 57559, "sentence_idx": 891, "label": "unlabeled"}, {"type": "text", "expr": "Romero et\u00a0al", "word_idx": 57595, "sentence_idx": 892, "label": "unlabeled"}, {"type": "text", "expr": "(2014)Romero, Ballas, Kahou, Chassang, Gatta, and\nBengio", "word_idx": 57607, "sentence_idx": 893, "label": "unlabeled"}, {"type": "text", "expr": "Romero, Adriana, Ballas, Nicolas, Kahou, Samira\u00a0Ebrahimi, Chassang, Antoine,\nGatta, Carlo, and Bengio, Yoshua", "word_idx": 57663, "sentence_idx": 894, "label": "unlabeled"}, {"type": "text", "expr": "Fitnets: Hints for thin deep nets", "word_idx": 57772, "sentence_idx": 895, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1412", "word_idx": 57805, "sentence_idx": 896, "label": "unlabeled"}, {"type": "text", "expr": "6550 , 2014", "word_idx": 57830, "sentence_idx": 897, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1412", "word_idx": 57841, "sentence_idx": 898, "label": "unlabeled"}, {"type": "text", "expr": "Russakovsky et\u00a0al", "word_idx": 57866, "sentence_idx": 899, "label": "unlabeled"}, {"type": "text", "expr": "(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,\nHuang, Karpathy, Khosla, Bernstein, et\u00a0al", "word_idx": 57883, "sentence_idx": 900, "label": "unlabeled"}, {"type": "text", "expr": " \nRussakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan, Satheesh, Sanjeev, Ma,\nSean, Huang, Zhiheng, Karpathy, Andrej, Khosla, Aditya, Bernstein, Michael,\net\u00a0al", "word_idx": 57975, "sentence_idx": 901, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Imagenet large scale visual recognition challenge", "word_idx": 58138, "sentence_idx": 902, "label": "unlabeled"}, {"type": "text", "expr": "Russakovsky et\u00a0al", "word_idx": 58190, "sentence_idx": 903, "label": "unlabeled"}, {"type": "text", "expr": "(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,\nHuang, Karpathy, Khosla, Bernstein, et\u00a0al", "word_idx": 58207, "sentence_idx": 904, "label": "unlabeled"}, {"type": "text", "expr": "Russakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan, Satheesh, Sanjeev, Ma,\nSean, Huang, Zhiheng, Karpathy, Andrej, Khosla, Aditya, Bernstein, Michael,\net\u00a0al", "word_idx": 58299, "sentence_idx": 905, "label": "unlabeled"}, {"type": "text", "expr": "Imagenet large scale visual recognition challenge", "word_idx": 58460, "sentence_idx": 906, "label": "unlabeled"}, {"type": "text", "expr": "International Journal of Computer Vision , 115(3):211\u2013252, 2015", "word_idx": 58509, "sentence_idx": 907, "label": "unlabeled"}, {"type": "text", "expr": "International Journal of Computer Vision", "word_idx": 58572, "sentence_idx": 908, "label": "unlabeled"}, {"type": "text", "expr": "Sau & Balasubramanian(2016)Sau and Balasubramanian \nSau, Bharat\u00a0Bhusan and Balasubramanian, Vineeth\u00a0N", "word_idx": 58612, "sentence_idx": 909, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Deep model compression: Distilling knowledge from noisy teachers", "word_idx": 58713, "sentence_idx": 910, "label": "unlabeled"}, {"type": "text", "expr": "Sau & Balasubramanian(2016)Sau and Balasubramanian", "word_idx": 58780, "sentence_idx": 911, "label": "unlabeled"}, {"type": "text", "expr": "Sau, Bharat\u00a0Bhusan and Balasubramanian, Vineeth\u00a0N", "word_idx": 58830, "sentence_idx": 912, "label": "unlabeled"}, {"type": "text", "expr": "Deep model compression: Distilling knowledge from noisy teachers", "word_idx": 58879, "sentence_idx": 913, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1610", "word_idx": 58943, "sentence_idx": 914, "label": "unlabeled"}, {"type": "text", "expr": "09650 , 2016", "word_idx": 58968, "sentence_idx": 915, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1610", "word_idx": 58980, "sentence_idx": 916, "label": "unlabeled"}, {"type": "text", "expr": "09650", "word_idx": 59005, "sentence_idx": 917, "label": "unlabeled"}, {"type": "text", "expr": "Simonyan & Zisserman(2014)Simonyan and Zisserman \nSimonyan, Karen and Zisserman, Andrew", "word_idx": 59010, "sentence_idx": 918, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Very deep convolutional networks for large-scale image recognition", "word_idx": 59097, "sentence_idx": 919, "label": "unlabeled"}, {"type": "text", "expr": "Simonyan & Zisserman(2014)Simonyan and Zisserman", "word_idx": 59166, "sentence_idx": 920, "label": "unlabeled"}, {"type": "text", "expr": "Simonyan, Karen and Zisserman, Andrew", "word_idx": 59214, "sentence_idx": 921, "label": "unlabeled"}, {"type": "text", "expr": "Very deep convolutional networks for large-scale image recognition", "word_idx": 59251, "sentence_idx": 922, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1409", "word_idx": 59317, "sentence_idx": 923, "label": "unlabeled"}, {"type": "text", "expr": "1556 , 2014", "word_idx": 59342, "sentence_idx": 924, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1409", "word_idx": 59353, "sentence_idx": 925, "label": "unlabeled"}, {"type": "text", "expr": "Srivastava et\u00a0al", "word_idx": 59378, "sentence_idx": 926, "label": "unlabeled"}, {"type": "text", "expr": "(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and\nSalakhutdinov \nSrivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex, Sutskever, Ilya, and\nSalakhutdinov, Ruslan", "word_idx": 59394, "sentence_idx": 927, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Dropout: A simple way to prevent neural networks from overfitting", "word_idx": 59560, "sentence_idx": 928, "label": "unlabeled"}, {"type": "text", "expr": "Srivastava et\u00a0al", "word_idx": 59628, "sentence_idx": 929, "label": "unlabeled"}, {"type": "text", "expr": "(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and\nSalakhutdinov", "word_idx": 59644, "sentence_idx": 930, "label": "unlabeled"}, {"type": "text", "expr": "Srivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex, Sutskever, Ilya, and\nSalakhutdinov, Ruslan", "word_idx": 59710, "sentence_idx": 931, "label": "unlabeled"}, {"type": "text", "expr": "Dropout: A simple way to prevent neural networks from overfitting", "word_idx": 59808, "sentence_idx": 932, "label": "unlabeled"}, {"type": "text", "expr": "The Journal of Machine Learning Research , 15(1):1929\u20131958, 2014", "word_idx": 59873, "sentence_idx": 933, "label": "unlabeled"}, {"type": "text", "expr": "The Journal of Machine Learning Research", "word_idx": 59937, "sentence_idx": 934, "label": "unlabeled"}, {"type": "text", "expr": "Wang et\u00a0al", "word_idx": 59977, "sentence_idx": 935, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Wang, Mohamed, Caruana, Bilmes, Plilipose,\nRichardson, Geras, Urban, and Aslan \nWang, Shengjie, Mohamed, Abdel-rahman, Caruana, Rich, Bilmes, Jeff, Plilipose,\nMatthai, Richardson, Matthew, Geras, Krzysztof, Urban, Gregor, and Aslan,\nOzlem", "word_idx": 59987, "sentence_idx": 936, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Analysis of deep neural networks with extended data jacobian matrix", "word_idx": 60231, "sentence_idx": 937, "label": "unlabeled"}, {"type": "text", "expr": "Wang et\u00a0al", "word_idx": 60301, "sentence_idx": 938, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Wang, Mohamed, Caruana, Bilmes, Plilipose,\nRichardson, Geras, Urban, and Aslan", "word_idx": 60311, "sentence_idx": 939, "label": "unlabeled"}, {"type": "text", "expr": "Wang, Shengjie, Mohamed, Abdel-rahman, Caruana, Rich, Bilmes, Jeff, Plilipose,\nMatthai, Richardson, Matthew, Geras, Krzysztof, Urban, Gregor, and Aslan,\nOzlem", "word_idx": 60395, "sentence_idx": 940, "label": "unlabeled"}, {"type": "text", "expr": "Analysis of deep neural networks with extended data jacobian matrix", "word_idx": 60553, "sentence_idx": 941, "label": "unlabeled"}, {"type": "text", "expr": "In  International Conference on Machine Learning , pp", "word_idx": 60620, "sentence_idx": 942, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0718\u2013726, 2016", "word_idx": 60673, "sentence_idx": 943, "label": "unlabeled"}, {"type": "text", "expr": "International Conference on Machine Learning", "word_idx": 60687, "sentence_idx": 944, "label": "unlabeled"}, {"type": "text", "expr": "Xu & Mannor(2012)Xu and Mannor \nXu, Huan and Mannor, Shie", "word_idx": 60731, "sentence_idx": 945, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Robustness and generalization", "word_idx": 60788, "sentence_idx": 946, "label": "unlabeled"}, {"type": "text", "expr": "Xu & Mannor(2012)Xu and Mannor", "word_idx": 60820, "sentence_idx": 947, "label": "unlabeled"}, {"type": "text", "expr": "Xu, Huan and Mannor, Shie", "word_idx": 60850, "sentence_idx": 948, "label": "unlabeled"}, {"type": "text", "expr": "Robustness and generalization", "word_idx": 60875, "sentence_idx": 949, "label": "unlabeled"}, {"type": "text", "expr": "Machine learning , 86(3):391\u2013423, 2012", "word_idx": 60904, "sentence_idx": 950, "label": "unlabeled"}, {"type": "text", "expr": "Machine learning", "word_idx": 60942, "sentence_idx": 951, "label": "unlabeled"}, {"type": "text", "expr": "Yosinski et\u00a0al", "word_idx": 60958, "sentence_idx": 952, "label": "unlabeled"}, {"type": "text", "expr": "(2014)Yosinski, Clune, Bengio, and\nLipson \nYosinski, Jason, Clune, Jeff, Bengio, Yoshua, and Lipson, Hod", "word_idx": 60972, "sentence_idx": 953, "label": "unlabeled"}, {"type": "text", "expr": "\n\n How transferable are features in deep neural networks?", "word_idx": 61076, "sentence_idx": 954, "label": "unlabeled"}, {"type": "text", "expr": "Yosinski et\u00a0al", "word_idx": 61133, "sentence_idx": 955, "label": "unlabeled"}, {"type": "text", "expr": "(2014)Yosinski, Clune, Bengio, and\nLipson", "word_idx": 61147, "sentence_idx": 956, "label": "unlabeled"}, {"type": "text", "expr": "Yosinski, Jason, Clune, Jeff, Bengio, Yoshua, and Lipson, Hod", "word_idx": 61188, "sentence_idx": 957, "label": "unlabeled"}, {"type": "text", "expr": "How transferable are features in deep neural networks?", "word_idx": 61249, "sentence_idx": 958, "label": "unlabeled"}, {"type": "text", "expr": "In  Advances in neural information processing systems , pp", "word_idx": 61303, "sentence_idx": 959, "label": "unlabeled"}, {"type": "text", "expr": "\u00a03320\u20133328, 2014", "word_idx": 61361, "sentence_idx": 960, "label": "unlabeled"}, {"type": "text", "expr": "Advances in neural information processing systems", "word_idx": 61377, "sentence_idx": 961, "label": "unlabeled"}, {"type": "text", "expr": "Zagoruyko & Komodakis(2017)Zagoruyko and\nKomodakis \nZagoruyko, Sergey and Komodakis, Nikos", "word_idx": 61426, "sentence_idx": 962, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Paying more attention to attention: Improving the performance of\nconvolutional neural networks via attention transfer", "word_idx": 61516, "sentence_idx": 963, "label": "unlabeled"}, {"type": "text", "expr": "Zagoruyko & Komodakis(2017)Zagoruyko and\nKomodakis", "word_idx": 61636, "sentence_idx": 964, "label": "unlabeled"}, {"type": "text", "expr": "Zagoruyko, Sergey and Komodakis, Nikos", "word_idx": 61686, "sentence_idx": 965, "label": "unlabeled"}, {"type": "text", "expr": "Paying more attention to attention: Improving the performance of\nconvolutional neural networks via attention transfer", "word_idx": 61724, "sentence_idx": 966, "label": "unlabeled"}, {"type": "text", "expr": "ICLR , 2017", "word_idx": 61841, "sentence_idx": 967, "label": "unlabeled"}, {"type": "text", "expr": "Supplementary Material", "word_idx": 61852, "sentence_idx": 968, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0A  Proof for Proposition 1", "word_idx": 61874, "sentence_idx": 969, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0A", "word_idx": 61909, "sentence_idx": 970, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 1 ", "word_idx": 61919, "sentence_idx": 971, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 1", "word_idx": 61933, "sentence_idx": 972, "label": "unlabeled"}, {"type": "text", "expr": "Consider the squared error cost function for matching soft targets of two neural networks with  $k$ -length targets ( $\\in\\mathbb{R}^{k}$ ), given by  $\\ell(\\mathcal{T}(\\mathbf{x}),\\mathcal{S}(\\mathbf{x}))=\\sum_{i=1}^{k}(\\mathcal{%\nT}^{i}(\\mathbf{x})-\\mathcal{S}^{i}(\\mathbf{x}))^{2}$ , where  $\\mathbf{x}\\in\\mathbb{R}^{D}$  is an input data point", "word_idx": 61946, "sentence_idx": 973, "label": "unlabeled"}, {"type": "text", "expr": " Let  $\\bm{\\xi}~{}(\\in\\mathbb{R}^{D})=\\sigma~{}\\mathbf{z}$  be a scaled version of a unit normal random variable  $\\mathbf{z}~{}\\in\\mathbb{R}^{D}$  with scaling factor  $\\sigma\\in\\mathbb{R}$ ", "word_idx": 62293, "sentence_idx": 974, "label": "unlabeled"}, {"type": "text", "expr": " Then the following is locally true", "word_idx": 62484, "sentence_idx": 975, "label": "unlabeled"}, {"type": "math", "expr": "$$\\in\\mathbb{R}^{k}$$", "word_idx": 62519, "sentence_idx": 976, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ell(\\mathcal{T}(\\mathbf{x}),\\mathcal{S}(\\mathbf{x}))=\\sum_{i=1}^{k}(\\mathcal{%\nT}^{i}(\\mathbf{x})-\\mathcal{S}^{i}(\\mathbf{x}))^{2}$$", "word_idx": 62536, "sentence_idx": 977, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}\\in\\mathbb{R}^{D}$$", "word_idx": 62667, "sentence_idx": 978, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{\\xi}~{}(\\in\\mathbb{R}^{D})=\\sigma~{}\\mathbf{z}$$", "word_idx": 62694, "sentence_idx": 979, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{z}~{}\\in\\mathbb{R}^{D}$$", "word_idx": 62744, "sentence_idx": 980, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma\\in\\mathbb{R}$$", "word_idx": 62774, "sentence_idx": 981, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\mathbb{E}_{\\bm{\\xi}}\\left[\\sum_{i=1}^{k}\\left(\\mathcal{T}^{i}(%\n\\mathbf{x}+\\bm{\\xi})-\\mathcal{S}^{i}(\\mathbf{x}+\\bm{\\xi})\\right)^{2}\\right]$", "word_idx": 62793, "sentence_idx": 982, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\mathbb{E}_{\\bm{\\xi}}\\left[\\sum_{i=1}^{k}\\left(\\mathcal{T}^{i}(%\n\\mathbf{x}+\\bm{\\xi})-\\mathcal{S}^{i}(\\mathbf{x}+\\bm{\\xi})\\right)^{2}\\right]$$", "word_idx": 62948, "sentence_idx": 983, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\sum_{i=1}^{k}\\left(\\mathcal{T}^{i}(\\mathbf{x})-\\mathcal{S}^{i}(%\n\\mathbf{x})\\right)^{2}+\\sigma^{2}\\sum_{i=1}^{k}\\|\\nabla_{x}\\mathcal{T}^{i}(%\n\\mathbf{x})-\\nabla_{x}\\mathcal{S}^{i}(\\mathbf{x})\\|_{2}^{2}$", "word_idx": 63101, "sentence_idx": 984, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\sum_{i=1}^{k}\\left(\\mathcal{T}^{i}(\\mathbf{x})-\\mathcal{S}^{i}(%\n\\mathbf{x})\\right)^{2}+\\sigma^{2}\\sum_{i=1}^{k}\\|\\nabla_{x}\\mathcal{T}^{i}(%\n\\mathbf{x})-\\nabla_{x}\\mathcal{S}^{i}(\\mathbf{x})\\|_{2}^{2}$$", "word_idx": 63319, "sentence_idx": 985, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle+\\mathcal{O}(\\sigma^{4})$", "word_idx": 63535, "sentence_idx": 986, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle+\\mathcal{O}(\\sigma^{4})$$", "word_idx": 63574, "sentence_idx": 987, "label": "unlabeled"}, {"type": "text", "expr": "Proof", "word_idx": 63611, "sentence_idx": 988, "label": "unlabeled"}, {"type": "text", "expr": "Proof", "word_idx": 63616, "sentence_idx": 989, "label": "unlabeled"}, {"type": "text", "expr": "There exists  $\\sigma$  and  $\\bm{\\xi}$  small enough that first-order Taylor series expansion holds true", "word_idx": 63621, "sentence_idx": 990, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma$$", "word_idx": 63726, "sentence_idx": 991, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{\\xi}$$", "word_idx": 63732, "sentence_idx": 992, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\mathbb{E}_{\\bm{\\xi}}\\left[\\sum_{i=1}^{k}\\left(\\mathcal{T}^{i}(%\n\\mathbf{x}+\\bm{\\xi})-\\mathcal{S}^{i}(\\mathbf{x}+\\bm{\\xi})\\right)^{2}\\right]$", "word_idx": 63740, "sentence_idx": 993, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\mathbb{E}_{\\bm{\\xi}}\\left[\\sum_{i=1}^{k}\\left(\\mathcal{T}^{i}(%\n\\mathbf{x}+\\bm{\\xi})-\\mathcal{S}^{i}(\\mathbf{x}+\\bm{\\xi})\\right)^{2}\\right]$$", "word_idx": 63895, "sentence_idx": 994, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=$", "word_idx": 64048, "sentence_idx": 995, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=$$", "word_idx": 64064, "sentence_idx": 996, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle~{}\\mathbb{E}_{\\bm{\\xi}}\\left[\\sum_{i=1}^{k}\\left(\\mathcal{T}^{i}%\n(\\mathbf{x})+\\bm{\\xi}^{T}\\nabla_{x}\\mathcal{T}^{i}(\\mathbf{x})-\\mathcal{S}^{i}%\n(\\mathbf{x})-\\bm{\\xi}^{T}\\nabla_{x}\\mathcal{S}^{i}(\\mathbf{x})\\right)^{2}\\right]$", "word_idx": 64078, "sentence_idx": 997, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle~{}\\mathbb{E}_{\\bm{\\xi}}\\left[\\sum_{i=1}^{k}\\left(\\mathcal{T}^{i}%\n(\\mathbf{x})+\\bm{\\xi}^{T}\\nabla_{x}\\mathcal{T}^{i}(\\mathbf{x})-\\mathcal{S}^{i}%\n(\\mathbf{x})-\\bm{\\xi}^{T}\\nabla_{x}\\mathcal{S}^{i}(\\mathbf{x})\\right)^{2}\\right]$$", "word_idx": 64320, "sentence_idx": 998, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle+$", "word_idx": 64560, "sentence_idx": 999, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle+$$", "word_idx": 64576, "sentence_idx": 1000, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\mathcal{O}(\\sigma^{4})$", "word_idx": 64590, "sentence_idx": 1001, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\mathcal{O}(\\sigma^{4})$$", "word_idx": 64628, "sentence_idx": 1002, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=$", "word_idx": 64664, "sentence_idx": 1003, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=$$", "word_idx": 64680, "sentence_idx": 1004, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\sum_{i=1}^{k}\\left(\\mathcal{T}^{i}(\\mathbf{x})-\\mathcal{S}^{i}(%\n\\mathbf{x})\\right)^{2}$", "word_idx": 64694, "sentence_idx": 1005, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\sum_{i=1}^{k}\\left(\\mathcal{T}^{i}(\\mathbf{x})-\\mathcal{S}^{i}(%\n\\mathbf{x})\\right)^{2}$$", "word_idx": 64797, "sentence_idx": 1006, "label": "unlabeled"}, {"type": "text", "expr": "+ (11)", "word_idx": 64898, "sentence_idx": 1007, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle+$", "word_idx": 64904, "sentence_idx": 1008, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle+$$", "word_idx": 64920, "sentence_idx": 1009, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\mathbb{E}_{\\bm{\\xi}}\\left[\\sum_{i=1}^{k}\\left[\\bm{\\xi}^{T}\\left(%\n\\nabla_{x}\\mathcal{T}^{i}(\\mathbf{x})-\\nabla_{x}\\mathcal{S}^{i}(\\mathbf{x})%\n\\right)\\right]^{2}\\right]+\\mathcal{O}(\\sigma^{4})$", "word_idx": 64934, "sentence_idx": 1010, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\mathbb{E}_{\\bm{\\xi}}\\left[\\sum_{i=1}^{k}\\left[\\bm{\\xi}^{T}\\left(%\n\\nabla_{x}\\mathcal{T}^{i}(\\mathbf{x})-\\nabla_{x}\\mathcal{S}^{i}(\\mathbf{x})%\n\\right)\\right]^{2}\\right]+\\mathcal{O}(\\sigma^{4})$$", "word_idx": 65142, "sentence_idx": 1011, "label": "unlabeled"}, {"type": "text", "expr": "To get equation  11 , we use the fact that mean of  $\\bm{\\xi}$  is zero", "word_idx": 65348, "sentence_idx": 1012, "label": "unlabeled"}, {"type": "text", "expr": " To complete the proof, we use the diagonal assumption on the covariance matrix of  $\\bm{\\xi}$ ", "word_idx": 65419, "sentence_idx": 1013, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{\\xi}$$", "word_idx": 65514, "sentence_idx": 1014, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{\\xi}$$", "word_idx": 65522, "sentence_idx": 1015, "label": "unlabeled"}, {"type": "text", "expr": "Proofs of other statements are similar", "word_idx": 65530, "sentence_idx": 1016, "label": "unlabeled"}, {"type": "text", "expr": " For proof for cross-entropy loss of Proposition 2, use a second order Taylor series expansion of  $\\log(\\cdot)$  in the first step", "word_idx": 65568, "sentence_idx": 1017, "label": "unlabeled"}, {"type": "math", "expr": "$$\\log(\\cdot)$$", "word_idx": 65699, "sentence_idx": 1018, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0B  Proof for Proposition 3", "word_idx": 65710, "sentence_idx": 1019, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0B", "word_idx": 65745, "sentence_idx": 1020, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 2 ", "word_idx": 65755, "sentence_idx": 1021, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 2", "word_idx": 65769, "sentence_idx": 1022, "label": "unlabeled"}, {"type": "text", "expr": "From the notations in the main text, we have", "word_idx": 65782, "sentence_idx": 1023, "label": "unlabeled"}, {"type": "text", "expr": "From the notations in the main text, we have", "word_idx": 65826, "sentence_idx": 1024, "label": "unlabeled"}, {"type": "text", "expr": "From the notations in the main text, we have", "word_idx": 65870, "sentence_idx": 1025, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\frac{1}{|\\mathcal{D}_{l}|}\\sum_{\\mathbf{x}\\sim\\mathcal{D}_{l}}%\n\\ell(f(\\mathbf{x}),g(\\mathbf{x}))$", "word_idx": 65914, "sentence_idx": 1026, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\frac{1}{|\\mathcal{D}_{l}|}\\sum_{\\mathbf{x}\\sim\\mathcal{D}_{l}}%\n\\ell(f(\\mathbf{x}),g(\\mathbf{x}))$$", "word_idx": 66027, "sentence_idx": 1027, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\leq\\max_{\\mathbf{x}\\sim\\mathcal{D}_{s}}\\ell(f(\\mathbf{x}),g(%\n\\mathbf{x}))$", "word_idx": 66138, "sentence_idx": 1028, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\leq\\max_{\\mathbf{x}\\sim\\mathcal{D}_{s}}\\ell(f(\\mathbf{x}),g(%\n\\mathbf{x}))$$", "word_idx": 66228, "sentence_idx": 1029, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle+\\mathrm{K}\\mathcal{H}_{a}(\\mathcal{D}_{l},\\mathcal{D}_{s})$", "word_idx": 66316, "sentence_idx": 1030, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle+\\mathrm{K}\\mathcal{H}_{a}(\\mathcal{D}_{l},\\mathcal{D}_{s})$$", "word_idx": 66390, "sentence_idx": 1031, "label": "unlabeled"}, {"type": "text", "expr": "Proof", "word_idx": 66462, "sentence_idx": 1032, "label": "unlabeled"}, {"type": "text", "expr": "Proof", "word_idx": 66467, "sentence_idx": 1033, "label": "unlabeled"}, {"type": "text", "expr": "Let us denote  $\\rho(\\mathbf{x})=\\ell(f(\\mathbf{x}),g(\\mathbf{x}))$  for convenience", "word_idx": 66472, "sentence_idx": 1034, "label": "unlabeled"}, {"type": "text", "expr": " Assume Lipschitz continuity for  $\\rho(\\mathbf{x})$  with Lipschitz constant  $\\mathrm{K}$ , and distance metric  $\\psi_{\\mathbf{x}}(\\cdot,\\cdot)$  in the input space -", "word_idx": 66556, "sentence_idx": 1035, "label": "unlabeled"}, {"type": "math", "expr": "$$\\rho(\\mathbf{x})=\\ell(f(\\mathbf{x}),g(\\mathbf{x}))$$", "word_idx": 66725, "sentence_idx": 1036, "label": "unlabeled"}, {"type": "math", "expr": "$$\\rho(\\mathbf{x})$$", "word_idx": 66775, "sentence_idx": 1037, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{K}$$", "word_idx": 66791, "sentence_idx": 1038, "label": "unlabeled"}, {"type": "math", "expr": "$$\\psi_{\\mathbf{x}}(\\cdot,\\cdot)$$", "word_idx": 66801, "sentence_idx": 1039, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\|\\rho(\\mathbf{x}_{1})-\\rho(\\mathbf{x}_{2})\\|_{1}\\leq\\mathrm{K}%\n\\psi_{\\mathbf{x}}(\\mathbf{x}_{1},\\mathbf{x}_{2})$", "word_idx": 66831, "sentence_idx": 1040, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\|\\rho(\\mathbf{x}_{1})-\\rho(\\mathbf{x}_{2})\\|_{1}\\leq\\mathrm{K}%\n\\psi_{\\mathbf{x}}(\\mathbf{x}_{1},\\mathbf{x}_{2})$$", "word_idx": 66959, "sentence_idx": 1041, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\implies\\rho(\\mathbf{x}_{1})\\leq\\rho(\\mathbf{x}_{2})+\\mathrm{K}%\n\\psi_{\\mathbf{x}}(\\mathbf{x}_{1},\\mathbf{x}_{2})$", "word_idx": 67085, "sentence_idx": 1042, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\implies\\rho(\\mathbf{x}_{1})\\leq\\rho(\\mathbf{x}_{2})+\\mathrm{K}%\n\\psi_{\\mathbf{x}}(\\mathbf{x}_{1},\\mathbf{x}_{2})$$", "word_idx": 67213, "sentence_idx": 1043, "label": "unlabeled"}, {"type": "text", "expr": "Assuming that  $\\rho(\\mathbf{x}_{1})\\geq\\rho(\\mathbf{x}_{2})$ ", "word_idx": 67339, "sentence_idx": 1044, "label": "unlabeled"}, {"type": "text", "expr": " Note that it holds even otherwise, but is trivial", "word_idx": 67401, "sentence_idx": 1045, "label": "unlabeled"}, {"type": "math", "expr": "$$\\rho(\\mathbf{x}_{1})\\geq\\rho(\\mathbf{x}_{2})$$", "word_idx": 67451, "sentence_idx": 1046, "label": "unlabeled"}, {"type": "text", "expr": "Now, for every datapoint  $\\mathbf{x}_{l}\\in\\mathcal{D}_{l}$ , there exists a point  $\\mathbf{x}_{s}\\in\\mathcal{D}_{s}$  such that  $\\psi_{\\mathbf{x}}(\\mathbf{x}_{s},\\mathbf{x}_{l})$  is the smallest among all points in  $\\mathcal{D}_{s}$ ", "word_idx": 67495, "sentence_idx": 1047, "label": "unlabeled"}, {"type": "text", "expr": " In other words, we look at the point in  $\\mathcal{D}_{s}$  closest to each point  $\\mathbf{x}_{l}$ ", "word_idx": 67734, "sentence_idx": 1048, "label": "unlabeled"}, {"type": "text", "expr": " Note that in this process only a subset of points  $\\mathrm{d}_{s}\\subseteq\\mathcal{D}_{s}$  are chosen, and individual points can be chosen multiple times", "word_idx": 67835, "sentence_idx": 1049, "label": "unlabeled"}, {"type": "text", "expr": " For these points, we can write", "word_idx": 67991, "sentence_idx": 1050, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}_{l}\\in\\mathcal{D}_{l}$$", "word_idx": 68022, "sentence_idx": 1051, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}_{s}\\in\\mathcal{D}_{s}$$", "word_idx": 68054, "sentence_idx": 1052, "label": "unlabeled"}, {"type": "math", "expr": "$$\\psi_{\\mathbf{x}}(\\mathbf{x}_{s},\\mathbf{x}_{l})$$", "word_idx": 68086, "sentence_idx": 1053, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{D}_{s}$$", "word_idx": 68134, "sentence_idx": 1054, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{D}_{s}$$", "word_idx": 68149, "sentence_idx": 1055, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}_{l}$$", "word_idx": 68164, "sentence_idx": 1056, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{d}_{s}\\subseteq\\mathcal{D}_{s}$$", "word_idx": 68178, "sentence_idx": 1057, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\rho(\\mathbf{x}_{l})\\leq\\rho(\\mathbf{x}_{s})+\\mathrm{K}\\psi_{%\n\\mathbf{x}}(\\mathbf{x}_{s},\\mathbf{x}_{l})$", "word_idx": 68216, "sentence_idx": 1058, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\rho(\\mathbf{x}_{l})\\leq\\rho(\\mathbf{x}_{s})+\\mathrm{K}\\psi_{%\n\\mathbf{x}}(\\mathbf{x}_{s},\\mathbf{x}_{l})$$", "word_idx": 68336, "sentence_idx": 1059, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\implies\\frac{1}{|\\mathcal{D}_{l}|}\\sum_{\\mathbf{x}_{l}\\sim%\n\\mathcal{D}_{l}}\\rho(\\mathbf{x}_{l})\\leq\\frac{1}{|\\mathcal{D}_{l}|}\\sum_{%\n\\mathbf{x}_{s}\\mathrm{~{}closest~{}to~{}}\\mathbf{x}_{l}}$", "word_idx": 68454, "sentence_idx": 1060, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\implies\\frac{1}{|\\mathcal{D}_{l}|}\\sum_{\\mathbf{x}_{l}\\sim%\n\\mathcal{D}_{l}}\\rho(\\mathbf{x}_{l})\\leq\\frac{1}{|\\mathcal{D}_{l}|}\\sum_{%\n\\mathbf{x}_{s}\\mathrm{~{}closest~{}to~{}}\\mathbf{x}_{l}}$$", "word_idx": 68661, "sentence_idx": 1061, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\rho(\\mathbf{x}_{s})$", "word_idx": 68866, "sentence_idx": 1062, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\rho(\\mathbf{x}_{s})$$", "word_idx": 68901, "sentence_idx": 1063, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle+\\frac{1}{|\\mathcal{D}_{l}|}\\sum_{\\mathbf{x}_{s}\\mathrm{~{}%\nclosest~{}to~{}}\\mathbf{x}_{l}}$", "word_idx": 68934, "sentence_idx": 1064, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle+\\frac{1}{|\\mathcal{D}_{l}|}\\sum_{\\mathbf{x}_{s}\\mathrm{~{}%\nclosest~{}to~{}}\\mathbf{x}_{l}}$$", "word_idx": 69041, "sentence_idx": 1065, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\mathrm{K}\\psi_{\\mathbf{x}}(\\mathbf{x}_{s},\\mathbf{x}_{l})$", "word_idx": 69146, "sentence_idx": 1066, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\mathrm{K}\\psi_{\\mathbf{x}}(\\mathbf{x}_{s},\\mathbf{x}_{l})$$", "word_idx": 69219, "sentence_idx": 1067, "label": "unlabeled"}, {"type": "text", "expr": "We see that  $\\frac{1}{|\\mathcal{D}_{l}|}\\sum_{\\mathbf{x}_{s}}\\rho(\\mathbf{x}_{s})\\leq\\max_{%\n\\mathbf{x}\\sim\\mathrm{d}_{s}}\\rho(\\mathbf{x})\\leq\\max_{\\mathbf{x}\\sim\\mathcal{%\nD}_{s}}\\rho(\\mathbf{x})$ , which is a consequence of the fact that the max is greater than any convex combination of elements", "word_idx": 69290, "sentence_idx": 1068, "label": "unlabeled"}, {"type": "math", "expr": "$$\\frac{1}{|\\mathcal{D}_{l}|}\\sum_{\\mathbf{x}_{s}}\\rho(\\mathbf{x}_{s})\\leq\\max_{%\n\\mathbf{x}\\sim\\mathrm{d}_{s}}\\rho(\\mathbf{x})\\leq\\max_{\\mathbf{x}\\sim\\mathcal{%\nD}_{s}}\\rho(\\mathbf{x})$$", "word_idx": 69589, "sentence_idx": 1069, "label": "unlabeled"}, {"type": "text", "expr": "Also, we have  $\\psi_{\\mathbf{x}}(\\mathbf{x}_{l},\\mathbf{x}_{s})\\leq\\mathcal{H}_{a}(\\mathcal{D%\n}_{l},\\mathcal{D}_{s})$ , which is the maximum distance between any two \u2018closest\u2019 points from  $\\mathcal{D}_{l}$  to  $\\mathcal{D}_{s}$  (by definition)", "word_idx": 69772, "sentence_idx": 1070, "label": "unlabeled"}, {"type": "math", "expr": "$$\\psi_{\\mathbf{x}}(\\mathbf{x}_{l},\\mathbf{x}_{s})\\leq\\mathcal{H}_{a}(\\mathcal{D%\n}_{l},\\mathcal{D}_{s})$$", "word_idx": 70020, "sentence_idx": 1071, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{D}_{l}$$", "word_idx": 70122, "sentence_idx": 1072, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{D}_{s}$$", "word_idx": 70137, "sentence_idx": 1073, "label": "unlabeled"}, {"type": "text", "expr": "Applying these bounds, we have the final result", "word_idx": 70152, "sentence_idx": 1074, "label": "unlabeled"}, {"type": "text", "expr": "Applying these bounds, we have the final result", "word_idx": 70199, "sentence_idx": 1075, "label": "unlabeled"}, {"type": "text", "expr": "1  Proof for Corollary", "word_idx": 70246, "sentence_idx": 1076, "label": "unlabeled"}, {"type": "text", "expr": "Corollary ", "word_idx": 70268, "sentence_idx": 1077, "label": "unlabeled"}, {"type": "text", "expr": "Corollary", "word_idx": 70278, "sentence_idx": 1078, "label": "unlabeled"}, {"type": "text", "expr": "For any superset  $\\mathrm{D^{\\prime}}_{s}\\supseteq\\mathcal{D}_{s}$  of the target dataset,  $\\mathcal{H}_{a}(\\mathcal{D}_{l},\\mathcal{D^{\\prime}}_{s})\\leq\\mathcal{H}_{a}(%\n\\mathcal{D}_{l},\\mathcal{D}_{s})$", "word_idx": 70287, "sentence_idx": 1079, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{D^{\\prime}}_{s}\\supseteq\\mathcal{D}_{s}$$", "word_idx": 70493, "sentence_idx": 1080, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{H}_{a}(\\mathcal{D}_{l},\\mathcal{D^{\\prime}}_{s})\\leq\\mathcal{H}_{a}(%\n\\mathcal{D}_{l},\\mathcal{D}_{s})$$", "word_idx": 70540, "sentence_idx": 1081, "label": "unlabeled"}, {"type": "text", "expr": "Proof", "word_idx": 70651, "sentence_idx": 1082, "label": "unlabeled"}, {"type": "text", "expr": "Proof", "word_idx": 70656, "sentence_idx": 1083, "label": "unlabeled"}, {"type": "text", "expr": "From the previous proof, we have  $\\rho(\\mathbf{x}_{l})\\leq\\rho(\\mathbf{x}_{s})+\\mathrm{K}\\psi_{\\mathbf{x}}(%\n\\mathbf{x}_{s},\\mathbf{x}_{l})$  for an individual point  $\\mathbf{x}_{l}$ ", "word_idx": 70661, "sentence_idx": 1084, "label": "unlabeled"}, {"type": "text", "expr": " Now if we have  $\\mathrm{D^{\\prime}}_{s}\\supseteq\\mathcal{D}_{s}$ , then we have  $\\rho(\\mathbf{x}_{l})\\leq\\rho(\\mathbf{x}^{\\prime}_{s})+\\mathrm{K}\\psi_{\\mathbf{%\nx}}(\\mathbf{x}^{\\prime}_{s},\\mathbf{x}_{l})$ , where  $\\mathbf{x}^{\\prime}_{s}$  is the new point closest to  $\\mathbf{x}_{l}$ ", "word_idx": 70846, "sentence_idx": 1085, "label": "unlabeled"}, {"type": "text", "expr": " It is clear that  $\\psi_{\\mathbf{x}}(\\mathbf{x}^{\\prime}_{s},\\mathbf{x}_{l})\\leq\\psi_{\\mathbf{x}}%\n(\\mathbf{x}_{s},\\mathbf{x}_{l})$  for all  $\\mathbf{x}_{l}$ ", "word_idx": 71137, "sentence_idx": 1086, "label": "unlabeled"}, {"type": "text", "expr": " Hence it follows that  $\\mathcal{H}_{a}(\\mathcal{D}_{l},\\mathcal{D^{\\prime}}_{s})\\leq\\mathcal{H}_{a}(%\n\\mathcal{D}_{l},\\mathcal{D}_{s})$ ", "word_idx": 71297, "sentence_idx": 1087, "label": "unlabeled"}, {"type": "math", "expr": "$$\\rho(\\mathbf{x}_{l})\\leq\\rho(\\mathbf{x}_{s})+\\mathrm{K}\\psi_{\\mathbf{x}}(%\n\\mathbf{x}_{s},\\mathbf{x}_{l})$$", "word_idx": 71435, "sentence_idx": 1088, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}_{l}$$", "word_idx": 71540, "sentence_idx": 1089, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{D^{\\prime}}_{s}\\supseteq\\mathcal{D}_{s}$$", "word_idx": 71554, "sentence_idx": 1090, "label": "unlabeled"}, {"type": "math", "expr": "$$\\rho(\\mathbf{x}_{l})\\leq\\rho(\\mathbf{x}^{\\prime}_{s})+\\mathrm{K}\\psi_{\\mathbf{%\nx}}(\\mathbf{x}^{\\prime}_{s},\\mathbf{x}_{l})$$", "word_idx": 71601, "sentence_idx": 1091, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}^{\\prime}_{s}$$", "word_idx": 71724, "sentence_idx": 1092, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}_{l}$$", "word_idx": 71747, "sentence_idx": 1093, "label": "unlabeled"}, {"type": "math", "expr": "$$\\psi_{\\mathbf{x}}(\\mathbf{x}^{\\prime}_{s},\\mathbf{x}_{l})\\leq\\psi_{\\mathbf{x}}%\n(\\mathbf{x}_{s},\\mathbf{x}_{l})$$", "word_idx": 71761, "sentence_idx": 1094, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}_{l}$$", "word_idx": 71872, "sentence_idx": 1095, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{H}_{a}(\\mathcal{D}_{l},\\mathcal{D^{\\prime}}_{s})\\leq\\mathcal{H}_{a}(%\n\\mathcal{D}_{l},\\mathcal{D}_{s})$$", "word_idx": 71886, "sentence_idx": 1096, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0C  Experimental details", "word_idx": 71997, "sentence_idx": 1097, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0C", "word_idx": 72029, "sentence_idx": 1098, "label": "unlabeled"}, {"type": "text", "expr": "1  VGG Network Architectures", "word_idx": 72039, "sentence_idx": 1099, "label": "unlabeled"}, {"type": "text", "expr": "The architecture for our networks follow the VGG design philosophy", "word_idx": 72067, "sentence_idx": 1100, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, we have blocks with the following elements:", "word_idx": 72133, "sentence_idx": 1101, "label": "unlabeled"}, {"type": "text", "expr": "The architecture for our networks follow the VGG design philosophy", "word_idx": 72191, "sentence_idx": 1102, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, we have blocks with the following elements:", "word_idx": 72257, "sentence_idx": 1103, "label": "unlabeled"}, {"type": "text", "expr": "$3\\times 3$  conv kernels with  $c$  channels of stride 1", "word_idx": 72315, "sentence_idx": 1104, "label": "unlabeled"}, {"type": "math", "expr": "$$3\\times 3$$", "word_idx": 72372, "sentence_idx": 1105, "label": "unlabeled"}, {"type": "text", "expr": "Batch Normalization", "word_idx": 72381, "sentence_idx": 1106, "label": "unlabeled"}, {"type": "text", "expr": "Batch Normalization", "word_idx": 72400, "sentence_idx": 1107, "label": "unlabeled"}, {"type": "text", "expr": "Whenever we use Max-pooling (M), we use stride  $2$  and window size  $2$ ", "word_idx": 72419, "sentence_idx": 1108, "label": "unlabeled"}, {"type": "text", "expr": "The architecture for VGG-9 is -  $[64-M-128-M-256-256-M-512-512-M-512-512-M]$ ", "word_idx": 72493, "sentence_idx": 1109, "label": "unlabeled"}, {"type": "text", "expr": " Here, the number stands for the number of convolution channels, and  $M$  represents max-pooling", "word_idx": 72571, "sentence_idx": 1110, "label": "unlabeled"}, {"type": "text", "expr": " At the end of all the convolutional and max-pooling layers, we have a Global Average Pooling (GAP) layer, after which we have a fully connected layer leading up to the final classes", "word_idx": 72668, "sentence_idx": 1111, "label": "unlabeled"}, {"type": "text", "expr": " Similar architecture is used for the case of both CIFAR and MIT Scene experiments", "word_idx": 72850, "sentence_idx": 1112, "label": "unlabeled"}, {"type": "math", "expr": "$$[64-M-128-M-256-256-M-512-512-M-512-512-M]$$", "word_idx": 72932, "sentence_idx": 1113, "label": "unlabeled"}, {"type": "text", "expr": "The architecture for VGG-4 is -  $[64-M-128-M-512-M]$ ", "word_idx": 72974, "sentence_idx": 1114, "label": "unlabeled"}, {"type": "math", "expr": "$$[64-M-128-M-512-M]$$", "word_idx": 73028, "sentence_idx": 1115, "label": "unlabeled"}, {"type": "text", "expr": "2  Loss function", "word_idx": 73046, "sentence_idx": 1116, "label": "unlabeled"}, {"type": "text", "expr": "The loss function for distillation experiments use the following form", "word_idx": 73062, "sentence_idx": 1117, "label": "unlabeled"}, {"type": "text", "expr": "The loss function for distillation experiments use the following form", "word_idx": 73131, "sentence_idx": 1118, "label": "unlabeled"}, {"type": "text", "expr": "$\\ell(\\mathcal{S},\\mathcal{T})=\\alpha\\times\\mathrm{(CE)}+\\beta\\times\\mathrm{(%\nMatch~{}Activations)}+\\gamma\\times\\mathrm{(Match~{}Jacobians)}$", "word_idx": 73200, "sentence_idx": 1119, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ell(\\mathcal{S},\\mathcal{T})=\\alpha\\times\\mathrm{(CE)}+\\beta\\times\\mathrm{(%\nMatch~{}Activations)}+\\gamma\\times\\mathrm{(Match~{}Jacobians)}$$", "word_idx": 73342, "sentence_idx": 1120, "label": "unlabeled"}, {"type": "text", "expr": "In our experiments,  $\\alpha,\\beta,\\gamma$  are either set to  $1$  or  $0$ ", "word_idx": 73482, "sentence_idx": 1121, "label": "unlabeled"}, {"type": "text", "expr": " In other words, all regularization constants are  $1$ ", "word_idx": 73558, "sentence_idx": 1122, "label": "unlabeled"}, {"type": "math", "expr": "$$\\alpha,\\beta,\\gamma$$", "word_idx": 73613, "sentence_idx": 1123, "label": "unlabeled"}, {"type": "text", "expr": "Here, \u2018CE\u2019 refers to cross-entropy with ground truth labels", "word_idx": 73632, "sentence_idx": 1124, "label": "unlabeled"}, {"type": "text", "expr": " \u2018Match Activations\u2019 refers to squared error term over pre-softmax activations of the form  $(y_{s}-y_{t})^{2}$ ", "word_idx": 73691, "sentence_idx": 1125, "label": "unlabeled"}, {"type": "text", "expr": " \u2018Match Jacobians\u2019 refers to the same squared error term, but for Jacobians", "word_idx": 73803, "sentence_idx": 1126, "label": "unlabeled"}, {"type": "math", "expr": "$$(y_{s}-y_{t})^{2}$$", "word_idx": 73878, "sentence_idx": 1127, "label": "unlabeled"}, {"type": "text", "expr": "For the MIT Scene experiments,  $\\alpha,\\beta,\\gamma$  are either set to  $10$  or  $0$ , depending on the specific method", "word_idx": 73895, "sentence_idx": 1128, "label": "unlabeled"}, {"type": "text", "expr": " To compute the Jacobian, we use average pooling over a  $feature~{}size/5$  window with a stride of  $1$ ", "word_idx": 74017, "sentence_idx": 1129, "label": "unlabeled"}, {"type": "text", "expr": " We match the Jacobian after the first residual block for resnet, and after the second max-pool for VGG", "word_idx": 74123, "sentence_idx": 1130, "label": "unlabeled"}, {"type": "text", "expr": " This corresponds to feature level \u201c1\u201d in the ablation experiments", "word_idx": 74226, "sentence_idx": 1131, "label": "unlabeled"}, {"type": "math", "expr": "$$\\alpha,\\beta,\\gamma$$", "word_idx": 74292, "sentence_idx": 1132, "label": "unlabeled"}, {"type": "math", "expr": "$$feature~{}size/5$$", "word_idx": 74311, "sentence_idx": 1133, "label": "unlabeled"}, {"type": "text", "expr": "3  Optimization", "word_idx": 74327, "sentence_idx": 1134, "label": "unlabeled"}, {"type": "text", "expr": "For CIFAR100 experiments, we run optimization for  $500$  epochs", "word_idx": 74342, "sentence_idx": 1135, "label": "unlabeled"}, {"type": "text", "expr": " We use the Adam optimizer, with an initial learning rate of  $1e-3$ , and a single learning rate annealing (to  $1e-4$ ) at  $400$  epochs", "word_idx": 74406, "sentence_idx": 1136, "label": "unlabeled"}, {"type": "text", "expr": " We used a batch size of  $128$ ", "word_idx": 74545, "sentence_idx": 1137, "label": "unlabeled"}, {"type": "math", "expr": "$$500$$", "word_idx": 74577, "sentence_idx": 1138, "label": "unlabeled"}, {"type": "math", "expr": "$$1e-3$$", "word_idx": 74580, "sentence_idx": 1139, "label": "unlabeled"}, {"type": "math", "expr": "$$1e-4$$", "word_idx": 74584, "sentence_idx": 1140, "label": "unlabeled"}, {"type": "math", "expr": "$$400$$", "word_idx": 74588, "sentence_idx": 1141, "label": "unlabeled"}, {"type": "math", "expr": "$$128$$", "word_idx": 74591, "sentence_idx": 1142, "label": "unlabeled"}, {"type": "text", "expr": "For MIT Scenes, we used SGD with momentum of  $09$ , for  $75$  epochs", "word_idx": 74594, "sentence_idx": 1143, "label": "unlabeled"}, {"type": "text", "expr": " The initial learning rate is  $1e-3$ , and it is reduced  $10$  times after  $40$  and  $60$  epochs", "word_idx": 74664, "sentence_idx": 1144, "label": "unlabeled"}, {"type": "text", "expr": " We used batch size  $8$ ", "word_idx": 74765, "sentence_idx": 1145, "label": "unlabeled"}, {"type": "text", "expr": " This is because the Jacobian computation is very memory intensive", "word_idx": 74790, "sentence_idx": 1146, "label": "unlabeled"}, {"type": "math", "expr": "$$0.9$$", "word_idx": 74856, "sentence_idx": 1147, "label": "unlabeled"}, {"type": "math", "expr": "$$1e-3$$", "word_idx": 74859, "sentence_idx": 1148, "label": "unlabeled"}, {"type": "text", "expr": "Generated  on Wed Mar  7 13:39:06 2018 by", "word_idx": 74863, "sentence_idx": 1149, "label": "unlabeled"}, {"type": "text", "expr": "LaTeXML", "word_idx": 74904, "sentence_idx": 1150, "label": "unlabeled"}]}