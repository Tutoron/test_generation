{"doc_name": "WaveNet_A_Generative_Model_for_Raw_Audio", "symbol_expr": "x_{t}", "ranked_sentence": ["One approach to modeling the conditional distributions  $p\\left(x_{t}\\mid x_{1},\\dots,x_{t-1}\\right)$  over the individual audio samples would be to use a mixture model such as a mixture density network   or mixture of conditional Gaussian scale mixtures (MCGSM)  ", "Each audio sample  $x_{t}$  is therefore conditioned on the samples at all previous timesteps", "$$p\\left(\\mathbf{x}\\right)=\\prod_{t=1}^{T}p\\left(x_{t}\\mid x_{1},\\dots,x_{t-1}\\right)$$", " By using causal convolutions, we make sure the model cannot violate the ordering in which we model the data: the prediction  $p\\left(x_{t+1}\\mid x_{1},,x_{t}\\right)$  emitted by the model at timestep  $t$  cannot depend on any of the future timesteps  $x_{t+1},x_{t+2},\\dots,x_{T}$  as shown in Fig", " The model outputs a categorical distribution over the next value  $x_{t}$  with a softmax layer and it is optimized to maximize the log-likelihood of the data w", "$p\\left(\\mathbf{x}\\right)=\\prod_{t=1}^{T}p\\left(x_{t}\\mid x_{1},\\dots,x_{t-1}\\right)$", "$$x_{t}$$", "$\\displaystyle x_{t}$", "$$x_{t}$$", "$$\\displaystyle x_{t}$$", "$$f\\left(x_{t}\\right)=\\operatorname{sign}(x_{t})\\frac{\\ln\\left(1+\\mu\\left|x_{t}%\n\\right|\\right)}{\\ln\\left(1+\\mu\\right)},$$", "$$-1<x_{t}<1$$", "$$p\\left(\\mathbf{x}\\mid\\mathbf{h}\\right)=\\prod_{t=1}^{T}p\\left(x_{t}\\mid x_{1},%\n\\dots,x_{t-1},\\mathbf{h}\\right).$$", "$f\\left(x_{t}\\right)=\\operatorname{sign}(x_{t})\\frac{\\ln\\left(1+\\mu\\left|x_{t}%\n\\right|\\right)}{\\ln\\left(1+\\mu\\right)},$", "$$p\\left(x_{t+1}\\mid x_{1},...,x_{t}\\right)$$", "where  $-1<x_{t}<1$  and  $\\mu=255$ ", "$$p\\left(x_{t}\\mid x_{1},\\dots,x_{t-1}\\right)$$", "$p\\left(\\mathbf{x}\\mid\\mathbf{h}\\right)=\\prod_{t=1}^{T}p\\left(x_{t}\\mid x_{1},%\n\\dots,x_{t-1},\\mathbf{h}\\right)$"]}