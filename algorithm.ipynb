{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "import graphviz\n",
    "import nltk\n",
    "from nltk.chunk import *\n",
    "from nltk.chunk.util import *\n",
    "from nltk.chunk.regexp import *\n",
    "from nltk import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordToVec(sentences):\n",
    "    vec = np.zeros(len(sentences))\n",
    "    word = [s.split(' ') for s in sentences]\n",
    "    #print(word)\n",
    "    model = Word2Vec(word, min_count=1)\n",
    "    #print(model.wv.get_vector(word[0][0]).shape)\n",
    "    for i in range(len(word)):\n",
    "        temp = np.zeros(100)\n",
    "        for j in range(len(word[i])):\n",
    "            temp += model.wv.get_vector(word[i][j])\n",
    "        # take average of the sentences\n",
    "        vec[i] = np.sum(temp)\n",
    "    #print(vec)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  toBe defVerb verb punctuation pronoun\n",
      "0    1       0    1           0       0\n",
      "1    1       1    1           0       0\n",
      "2    0       0    1           0       0\n",
      "3    0       0    0           1       0\n",
      "4    0       0    0           0       1\n"
     ]
    }
   ],
   "source": [
    "def ruleBasedSearch(sentenceList):\n",
    "    # return new dataframe, colName: to be, other verb, verb, punctuation, pronoun\n",
    "    newPd = pd.DataFrame(columns=['toBe','defVerb','verb','punctuation','pronoun'])\n",
    "    for sen in sentenceList:\n",
    "        #print(sen)\n",
    "        result = [0]*5\n",
    "        # tobe\n",
    "        reTOBE = re.compile(r'\\b(be|is|are|was|were|being{1})\\b', flags=re.IGNORECASE)\n",
    "        if (reTOBE.findall(sen)!=[]):\n",
    "            result[0] = 1\n",
    "        # defVerb\n",
    "        reDefVerb = re.compile(r'\\b(as|define|defines|defined|mean|means|meant|refer|refers|referred{1})\\b', flags=re.IGNORECASE)\n",
    "        if (reDefVerb.findall(sen)!=[]):\n",
    "            result[1] = 1\n",
    "        # punctuation\n",
    "        rePunc = re.compile(r'(:|-|\\()')\n",
    "        if (rePunc.findall(sen)!=[]):\n",
    "            result[3] = 1\n",
    "        \n",
    "        \n",
    "        tokens = nltk.word_tokenize(sen)\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        #print(tagged)\n",
    "\n",
    "        tagged_text_string = \" \".join([\"{}/{}\".format(word,pos) for word, pos in tagged])\n",
    "        gold_chunked_text = tagstr2tree(tagged_text_string)\n",
    "        unchunked_text = gold_chunked_text.flatten()\n",
    "        \n",
    "        # verb\n",
    "        chunk_rule = ChunkRule(\"<VB|VBG|VBD|VBN|VBP|VBZ>\", \"Chunk verb\")\n",
    "        chunk_parser = RegexpChunkParser([chunk_rule], chunk_label='NP')\n",
    "        chunked_text = chunk_parser.parse(unchunked_text)\n",
    "        #print(chunked_text)\n",
    "        if (chunked_text.height()>=3):\n",
    "            result[2] = 1\n",
    "        # pronoun\n",
    "        chunk_rule1 = ChunkRule(\"<PRP|PRP\\$>\", \"Chunk pronoun\")\n",
    "        chunk_parser1 = RegexpChunkParser([chunk_rule1], chunk_label='NP')\n",
    "        chunked_text1 = chunk_parser1.parse(unchunked_text)\n",
    "        if (chunked_text1.height()>=3):\n",
    "            result[4] = 1\n",
    "        \n",
    "        \"\"\"\n",
    "        posTag = ' '.join(pos for _,pos in nltk.pos_tag(nltk.word_tokenize(sen)))\n",
    "        print(posTag)\n",
    "        # verb\n",
    "        reVerb = re.compile(r'(VB?)', flags=re.IGNORECASE)\n",
    "        if (reVerb.findall(sen)!=[]):\n",
    "            result[2] = 1\n",
    "        # pronoun\n",
    "        rePronoun = re.compile(r'(PRP)', flags=re.IGNORECASE)\n",
    "        if (rePronoun.findall(sen)!=[]):\n",
    "            result[4] = 1\n",
    "        \"\"\"\n",
    "        newPd.loc[len(newPd)] = result\n",
    "    return newPd\n",
    "\n",
    "#test = [\"is not sth\",\"a is defined as sth\",\"i am moving a car\",\"a() sth\",\"in our model\"]\n",
    "#print(ruleBasedSearch(test))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             sentence   section  \\\n",
      "0   In particular consider the conditional log-pro...  0.222222   \n",
      "1   With this scoring function in mind, our main f...  0.222222   \n",
      "2   The distribution of interest, EQUATION, is a c...  0.333333   \n",
      "3         The full model is: EQUATION, OBJECT, OBJECT  0.370370   \n",
      "4   The parameters are EQUATION where OBJECT is a ...  0.370370   \n",
      "5   Once we have defined the local conditional mod...  0.444443   \n",
      "6                            OBJECT, EQUATION, OBJECT  0.444444   \n",
      "7   \\Require{Parameters EQUATION, beam size OBJECT...  0.444444   \n",
      "8   We do this by modifying our scoring function t...  0.444444   \n",
      "9   After training the main neural model, we fix E...  0.444444   \n",
      "10  where EQUATION is a size of the mini-batch and...  0.600000   \n",
      "11  The leading term of the number of multiply-add...  0.714286   \n",
      "12  By the weak law of large numbers, when EQUATIO...  0.600000   \n",
      "13                           OBJECT, EQUATION, OBJECT  0.600000   \n",
      "14                           OBJECT, EQUATION, OBJECT  0.600000   \n",
      "\n",
      "    sameSubSection  sameSection  distanceToSection  numAppearance  \\\n",
      "0                0            0                  3            0.1   \n",
      "1                0            0                  3            0.2   \n",
      "2                0            0                  2            0.3   \n",
      "3                0            0                  1            0.4   \n",
      "4                0            0                  1            0.5   \n",
      "5                0            1                  0            0.6   \n",
      "6                1            1                  0            0.7   \n",
      "7                1            1                  0            0.8   \n",
      "8                1            1                  0            0.9   \n",
      "9                1            1                  0            1.0   \n",
      "10               0            0                  1            0.6   \n",
      "11               1            1                  0            1.0   \n",
      "12               0            0                  1            0.8   \n",
      "13               0            0                  1            0.2   \n",
      "14               0            0                  1            0.4   \n",
      "\n",
      "    distanceToSymbol  label toBe defVerb verb punctuation pronoun  \n",
      "0                  7      0    0       0    1           1       0  \n",
      "1                  6      1    1       0    1           1       1  \n",
      "2                  5      0    1       0    1           0       0  \n",
      "3                  4      1    1       0    1           1       0  \n",
      "4                  3      2    1       0    1           0       0  \n",
      "5                  2      1    0       1    1           1       1  \n",
      "6                  1      0    0       0    0           0       0  \n",
      "7                  1      0    0       0    0           0       0  \n",
      "8                  2      0    1       1    1           1       1  \n",
      "9                  3      1    0       0    1           0       1  \n",
      "10                 2      2    1       0    1           1       0  \n",
      "11                 1      2    1       0    1           1       1  \n",
      "12                 1      1    1       1    1           1       0  \n",
      "13                 2      0    0       0    0           0       0  \n",
      "14                 3      0    0       0    0           0       0  \n"
     ]
    }
   ],
   "source": [
    "#####\n",
    "# file strcture:\n",
    "# 1. sentence: equation with symbol replaced with EQUATION\n",
    "#              equation without symbol replaced with OBJECT\n",
    "# 2. section sentence appears: number of section/total section\n",
    "#             e.g.: Introduction:1/n, Section 3.1 (there are 3.1-3.3):3.33333/n\n",
    "# 3. sameSubSection: is the symbol selected the same subsection as the sentence, binary 0/1\n",
    "# 4. sameSection: is the symbol selected the same section as the sentence, binary 0/1\n",
    "# 5. distanceToSection: how many sections between sentence & symbol\n",
    "# 6. number of appearance: number of symbol appearance/total number of appearance\n",
    "# 7. distanceToSymbol: how many sentences between current sentence & symbol\n",
    "# 8. label: 0 - weak, 1 - medium, 2 - strong\n",
    "\n",
    "df = pd.read_csv('testset_jenny.csv', names=['sentence','section','sameSubSection','sameSection','distanceToSection'\n",
    "                                             ,'numAppearance','distanceToSymbol','label'], header=None)\n",
    "#print(type(df[df.columns[0]]))\n",
    "\"\"\" #word2Vec representation\n",
    "vec = wordToVec(df[df.columns[0]].tolist())\n",
    "df.insert(1,\"vec\",vec,allow_duplicates=True)\n",
    "\"\"\"\n",
    "\n",
    "vec = ruleBasedSearch(df[df.columns[0]].tolist())\n",
    "df = pd.concat([df,vec], axis=1)\n",
    "print(df)\n",
    "\n",
    "#############\n",
    "# now the columns are: 'sentence','section','sameSubSection','sameSection','distanceToSection','numAppearance'\n",
    "#     ,'distanceToSymbol','label','toBe','defVerb','verb','punctuation','pronoun'\n",
    "#############\n",
    " \n",
    "#X = df[['vec','section','numAppearance']]\n",
    "#X = df[['section','sameSubSection','sameSection','distanceToSection','distanceToSymbol']]\n",
    "#X = df[['sameSubSection','sameSection','distanceToSection','distanceToSymbol']]\n",
    "X = df[['toBe','defVerb','verb','punctuation','pronoun','section','sameSubSection','sameSection','distanceToSection','numAppearance','distanceToSymbol']]\n",
    "Y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algo coefficients:\n",
      "[[-5.41600090e-01  4.58399910e-01 -7.44459086e-01 -7.44459086e-01\n",
      "  -1.00000000e+00 -1.33061261e-02  3.40104084e-01  3.40104084e-01\n",
      "   2.54177924e-01 -4.00611704e-01  1.87201026e-01]\n",
      " [-8.66676922e-01  1.00000000e+00 -8.66676922e-01 -1.71383179e-01\n",
      "   0.00000000e+00 -1.73396725e-01  3.33836269e-01  3.33836269e-01\n",
      "  -2.91300115e-02 -2.37780885e-01  5.80282503e-01]\n",
      " [-6.23476788e-01  1.00000000e+00 -4.44089210e-16  3.76523212e-01\n",
      "   1.28964281e-01 -1.23887292e-01  1.28964281e-01  1.28964281e-01\n",
      "  -1.28964281e-01  1.77378568e-01  8.60060733e-01]]\n",
      "intercept:\n",
      "[ 0.45918149  0.01970929 -2.08922388]\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(X,Y)\n",
    "print(\"algo coefficients:\")\n",
    "print(clf.coef_)\n",
    "print(\"intercept:\")\n",
    "print(clf.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "print(clf.predict([list(X.iloc[4])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algo coefficients:\n",
      "[[-4.48583225e-01  3.10902622e-01 -8.21451143e-01 -3.48818405e-01\n",
      "  -4.64027397e-01 -2.72147493e-02  4.68629335e-01  2.53647512e-01\n",
      "   1.64359361e-01 -2.13806896e-01  2.25532421e-01]\n",
      " [-1.98061115e-01  3.57779187e-01  3.95049388e-01  3.48169638e-01\n",
      "   4.74558048e-01 -7.73825036e-02 -3.60438712e-01 -6.08508559e-02\n",
      "  -1.02843539e-01  1.55858566e-01  1.81404935e-01]\n",
      " [ 6.46644340e-01 -6.68681809e-01  4.26401755e-01  6.48766071e-04\n",
      "  -1.05306503e-02  1.04597253e-01 -1.08190622e-01 -1.92796656e-01\n",
      "  -6.15158224e-02  5.79483296e-02 -4.06937356e-01]]\n",
      "intercept:\n",
      "[ 0.67625073 -0.8383667   0.16211597]\n",
      "difference in coefficients:\n",
      "[[-0.09301686  0.14749729  0.07699206 -0.39564068 -0.5359726   0.01390862\n",
      "  -0.12852525  0.08645657  0.08981856 -0.18680481 -0.03833139]\n",
      " [-0.66861581  0.64222081 -1.26172631 -0.51955282 -0.47455805 -0.09601422\n",
      "   0.69427498  0.39468712  0.07371353 -0.39363945  0.39887757]\n",
      " [-1.27012113  1.66868181 -0.42640176  0.37587445  0.13949493 -0.22848455\n",
      "   0.2371549   0.32176094 -0.06744846  0.11943024  1.26699809]]\n",
      "difference in intercept:\n",
      "[-0.21706923  0.85807599 -2.25133985]\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Logit\n",
    "mlr = lr(multi_class=\"multinomial\", solver=\"newton-cg\")\n",
    "mlr.fit(X,Y)\n",
    "print(\"algo coefficients:\")\n",
    "print(mlr.coef_)\n",
    "print(\"intercept:\")\n",
    "print(mlr.intercept_)\n",
    "print(\"difference in coefficients:\")\n",
    "print(clf.coef_-mlr.coef_)\n",
    "print(\"difference in intercept:\")\n",
    "print(clf.intercept_-mlr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "print(mlr.predict([list(X.iloc[3])]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ffa460a24b01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# decision tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#print(dtree.decision_path(X))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m dot_data = tree.export_graphviz(dtree, out_file=None,feature_names=list(X.columns), \n",
      "\u001b[0;31mNameError\u001b[0m: name 'tree' is not defined"
     ]
    }
   ],
   "source": [
    "# decision tree\n",
    "dtree = tree.DecisionTreeClassifier()\n",
    "dtree.fit(X,Y)\n",
    "#print(dtree.decision_path(X))\n",
    "dot_data = tree.export_graphviz(dtree, out_file=None,feature_names=list(X.columns), \n",
    "                                filled=True,special_characters=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "print(dtree.predict([list(X.iloc[4])]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  With/IN\n",
      "  this/DT\n",
      "  (NP scoring/VBG)\n",
      "  function/NN\n",
      "  in/IN\n",
      "  mind/NN\n",
      "  ,/,\n",
      "  to/TO\n",
      "  is/VBZ\n",
      "  are/VBP\n",
      "  our/PRP$\n",
      "  main/JJ\n",
      "  focus/NN\n",
      "  will/MD\n",
      "  (NP be/VB)\n",
      "  on/IN\n",
      "  (NP modelling/VBG)\n",
      "  the/DT\n",
      "  local/JJ\n",
      "  conditional/JJ\n",
      "  distribution/NN\n",
      "  :/:\n",
      "  EQUATION/NN\n",
      "  ./.)\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nentities = nltk.chunk.ne_chunk(tagged)\\nprint(entities)\\n\\nif 'PRP' in {pos for _,pos in nltk.pos_tag(nltk.word_tokenize(testSen))}:\\n    print('true')\\n#nltk.regexp_tokenize(testSen, r'[\\\\!]\\\\s*', gaps=True)\\nimport re\\n# t = re.search(r'is|this',testSen)\\n# t.groups()\\nregex = re.compile(r'\\x08(in minds|with|thiss{1})\\x08', flags=re.IGNORECASE)\\nprint(regex.findall(testSen))\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test with NLTK\n",
    "\n",
    "testSen=\"With this scoring function in mind, to is are our main focus will be on modelling the local conditional distribution: EQUATION.\"\n",
    "\n",
    "tokens = nltk.word_tokenize(testSen)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "#print(tagged)\n",
    "\n",
    "tagged_text_string = \" \".join([\"{}/{}\".format(word,pos) for word, pos in tagged])\n",
    "gold_chunked_text = tagstr2tree(tagged_text_string)\n",
    "unchunked_text = gold_chunked_text.flatten()\n",
    "\n",
    "chunk_rule = ChunkRule(\"<VBG|VB|VBD>\", \"Chunk test\")\n",
    "chunk_parser = RegexpChunkParser([chunk_rule], chunk_label='NP')\n",
    "chunked_text = chunk_parser.parse(unchunked_text)\n",
    "print(chunked_text)\n",
    "print(chunked_text.height())\n",
    "\n",
    "\"\"\"\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "print(entities)\n",
    "\n",
    "if 'PRP' in {pos for _,pos in nltk.pos_tag(nltk.word_tokenize(testSen))}:\n",
    "    print('true')\n",
    "#nltk.regexp_tokenize(testSen, r'[\\!]\\s*', gaps=True)\n",
    "import re\n",
    "# t = re.search(r'is|this',testSen)\n",
    "# t.groups()\n",
    "regex = re.compile(r'\\b(in minds|with|thiss{1})\\b', flags=re.IGNORECASE)\n",
    "print(regex.findall(testSen))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
