{"Neural_Baby_Talk": [{"type": "text", "expr": "Neural Baby Talk", "word_idx": 0, "sentence_idx": 0, "label": "unlabeled"}, {"type": "text", "expr": "Neural Baby Talk", "word_idx": 16, "sentence_idx": 1, "label": "unlabeled"}, {"type": "text", "expr": "Equal contribution", "word_idx": 32, "sentence_idx": 2, "label": "unlabeled"}, {"type": "text", "expr": "Jiasen Lu ${}^{1}$  \u2003Jianwei Yang ${}^{1}$  \u2003Dhruv Batra ${}^{1,2}$  \u2003Devi Parikh ${}^{1,2}$ ${}^{1}$ Georgia Institute of Technology \u2003 ${}^{2}$ Facebook AI Research\n {jiasenlu, jw2yang, dbatra, parikh}@gatech", "word_idx": 50, "sentence_idx": 3, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{1}$$", "word_idx": 259, "sentence_idx": 4, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{1}$$", "word_idx": 265, "sentence_idx": 5, "label": "unlabeled"}, {"type": "text", "expr": "1 footnotemark:", "word_idx": 271, "sentence_idx": 6, "label": "unlabeled"}, {"type": "text", "expr": "footnotemark:", "word_idx": 286, "sentence_idx": 7, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{1,2}$$", "word_idx": 299, "sentence_idx": 8, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{1,2}$$", "word_idx": 307, "sentence_idx": 9, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{1}$$", "word_idx": 315, "sentence_idx": 10, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{2}$$", "word_idx": 321, "sentence_idx": 11, "label": "unlabeled"}, {"type": "text", "expr": "{jiasenlu, jw2yang, dbatra, parikh}@gatech", "word_idx": 327, "sentence_idx": 12, "label": "unlabeled"}, {"type": "text", "expr": "Equal contribution", "word_idx": 369, "sentence_idx": 13, "label": "unlabeled"}, {"type": "text", "expr": "Equal contribution", "word_idx": 387, "sentence_idx": 14, "label": "unlabeled"}, {"type": "text", "expr": "Abstract", "word_idx": 405, "sentence_idx": 15, "label": "unlabeled"}, {"type": "text", "expr": "Abstract", "word_idx": 413, "sentence_idx": 16, "label": "unlabeled"}, {"type": "text", "expr": "We introduce a novel framework for image captioning that can produce natural language explicitly grounded in entities that object detectors find in the image", "word_idx": 421, "sentence_idx": 17, "label": "unlabeled"}, {"type": "text", "expr": "\nOur approach reconciles classical slot filling approaches (that are generally better grounded in images) with modern neural captioning approaches (that are generally more natural sounding and accurate)", "word_idx": 578, "sentence_idx": 18, "label": "unlabeled"}, {"type": "text", "expr": "\nOur approach first generates a sentence \u2018template\u2019 with slot locations explicitly tied to specific image regions", "word_idx": 780, "sentence_idx": 19, "label": "unlabeled"}, {"type": "text", "expr": " These slots are then filled in by visual concepts identified in the regions by object detectors", "word_idx": 893, "sentence_idx": 20, "label": "unlabeled"}, {"type": "text", "expr": " The entire architecture (sentence template generation and slot filling with object detectors) is end-to-end differentiable", "word_idx": 989, "sentence_idx": 21, "label": "unlabeled"}, {"type": "text", "expr": "\nWe verify the effectiveness of our proposed model on different image captioning tasks", "word_idx": 1112, "sentence_idx": 22, "label": "unlabeled"}, {"type": "text", "expr": " On standard image captioning and novel object captioning, our model reaches state-of-the-art on both COCO and Flickr30k datasets", "word_idx": 1198, "sentence_idx": 23, "label": "unlabeled"}, {"type": "text", "expr": " We also demonstrate that our model has unique advantages when the train and test distributions of scene compositions \u2013 and hence language priors of associated captions \u2013 are different", "word_idx": 1327, "sentence_idx": 24, "label": "unlabeled"}, {"type": "text", "expr": " Code has been made available at:  https://github", "word_idx": 1511, "sentence_idx": 25, "label": "unlabeled"}, {"type": "text", "expr": "com/jiasenlu/NeuralBabyTalk ", "word_idx": 1560, "sentence_idx": 26, "label": "unlabeled"}, {"type": "text", "expr": "https://github", "word_idx": 1588, "sentence_idx": 27, "label": "unlabeled"}, {"type": "text", "expr": "com/jiasenlu/NeuralBabyTalk", "word_idx": 1602, "sentence_idx": 28, "label": "unlabeled"}, {"type": "text", "expr": "\\cvprfinalcopy", "word_idx": 1629, "sentence_idx": 29, "label": "unlabeled"}, {"type": "text", "expr": "1  Introduction", "word_idx": 1643, "sentence_idx": 30, "label": "unlabeled"}, {"type": "text", "expr": "Image captioning is a challenging problem\nthat lies at the intersection of computer vision and natural language processing", "word_idx": 1658, "sentence_idx": 31, "label": "unlabeled"}, {"type": "text", "expr": "\nIt involves generating a natural language sentence that accurately summarizes the contents of an image", "word_idx": 1780, "sentence_idx": 32, "label": "unlabeled"}, {"type": "text", "expr": "\nImage captioning is also an important first step towards real-world applications with significant practical impact, ranging from aiding visually impaired users\nto personal assistants to human-robot interaction  ", "word_idx": 1883, "sentence_idx": 33, "label": "unlabeled"}, {"type": "text", "expr": "State-of-art image captioning models today tend to be monolithic neural models, essentially of the \u201cencoder-decoder\u201d paradigm", "word_idx": 2095, "sentence_idx": 34, "label": "unlabeled"}, {"type": "text", "expr": " Images are encoded into a vector with a convolutional neural network (CNN), and captions are decoded from this vector using a Recurrent Neural Network (RNN), with the entire system trained end-to-end", "word_idx": 2220, "sentence_idx": 35, "label": "unlabeled"}, {"type": "text", "expr": " While there are many recent extensions of this basic idea to include attention\u00a0 , it is well-understood that models still lack visual grounding ( \\ie , do not associate named concepts to pixels in the image)", "word_idx": 2420, "sentence_idx": 36, "label": "unlabeled"}, {"type": "text", "expr": "\nThey often tend to \u2018look\u2019 at different regions than humans would and tend to copy captions from training data  ", "word_idx": 2628, "sentence_idx": 37, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:  Example captions generated by (a) Baby Talk\u00a0 , (c) neural image captioning\u00a0  and (b) our Neural Baby Talk approach", "word_idx": 2740, "sentence_idx": 38, "label": "unlabeled"}, {"type": "text", "expr": " Our method generates the sentence \u201ctemplate\u201d with slot locations (illustrated with filled boxes) explicitly tied to image regions (drawn in the image in corresponding colors)", "word_idx": 2865, "sentence_idx": 39, "label": "unlabeled"}, {"type": "text", "expr": " These slots are then filled by object detectors with concepts found in regions", "word_idx": 3040, "sentence_idx": 40, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:", "word_idx": 3119, "sentence_idx": 41, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:  From left to right is the generated caption using the same captioning model but with different detectors: 1) No detector; 2) A weak detector that only detects \u201cperson\u201d and \u201csandwich\u201d; 3) A detector trained on COCO\u00a0  categories (including \u201cteddy bear\u201d)", "word_idx": 3128, "sentence_idx": 42, "label": "unlabeled"}, {"type": "text", "expr": " 4) A detector that can detect novel concepts (e", "word_idx": 3390, "sentence_idx": 43, "label": "unlabeled"}, {"type": "text", "expr": " Ted\u201d and \u201cpie\u201d that never occurred in the captioning training data)", "word_idx": 3438, "sentence_idx": 44, "label": "unlabeled"}, {"type": "text", "expr": " Different colors show a correspondence between the visual word and grounding regions", "word_idx": 3506, "sentence_idx": 45, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:", "word_idx": 3591, "sentence_idx": 46, "label": "unlabeled"}, {"type": "text", "expr": "For instance, in Fig", "word_idx": 3600, "sentence_idx": 47, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 1  a neural image captioning approach\u00a0  describes the image\nas \u201cA dog is sitting on a couch with a toy", "word_idx": 3620, "sentence_idx": 48, "label": "unlabeled"}, {"type": "text", "expr": "\u201d This is not quite accurate", "word_idx": 3724, "sentence_idx": 49, "label": "unlabeled"}, {"type": "text", "expr": " But if one were to  really  squint at the image, it (arguably) does perhaps look like a scene where a dog  could  be sitting on a couch with a toy", "word_idx": 3752, "sentence_idx": 50, "label": "unlabeled"}, {"type": "text", "expr": "\nIt certainly is common to find dogs sitting on couches with toys", "word_idx": 3899, "sentence_idx": 51, "label": "unlabeled"}, {"type": "text", "expr": "\nA-priori, the description is reasonable", "word_idx": 3964, "sentence_idx": 52, "label": "unlabeled"}, {"type": "text", "expr": "\nThat\u2019s exactly what today\u2019s neural captioning models tend to do \u2013 produce generic  plausible  captions based\non the language model  that match\na first-glance gist of the scene", "word_idx": 4004, "sentence_idx": 53, "label": "unlabeled"}, {"type": "text", "expr": " While this may suffice for common scenes, images that differ\nfrom canonical scenes \u2013 given the diversity in our visual world, there are  plenty  of\nsuch images \u2013 tend to be underserved by these models", "word_idx": 4180, "sentence_idx": 54, "label": "unlabeled"}, {"type": "text", "expr": "really", "word_idx": 4381, "sentence_idx": 55, "label": "unlabeled"}, {"type": "text", "expr": "could", "word_idx": 4387, "sentence_idx": 56, "label": "unlabeled"}, {"type": "text", "expr": "plausible", "word_idx": 4392, "sentence_idx": 57, "label": "unlabeled"}, {"type": "text", "expr": "1 frequently, directly reproduced from a caption in the training data", "word_idx": 4401, "sentence_idx": 58, "label": "unlabeled"}, {"type": "text", "expr": "plenty", "word_idx": 4470, "sentence_idx": 59, "label": "unlabeled"}, {"type": "text", "expr": "If we take a step back \u2013 do we really need the language model to do the heavy lifting in image captioning?\nGiven the unprecedented progress we are seeing in object recognition \n(e", "word_idx": 4476, "sentence_idx": 60, "label": "unlabeled"}, {"type": "text", "expr": ", object detection, semantic segmentation, instance segmentation, pose estimation),\nit seems like the vision pipeline can certainly do better than rely on just a first-glance gist of the scene", "word_idx": 4655, "sentence_idx": 61, "label": "unlabeled"}, {"type": "text", "expr": "\nIn fact, today\u2019s state-of-the-art object detectors can successfully detect the table and cake in the image\nin Fig", "word_idx": 4847, "sentence_idx": 62, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 1 (c)! The caption ought to be able to talk about the table and cake  actually detected  as opposed to letting the language model hallucinate a couch and a toy simply because that sounds plausible", "word_idx": 4961, "sentence_idx": 63, "label": "unlabeled"}, {"type": "text", "expr": ", 11%\nabsolute increase in average precision in object detection in the COCO challenge in the last year", "word_idx": 5159, "sentence_idx": 64, "label": "unlabeled"}, {"type": "text", "expr": "actually detected", "word_idx": 5262, "sentence_idx": 65, "label": "unlabeled"}, {"type": "text", "expr": "Interestingly, some of the first attempts at image captioning\u00a0  \u2013 before the deep learning \u201crevolution\u201d \u2013\nrelied heavily on outputs of object detectors and attribute classifiers to describe images", "word_idx": 5279, "sentence_idx": 66, "label": "unlabeled"}, {"type": "text", "expr": "\nFor instance, consider the output of Baby Talk\u00a0  in Fig", "word_idx": 5475, "sentence_idx": 67, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 1 , that used a slot filling\napproach to talk about all the objects and attributes found in the scene via a templated caption", "word_idx": 5531, "sentence_idx": 68, "label": "unlabeled"}, {"type": "text", "expr": " The language\nis unnatural but the caption is very much grounded in what the model\nsees in the image", "word_idx": 5658, "sentence_idx": 69, "label": "unlabeled"}, {"type": "text", "expr": " Today\u2019s approaches fall at the other extreme on the spectrum \u2013\nthe language generated by modern neural image captioning approaches is much more natural but tends to be much less grounded in the image", "word_idx": 5758, "sentence_idx": 70, "label": "unlabeled"}, {"type": "text", "expr": "In this paper, we introduce Neural Baby Talk that reconciles these methodologies", "word_idx": 5958, "sentence_idx": 71, "label": "unlabeled"}, {"type": "text", "expr": " It produces natural language  explicitly  grounded in entities found by object detectors", "word_idx": 6038, "sentence_idx": 72, "label": "unlabeled"}, {"type": "text", "expr": " It is a neural approach that generates a sentence\n\u201ctemplate\u201d with slot locations explicitly tied to image regions", "word_idx": 6127, "sentence_idx": 73, "label": "unlabeled"}, {"type": "text", "expr": " These slots are then filled by object recognizers with concepts found in the regions", "word_idx": 6241, "sentence_idx": 74, "label": "unlabeled"}, {"type": "text", "expr": " The entire approach is trained end-to-end", "word_idx": 6326, "sentence_idx": 75, "label": "unlabeled"}, {"type": "text", "expr": " This results in natural\nsounding and grounded captions", "word_idx": 6368, "sentence_idx": 76, "label": "unlabeled"}, {"type": "text", "expr": "explicitly", "word_idx": 6423, "sentence_idx": 77, "label": "unlabeled"}, {"type": "text", "expr": "Our main technical contribution is a novel neural decoder for grounded image captioning", "word_idx": 6433, "sentence_idx": 78, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, at each time step, the model decides whether to generate a word from the textual vocabulary\nor generate a \u201cvisual\u201d word", "word_idx": 6520, "sentence_idx": 79, "label": "unlabeled"}, {"type": "text", "expr": " The visual word is essentially a token that will hold the slot for\na word that is to describe a specific region in the image", "word_idx": 6654, "sentence_idx": 80, "label": "unlabeled"}, {"type": "text", "expr": "\nFor instance, for the image\nin Fig", "word_idx": 6779, "sentence_idx": 81, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 1 , the generated sequence may be \u201cA  $<$ region $-$ 17 $>$  is sitting at a  $<$ region $-$ 123 $>$  with a  $<$ region $-3$ $>$ ", "word_idx": 6814, "sentence_idx": 82, "label": "unlabeled"}, {"type": "text", "expr": "\u201d\nThe visual\nwords ( $<$ region $-$ [", "word_idx": 6946, "sentence_idx": 83, "label": "unlabeled"}, {"type": "text", "expr": "] $>$ \u2019s) are then filled in during a second stage\nthat classifies each of the indicated regions (e", "word_idx": 6983, "sentence_idx": 84, "label": "unlabeled"}, {"type": "text", "expr": ",  $<$ region $-$ 17 $>$ $\\rightarrow$ puppy,\n $<$ region $-$ 123 $>$ $\\rightarrow$ table), resulting in\na final description of \u201cA puppy is sitting at a table with a cake", "word_idx": 7082, "sentence_idx": 85, "label": "unlabeled"}, {"type": "text", "expr": "\u201d \u2013 a free-form natural language description that is grounded in the image", "word_idx": 7252, "sentence_idx": 86, "label": "unlabeled"}, {"type": "text", "expr": " One nice feature of our model is that it allows for different object detectors to be plugged in easily", "word_idx": 7326, "sentence_idx": 87, "label": "unlabeled"}, {"type": "text", "expr": " As a result, a variety of captions can be produced for the same image using different detection backends", "word_idx": 7429, "sentence_idx": 88, "label": "unlabeled"}, {"type": "text", "expr": " See Fig", "word_idx": 7534, "sentence_idx": 89, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 2  for an illustration", "word_idx": 7542, "sentence_idx": 90, "label": "unlabeled"}, {"type": "math", "expr": "$$\\rightarrow$$", "word_idx": 7566, "sentence_idx": 91, "label": "unlabeled"}, {"type": "math", "expr": "$$\\rightarrow$$", "word_idx": 7577, "sentence_idx": 92, "label": "unlabeled"}, {"type": "text", "expr": "Contributions:  Our contributions are as follows:", "word_idx": 7588, "sentence_idx": 93, "label": "unlabeled"}, {"type": "text", "expr": "Contributions:", "word_idx": 7637, "sentence_idx": 94, "label": "unlabeled"}, {"type": "text", "expr": "We present Neural Baby Talk \u2013 a novel framework for visually grounded image captioning that explicitly localizes objects in the image while generating free-form natural language descriptions", "word_idx": 7651, "sentence_idx": 95, "label": "unlabeled"}, {"type": "text", "expr": "We present Neural Baby Talk \u2013 a novel framework for visually grounded image captioning that explicitly localizes objects in the image while generating free-form natural language descriptions", "word_idx": 7841, "sentence_idx": 96, "label": "unlabeled"}, {"type": "text", "expr": "Ours is a two-stage approach that first generates a hybrid template that contains a mix of (text) words and slots explicitly associated with image regions, and then fills in the slots with (text) words by recognizing the content in the corresponding image regions", "word_idx": 8031, "sentence_idx": 97, "label": "unlabeled"}, {"type": "text", "expr": "Ours is a two-stage approach that first generates a hybrid template that contains a mix of (text) words and slots explicitly associated with image regions, and then fills in the slots with (text) words by recognizing the content in the corresponding image regions", "word_idx": 8294, "sentence_idx": 98, "label": "unlabeled"}, {"type": "text", "expr": "We propose a robust image captioning task to benchmark compositionality of image captioning algorithms where at test time the model encounters images containing known objects but in novel combinations (e", "word_idx": 8557, "sentence_idx": 99, "label": "unlabeled"}, {"type": "text", "expr": ", the model has seen dogs on couches and people at tables during training, but at test time encounters a dog at a table)", "word_idx": 8760, "sentence_idx": 100, "label": "unlabeled"}, {"type": "text", "expr": " Generalizing to such novel compositions is one way to demonstrate image grounding as opposed to simply leveraging correlations from training data", "word_idx": 8880, "sentence_idx": 101, "label": "unlabeled"}, {"type": "text", "expr": "We propose a robust image captioning task to benchmark compositionality of image captioning algorithms where at test time the model encounters images containing known objects but in novel combinations (e", "word_idx": 9026, "sentence_idx": 102, "label": "unlabeled"}, {"type": "text", "expr": ", the model has seen dogs on couches and people at tables during training, but at test time encounters a dog at a table)", "word_idx": 9229, "sentence_idx": 103, "label": "unlabeled"}, {"type": "text", "expr": " Generalizing to such novel compositions is one way to demonstrate image grounding as opposed to simply leveraging correlations from training data", "word_idx": 9349, "sentence_idx": 104, "label": "unlabeled"}, {"type": "text", "expr": "Our proposed method achieves state-of-the-art performance on COCO and Flickr30k datasets on the standard image captioning task, and significantly outperforms existing approaches on the robust image captioning and novel object captioning tasks", "word_idx": 9495, "sentence_idx": 105, "label": "unlabeled"}, {"type": "text", "expr": "Our proposed method achieves state-of-the-art performance on COCO and Flickr30k datasets on the standard image captioning task, and significantly outperforms existing approaches on the robust image captioning and novel object captioning tasks", "word_idx": 9737, "sentence_idx": 106, "label": "unlabeled"}, {"type": "text", "expr": "2  Related Work", "word_idx": 9979, "sentence_idx": 107, "label": "unlabeled"}, {"type": "text", "expr": "Some of the earlier approaches generated templated image captions via slot-filling", "word_idx": 9994, "sentence_idx": 108, "label": "unlabeled"}, {"type": "text", "expr": "\nFor instance,\nKulkarni  \\etal \u00a0  detect objects, attributes, and prepositions, jointly reason about these through a CRF, and finally fill appropriate slots in a template", "word_idx": 10076, "sentence_idx": 109, "label": "unlabeled"}, {"type": "text", "expr": " Farhadi  \\etal  compute a triplet for a scene, and use this templated \u201cmeaning\u201d representation to retrieve a caption from a database", "word_idx": 10246, "sentence_idx": 110, "label": "unlabeled"}, {"type": "text", "expr": "   use more powerful language templates such as a syntactically well-formed tree", "word_idx": 10379, "sentence_idx": 111, "label": "unlabeled"}, {"type": "text", "expr": " These approaches tend to either produce captions that are relevant to the image but not natural sounding, or captions that are natural ( \\eg retrieved from a database of captions) but may not be sufficiently grounded in the image", "word_idx": 10459, "sentence_idx": 112, "label": "unlabeled"}, {"type": "text", "expr": "\\etal", "word_idx": 10689, "sentence_idx": 113, "label": "unlabeled"}, {"type": "text", "expr": "\\etal", "word_idx": 10694, "sentence_idx": 114, "label": "unlabeled"}, {"type": "text", "expr": "Neural models for image captioning have been receiving increased attention in the last few years\u00a0 ", "word_idx": 10699, "sentence_idx": 115, "label": "unlabeled"}, {"type": "text", "expr": " State-of-the-art neural approaches include attention mechanisms\u00a0  that identify regions in the image to \u201cground\u201d emitted words", "word_idx": 10797, "sentence_idx": 116, "label": "unlabeled"}, {"type": "text", "expr": " In practice, these attention regions tend to be quite blurry, and rarely correspond to semantically meaningful individual entities (e", "word_idx": 10924, "sentence_idx": 117, "label": "unlabeled"}, {"type": "text", "expr": ", objects instances) in the image", "word_idx": 11058, "sentence_idx": 118, "label": "unlabeled"}, {"type": "text", "expr": " Our approach grounds words in object detections, which by design identify concrete semantic entities (object instances) in the image", "word_idx": 11091, "sentence_idx": 119, "label": "unlabeled"}, {"type": "text", "expr": "There has been some recent interest in grounding natural language in images", "word_idx": 11224, "sentence_idx": 120, "label": "unlabeled"}, {"type": "text", "expr": " Dense Captioning   generates descriptions for specific image regions", "word_idx": 11299, "sentence_idx": 121, "label": "unlabeled"}, {"type": "text", "expr": " In contrast, our model produces captions for the entire image, with words grounded in concrete entities in the image", "word_idx": 11368, "sentence_idx": 122, "label": "unlabeled"}, {"type": "text", "expr": "\nAnother related line of work is on resolving referring expressions\u00a0 \n(or description-based object retrieval\u00a0  \u2013 given a description of an object in the image, identify which object is being referred to) or referring expression generation\u00a0  (given an object in the image, generate a discriminative description of the object)", "word_idx": 11485, "sentence_idx": 123, "label": "unlabeled"}, {"type": "text", "expr": " While the interest in grounded language is in common, our task is different", "word_idx": 11809, "sentence_idx": 124, "label": "unlabeled"}, {"type": "text", "expr": "One natural strength of our model is its ability to incorporate different object detectors, including the ability to generate captions with novel objects (never seen before in training captions)", "word_idx": 11885, "sentence_idx": 125, "label": "unlabeled"}, {"type": "text", "expr": " In that context, our work is related to prior works on novel object captioning  ", "word_idx": 12079, "sentence_idx": 126, "label": "unlabeled"}, {"type": "text", "expr": " As we describe in Sec", "word_idx": 12160, "sentence_idx": 127, "label": "unlabeled"}, {"type": "text", "expr": "3 , our method outperforms these approaches by 14", "word_idx": 12182, "sentence_idx": 128, "label": "unlabeled"}, {"type": "text", "expr": "6% on the averaged F1 score", "word_idx": 12231, "sentence_idx": 129, "label": "unlabeled"}, {"type": "text", "expr": "3  Method", "word_idx": 12258, "sentence_idx": 130, "label": "unlabeled"}, {"type": "text", "expr": "Given an image  $\\bm{I}$ , the goal of our method is to generate visually grounded descriptions  $\\bm{y}=\\{y_{1},\\ldots,y_{T}\\}$ ", "word_idx": 12267, "sentence_idx": 131, "label": "unlabeled"}, {"type": "text", "expr": " Let  $\\bm{r}_{\\bm{I}}=\\{r_{1},,r_{N}\\}$  be the set of  $N$  images regions extracted from  $\\bm{I}$ ", "word_idx": 12396, "sentence_idx": 132, "label": "unlabeled"}, {"type": "text", "expr": " When generating an entity word in the caption, we want to ground it in a specific image region  $r\\in\\bm{r}_{\\bm{I}}$ ", "word_idx": 12498, "sentence_idx": 133, "label": "unlabeled"}, {"type": "text", "expr": " Following the standard supervised learning paradigm, we learn parameters  $\\bm{\\theta}$  of our model by maximizing the likelihood of the correct caption:", "word_idx": 12617, "sentence_idx": 134, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{I}$$", "word_idx": 12772, "sentence_idx": 135, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{y}=\\{y_{1},\\ldots,y_{T}\\}$$", "word_idx": 12778, "sentence_idx": 136, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{r}_{\\bm{I}}=\\{r_{1},...,r_{N}\\}$$", "word_idx": 12807, "sentence_idx": 137, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{I}$$", "word_idx": 12842, "sentence_idx": 138, "label": "unlabeled"}, {"type": "math", "expr": "$$r\\in\\bm{r}_{\\bm{I}}$$", "word_idx": 12848, "sentence_idx": 139, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{\\theta}$$", "word_idx": 12867, "sentence_idx": 140, "label": "unlabeled"}, {"type": "text", "expr": "$\\bm{\\theta}^{*}=\\arg\\max_{\\bm{\\theta}}\\sum_{(\\bm{I},\\bm{y})}\\log p(\\bm{y}|\\bm{%\nI};\\bm{\\theta})\\vspace{-5pt}$", "word_idx": 12878, "sentence_idx": 141, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{\\theta}^{*}=\\arg\\max_{\\bm{\\theta}}\\sum_{(\\bm{I},\\bm{y})}\\log p(\\bm{y}|\\bm{%\nI};\\bm{\\theta})\\vspace{-5pt}$$", "word_idx": 12988, "sentence_idx": 142, "label": "unlabeled"}, {"type": "text", "expr": "Using chain rule, the joint probability distribution can be decomposed over a sequence of tokens:", "word_idx": 13096, "sentence_idx": 143, "label": "unlabeled"}, {"type": "text", "expr": "Using chain rule, the joint probability distribution can be decomposed over a sequence of tokens:", "word_idx": 13193, "sentence_idx": 144, "label": "unlabeled"}, {"type": "text", "expr": "$p(\\bm{y}|\\bm{I})=\\prod_{t=1}^{T}p(y_{t}|\\bm{y}_{1:t-1},\\bm{I})\\vspace{-5pt}$", "word_idx": 13290, "sentence_idx": 145, "label": "unlabeled"}, {"type": "math", "expr": "$$p(\\bm{y}|\\bm{I})=\\prod_{t=1}^{T}p(y_{t}|\\bm{y}_{1:t-1},\\bm{I})\\vspace{-5pt}$$", "word_idx": 13367, "sentence_idx": 146, "label": "unlabeled"}, {"type": "text", "expr": "where we drop the dependency on model parameters to avoid notational clutter", "word_idx": 13442, "sentence_idx": 147, "label": "unlabeled"}, {"type": "text", "expr": " We introduce a latent variable  $r_{t}$  to denote a specific image region so that  $y_{t}$  can explicitly ground in it", "word_idx": 13518, "sentence_idx": 148, "label": "unlabeled"}, {"type": "text", "expr": " Thus the probability of  $y_{t}$  is decomposed to:", "word_idx": 13639, "sentence_idx": 149, "label": "unlabeled"}, {"type": "math", "expr": "$$r_{t}$$", "word_idx": 13691, "sentence_idx": 150, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}$$", "word_idx": 13696, "sentence_idx": 151, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}$$", "word_idx": 13701, "sentence_idx": 152, "label": "unlabeled"}, {"type": "text", "expr": "$p(y_{t}|\\bm{y}_{1:t-1},\\bm{I})=p(y_{t}|r_{t},\\bm{y}_{1:t-1},\\bm{I})p(r_{t}|\\bm%\n{y}_{1:t-1},\\bm{I})\\vspace{-5pt}$", "word_idx": 13706, "sentence_idx": 153, "label": "unlabeled"}, {"type": "math", "expr": "$$p(y_{t}|\\bm{y}_{1:t-1},\\bm{I})=p(y_{t}|r_{t},\\bm{y}_{1:t-1},\\bm{I})p(r_{t}|\\bm%\n{y}_{1:t-1},\\bm{I})\\vspace{-5pt}$$", "word_idx": 13820, "sentence_idx": 154, "label": "unlabeled"}, {"type": "text", "expr": "In our framework,  $y_{t}$  can be of one of two types: a visual word or a textual word, denoted as  $y^{vis}$  and  $y^{txt}$  respectively", "word_idx": 13932, "sentence_idx": 155, "label": "unlabeled"}, {"type": "text", "expr": " A visual word  $y^{vis}$  is a type of word that is grounded in a specific image region drawn from  $\\bm{r}_{\\bm{I}}$ ", "word_idx": 14072, "sentence_idx": 156, "label": "unlabeled"}, {"type": "text", "expr": " A textual word  $y^{txt}$  is a word from the remainder of the caption", "word_idx": 14191, "sentence_idx": 157, "label": "unlabeled"}, {"type": "text", "expr": " It is drawn from the language model , which is associated with a \u201cdefault\u201d sentinel \u201cregion\u201d  $\\tilde{r}$  obtained from the language model\u00a0  (discussed in Sec", "word_idx": 14262, "sentence_idx": 158, "label": "unlabeled"}, {"type": "text", "expr": " For example, as illustrated in Fig", "word_idx": 14422, "sentence_idx": 159, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 1 , \u201cpuppy\u201d and \u201ccake\u201d grounded in the bounding box of category \u201cdog\u201d and \u201ccake\u201d respectively, are visual words", "word_idx": 14457, "sentence_idx": 160, "label": "unlabeled"}, {"type": "text", "expr": " While \u201cwith\u201d and \u201csitting\u201d are not associated with any image regions and thus are textual words", "word_idx": 14570, "sentence_idx": 161, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}$$", "word_idx": 14666, "sentence_idx": 162, "label": "unlabeled"}, {"type": "math", "expr": "$$y^{vis}$$", "word_idx": 14671, "sentence_idx": 163, "label": "unlabeled"}, {"type": "math", "expr": "$$y^{txt}$$", "word_idx": 14678, "sentence_idx": 164, "label": "unlabeled"}, {"type": "math", "expr": "$$y^{vis}$$", "word_idx": 14685, "sentence_idx": 165, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{r}_{\\bm{I}}$$", "word_idx": 14692, "sentence_idx": 166, "label": "unlabeled"}, {"type": "math", "expr": "$$y^{txt}$$", "word_idx": 14707, "sentence_idx": 167, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{r}$$", "word_idx": 14714, "sentence_idx": 168, "label": "unlabeled"}, {"type": "text", "expr": "With this, Eq", "word_idx": 14723, "sentence_idx": 169, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 1  can be decomposed into two cascaded objectives", "word_idx": 14736, "sentence_idx": 170, "label": "unlabeled"}, {"type": "text", "expr": " First, maximizing the probability of generating the sentence \u201ctemplate\u201d", "word_idx": 14787, "sentence_idx": 171, "label": "unlabeled"}, {"type": "text", "expr": "\nA sequence of grounding regions associated with the visual words interspersed with the textual words can be viewed as a sentence \u201ctemplate\u201d, where the grounding regions are slots to be filled in with visual words", "word_idx": 14859, "sentence_idx": 172, "label": "unlabeled"}, {"type": "text", "expr": "  An example template (Fig", "word_idx": 15072, "sentence_idx": 173, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 3 ) is \u201cA  $<$ region $-$ 2 $>$  is laying on the  $<$ region $-$ 4 $>$  near a  $<$ region $-$ 7 $>$ ", "word_idx": 15098, "sentence_idx": 174, "label": "unlabeled"}, {"type": "text", "expr": " Second, maximizing the probability of visual words  $y_{t}^{vis}$  conditioned on the grounding regions and object detection information, e", "word_idx": 15202, "sentence_idx": 175, "label": "unlabeled"}, {"type": "text", "expr": ", categories recognized by detector", "word_idx": 15342, "sentence_idx": 176, "label": "unlabeled"}, {"type": "text", "expr": " In the template example above, the model will fill the slots with \u2018cat\u2019, \u2018laptop\u2019 and \u2018chair\u2019 respectively", "word_idx": 15377, "sentence_idx": 177, "label": "unlabeled"}, {"type": "text", "expr": "3 Our approach is not limited to any pre-specified bank of templates", "word_idx": 15484, "sentence_idx": 178, "label": "unlabeled"}, {"type": "text", "expr": " Rather, our approach automatically generates a template (with placeholders \u2013 slots \u2013 for visually grounded words), which may be any one of the exponentially many possible templates", "word_idx": 15552, "sentence_idx": 179, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}^{vis}$$", "word_idx": 15733, "sentence_idx": 180, "label": "unlabeled"}, {"type": "text", "expr": "In the following, we first describe how we generate the slotted caption template (Sec", "word_idx": 15744, "sentence_idx": 181, "label": "unlabeled"}, {"type": "text", "expr": "1 ), and then how the slots are filled in to obtain the final image description (Sec", "word_idx": 15829, "sentence_idx": 182, "label": "unlabeled"}, {"type": "text", "expr": " The overall objective function is described in Sec", "word_idx": 15913, "sentence_idx": 183, "label": "unlabeled"}, {"type": "text", "expr": "3  and the implementation details in Sec", "word_idx": 15964, "sentence_idx": 184, "label": "unlabeled"}, {"type": "text", "expr": "1  \u201cSlotted\u201d Caption Template Generation", "word_idx": 16004, "sentence_idx": 185, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:  One block of the proposed approach", "word_idx": 16044, "sentence_idx": 186, "label": "unlabeled"}, {"type": "text", "expr": " Given an image, proposals from any object detector and current word \u201cA\u201d, the figure shows the process to predict the next visual word \u201ccat\u201d", "word_idx": 16089, "sentence_idx": 187, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:", "word_idx": 16229, "sentence_idx": 188, "label": "unlabeled"}, {"type": "text", "expr": "Given an image  $\\bm{I}$ , and the corresponding caption  $\\bm{y}$ , the candidate grounding regions are obtained by using a pre-trained Faster-RCNN network  ", "word_idx": 16238, "sentence_idx": 189, "label": "unlabeled"}, {"type": "text", "expr": " To generate the caption \u201ctemplate\u201d, we use a recurrent neural network, which is commonly used as the decoder for image captioning  ", "word_idx": 16396, "sentence_idx": 190, "label": "unlabeled"}, {"type": "text", "expr": " At each time step, we compute the RNN hidden state  $\\bm{h}_{t}$  according to the previous hidden state  $\\bm{h}_{t-1}$  and the input  $\\bm{x}_{t}$  such that  $\\bm{h}_{t}=\\mathrm{RNN}(\\bm{x}_{t},\\bm{h}_{t-1})$ ", "word_idx": 16528, "sentence_idx": 191, "label": "unlabeled"}, {"type": "text", "expr": " At training time,  $x_{t}$  is the ground truth token (teacher forcing) and at test time is the sampled token  $y_{t-1}$ ", "word_idx": 16742, "sentence_idx": 192, "label": "unlabeled"}, {"type": "text", "expr": " Our decoder consists of an attention based LSTM layer   that takes convolution feature maps as input", "word_idx": 16864, "sentence_idx": 193, "label": "unlabeled"}, {"type": "text", "expr": " Details can be found in Sec", "word_idx": 16965, "sentence_idx": 194, "label": "unlabeled"}, {"type": "text", "expr": " To generate the \u201cslot\u201d for visual words, we use a pointer network   that modulates a content-based attention mechanism over the grounding regions", "word_idx": 16993, "sentence_idx": 195, "label": "unlabeled"}, {"type": "text", "expr": " Let  $\\bm{v_{t}}\\in\\mathcal{R}^{d\\times 1}$  be the region feature of  ${r}_{t}$ , which is calculated based on Faster R-CNN", "word_idx": 17139, "sentence_idx": 196, "label": "unlabeled"}, {"type": "text", "expr": " We compute the pointing vector with:", "word_idx": 17264, "sentence_idx": 197, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{I}$$", "word_idx": 17301, "sentence_idx": 198, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{y}$$", "word_idx": 17307, "sentence_idx": 199, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{h}_{t}$$", "word_idx": 17313, "sentence_idx": 200, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{h}_{t-1}$$", "word_idx": 17323, "sentence_idx": 201, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{x}_{t}$$", "word_idx": 17335, "sentence_idx": 202, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{h}_{t}=\\mathrm{RNN}(\\bm{x}_{t},\\bm{h}_{t-1})$$", "word_idx": 17345, "sentence_idx": 203, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t}$$", "word_idx": 17393, "sentence_idx": 204, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t-1}$$", "word_idx": 17398, "sentence_idx": 205, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{v_{t}}\\in\\mathcal{R}^{d\\times 1}$$", "word_idx": 17405, "sentence_idx": 206, "label": "unlabeled"}, {"type": "math", "expr": "$${r}_{t}$$", "word_idx": 17441, "sentence_idx": 207, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle u_{i}^{t}$", "word_idx": 17448, "sentence_idx": 208, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle u_{i}^{t}$$", "word_idx": 17473, "sentence_idx": 209, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\bm{w}_{h}^{T}\\tanh(\\bm{W}_{v}\\bm{v}_{t}+\\bm{W}_{z}\\bm{h}_{t})$", "word_idx": 17496, "sentence_idx": 210, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\bm{w}_{h}^{T}\\tanh(\\bm{W}_{v}\\bm{v}_{t}+\\bm{W}_{z}\\bm{h}_{t})$$", "word_idx": 17574, "sentence_idx": 211, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\bm{P}_{\\bm{r}_{\\bm{I}}}^{t}$", "word_idx": 17650, "sentence_idx": 212, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\bm{P}_{\\bm{r}_{\\bm{I}}}^{t}$$", "word_idx": 17693, "sentence_idx": 213, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\textrm{softmax}(\\bm{u}^{t})$", "word_idx": 17734, "sentence_idx": 214, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\textrm{softmax}(\\bm{u}^{t})$$", "word_idx": 17778, "sentence_idx": 215, "label": "unlabeled"}, {"type": "text", "expr": "where  $\\bm{W}_{v}\\in\\mathbb{R}^{m\\times d}$ ,  $\\bm{W}_{z}\\in\\mathbb{R}^{d\\times d}$  and  $\\bm{w}_{h}\\in\\mathbb{R}^{d\\times 1}$  are parameters to be learned", "word_idx": 17820, "sentence_idx": 216, "label": "unlabeled"}, {"type": "text", "expr": " The  softmax  normalizes the vector  $\\bm{u}^{t}$  to be a distribution over grounding regions  $\\bm{r}_{\\bm{I}}$ ", "word_idx": 17979, "sentence_idx": 217, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{W}_{v}\\in\\mathbb{R}^{m\\times d}$$", "word_idx": 18094, "sentence_idx": 218, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{W}_{z}\\in\\mathbb{R}^{d\\times d}$$", "word_idx": 18129, "sentence_idx": 219, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{w}_{h}\\in\\mathbb{R}^{d\\times 1}$$", "word_idx": 18164, "sentence_idx": 220, "label": "unlabeled"}, {"type": "text", "expr": "softmax", "word_idx": 18199, "sentence_idx": 221, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{u}^{t}$$", "word_idx": 18206, "sentence_idx": 222, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{r}_{\\bm{I}}$$", "word_idx": 18216, "sentence_idx": 223, "label": "unlabeled"}, {"type": "text", "expr": "Since textual words  $y_{t}^{txt}$  are not tied to specific regions in the image, inspired by\u00a0 , we add a \u201cvisual sentinel\u201d  $\\tilde{r}$  as a latent variable to serve as dummy grounding for the textual word", "word_idx": 18231, "sentence_idx": 224, "label": "unlabeled"}, {"type": "text", "expr": " The visual sentinel can be thought of as a latent representation of what the decoder already knows about the image", "word_idx": 18439, "sentence_idx": 225, "label": "unlabeled"}, {"type": "text", "expr": "\nThe probability of a textual word  $y_{t}^{txt}$  then is:", "word_idx": 18554, "sentence_idx": 226, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}^{txt}$$", "word_idx": 18613, "sentence_idx": 227, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{r}$$", "word_idx": 18624, "sentence_idx": 228, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}^{txt}$$", "word_idx": 18633, "sentence_idx": 229, "label": "unlabeled"}, {"type": "text", "expr": "$p(y_{t}^{txt}|\\bm{y}_{1:t-1})=p(y_{t}^{txt}|\\tilde{r},\\bm{y}_{1:t-1})p(\\tilde{%\nr}|\\bm{y}_{1:t-1})$", "word_idx": 18644, "sentence_idx": 230, "label": "unlabeled"}, {"type": "math", "expr": "$$p(y_{t}^{txt}|\\bm{y}_{1:t-1})=p(y_{t}^{txt}|\\tilde{r},\\bm{y}_{1:t-1})p(\\tilde{%\nr}|\\bm{y}_{1:t-1})$$", "word_idx": 18744, "sentence_idx": 231, "label": "unlabeled"}, {"type": "text", "expr": "where we drop the dependency on  $\\bm{I}$  to avoid clutter", "word_idx": 18842, "sentence_idx": 232, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{I}$$", "word_idx": 18901, "sentence_idx": 233, "label": "unlabeled"}, {"type": "text", "expr": "We first describe how the visual sentinel is computed, and then how the textual words are determined based on the visual sentinel", "word_idx": 18907, "sentence_idx": 234, "label": "unlabeled"}, {"type": "text", "expr": " Following\u00a0 , when the decoder RNN is an LSTM  , the representation for visual sentinel  $\\bm{s}_{t}$  can be obtained by:", "word_idx": 19036, "sentence_idx": 235, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{s}_{t}$$", "word_idx": 19158, "sentence_idx": 236, "label": "unlabeled"}, {"type": "text", "expr": "= (7)", "word_idx": 19168, "sentence_idx": 237, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\bm{g}_{t}$", "word_idx": 19173, "sentence_idx": 238, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\bm{g}_{t}$$", "word_idx": 19198, "sentence_idx": 239, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=$", "word_idx": 19221, "sentence_idx": 240, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=$$", "word_idx": 19237, "sentence_idx": 241, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\sigma\\left(\\bm{W}_{x}\\bm{x}_{t}+\\bm{W}_{h}\\bm{h}_{t-1}\\right)$", "word_idx": 19251, "sentence_idx": 242, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\sigma\\left(\\bm{W}_{x}\\bm{x}_{t}+\\bm{W}_{h}\\bm{h}_{t-1}\\right)$$", "word_idx": 19328, "sentence_idx": 243, "label": "unlabeled"}, {"type": "text", "expr": "= (8)", "word_idx": 19403, "sentence_idx": 244, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\bm{s}_{t}$", "word_idx": 19408, "sentence_idx": 245, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\bm{s}_{t}$$", "word_idx": 19433, "sentence_idx": 246, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=$", "word_idx": 19456, "sentence_idx": 247, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=$$", "word_idx": 19472, "sentence_idx": 248, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\bm{g}_{t}\\odot\\tanh\\left(\\bm{c}_{t}\\right)$", "word_idx": 19486, "sentence_idx": 249, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\bm{g}_{t}\\odot\\tanh\\left(\\bm{c}_{t}\\right)$$", "word_idx": 19544, "sentence_idx": 250, "label": "unlabeled"}, {"type": "text", "expr": "where  $\\bm{W}_{x}\\in\\mathbb{R}^{d\\times d}$ ,  $\\bm{W}_{h}\\in\\mathbb{R}^{d\\times d}$ ", "word_idx": 19600, "sentence_idx": 251, "label": "unlabeled"}, {"type": "text", "expr": "  $\\bm{x}_{t}$  is the LSTM input at time step  $t$ , and  $\\bm{g}_{t}$  is the gate applied on the cell state  $\\bm{c}_{t}$ ", "word_idx": 19686, "sentence_idx": 252, "label": "unlabeled"}, {"type": "text", "expr": "  $\\odot$  represents element-wise product,  $\\sigma$  the logistic sigmoid activation", "word_idx": 19811, "sentence_idx": 253, "label": "unlabeled"}, {"type": "text", "expr": " Modifying Eq", "word_idx": 19897, "sentence_idx": 254, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 5 , the probability over the grounding regions including the visual sentinel is:", "word_idx": 19910, "sentence_idx": 255, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{W}_{x}\\in\\mathbb{R}^{d\\times d}$$", "word_idx": 19992, "sentence_idx": 256, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{W}_{h}\\in\\mathbb{R}^{d\\times d}$$", "word_idx": 20027, "sentence_idx": 257, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{x}_{t}$$", "word_idx": 20062, "sentence_idx": 258, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{g}_{t}$$", "word_idx": 20072, "sentence_idx": 259, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{c}_{t}$$", "word_idx": 20082, "sentence_idx": 260, "label": "unlabeled"}, {"type": "math", "expr": "$$\\odot$$", "word_idx": 20092, "sentence_idx": 261, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma$$", "word_idx": 20097, "sentence_idx": 262, "label": "unlabeled"}, {"type": "text", "expr": "$\\bm{P}_{\\bm{r}}^{t}=\\textrm{softmax}([\\bm{u}^{t};\\bm{w}_{h}^{T}\\tanh(\\bm{W}_{s%\n}\\bm{s}_{t}+\\bm{W}_{z}\\bm{h}_{t})])\\vspace{-5pt}$", "word_idx": 20103, "sentence_idx": 263, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{P}_{\\bm{r}}^{t}=\\textrm{softmax}([\\bm{u}^{t};\\bm{w}_{h}^{T}\\tanh(\\bm{W}_{s%\n}\\bm{s}_{t}+\\bm{W}_{z}\\bm{h}_{t})])\\vspace{-5pt}$$", "word_idx": 20233, "sentence_idx": 264, "label": "unlabeled"}, {"type": "text", "expr": "where  $\\bm{W}_{s}\\in\\mathbb{R}^{d\\times d}$  and  $\\bm{W}_{z}\\in\\mathbb{R}^{d\\times d}$  are the parameters", "word_idx": 20361, "sentence_idx": 265, "label": "unlabeled"}, {"type": "text", "expr": " Notably,  $\\bm{W}_{z}$  and  $\\bm{w}_{h}$  are the same parameters as in Eq", "word_idx": 20469, "sentence_idx": 266, "label": "unlabeled"}, {"type": "text", "expr": "  $\\bm{P}_{\\bm{r}}^{t}$  is the probability distribution over grounding regions  $\\bm{r}_{\\bm{I}}$  and visual sentinel  $\\tilde{r}$ ", "word_idx": 20545, "sentence_idx": 267, "label": "unlabeled"}, {"type": "text", "expr": " The last element of the vector in Eq", "word_idx": 20678, "sentence_idx": 268, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 9  captures  $p(\\tilde{r}|\\bm{y}_{1:t-1})$ ", "word_idx": 20715, "sentence_idx": 269, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{W}_{s}\\in\\mathbb{R}^{d\\times d}$$", "word_idx": 20760, "sentence_idx": 270, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{W}_{z}\\in\\mathbb{R}^{d\\times d}$$", "word_idx": 20795, "sentence_idx": 271, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{W}_{z}$$", "word_idx": 20830, "sentence_idx": 272, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{w}_{h}$$", "word_idx": 20840, "sentence_idx": 273, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{P}_{\\bm{r}}^{t}$$", "word_idx": 20850, "sentence_idx": 274, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{r}_{\\bm{I}}$$", "word_idx": 20869, "sentence_idx": 275, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{r}$$", "word_idx": 20884, "sentence_idx": 276, "label": "unlabeled"}, {"type": "math", "expr": "$$p(\\tilde{r}|\\bm{y}_{1:t-1})$$", "word_idx": 20893, "sentence_idx": 277, "label": "unlabeled"}, {"type": "text", "expr": "We feed the hidden state  $\\bm{h}_{t}$  into a  softmax  layer to obtain the probability over textual words conditioned on the image, all previous words, and the visual sentinel:", "word_idx": 20920, "sentence_idx": 278, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{h}_{t}$$", "word_idx": 21098, "sentence_idx": 279, "label": "unlabeled"}, {"type": "text", "expr": "softmax", "word_idx": 21108, "sentence_idx": 280, "label": "unlabeled"}, {"type": "text", "expr": "$\\bm{P}_{txt}^{t}=\\textrm{softmax}\\left(\\bm{W}_{q}\\bm{h}_{t}\\right)\\vspace{-5pt}$", "word_idx": 21115, "sentence_idx": 281, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{P}_{txt}^{t}=\\textrm{softmax}\\left(\\bm{W}_{q}\\bm{h}_{t}\\right)\\vspace{-5pt}$$", "word_idx": 21196, "sentence_idx": 282, "label": "unlabeled"}, {"type": "text", "expr": "where  $\\bm{W}_{q}\\in\\mathbb{R}^{V\\times d}$ ,  $d$  is hidden state size, and  $V$  is textual vocabulary size", "word_idx": 21275, "sentence_idx": 283, "label": "unlabeled"}, {"type": "text", "expr": "\nPlugging in Eq", "word_idx": 21386, "sentence_idx": 284, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 10  and  $p(\\tilde{r}|\\bm{y}_{1:t-1})$  from the last element of the vector in Eq", "word_idx": 21401, "sentence_idx": 285, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 9  into Eq", "word_idx": 21484, "sentence_idx": 286, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 6  gives us the probability of generating a textual word in the template", "word_idx": 21496, "sentence_idx": 287, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{W}_{q}\\in\\mathbb{R}^{V\\times d}$$", "word_idx": 21570, "sentence_idx": 288, "label": "unlabeled"}, {"type": "math", "expr": "$$p(\\tilde{r}|\\bm{y}_{1:t-1})$$", "word_idx": 21605, "sentence_idx": 289, "label": "unlabeled"}, {"type": "text", "expr": "2  Caption Refinement: Filling in The Slots", "word_idx": 21632, "sentence_idx": 290, "label": "unlabeled"}, {"type": "text", "expr": "To fill the slots in the generated template with visual words grounded in image regions, we leverage the outputs of an object detection network", "word_idx": 21675, "sentence_idx": 291, "label": "unlabeled"}, {"type": "text", "expr": " Given a grounding region, the category can be obtained through any detection framework  ", "word_idx": 21818, "sentence_idx": 292, "label": "unlabeled"}, {"type": "text", "expr": " But outputs of detection networks are typically singular coarse labels  \\eg \u201cdog\u201d", "word_idx": 21907, "sentence_idx": 293, "label": "unlabeled"}, {"type": "text", "expr": " Captions often refer to these entities in a fine-grained fashion  \\eg \u201cpuppy\u201d or in the plural form \u201cdogs\u201d", "word_idx": 21989, "sentence_idx": 294, "label": "unlabeled"}, {"type": "text", "expr": " In order to accommodate for these linguistic variations, the visual word  $y^{vis}$  in our model is a refinement of the category name by considering the following two factors:\nFirst, determine the plurality \u2013 whether it should be singular or plural", "word_idx": 22096, "sentence_idx": 295, "label": "unlabeled"}, {"type": "text", "expr": " Second, determine the fine-grained class (if any)", "word_idx": 22346, "sentence_idx": 296, "label": "unlabeled"}, {"type": "text", "expr": " Using two single layer MLPs with ReLU activation  $f(\\cdot)$ , we compute them with:", "word_idx": 22396, "sentence_idx": 297, "label": "unlabeled"}, {"type": "math", "expr": "$$y^{vis}$$", "word_idx": 22481, "sentence_idx": 298, "label": "unlabeled"}, {"type": "math", "expr": "$$f(\\cdot)$$", "word_idx": 22488, "sentence_idx": 299, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\bm{P}_{b}^{t}$", "word_idx": 22496, "sentence_idx": 300, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\bm{P}_{b}^{t}$$", "word_idx": 22525, "sentence_idx": 301, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\textrm{softmax}\\left(\\bm{W}_{b}f_{b}\\left(\\left[\\bm{v}_{t};\\bm{%\nh}_{t}\\right]\\right)\\right)$", "word_idx": 22552, "sentence_idx": 302, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\textrm{softmax}\\left(\\bm{W}_{b}f_{b}\\left(\\left[\\bm{v}_{t};\\bm{%\nh}_{t}\\right]\\right)\\right)$$", "word_idx": 22661, "sentence_idx": 303, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\bm{P}_{g}^{t}$", "word_idx": 22768, "sentence_idx": 304, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\bm{P}_{g}^{t}$$", "word_idx": 22797, "sentence_idx": 305, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\textrm{softmax}\\left(\\bm{U}^{T}\\bm{W}_{g}f_{g}\\left(\\left[\\bm{v%\n}_{t};\\bm{h}_{t}\\right]\\right)\\right)$", "word_idx": 22824, "sentence_idx": 306, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\textrm{softmax}\\left(\\bm{U}^{T}\\bm{W}_{g}f_{g}\\left(\\left[\\bm{v%\n}_{t};\\bm{h}_{t}\\right]\\right)\\right)$$", "word_idx": 22943, "sentence_idx": 307, "label": "unlabeled"}, {"type": "text", "expr": "$\\bm{W}_{b}\\in\\mathbb{R}^{2\\times d}$ ,  $\\bm{W}_{g}\\in\\mathbb{R}^{300\\times d}$  are the weight parameters", "word_idx": 23060, "sentence_idx": 308, "label": "unlabeled"}, {"type": "text", "expr": "  $\\bm{U}\\in\\mathbb{R}^{300\\times k}$  is the glove vector embeddings   for  $k$  fine-grained words associated with the category name", "word_idx": 23167, "sentence_idx": 309, "label": "unlabeled"}, {"type": "text", "expr": " The visual word  $y_{t}^{vis}$  is then determined by plurality and fine-grained class ( \\eg , if plurality is plural, and the fine-grained class is \u201cpuppy\u201d, the visual word will be \u201cpuppies\u201d)", "word_idx": 23301, "sentence_idx": 310, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{W}_{b}\\in\\mathbb{R}^{2\\times d}$$", "word_idx": 23494, "sentence_idx": 311, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{W}_{g}\\in\\mathbb{R}^{300\\times d}$$", "word_idx": 23529, "sentence_idx": 312, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{U}\\in\\mathbb{R}^{300\\times k}$$", "word_idx": 23566, "sentence_idx": 313, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}^{vis}$$", "word_idx": 23599, "sentence_idx": 314, "label": "unlabeled"}, {"type": "text", "expr": "3  Objective", "word_idx": 23610, "sentence_idx": 315, "label": "unlabeled"}, {"type": "text", "expr": "Most standard image captioning datasets ( \\eg COCO  ) do not contain phrase grounding annotations, while some datasets do ( \\eg Flickr30k  )", "word_idx": 23622, "sentence_idx": 316, "label": "unlabeled"}, {"type": "text", "expr": " Our training objective (presented next) can incorporate different kinds of supervision\n\u2013 be it strong annotations\nindicating which words in the caption are grounded in which\nboxes in the image, or weak supervision where objects are\nannotated in the image but are not aligned to words in the\ncaption", "word_idx": 23762, "sentence_idx": 317, "label": "unlabeled"}, {"type": "text", "expr": "\nGiven the target ground truth caption  $\\bm{y}_{1:T}^{*}$  and a image captioning model with parameters  $\\bm{\\theta}$ , we minimize the cross entropy loss:", "word_idx": 24061, "sentence_idx": 318, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{y}_{1:T}^{*}$$", "word_idx": 24218, "sentence_idx": 319, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{\\theta}$$", "word_idx": 24234, "sentence_idx": 320, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle L(\\bm{\\theta})$", "word_idx": 24245, "sentence_idx": 321, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle L(\\bm{\\theta})$$", "word_idx": 24275, "sentence_idx": 322, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=-\\sum_{t=1}^{T}\\log\\Big{(}\\overbrace{p(y_{t}^{*}|\\tilde{r},\\bm{y%\n}_{1:t-1}^{*})p(\\tilde{r}|\\bm{y}_{1:t-1}^{*})\\mathbbm{1}_{(y_{t}^{*}=y^{%\n\\textrm{txt}})}}^{\\begin{subarray}{c}\\text{Textual word probability}%\n\\end{subarray}}+$", "word_idx": 24303, "sentence_idx": 323, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=-\\sum_{t=1}^{T}\\log\\Big{(}\\overbrace{p(y_{t}^{*}|\\tilde{r},\\bm{y%\n}_{1:t-1}^{*})p(\\tilde{r}|\\bm{y}_{1:t-1}^{*})\\mathbbm{1}_{(y_{t}^{*}=y^{%\n\\textrm{txt}})}}^{\\begin{subarray}{c}\\text{Textual word probability}%\n\\end{subarray}}+$$", "word_idx": 24545, "sentence_idx": 324, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\underbrace{p\\left(b_{t}^{*},s_{t}^{*}|\\bm{r}_{t},\\bm{y}_{1:t-1}^%\n{*}\\right)}_{\\text{Caption refinement}}\\big{(}\\underbrace{\\dfrac{1}{m}\\sum_{i=%\n1}^{m}p\\left(r_{t}^{i}|\\bm{y}^{*}_{1:t-1}\\right)\\big{)}\\mathbbm{1}_{(y_{t}^{*}%\n=y^{\\textrm{vis}})}}_{\\begin{subarray}{c}\\text{Averaged target region %\nprobability}\\end{subarray}}\\Big{)}$", "word_idx": 24785, "sentence_idx": 325, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\underbrace{p\\left(b_{t}^{*},s_{t}^{*}|\\bm{r}_{t},\\bm{y}_{1:t-1}^%\n{*}\\right)}_{\\text{Caption refinement}}\\big{(}\\underbrace{\\dfrac{1}{m}\\sum_{i=%\n1}^{m}p\\left(r_{t}^{i}|\\bm{y}^{*}_{1:t-1}\\right)\\big{)}\\mathbbm{1}_{(y_{t}^{*}%\n=y^{\\textrm{vis}})}}_{\\begin{subarray}{c}\\text{Averaged target region %\nprobability}\\end{subarray}}\\Big{)}$$", "word_idx": 25133, "sentence_idx": 326, "label": "unlabeled"}, {"type": "text", "expr": "where  $y_{t}^{*}$  is the word from the ground truth caption at time  $t$ ", "word_idx": 25479, "sentence_idx": 327, "label": "unlabeled"}, {"type": "text", "expr": "  $\\mathbbm{1}_{(y_{t}^{*}=y^{\\textrm{txt}})}$  is the indicator function which\nequals to 1 if  $y_{t}^{*}$  is textual word and 0 otherwise", "word_idx": 25554, "sentence_idx": 328, "label": "unlabeled"}, {"type": "text", "expr": "  $b_{t}^{*}$  and  $s_{t}^{*}$ \nare the target ground truth plurality and find-grained class", "word_idx": 25694, "sentence_idx": 329, "label": "unlabeled"}, {"type": "text", "expr": "\n $\\{r_{t}^{i}\\}_{i=1}^{m}\\in\\bm{r_{I}}$  are the target grounding regions of the visual word at time  $t$ ", "word_idx": 25787, "sentence_idx": 330, "label": "unlabeled"}, {"type": "text", "expr": " We maximize the averaged log probability of the target grounding regions", "word_idx": 25894, "sentence_idx": 331, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}^{*}$$", "word_idx": 25967, "sentence_idx": 332, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbbm{1}_{(y_{t}^{*}=y^{\\textrm{txt}})}$$", "word_idx": 25976, "sentence_idx": 333, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}^{*}$$", "word_idx": 26018, "sentence_idx": 334, "label": "unlabeled"}, {"type": "math", "expr": "$$b_{t}^{*}$$", "word_idx": 26027, "sentence_idx": 335, "label": "unlabeled"}, {"type": "math", "expr": "$$s_{t}^{*}$$", "word_idx": 26036, "sentence_idx": 336, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{r_{t}^{i}\\}_{i=1}^{m}\\in\\bm{r_{I}}$$", "word_idx": 26045, "sentence_idx": 337, "label": "unlabeled"}, {"type": "text", "expr": "Visual word extraction", "word_idx": 26081, "sentence_idx": 338, "label": "unlabeled"}, {"type": "text", "expr": "  During training, visual words in a caption are dynamically identified by matching the base form of each word (using the Stanford lemmatization\ntoolbox\u00a0 ) against a vocabulary\nof visual words (details of how to get visual word can be found in dataset Sec", "word_idx": 26103, "sentence_idx": 339, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 4 )", "word_idx": 26358, "sentence_idx": 340, "label": "unlabeled"}, {"type": "text", "expr": "\nThe grounding regions  $\\{r_{t}^{i}\\}_{i=1}^{m}$  for a visual word  $y_{t}$  is\nidentified by computing the IoU of all boxes detected by the\nobject detection network with the ground truth bounding box\nassociated with the category corresponding to  $y_{t}$ ", "word_idx": 26363, "sentence_idx": 341, "label": "unlabeled"}, {"type": "text", "expr": "\nIf the score exceeds a threshold of  $05$  and the grounding region label\nmatches the visual word, the bounding boxes\nare selected as the grounding regions", "word_idx": 26621, "sentence_idx": 342, "label": "unlabeled"}, {"type": "text", "expr": ", given a target visual word \u201ccat\u201d, if there are no proposals that match the target bounding box, the model predicts the textual word \u201ccat\u201d instead", "word_idx": 26777, "sentence_idx": 343, "label": "unlabeled"}, {"type": "text", "expr": "Visual word extraction", "word_idx": 26924, "sentence_idx": 344, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{r_{t}^{i}\\}_{i=1}^{m}$$", "word_idx": 26946, "sentence_idx": 345, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}$$", "word_idx": 26969, "sentence_idx": 346, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}$$", "word_idx": 26974, "sentence_idx": 347, "label": "unlabeled"}, {"type": "math", "expr": "$$0.5$$", "word_idx": 26979, "sentence_idx": 348, "label": "unlabeled"}, {"type": "text", "expr": "4  Implementation Details", "word_idx": 26982, "sentence_idx": 349, "label": "unlabeled"}, {"type": "text", "expr": "Detection model", "word_idx": 27007, "sentence_idx": 350, "label": "unlabeled"}, {"type": "text", "expr": "  We use Faster R-CNN   with ResNet-101   to obtain region proposals for the image", "word_idx": 27022, "sentence_idx": 351, "label": "unlabeled"}, {"type": "text", "expr": " We use an IoU threshold of 0", "word_idx": 27104, "sentence_idx": 352, "label": "unlabeled"}, {"type": "text", "expr": "7 for region proposal suppression and 0", "word_idx": 27133, "sentence_idx": 353, "label": "unlabeled"}, {"type": "text", "expr": "3 for class suppressions", "word_idx": 27172, "sentence_idx": 354, "label": "unlabeled"}, {"type": "text", "expr": " A class detection confidence threshold of 0", "word_idx": 27196, "sentence_idx": 355, "label": "unlabeled"}, {"type": "text", "expr": "5 is used to select regions", "word_idx": 27240, "sentence_idx": 356, "label": "unlabeled"}, {"type": "text", "expr": "Detection model", "word_idx": 27267, "sentence_idx": 357, "label": "unlabeled"}, {"type": "text", "expr": "Region feature", "word_idx": 27282, "sentence_idx": 358, "label": "unlabeled"}, {"type": "text", "expr": "  We use a pre-trained ResNet-101   in our model", "word_idx": 27296, "sentence_idx": 359, "label": "unlabeled"}, {"type": "text", "expr": " The image is first resized to  $576\\times 576$  and we random crop  $512\\times 512$  as the input to the CNN network", "word_idx": 27344, "sentence_idx": 360, "label": "unlabeled"}, {"type": "text", "expr": " Given proposals from the pre-trained detection model, the feature  $\\bm{v}_{i}$  for region  $i$  is a concatenation of 3 different features  $\\bm{v}_{i}=[\\bm{v}_{i}^{p};\\bm{v}_{i}^{l};\\bm{v}_{i}^{g}]$  where  $\\bm{v}_{i}^{p}$  is the pooling feature of RoI align layer   given the proposal coordinates,  $\\bm{v}_{i}^{l}$  is the location feature and  $\\bm{v}_{i}^{g}$  is the glove vector embedding of the class label for region  $i$ ", "word_idx": 27461, "sentence_idx": 361, "label": "unlabeled"}, {"type": "text", "expr": "\nLet  $x_{\\text{min}},y_{\\text{min}},x_{\\text{max}},y_{\\text{max}}$  be the bounding box coordinates of the region  $b$ ;  $W_{I}$  and  $H_{I}$  be the width and height of the image  $I$ ", "word_idx": 27897, "sentence_idx": 362, "label": "unlabeled"}, {"type": "text", "expr": " Then the location feature  $\\bm{v}_{i}^{l}$  can be obtained by projecting the normalized location  $[\\dfrac{x_{\\text{min}}}{W_{I}},\\dfrac{y_{\\text{min}}}{H_{I}},\\dfrac{x_{\\text{%\nmax}}}{W_{I}},\\dfrac{y_{\\text{max}}}{H_{I}}]$  into another embedding space", "word_idx": 28085, "sentence_idx": 363, "label": "unlabeled"}, {"type": "text", "expr": "Region feature", "word_idx": 28341, "sentence_idx": 364, "label": "unlabeled"}, {"type": "math", "expr": "$$576\\times 576$$", "word_idx": 28355, "sentence_idx": 365, "label": "unlabeled"}, {"type": "math", "expr": "$$512\\times 512$$", "word_idx": 28368, "sentence_idx": 366, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{v}_{i}$$", "word_idx": 28381, "sentence_idx": 367, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{v}_{i}=[\\bm{v}_{i}^{p};\\bm{v}_{i}^{l};\\bm{v}_{i}^{g}]$$", "word_idx": 28391, "sentence_idx": 368, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{v}_{i}^{p}$$", "word_idx": 28448, "sentence_idx": 369, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{v}_{i}^{l}$$", "word_idx": 28462, "sentence_idx": 370, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{v}_{i}^{g}$$", "word_idx": 28476, "sentence_idx": 371, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{\\text{min}},y_{\\text{min}},x_{\\text{max}},y_{\\text{max}}$$", "word_idx": 28490, "sentence_idx": 372, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{I}$$", "word_idx": 28549, "sentence_idx": 373, "label": "unlabeled"}, {"type": "math", "expr": "$$H_{I}$$", "word_idx": 28554, "sentence_idx": 374, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{v}_{i}^{l}$$", "word_idx": 28559, "sentence_idx": 375, "label": "unlabeled"}, {"type": "math", "expr": "$$[\\dfrac{x_{\\text{min}}}{W_{I}},\\dfrac{y_{\\text{min}}}{H_{I}},\\dfrac{x_{\\text{%\nmax}}}{W_{I}},\\dfrac{y_{\\text{max}}}{H_{I}}]$$", "word_idx": 28573, "sentence_idx": 376, "label": "unlabeled"}, {"type": "text", "expr": "Language model", "word_idx": 28696, "sentence_idx": 377, "label": "unlabeled"}, {"type": "text", "expr": "  We use an attention model with two LSTM layers\u00a0  as our base attention model", "word_idx": 28710, "sentence_idx": 378, "label": "unlabeled"}, {"type": "text", "expr": "\nGiven  $N$  region features from detection proposals  $\\bm{V}=\\{\\bm{v}_{1},\\ldots,\\bm{v}_{N}\\}$  and CNN features from the last convolution layer at  $K$  grids  $\\bm{\\hat{V}}=\\{\\bm{\\hat{v}}_{1},\\ldots,\\bm{\\hat{v}}_{K}\\}$ , the language model has two separate attention layers shown in Fig\u00a0 4 ", "word_idx": 28788, "sentence_idx": 379, "label": "unlabeled"}, {"type": "text", "expr": " The attention distribution over the image features for detection proposals is:", "word_idx": 29082, "sentence_idx": 380, "label": "unlabeled"}, {"type": "text", "expr": "Language model", "word_idx": 29161, "sentence_idx": 381, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{V}=\\{\\bm{v}_{1},\\ldots,\\bm{v}_{N}\\}$$", "word_idx": 29175, "sentence_idx": 382, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{\\hat{V}}=\\{\\bm{\\hat{v}}_{1},\\ldots,\\bm{\\hat{v}}_{K}\\}$$", "word_idx": 29214, "sentence_idx": 383, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\bm{z}_{t}$", "word_idx": 29271, "sentence_idx": 384, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\bm{z}_{t}$$", "word_idx": 29296, "sentence_idx": 385, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\bm{w}_{z}^{T}\\tanh\\left(\\bm{W}_{v}\\bm{V}+(\\bm{W_{g}\\bm{h}_{t}})%\n\\mathbbm{1}^{T}\\right)$", "word_idx": 29319, "sentence_idx": 386, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\bm{w}_{z}^{T}\\tanh\\left(\\bm{W}_{v}\\bm{V}+(\\bm{W_{g}\\bm{h}_{t}})%\n\\mathbbm{1}^{T}\\right)$$", "word_idx": 29423, "sentence_idx": 387, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\bm{\\alpha}_{t}$", "word_idx": 29525, "sentence_idx": 388, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\bm{\\alpha}_{t}$$", "word_idx": 29555, "sentence_idx": 389, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\mathrm{softmax}(\\bm{z}_{t})$", "word_idx": 29583, "sentence_idx": 390, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\mathrm{softmax}(\\bm{z}_{t})$$", "word_idx": 29627, "sentence_idx": 391, "label": "unlabeled"}, {"type": "text", "expr": "where  $\\bm{W}_{v}\\in\\mathbb{R}^{m\\times d}$ ,  $\\bm{W}_{g}\\in\\mathbb{R}^{d\\times d}$  and  $\\bm{w}\\in\\mathbb{R}^{d\\times 1}$ ", "word_idx": 29669, "sentence_idx": 392, "label": "unlabeled"}, {"type": "text", "expr": "  $\\mathbbm{1}\\in\\mathbb{R}^{N}$  is a vector with all elements set to 1", "word_idx": 29795, "sentence_idx": 393, "label": "unlabeled"}, {"type": "text", "expr": "  $\\bm{\\alpha}_{t}$  is the attention weight over  $N$  image location features", "word_idx": 29867, "sentence_idx": 394, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{W}_{v}\\in\\mathbb{R}^{m\\times d}$$", "word_idx": 29946, "sentence_idx": 395, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{W}_{g}\\in\\mathbb{R}^{d\\times d}$$", "word_idx": 29981, "sentence_idx": 396, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{w}\\in\\mathbb{R}^{d\\times 1}$$", "word_idx": 30016, "sentence_idx": 397, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbbm{1}\\in\\mathbb{R}^{N}$$", "word_idx": 30047, "sentence_idx": 398, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{\\alpha}_{t}$$", "word_idx": 30075, "sentence_idx": 399, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:  Language model used in our approach", "word_idx": 30090, "sentence_idx": 400, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:", "word_idx": 30136, "sentence_idx": 401, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a05:  Generated captions and corresponding visual grounding regions on the standard image captioning task (Top: COCO, Bottom: Flickr30k)", "word_idx": 30145, "sentence_idx": 402, "label": "unlabeled"}, {"type": "text", "expr": " Different colors show a correspondence between the visual words and grounding regions", "word_idx": 30286, "sentence_idx": 403, "label": "unlabeled"}, {"type": "text", "expr": " Grey regions are the proposals not selected in the caption", "word_idx": 30372, "sentence_idx": 404, "label": "unlabeled"}, {"type": "text", "expr": " First 3 columns show success and last column shows failure cases (words are grounded in the wrong region)", "word_idx": 30431, "sentence_idx": 405, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a05:", "word_idx": 30537, "sentence_idx": 406, "label": "unlabeled"}, {"type": "text", "expr": "Training details", "word_idx": 30546, "sentence_idx": 407, "label": "unlabeled"}, {"type": "text", "expr": "  In our experiments, we use a two layer LSTM with hidden size  $1024$ ", "word_idx": 30562, "sentence_idx": 408, "label": "unlabeled"}, {"type": "text", "expr": " The number of hidden units in the attention layer and the size of the input word embedding are  $512$ ", "word_idx": 30633, "sentence_idx": 409, "label": "unlabeled"}, {"type": "text", "expr": " We use the Adam   optimizer with an initial learning rate of  $5\\times 10^{-4}$  and anneal the learning rate\nby a factor of  $08$  every three epochs", "word_idx": 30736, "sentence_idx": 410, "label": "unlabeled"}, {"type": "text", "expr": " We train the model up to 50 epochs with early stopping", "word_idx": 30887, "sentence_idx": 411, "label": "unlabeled"}, {"type": "text", "expr": " Note that we do not finetune the CNN network during training", "word_idx": 30942, "sentence_idx": 412, "label": "unlabeled"}, {"type": "text", "expr": " We set the batch size to be 100 for COCO   and 50 for Flickr30k  ", "word_idx": 31003, "sentence_idx": 413, "label": "unlabeled"}, {"type": "text", "expr": "Training details", "word_idx": 31069, "sentence_idx": 414, "label": "unlabeled"}, {"type": "math", "expr": "$$1024$$", "word_idx": 31085, "sentence_idx": 415, "label": "unlabeled"}, {"type": "math", "expr": "$$512$$", "word_idx": 31089, "sentence_idx": 416, "label": "unlabeled"}, {"type": "math", "expr": "$$5\\times 10^{-4}$$", "word_idx": 31092, "sentence_idx": 417, "label": "unlabeled"}, {"type": "math", "expr": "$$0.8$$", "word_idx": 31107, "sentence_idx": 418, "label": "unlabeled"}, {"type": "text", "expr": "4  Experimental Results", "word_idx": 31110, "sentence_idx": 419, "label": "unlabeled"}, {"type": "text", "expr": "Datasets", "word_idx": 31133, "sentence_idx": 420, "label": "unlabeled"}, {"type": "text", "expr": "  We experiment with two datasets", "word_idx": 31141, "sentence_idx": 421, "label": "unlabeled"}, {"type": "text", "expr": " Flickr30k Entities   contains 275,755 bounding boxes from 31,783\nimages associated with natural language phrases", "word_idx": 31174, "sentence_idx": 422, "label": "unlabeled"}, {"type": "text", "expr": " Each image is annotated with\n5 crowdsourced captions", "word_idx": 31287, "sentence_idx": 423, "label": "unlabeled"}, {"type": "text", "expr": " For each annotated phrase in the caption, we identify visual words by selecting the inner most NP (noun phrase) tag from the Stanford part-of-speech tagger\u00a0 ", "word_idx": 31340, "sentence_idx": 424, "label": "unlabeled"}, {"type": "text", "expr": "\nWe use Stanford Lemmatization Toolbox   to\nget the base form of the entity words resulting in 2,567 unique words", "word_idx": 31498, "sentence_idx": 425, "label": "unlabeled"}, {"type": "text", "expr": "Datasets", "word_idx": 31611, "sentence_idx": 426, "label": "unlabeled"}, {"type": "text", "expr": "COCO   contains 82,783, 40,504 and 40,775 images for training, validation and testing\nrespectively", "word_idx": 31619, "sentence_idx": 427, "label": "unlabeled"}, {"type": "text", "expr": " Each image has around 5 crowdsourced captions", "word_idx": 31717, "sentence_idx": 428, "label": "unlabeled"}, {"type": "text", "expr": " Unlike Flickr30k Entities, COCO does not have bounding box annotations associated with specific phrases or entities in the caption", "word_idx": 31763, "sentence_idx": 429, "label": "unlabeled"}, {"type": "text", "expr": " To identify visual words, we manually constructed an object category to word mapping that maps object categories like  $<$ person $>$  to a list of potential fine-grained labels like [\u201cchild\u201d, \u201cbaker\u201d, \u2026]", "word_idx": 31894, "sentence_idx": 430, "label": "unlabeled"}, {"type": "text", "expr": " This results in 80 categories with a total of 413 fine-grained classes", "word_idx": 32099, "sentence_idx": 431, "label": "unlabeled"}, {"type": "text", "expr": " See supp", "word_idx": 32170, "sentence_idx": 432, "label": "unlabeled"}, {"type": "text", "expr": " for details", "word_idx": 32179, "sentence_idx": 433, "label": "unlabeled"}, {"type": "text", "expr": "Method BLEU1 BLEU4 METEOR CIDEr SPICE", "word_idx": 32191, "sentence_idx": 434, "label": "unlabeled"}, {"type": "text", "expr": "Method", "word_idx": 32228, "sentence_idx": 435, "label": "unlabeled"}, {"type": "text", "expr": "BLEU1", "word_idx": 32234, "sentence_idx": 436, "label": "unlabeled"}, {"type": "text", "expr": "BLEU1", "word_idx": 32239, "sentence_idx": 437, "label": "unlabeled"}, {"type": "text", "expr": "BLEU4", "word_idx": 32244, "sentence_idx": 438, "label": "unlabeled"}, {"type": "text", "expr": "BLEU4", "word_idx": 32249, "sentence_idx": 439, "label": "unlabeled"}, {"type": "text", "expr": "METEOR", "word_idx": 32254, "sentence_idx": 440, "label": "unlabeled"}, {"type": "text", "expr": "METEOR", "word_idx": 32260, "sentence_idx": 441, "label": "unlabeled"}, {"type": "text", "expr": "CIDEr", "word_idx": 32266, "sentence_idx": 442, "label": "unlabeled"}, {"type": "text", "expr": "CIDEr", "word_idx": 32271, "sentence_idx": 443, "label": "unlabeled"}, {"type": "text", "expr": "SPICE", "word_idx": 32276, "sentence_idx": 444, "label": "unlabeled"}, {"type": "text", "expr": "SPICE", "word_idx": 32281, "sentence_idx": 445, "label": "unlabeled"}, {"type": "text", "expr": "5 - -", "word_idx": 32286, "sentence_idx": 446, "label": "unlabeled"}, {"type": "text", "expr": "Hard-Attention", "word_idx": 32291, "sentence_idx": 447, "label": "unlabeled"}, {"type": "text", "expr": "9 - -", "word_idx": 32305, "sentence_idx": 448, "label": "unlabeled"}, {"type": "text", "expr": "ATT-FCN", "word_idx": 32310, "sentence_idx": 449, "label": "unlabeled"}, {"type": "text", "expr": "Adaptive", "word_idx": 32317, "sentence_idx": 450, "label": "unlabeled"}, {"type": "text", "expr": "NBT 69", "word_idx": 32325, "sentence_idx": 451, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\mathrm{oracle}}$$", "word_idx": 32331, "sentence_idx": 452, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:  Performance on the test portion of Karpathy\u00a0 \\etal \u00a0 \u2019s splits on Flickr30k Entities dataset", "word_idx": 32351, "sentence_idx": 453, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:", "word_idx": 32453, "sentence_idx": 454, "label": "unlabeled"}, {"type": "text", "expr": "\\etal", "word_idx": 32461, "sentence_idx": 455, "label": "unlabeled"}, {"type": "text", "expr": "Detector pre-training", "word_idx": 32466, "sentence_idx": 456, "label": "unlabeled"}, {"type": "text", "expr": " \nWe use open an source implementation   of Faster-RCNN   to train the detector", "word_idx": 32487, "sentence_idx": 457, "label": "unlabeled"}, {"type": "text", "expr": " For Flickr30K Entities, we use visual words that occur at least 100 times as detection labels, resulting in a total of  $460$  detection labels", "word_idx": 32566, "sentence_idx": 458, "label": "unlabeled"}, {"type": "text", "expr": " Since detection labels and visual words have a one-to-one mapping, we do not have fine-grained classes for the Flickr30K Entities dataset \u2013 the caption refinement process only determines the plurality of detection labels", "word_idx": 32710, "sentence_idx": 459, "label": "unlabeled"}, {"type": "text", "expr": " For COCO, ground truth detection annotations are used to train the object detector", "word_idx": 32931, "sentence_idx": 460, "label": "unlabeled"}, {"type": "text", "expr": "Detector pre-training", "word_idx": 33014, "sentence_idx": 461, "label": "unlabeled"}, {"type": "math", "expr": "$$460$$", "word_idx": 33035, "sentence_idx": 462, "label": "unlabeled"}, {"type": "text", "expr": "Caption pre-processing", "word_idx": 33038, "sentence_idx": 463, "label": "unlabeled"}, {"type": "text", "expr": "  We truncate captions longer than 16 words for both COCO and Flickr30k Entities dataset", "word_idx": 33060, "sentence_idx": 464, "label": "unlabeled"}, {"type": "text", "expr": " We then build a vocabulary of words that occur at least 5 times in the training set, resulting in 9,587 and 6,864 words for COCO and Flickr30k Entities, respectively", "word_idx": 33148, "sentence_idx": 465, "label": "unlabeled"}, {"type": "text", "expr": "Caption pre-processing", "word_idx": 33314, "sentence_idx": 466, "label": "unlabeled"}, {"type": "text", "expr": "Method BLEU1 BLEU4 METEOR CIDEr SPICE", "word_idx": 33336, "sentence_idx": 467, "label": "unlabeled"}, {"type": "text", "expr": "Method", "word_idx": 33373, "sentence_idx": 468, "label": "unlabeled"}, {"type": "text", "expr": "BLEU1", "word_idx": 33379, "sentence_idx": 469, "label": "unlabeled"}, {"type": "text", "expr": "BLEU1", "word_idx": 33384, "sentence_idx": 470, "label": "unlabeled"}, {"type": "text", "expr": "BLEU4", "word_idx": 33389, "sentence_idx": 471, "label": "unlabeled"}, {"type": "text", "expr": "BLEU4", "word_idx": 33394, "sentence_idx": 472, "label": "unlabeled"}, {"type": "text", "expr": "METEOR", "word_idx": 33399, "sentence_idx": 473, "label": "unlabeled"}, {"type": "text", "expr": "METEOR", "word_idx": 33405, "sentence_idx": 474, "label": "unlabeled"}, {"type": "text", "expr": "CIDEr", "word_idx": 33411, "sentence_idx": 475, "label": "unlabeled"}, {"type": "text", "expr": "CIDEr", "word_idx": 33416, "sentence_idx": 476, "label": "unlabeled"}, {"type": "text", "expr": "SPICE", "word_idx": 33421, "sentence_idx": 477, "label": "unlabeled"}, {"type": "text", "expr": "SPICE", "word_idx": 33426, "sentence_idx": 478, "label": "unlabeled"}, {"type": "text", "expr": "6 108", "word_idx": 33431, "sentence_idx": 479, "label": "unlabeled"}, {"type": "text", "expr": "Adaptive", "word_idx": 33436, "sentence_idx": 480, "label": "unlabeled"}, {"type": "text", "expr": "0 101", "word_idx": 33444, "sentence_idx": 481, "label": "unlabeled"}, {"type": "text", "expr": "Att2in", "word_idx": 33449, "sentence_idx": 482, "label": "unlabeled"}, {"type": "text", "expr": "1 105", "word_idx": 33455, "sentence_idx": 483, "label": "unlabeled"}, {"type": "text", "expr": "Up-Down", "word_idx": 33460, "sentence_idx": 484, "label": "unlabeled"}, {"type": "text", "expr": "3 111", "word_idx": 33467, "sentence_idx": 485, "label": "unlabeled"}, {"type": "text", "expr": "Att2in", "word_idx": 33472, "sentence_idx": 486, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{*}$$", "word_idx": 33478, "sentence_idx": 487, "label": "unlabeled"}, {"type": "text", "expr": "7 120", "word_idx": 33484, "sentence_idx": 488, "label": "unlabeled"}, {"type": "text", "expr": "Up-Down", "word_idx": 33489, "sentence_idx": 489, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\dagger}$$", "word_idx": 33496, "sentence_idx": 490, "label": "unlabeled"}, {"type": "text", "expr": "NBT 75", "word_idx": 33508, "sentence_idx": 491, "label": "unlabeled"}, {"type": "text", "expr": "1 107", "word_idx": 33514, "sentence_idx": 492, "label": "unlabeled"}, {"type": "text", "expr": "4 108", "word_idx": 33519, "sentence_idx": 493, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\mathrm{oracle}}$$", "word_idx": 33524, "sentence_idx": 494, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:  Performance on the test portion of Karpathy\u00a0 \\etal \u00a0 \u2019s splits on COCO dataset", "word_idx": 33544, "sentence_idx": 495, "label": "unlabeled"}, {"type": "text", "expr": "  ${*}$  directly optimizes the CIDEr Metric,  $\\dagger$  uses better image features, and are thus not directly comparable", "word_idx": 33632, "sentence_idx": 496, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:", "word_idx": 33754, "sentence_idx": 497, "label": "unlabeled"}, {"type": "text", "expr": "\\etal", "word_idx": 33762, "sentence_idx": 498, "label": "unlabeled"}, {"type": "math", "expr": "$${*}$$", "word_idx": 33767, "sentence_idx": 499, "label": "unlabeled"}, {"type": "math", "expr": "$$\\dagger$$", "word_idx": 33770, "sentence_idx": 500, "label": "unlabeled"}, {"type": "text", "expr": "1  Standard Image Captioning", "word_idx": 33777, "sentence_idx": 501, "label": "unlabeled"}, {"type": "text", "expr": "For standard image captioning, we use splits from Karpathy\u00a0 \\etal \u00a0  on COCO/Flickr30k", "word_idx": 33805, "sentence_idx": 502, "label": "unlabeled"}, {"type": "text", "expr": " We report results using the COCO captioning evaluation toolkit  , which reports the widely used automatic evaluation metrics, BLEU  , METEOR  , CIDEr   and SPICE  ", "word_idx": 33891, "sentence_idx": 503, "label": "unlabeled"}, {"type": "text", "expr": "\\etal", "word_idx": 34055, "sentence_idx": 504, "label": "unlabeled"}, {"type": "text", "expr": "We present our methods trained on different object detectors: Flickr and COCO", "word_idx": 34060, "sentence_idx": 505, "label": "unlabeled"}, {"type": "text", "expr": "\nWe compare our approach (referred to as NBT) to recently proposed Hard-Attention  , ATT-FCN   and Adaptive   on Flickr30k, and Att2in  , Up-Down\u00a0  on COCO", "word_idx": 34137, "sentence_idx": 506, "label": "unlabeled"}, {"type": "text", "expr": "\nSince object detectors have not yet achieved near-perfect accuracies on these datasets, we also report the performance of our model under an oracle setting, where the ground truth object region and category is also provided during test time", "word_idx": 34292, "sentence_idx": 507, "label": "unlabeled"}, {"type": "text", "expr": " (referred to as NBT ${}^{\\mathrm{oracle}}$ )\nThis can be viewed as the upper bound of our method when we have perfect object detectors", "word_idx": 34533, "sentence_idx": 508, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\mathrm{oracle}}$$", "word_idx": 34668, "sentence_idx": 509, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a06:  Generated captions and corresponding visual grounding regions for the robust image captioning task", "word_idx": 34688, "sentence_idx": 510, "label": "unlabeled"}, {"type": "text", "expr": " \u201ccat-remote\u201d, \u201cman-bird\u201d, \u201cdog-skateboard\u201d and \u201corange-bird\u201d are co-occurring categories excluded in the training split", "word_idx": 34797, "sentence_idx": 511, "label": "unlabeled"}, {"type": "text", "expr": " First 3 columns show success and last column shows failure case (orange was not mentioned)", "word_idx": 34917, "sentence_idx": 512, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a06:", "word_idx": 35008, "sentence_idx": 513, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a0 1  shows results on the Flickr30k dataset", "word_idx": 35017, "sentence_idx": 514, "label": "unlabeled"}, {"type": "text", "expr": " We see that our method achieves state of the art on all automatic evaluation metrics, outperforming the previous state-of-art model Adaptive   by 2", "word_idx": 35065, "sentence_idx": 515, "label": "unlabeled"}, {"type": "text", "expr": "0 and 4", "word_idx": 35213, "sentence_idx": 516, "label": "unlabeled"}, {"type": "text", "expr": "4 on BLEU4 and CIDEr", "word_idx": 35220, "sentence_idx": 517, "label": "unlabeled"}, {"type": "text", "expr": "\nWhen using ground truth proposals, NBT ${}^{\\mathrm{oracle}}$  significantly outperforms previous methods, improving 5", "word_idx": 35240, "sentence_idx": 518, "label": "unlabeled"}, {"type": "text", "expr": "1 on SPICE, which implies that our method could further benefit from improved object detectors", "word_idx": 35359, "sentence_idx": 519, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\mathrm{oracle}}$$", "word_idx": 35453, "sentence_idx": 520, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a0 2  shows results on the COCO dataset", "word_idx": 35473, "sentence_idx": 521, "label": "unlabeled"}, {"type": "text", "expr": " Our method outperforms 4 out of 5 automatic evaluation metrics compared to the state of the art\u00a0  without using better visual features or directly optimizing the CIDEr metric", "word_idx": 35516, "sentence_idx": 522, "label": "unlabeled"}, {"type": "text", "expr": " Interestingly, the NBT ${}^{\\mathrm{oracle}}$  has little improvement over NBT", "word_idx": 35691, "sentence_idx": 523, "label": "unlabeled"}, {"type": "text", "expr": " We suspect the reason is that explicit ground truth annotation is absent for visual words", "word_idx": 35770, "sentence_idx": 524, "label": "unlabeled"}, {"type": "text", "expr": " Our model can be further improved with explicit co-reference supervision where the ground truth location annotation of the visual word is provided", "word_idx": 35860, "sentence_idx": 525, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 5  shows qualitative results on both datasets", "word_idx": 36007, "sentence_idx": 526, "label": "unlabeled"}, {"type": "text", "expr": " We see that our model learns to correctly identify the visual word, and ground it in image regions even under weak supervision (COCO)", "word_idx": 36054, "sentence_idx": 527, "label": "unlabeled"}, {"type": "text", "expr": " Our model is also robust to erroneous detections and produces correct captions (3rd column)", "word_idx": 36188, "sentence_idx": 528, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\mathrm{oracle}}$$", "word_idx": 36280, "sentence_idx": 529, "label": "unlabeled"}, {"type": "text", "expr": "Method BLEU4 METEOR CIDEr SPICE Accuracy", "word_idx": 36300, "sentence_idx": 530, "label": "unlabeled"}, {"type": "text", "expr": "Method", "word_idx": 36340, "sentence_idx": 531, "label": "unlabeled"}, {"type": "text", "expr": "BLEU4", "word_idx": 36346, "sentence_idx": 532, "label": "unlabeled"}, {"type": "text", "expr": "METEOR", "word_idx": 36351, "sentence_idx": 533, "label": "unlabeled"}, {"type": "text", "expr": "CIDEr", "word_idx": 36357, "sentence_idx": 534, "label": "unlabeled"}, {"type": "text", "expr": "SPICE", "word_idx": 36362, "sentence_idx": 535, "label": "unlabeled"}, {"type": "text", "expr": "Accuracy", "word_idx": 36367, "sentence_idx": 536, "label": "unlabeled"}, {"type": "text", "expr": "Att2in", "word_idx": 36375, "sentence_idx": 537, "label": "unlabeled"}, {"type": "text", "expr": "Up-Down", "word_idx": 36381, "sentence_idx": 538, "label": "unlabeled"}, {"type": "text", "expr": "NBT 31", "word_idx": 36388, "sentence_idx": 539, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\mathrm{oracle}}$$", "word_idx": 36394, "sentence_idx": 540, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a03:  Performance on the test portion of the robust image captioning split on COCO dataset", "word_idx": 36414, "sentence_idx": 541, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a03:", "word_idx": 36508, "sentence_idx": 542, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a07:  Generated captions and corresponding visual grounding regions for the novel object captioning task", "word_idx": 36516, "sentence_idx": 543, "label": "unlabeled"}, {"type": "text", "expr": " \u201czebra\u201d, \u201ctennis racket\u201d, \u201cbus\u201d and \u201cpizza\u201d are categories excluded in the training split", "word_idx": 36625, "sentence_idx": 544, "label": "unlabeled"}, {"type": "text", "expr": " First 3 columns show success and last column shows a failure case", "word_idx": 36715, "sentence_idx": 545, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a07:", "word_idx": 36781, "sentence_idx": 546, "label": "unlabeled"}, {"type": "text", "expr": "Out-of-Domain Test Data In-Domain Test Data", "word_idx": 36790, "sentence_idx": 547, "label": "unlabeled"}, {"type": "text", "expr": "Out-of-Domain Test Data", "word_idx": 36833, "sentence_idx": 548, "label": "unlabeled"}, {"type": "text", "expr": "Out-of-Domain Test Data", "word_idx": 36856, "sentence_idx": 549, "label": "unlabeled"}, {"type": "text", "expr": "In-Domain Test Data", "word_idx": 36879, "sentence_idx": 550, "label": "unlabeled"}, {"type": "text", "expr": "In-Domain Test Data", "word_idx": 36898, "sentence_idx": 551, "label": "unlabeled"}, {"type": "text", "expr": "Method bottle bus couch microwave pizza racket suitcase zebra Avg SPICE METEOR CIDEr SPICE METEOR CIDER", "word_idx": 36917, "sentence_idx": 552, "label": "unlabeled"}, {"type": "text", "expr": "Method", "word_idx": 37020, "sentence_idx": 553, "label": "unlabeled"}, {"type": "text", "expr": "bottle", "word_idx": 37026, "sentence_idx": 554, "label": "unlabeled"}, {"type": "text", "expr": "bottle", "word_idx": 37032, "sentence_idx": 555, "label": "unlabeled"}, {"type": "text", "expr": "couch", "word_idx": 37038, "sentence_idx": 556, "label": "unlabeled"}, {"type": "text", "expr": "couch", "word_idx": 37043, "sentence_idx": 557, "label": "unlabeled"}, {"type": "text", "expr": "microwave", "word_idx": 37048, "sentence_idx": 558, "label": "unlabeled"}, {"type": "text", "expr": "microwave", "word_idx": 37057, "sentence_idx": 559, "label": "unlabeled"}, {"type": "text", "expr": "pizza", "word_idx": 37066, "sentence_idx": 560, "label": "unlabeled"}, {"type": "text", "expr": "pizza", "word_idx": 37071, "sentence_idx": 561, "label": "unlabeled"}, {"type": "text", "expr": "racket", "word_idx": 37076, "sentence_idx": 562, "label": "unlabeled"}, {"type": "text", "expr": "racket", "word_idx": 37082, "sentence_idx": 563, "label": "unlabeled"}, {"type": "text", "expr": "suitcase", "word_idx": 37088, "sentence_idx": 564, "label": "unlabeled"}, {"type": "text", "expr": "suitcase", "word_idx": 37096, "sentence_idx": 565, "label": "unlabeled"}, {"type": "text", "expr": "zebra", "word_idx": 37104, "sentence_idx": 566, "label": "unlabeled"}, {"type": "text", "expr": "zebra", "word_idx": 37109, "sentence_idx": 567, "label": "unlabeled"}, {"type": "text", "expr": "SPICE", "word_idx": 37114, "sentence_idx": 568, "label": "unlabeled"}, {"type": "text", "expr": "SPICE", "word_idx": 37119, "sentence_idx": 569, "label": "unlabeled"}, {"type": "text", "expr": "METEOR", "word_idx": 37124, "sentence_idx": 570, "label": "unlabeled"}, {"type": "text", "expr": "METEOR", "word_idx": 37130, "sentence_idx": 571, "label": "unlabeled"}, {"type": "text", "expr": "CIDEr", "word_idx": 37136, "sentence_idx": 572, "label": "unlabeled"}, {"type": "text", "expr": "CIDEr", "word_idx": 37141, "sentence_idx": 573, "label": "unlabeled"}, {"type": "text", "expr": "SPICE", "word_idx": 37146, "sentence_idx": 574, "label": "unlabeled"}, {"type": "text", "expr": "SPICE", "word_idx": 37151, "sentence_idx": 575, "label": "unlabeled"}, {"type": "text", "expr": "METEOR", "word_idx": 37156, "sentence_idx": 576, "label": "unlabeled"}, {"type": "text", "expr": "METEOR", "word_idx": 37162, "sentence_idx": 577, "label": "unlabeled"}, {"type": "text", "expr": "CIDER", "word_idx": 37168, "sentence_idx": 578, "label": "unlabeled"}, {"type": "text", "expr": "CIDER", "word_idx": 37173, "sentence_idx": 579, "label": "unlabeled"}, {"type": "text", "expr": "1 - 21", "word_idx": 37178, "sentence_idx": 580, "label": "unlabeled"}, {"type": "text", "expr": "4 - - - -", "word_idx": 37184, "sentence_idx": 581, "label": "unlabeled"}, {"type": "text", "expr": "7 - 23", "word_idx": 37193, "sentence_idx": 582, "label": "unlabeled"}, {"type": "text", "expr": "0 - - - -", "word_idx": 37199, "sentence_idx": 583, "label": "unlabeled"}, {"type": "text", "expr": "C-LSTM", "word_idx": 37208, "sentence_idx": 584, "label": "unlabeled"}, {"type": "text", "expr": "Base+T4", "word_idx": 37214, "sentence_idx": 585, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{*}$$", "word_idx": 37221, "sentence_idx": 586, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\dagger}$$", "word_idx": 37227, "sentence_idx": 587, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\dagger}$$", "word_idx": 37239, "sentence_idx": 588, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\dagger}$$", "word_idx": 37251, "sentence_idx": 589, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a04:  Evaluation of captions generated using the proposed method", "word_idx": 37263, "sentence_idx": 590, "label": "unlabeled"}, {"type": "text", "expr": " G means greedy decoding, and T1 $-$ 2 means using constrained beam search   with 1 $-$ 2 top detected concepts", "word_idx": 37331, "sentence_idx": 591, "label": "unlabeled"}, {"type": "text", "expr": "  $*$  is the result using VGG-16   and  $\\dagger$  is the result using ResNet-101", "word_idx": 37442, "sentence_idx": 592, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a04:", "word_idx": 37524, "sentence_idx": 593, "label": "unlabeled"}, {"type": "math", "expr": "$$\\dagger$$", "word_idx": 37532, "sentence_idx": 594, "label": "unlabeled"}, {"type": "text", "expr": "2  Robust Image Captioning", "word_idx": 37539, "sentence_idx": 595, "label": "unlabeled"}, {"type": "text", "expr": "To quantitatively evaluate image captioning models for novel scene compositions, we present a new split of the COCO dataset, called the robust-COCO split", "word_idx": 37565, "sentence_idx": 596, "label": "unlabeled"}, {"type": "text", "expr": " This new split is created by re-organizing the train and val splits of the COCO dataset such that the distribution of co-occurring objects in train is different from test", "word_idx": 37718, "sentence_idx": 597, "label": "unlabeled"}, {"type": "text", "expr": " We also present a new metric to evaluate grounding", "word_idx": 37889, "sentence_idx": 598, "label": "unlabeled"}, {"type": "text", "expr": "To quantitatively evaluate image captioning models for novel scene compositions, we present a new split of the COCO dataset, called the robust-COCO split", "word_idx": 37940, "sentence_idx": 599, "label": "unlabeled"}, {"type": "text", "expr": " This new split is created by re-organizing the train and val splits of the COCO dataset such that the distribution of co-occurring objects in train is different from test", "word_idx": 38093, "sentence_idx": 600, "label": "unlabeled"}, {"type": "text", "expr": " We also present a new metric to evaluate grounding", "word_idx": 38264, "sentence_idx": 601, "label": "unlabeled"}, {"type": "text", "expr": "Robust split", "word_idx": 38315, "sentence_idx": 602, "label": "unlabeled"}, {"type": "text", "expr": "  To create the new split, we first identify entity words that belong to the 80 COCO object categories by following the same pre-processing procedure", "word_idx": 38327, "sentence_idx": 603, "label": "unlabeled"}, {"type": "text", "expr": "\nFor each image, we get a list of object categories that are mentioned in the caption", "word_idx": 38476, "sentence_idx": 604, "label": "unlabeled"}, {"type": "text", "expr": " We then calculate the co-occurrence statistics for these 80 object categories", "word_idx": 38561, "sentence_idx": 605, "label": "unlabeled"}, {"type": "text", "expr": "\nStarting from the least co-occurring category pairs, we greedily add them to the test set and ensure that for each category, at least half the instances of each category are in the train set", "word_idx": 38639, "sentence_idx": 606, "label": "unlabeled"}, {"type": "text", "expr": " As a result, there are sufficient examples from each category in train, but at test time we see novel compositions (pairs) of categories", "word_idx": 38830, "sentence_idx": 607, "label": "unlabeled"}, {"type": "text", "expr": " Remaining images are assigned to the training set", "word_idx": 38967, "sentence_idx": 608, "label": "unlabeled"}, {"type": "text", "expr": "\nThe final split has 110,234/3,915/9,138 images in train/val/test respectively", "word_idx": 39017, "sentence_idx": 609, "label": "unlabeled"}, {"type": "text", "expr": "Robust split", "word_idx": 39095, "sentence_idx": 610, "label": "unlabeled"}, {"type": "text", "expr": "Evaluation metric", "word_idx": 39107, "sentence_idx": 611, "label": "unlabeled"}, {"type": "text", "expr": " \nTo evaluate visual grounding on the robust-COCO split, we want a metric that indicates whether or not a generated caption includes the new object combination", "word_idx": 39124, "sentence_idx": 612, "label": "unlabeled"}, {"type": "text", "expr": " Common automatic evaluation metrics such as BLEU   and CIDEr   measure the overall sentence fluency", "word_idx": 39283, "sentence_idx": 613, "label": "unlabeled"}, {"type": "text", "expr": " We also measure whether the generated caption contains the novel co-occurring categories that exist in the ground truth caption", "word_idx": 39383, "sentence_idx": 614, "label": "unlabeled"}, {"type": "text", "expr": " A generated caption is deemed 100% accurate if it contains at least one mention of the  compositionally novel  category-pairs in any ground truth annotation that describe the image", "word_idx": 39511, "sentence_idx": 615, "label": "unlabeled"}, {"type": "text", "expr": "Evaluation metric", "word_idx": 39692, "sentence_idx": 616, "label": "unlabeled"}, {"type": "text", "expr": "compositionally novel", "word_idx": 39709, "sentence_idx": 617, "label": "unlabeled"}, {"type": "text", "expr": "Results and analysis", "word_idx": 39730, "sentence_idx": 618, "label": "unlabeled"}, {"type": "text", "expr": " \nWe compare our method with state of the art Att2in\u00a0  and Up-Down  ", "word_idx": 39750, "sentence_idx": 619, "label": "unlabeled"}, {"type": "text", "expr": " These are implemented using the open source implementation from   that can replicate results on Karpathy\u2019s split", "word_idx": 39818, "sentence_idx": 620, "label": "unlabeled"}, {"type": "text", "expr": " We follow the experimental setting from   and train the model using the robust-COCO train set", "word_idx": 39931, "sentence_idx": 621, "label": "unlabeled"}, {"type": "text", "expr": " Table\u00a0 3  shows the results on the robust-COCO split", "word_idx": 40025, "sentence_idx": 622, "label": "unlabeled"}, {"type": "text", "expr": " As we can see, all models perform worse on the robust-COCO split than the Karpathy\u2019s split by 2 $\\sim$ 3 points in general", "word_idx": 40078, "sentence_idx": 623, "label": "unlabeled"}, {"type": "text", "expr": " Our method outperforms the previous state of the art methods on all metrics, outperforming Up-Down   by 2", "word_idx": 40201, "sentence_idx": 624, "label": "unlabeled"}, {"type": "text", "expr": "7 on the proposed metric", "word_idx": 40307, "sentence_idx": 625, "label": "unlabeled"}, {"type": "text", "expr": " The oracle setting (NBT ${}^{\\mathrm{oracle}}$ ) has consistent improvements on all metrics, improving 3", "word_idx": 40331, "sentence_idx": 626, "label": "unlabeled"}, {"type": "text", "expr": "3 on the proposed metric", "word_idx": 40436, "sentence_idx": 627, "label": "unlabeled"}, {"type": "text", "expr": "Results and analysis", "word_idx": 40460, "sentence_idx": 628, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sim$$", "word_idx": 40480, "sentence_idx": 629, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\mathrm{oracle}}$$", "word_idx": 40484, "sentence_idx": 630, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 6  shows qualitative results on the robust image captioning task", "word_idx": 40504, "sentence_idx": 631, "label": "unlabeled"}, {"type": "text", "expr": " Our model successfully produces a caption with novel compositions, such as \u201ccat-remote\u201d, \u201cman-bird\u201d and \u201cdog-skateboard\u201d to describe the image", "word_idx": 40570, "sentence_idx": 632, "label": "unlabeled"}, {"type": "text", "expr": " The last column shows failure cases where our model didn\u2019t select \u201corange\u201d in the caption", "word_idx": 40713, "sentence_idx": 633, "label": "unlabeled"}, {"type": "text", "expr": " We can force our model to produce a caption containing \u201corange\u201d and \u201cbird\u201d using constrained beam search  , further illustrated in Sec", "word_idx": 40803, "sentence_idx": 634, "label": "unlabeled"}, {"type": "text", "expr": "3  Novel Object Captioning", "word_idx": 40938, "sentence_idx": 635, "label": "unlabeled"}, {"type": "text", "expr": "Since our model directly fills the \u201cslotted\u201d caption template with the concept, it can seamlessly generate descriptions for out-of-domain images", "word_idx": 40964, "sentence_idx": 636, "label": "unlabeled"}, {"type": "text", "expr": "\nWe replicated an existing experimental design   on COCO which excludes all the image-sentence pairs that contain at least one of eight objects in COCO", "word_idx": 41108, "sentence_idx": 637, "label": "unlabeled"}, {"type": "text", "expr": " The excluded objects are \u2018bottle\u2019, \u201cbus\u201d, \u201ccouch\u201d, \u201cmicrowave\u201d, \u201cpizza\u201d, \u201cracket\u201d, \u201csuitcase\u201d and \u201czebra\u201d", "word_idx": 41259, "sentence_idx": 638, "label": "unlabeled"}, {"type": "text", "expr": " We follow the same splits for training, validation, and testing as in prior work  ", "word_idx": 41365, "sentence_idx": 639, "label": "unlabeled"}, {"type": "text", "expr": " We use Faster R-CNN in conjunction with ResNet-101 which is pre-trained on COCO train split as the detection model", "word_idx": 41448, "sentence_idx": 640, "label": "unlabeled"}, {"type": "text", "expr": " Note that we do not pre-train the language model using COCO captions as in  , and simply replace the novel object\u2019s word embedding with an existing one which belongs to the same super-category in COCO (e", "word_idx": 41563, "sentence_idx": 641, "label": "unlabeled"}, {"type": "text", "expr": ", bus  $\\leftarrow$  car)", "word_idx": 41767, "sentence_idx": 642, "label": "unlabeled"}, {"type": "math", "expr": "$$\\leftarrow$$", "word_idx": 41792, "sentence_idx": 643, "label": "unlabeled"}, {"type": "text", "expr": "Following  , the test set is split into in-domain and out-of-domain subsets", "word_idx": 41802, "sentence_idx": 644, "label": "unlabeled"}, {"type": "text", "expr": " We report F1 as in  , which checks if the specific excluded object is mentioned in the generated caption", "word_idx": 41877, "sentence_idx": 645, "label": "unlabeled"}, {"type": "text", "expr": " To evaluate the quality of the generated caption, we use SPICE, METEOR and CIDEr metrics and the scores on out-of-domain test data are macro-averaged across eight excluded categories", "word_idx": 41982, "sentence_idx": 646, "label": "unlabeled"}, {"type": "text", "expr": " For consistency with previous work  , the inverse document frequency statistics used by CIDEr are determined across the entire test set", "word_idx": 42165, "sentence_idx": 647, "label": "unlabeled"}, {"type": "text", "expr": "As illustrated in Table\u00a0 4 , simply using greedy decoding, our model (NBT ${}^{*}$ +G) can successfully caption novel concepts with minimum changes to the model", "word_idx": 42301, "sentence_idx": 648, "label": "unlabeled"}, {"type": "text", "expr": " When using ResNet-101 and constrained beam search  , our model significantly outperforms prior works under F1 scores, SPICE, METEOR, and CIDEr, across both out-of-domain and in-domain test data", "word_idx": 42461, "sentence_idx": 649, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, NBT ${}^{\\dagger}$ +T2 outperforms the previous state-of-art model C-LSTM by 14", "word_idx": 42655, "sentence_idx": 650, "label": "unlabeled"}, {"type": "text", "expr": "6% on average F1 scores", "word_idx": 42749, "sentence_idx": 651, "label": "unlabeled"}, {"type": "text", "expr": " From the category F1 scores, we can see that our model is less likely to select small objects, e", "word_idx": 42772, "sentence_idx": 652, "label": "unlabeled"}, {"type": "text", "expr": " \u201cbottle\u201d, \u201cracket\u201d when only using the greedy decoding", "word_idx": 42869, "sentence_idx": 653, "label": "unlabeled"}, {"type": "text", "expr": " Since the visual words are grounded at the object-level, by using  , our model was able to significantly boost the captioning performance on out-of-domain images", "word_idx": 42924, "sentence_idx": 654, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 7  shows qualitative novel object captioning results", "word_idx": 43086, "sentence_idx": 655, "label": "unlabeled"}, {"type": "text", "expr": " Also see rightmost example in Fig", "word_idx": 43140, "sentence_idx": 656, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{*}$$", "word_idx": 43174, "sentence_idx": 657, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\dagger}$$", "word_idx": 43180, "sentence_idx": 658, "label": "unlabeled"}, {"type": "text", "expr": "5  Conclusion", "word_idx": 43192, "sentence_idx": 659, "label": "unlabeled"}, {"type": "text", "expr": "In this paper, we introduce Neural Baby Talk, a novel image captioning framework that produces natural language explicitly grounded in entities object detectors find in images", "word_idx": 43205, "sentence_idx": 660, "label": "unlabeled"}, {"type": "text", "expr": " Our approach is a two-stage approach that first generates a hybrid template that contains a mix of words from a text vocabulary as well as slots corresponding to image regions", "word_idx": 43380, "sentence_idx": 661, "label": "unlabeled"}, {"type": "text", "expr": " It then fills the slots based on categories recognized by object detectors in the image regions", "word_idx": 43556, "sentence_idx": 662, "label": "unlabeled"}, {"type": "text", "expr": " We also introduce a robust image captioning split by re-organizing the train and val splits of the COCO dataset", "word_idx": 43652, "sentence_idx": 663, "label": "unlabeled"}, {"type": "text", "expr": " Experimental results on standard, robust, and novel object image captioning tasks validate the effectiveness of our proposed approach", "word_idx": 43764, "sentence_idx": 664, "label": "unlabeled"}, {"type": "text", "expr": "In this paper, we introduce Neural Baby Talk, a novel image captioning framework that produces natural language explicitly grounded in entities object detectors find in images", "word_idx": 43898, "sentence_idx": 665, "label": "unlabeled"}, {"type": "text", "expr": " Our approach is a two-stage approach that first generates a hybrid template that contains a mix of words from a text vocabulary as well as slots corresponding to image regions", "word_idx": 44073, "sentence_idx": 666, "label": "unlabeled"}, {"type": "text", "expr": " It then fills the slots based on categories recognized by object detectors in the image regions", "word_idx": 44249, "sentence_idx": 667, "label": "unlabeled"}, {"type": "text", "expr": " We also introduce a robust image captioning split by re-organizing the train and val splits of the COCO dataset", "word_idx": 44345, "sentence_idx": 668, "label": "unlabeled"}, {"type": "text", "expr": " Experimental results on standard, robust, and novel object image captioning tasks validate the effectiveness of our proposed approach", "word_idx": 44457, "sentence_idx": 669, "label": "unlabeled"}, {"type": "text", "expr": "Acknowledgements  \nThis work was funded in part by: NSF CAREER awards to DB, DP; ONR YIP awards to DP, DB; ONR Grants N00014-14-1-{0679,2713}; PGA Family Foundation award to DP; Google FRAs to DP, DB; and Amazon ARAs to DP, DB; DARPA XAI grant to DB, DP", "word_idx": 44591, "sentence_idx": 670, "label": "unlabeled"}, {"type": "text", "expr": "This work was funded in part by: NSF CAREER awards to DB, DP; ONR YIP awards to DP, DB; ONR Grants N00014-14-1-{0679,2713}; PGA Family Foundation award to DP; Google FRAs to DP, DB; and Amazon ARAs to DP, DB; DARPA XAI grant to DB, DP", "word_idx": 44844, "sentence_idx": 671, "label": "unlabeled"}, {"type": "text", "expr": "6  Appendix: COCO Fine-grained Categories", "word_idx": 45078, "sentence_idx": 672, "label": "unlabeled"}, {"type": "text", "expr": "The COCO   dataset does not have bounding box annotations associated with specific phrases or entities in the caption", "word_idx": 45119, "sentence_idx": 673, "label": "unlabeled"}, {"type": "text", "expr": " We use category level detection annotations and create a category mapping list that maps the object categories like  $<$ Person $>$  to a list of potential fine-grained labels like [\u201cchild\u201d, \u201cman\u201d, \u201cbaker\u201d,\u2026]", "word_idx": 45236, "sentence_idx": 674, "label": "unlabeled"}, {"type": "text", "expr": " We first use the Stanford lemmatization toolbox   to get the base form of the entity words in the caption", "word_idx": 45445, "sentence_idx": 675, "label": "unlabeled"}, {"type": "text", "expr": " For each category class, we retrieve the top 200 similar words in the WordVec   space", "word_idx": 45551, "sentence_idx": 676, "label": "unlabeled"}, {"type": "text", "expr": " We then manually verify each word in the list, resulting in 413 fine-grained classes", "word_idx": 45637, "sentence_idx": 677, "label": "unlabeled"}, {"type": "text", "expr": " A complete list of the fine-grained class for each object category can be found in Table\u00a0 5  and Table\u00a0 6 ", "word_idx": 45722, "sentence_idx": 678, "label": "unlabeled"}, {"type": "text", "expr": "The COCO", "word_idx": 45829, "sentence_idx": 679, "label": "unlabeled"}, {"type": "text", "expr": "dataset does not have bounding box annotations associated with specific phrases or entities in the caption", "word_idx": 45837, "sentence_idx": 680, "label": "unlabeled"}, {"type": "text", "expr": " We use category level detection annotations and create a category mapping list that maps the object categories like", "word_idx": 45943, "sentence_idx": 681, "label": "unlabeled"}, {"type": "text", "expr": "Person", "word_idx": 46059, "sentence_idx": 682, "label": "unlabeled"}, {"type": "text", "expr": "to a list of potential fine-grained labels like [\u201cchild\u201d, \u201cman\u201d, \u201cbaker\u201d,\u2026]", "word_idx": 46065, "sentence_idx": 683, "label": "unlabeled"}, {"type": "text", "expr": " We first use the Stanford lemmatization toolbox", "word_idx": 46140, "sentence_idx": 684, "label": "unlabeled"}, {"type": "text", "expr": "to get the base form of the entity words in the caption", "word_idx": 46188, "sentence_idx": 685, "label": "unlabeled"}, {"type": "text", "expr": " For each category class, we retrieve the top 200 similar words in the WordVec", "word_idx": 46243, "sentence_idx": 686, "label": "unlabeled"}, {"type": "text", "expr": "space", "word_idx": 46321, "sentence_idx": 687, "label": "unlabeled"}, {"type": "text", "expr": " We then manually verify each word in the list, resulting in 413 fine-grained classes", "word_idx": 46326, "sentence_idx": 688, "label": "unlabeled"}, {"type": "text", "expr": " A complete list of the fine-grained class for each object category can be found in Table", "word_idx": 46411, "sentence_idx": 689, "label": "unlabeled"}, {"type": "text", "expr": "and Table", "word_idx": 46500, "sentence_idx": 690, "label": "unlabeled"}, {"type": "text", "expr": "Object category Fine-grained class", "word_idx": 46509, "sentence_idx": 691, "label": "unlabeled"}, {"type": "text", "expr": "Object category", "word_idx": 46543, "sentence_idx": 692, "label": "unlabeled"}, {"type": "text", "expr": "Fine-grained class", "word_idx": 46558, "sentence_idx": 693, "label": "unlabeled"}, {"type": "text", "expr": "person, girl, boy, man, woman, kid, child, chef, baker, people, adult, rider, children, baby, worker, passenger, sister, biker, policeman,", "word_idx": 46576, "sentence_idx": 694, "label": "unlabeled"}, {"type": "text", "expr": "person", "word_idx": 46714, "sentence_idx": 695, "label": "unlabeled"}, {"type": "text", "expr": "person, girl, boy, man, woman, kid, child, chef, baker, people, adult, rider, children, baby, worker, passenger, sister, biker, policeman,", "word_idx": 46720, "sentence_idx": 696, "label": "unlabeled"}, {"type": "text", "expr": "person, girl, boy, man, woman, kid, child, chef, baker, people, adult, rider, children, baby, worker, passenger, sister, biker, policeman,", "word_idx": 46858, "sentence_idx": 697, "label": "unlabeled"}, {"type": "text", "expr": "officer, lady, cowboy, bride, groom, male, female, guy, traveler, mother, father, gentleman, pitcher, player, skier, snowboarder,", "word_idx": 46996, "sentence_idx": 698, "label": "unlabeled"}, {"type": "text", "expr": "officer, lady, cowboy, bride, groom, male, female, guy, traveler, mother, father, gentleman, pitcher, player, skier, snowboarder,", "word_idx": 47125, "sentence_idx": 699, "label": "unlabeled"}, {"type": "text", "expr": "officer, lady, cowboy, bride, groom, male, female, guy, traveler, mother, father, gentleman, pitcher, player, skier, snowboarder,", "word_idx": 47254, "sentence_idx": 700, "label": "unlabeled"}, {"type": "text", "expr": "skater, skateboarder, foreigner, caller, offender, coworker, trespasser, patient, politician, soldier, grandchild, serviceman, walker,", "word_idx": 47383, "sentence_idx": 701, "label": "unlabeled"}, {"type": "text", "expr": "skater, skateboarder, foreigner, caller, offender, coworker, trespasser, patient, politician, soldier, grandchild, serviceman, walker,", "word_idx": 47517, "sentence_idx": 702, "label": "unlabeled"}, {"type": "text", "expr": "skater, skateboarder, foreigner, caller, offender, coworker, trespasser, patient, politician, soldier, grandchild, serviceman, walker,", "word_idx": 47651, "sentence_idx": 703, "label": "unlabeled"}, {"type": "text", "expr": "drinker, doctor, bicyclist, thief, buyer, teenager, student, camper, driver, solider, hunter, shopper, villager, cop", "word_idx": 47785, "sentence_idx": 704, "label": "unlabeled"}, {"type": "text", "expr": "drinker, doctor, bicyclist, thief, buyer, teenager, student, camper, driver, solider, hunter, shopper, villager, cop", "word_idx": 47901, "sentence_idx": 705, "label": "unlabeled"}, {"type": "text", "expr": "drinker, doctor, bicyclist, thief, buyer, teenager, student, camper, driver, solider, hunter, shopper, villager, cop", "word_idx": 48017, "sentence_idx": 706, "label": "unlabeled"}, {"type": "text", "expr": "bicycle, bike, unicycle, minibike, trike", "word_idx": 48133, "sentence_idx": 707, "label": "unlabeled"}, {"type": "text", "expr": "bicycle", "word_idx": 48173, "sentence_idx": 708, "label": "unlabeled"}, {"type": "text", "expr": "bicycle, bike, unicycle, minibike, trike", "word_idx": 48180, "sentence_idx": 709, "label": "unlabeled"}, {"type": "text", "expr": "bicycle, bike, unicycle, minibike, trike", "word_idx": 48220, "sentence_idx": 710, "label": "unlabeled"}, {"type": "text", "expr": "car, automobile, van, minivan, sedan, suv, hatchback, cab, jeep, coupe, taxicab, limo, taxi", "word_idx": 48260, "sentence_idx": 711, "label": "unlabeled"}, {"type": "text", "expr": "car, automobile, van, minivan, sedan, suv, hatchback, cab, jeep, coupe, taxicab, limo, taxi", "word_idx": 48351, "sentence_idx": 712, "label": "unlabeled"}, {"type": "text", "expr": "car, automobile, van, minivan, sedan, suv, hatchback, cab, jeep, coupe, taxicab, limo, taxi", "word_idx": 48442, "sentence_idx": 713, "label": "unlabeled"}, {"type": "text", "expr": "motorcycle, scooter, motor bike, motor cycle, motorbike, moped", "word_idx": 48533, "sentence_idx": 714, "label": "unlabeled"}, {"type": "text", "expr": "motorcycle", "word_idx": 48595, "sentence_idx": 715, "label": "unlabeled"}, {"type": "text", "expr": "motorcycle, scooter, motor bike, motor cycle, motorbike, moped", "word_idx": 48605, "sentence_idx": 716, "label": "unlabeled"}, {"type": "text", "expr": "motorcycle, scooter, motor bike, motor cycle, motorbike, moped", "word_idx": 48667, "sentence_idx": 717, "label": "unlabeled"}, {"type": "text", "expr": "airplane, jetliner, plane, air plane, monoplane, aircraft, jet, airbus, biplane, seaplane\nbus, minibus, trolley", "word_idx": 48729, "sentence_idx": 718, "label": "unlabeled"}, {"type": "text", "expr": "airplane", "word_idx": 48840, "sentence_idx": 719, "label": "unlabeled"}, {"type": "text", "expr": "airplane, jetliner, plane, air plane, monoplane, aircraft, jet, airbus, biplane, seaplane\nbus, minibus, trolley", "word_idx": 48848, "sentence_idx": 720, "label": "unlabeled"}, {"type": "text", "expr": "airplane, jetliner, plane, air plane, monoplane, aircraft, jet, airbus, biplane, seaplane\nbus, minibus, trolley", "word_idx": 48959, "sentence_idx": 721, "label": "unlabeled"}, {"type": "text", "expr": "bus, minibus, schoolbus, trolley", "word_idx": 49070, "sentence_idx": 722, "label": "unlabeled"}, {"type": "text", "expr": "bus, minibus, schoolbus, trolley", "word_idx": 49102, "sentence_idx": 723, "label": "unlabeled"}, {"type": "text", "expr": "bus, minibus, schoolbus, trolley", "word_idx": 49134, "sentence_idx": 724, "label": "unlabeled"}, {"type": "text", "expr": "train, locomotive, tramway, caboose", "word_idx": 49166, "sentence_idx": 725, "label": "unlabeled"}, {"type": "text", "expr": "train", "word_idx": 49201, "sentence_idx": 726, "label": "unlabeled"}, {"type": "text", "expr": "train, locomotive, tramway, caboose", "word_idx": 49206, "sentence_idx": 727, "label": "unlabeled"}, {"type": "text", "expr": "train, locomotive, tramway, caboose", "word_idx": 49241, "sentence_idx": 728, "label": "unlabeled"}, {"type": "text", "expr": "truck, pickup, lorry, hauler, firetruck", "word_idx": 49276, "sentence_idx": 729, "label": "unlabeled"}, {"type": "text", "expr": "truck", "word_idx": 49315, "sentence_idx": 730, "label": "unlabeled"}, {"type": "text", "expr": "truck, pickup, lorry, hauler, firetruck", "word_idx": 49320, "sentence_idx": 731, "label": "unlabeled"}, {"type": "text", "expr": "truck, pickup, lorry, hauler, firetruck", "word_idx": 49359, "sentence_idx": 732, "label": "unlabeled"}, {"type": "text", "expr": "boat, ship, liner, sailboat, motorboat, dinghy, powerboat, speedboat, canoe, skiff, yacht, kayak, catamaran, pontoon, houseboat, vessel,", "word_idx": 49398, "sentence_idx": 733, "label": "unlabeled"}, {"type": "text", "expr": "boat, ship, liner, sailboat, motorboat, dinghy, powerboat, speedboat, canoe, skiff, yacht, kayak, catamaran, pontoon, houseboat, vessel,", "word_idx": 49534, "sentence_idx": 734, "label": "unlabeled"}, {"type": "text", "expr": "boat, ship, liner, sailboat, motorboat, dinghy, powerboat, speedboat, canoe, skiff, yacht, kayak, catamaran, pontoon, houseboat, vessel,", "word_idx": 49670, "sentence_idx": 735, "label": "unlabeled"}, {"type": "text", "expr": "rowboat, trawler, ferryboat, watercraft, tugboat, schooner, barge, ferry, sailboard, paddleboat, lifeboat, freighter, steamboat, riverboat,", "word_idx": 49806, "sentence_idx": 736, "label": "unlabeled"}, {"type": "text", "expr": "rowboat, trawler, ferryboat, watercraft, tugboat, schooner, barge, ferry, sailboard, paddleboat, lifeboat, freighter, steamboat, riverboat,", "word_idx": 49945, "sentence_idx": 737, "label": "unlabeled"}, {"type": "text", "expr": "rowboat, trawler, ferryboat, watercraft, tugboat, schooner, barge, ferry, sailboard, paddleboat, lifeboat, freighter, steamboat, riverboat,", "word_idx": 50084, "sentence_idx": 738, "label": "unlabeled"}, {"type": "text", "expr": "surfboard, battleship, steamship", "word_idx": 50223, "sentence_idx": 739, "label": "unlabeled"}, {"type": "text", "expr": "surfboard, battleship, steamship", "word_idx": 50255, "sentence_idx": 740, "label": "unlabeled"}, {"type": "text", "expr": "surfboard, battleship, steamship", "word_idx": 50287, "sentence_idx": 741, "label": "unlabeled"}, {"type": "text", "expr": "traffic light, street light, traffic signal, stop light, streetlight, stoplight", "word_idx": 50319, "sentence_idx": 742, "label": "unlabeled"}, {"type": "text", "expr": "traffic light", "word_idx": 50398, "sentence_idx": 743, "label": "unlabeled"}, {"type": "text", "expr": "traffic light, street light, traffic signal, stop light, streetlight, stoplight", "word_idx": 50411, "sentence_idx": 744, "label": "unlabeled"}, {"type": "text", "expr": "traffic light, street light, traffic signal, stop light, streetlight, stoplight", "word_idx": 50490, "sentence_idx": 745, "label": "unlabeled"}, {"type": "text", "expr": "fire hydrant, hydrant", "word_idx": 50569, "sentence_idx": 746, "label": "unlabeled"}, {"type": "text", "expr": "fire hydrant", "word_idx": 50590, "sentence_idx": 747, "label": "unlabeled"}, {"type": "text", "expr": "fire hydrant, hydrant", "word_idx": 50602, "sentence_idx": 748, "label": "unlabeled"}, {"type": "text", "expr": "fire hydrant, hydrant", "word_idx": 50623, "sentence_idx": 749, "label": "unlabeled"}, {"type": "text", "expr": "stop sign, street sign", "word_idx": 50644, "sentence_idx": 750, "label": "unlabeled"}, {"type": "text", "expr": "stop sign", "word_idx": 50666, "sentence_idx": 751, "label": "unlabeled"}, {"type": "text", "expr": "stop sign, street sign", "word_idx": 50675, "sentence_idx": 752, "label": "unlabeled"}, {"type": "text", "expr": "stop sign, street sign", "word_idx": 50697, "sentence_idx": 753, "label": "unlabeled"}, {"type": "text", "expr": "parking meter", "word_idx": 50719, "sentence_idx": 754, "label": "unlabeled"}, {"type": "text", "expr": "parking meter", "word_idx": 50732, "sentence_idx": 755, "label": "unlabeled"}, {"type": "text", "expr": "parking meter", "word_idx": 50745, "sentence_idx": 756, "label": "unlabeled"}, {"type": "text", "expr": "parking meter", "word_idx": 50758, "sentence_idx": 757, "label": "unlabeled"}, {"type": "text", "expr": "bench, pew", "word_idx": 50771, "sentence_idx": 758, "label": "unlabeled"}, {"type": "text", "expr": "bench", "word_idx": 50781, "sentence_idx": 759, "label": "unlabeled"}, {"type": "text", "expr": "bench, pew", "word_idx": 50786, "sentence_idx": 760, "label": "unlabeled"}, {"type": "text", "expr": "bench, pew", "word_idx": 50796, "sentence_idx": 761, "label": "unlabeled"}, {"type": "text", "expr": "cat, kitten, feline, tabby", "word_idx": 50806, "sentence_idx": 762, "label": "unlabeled"}, {"type": "text", "expr": "cat, kitten, feline, tabby", "word_idx": 50832, "sentence_idx": 763, "label": "unlabeled"}, {"type": "text", "expr": "cat, kitten, feline, tabby", "word_idx": 50858, "sentence_idx": 764, "label": "unlabeled"}, {"type": "text", "expr": "dog, puppy, beagle, pup, chihuahua, schnauzer, dachshund, rottweiler, canine, pitbull, collie, pug, terrier, poodle, labrador, doggie,", "word_idx": 50884, "sentence_idx": 765, "label": "unlabeled"}, {"type": "text", "expr": "dog, puppy, beagle, pup, chihuahua, schnauzer, dachshund, rottweiler, canine, pitbull, collie, pug, terrier, poodle, labrador, doggie,", "word_idx": 51018, "sentence_idx": 766, "label": "unlabeled"}, {"type": "text", "expr": "dog, puppy, beagle, pup, chihuahua, schnauzer, dachshund, rottweiler, canine, pitbull, collie, pug, terrier, poodle, labrador, doggie,", "word_idx": 51152, "sentence_idx": 767, "label": "unlabeled"}, {"type": "text", "expr": "doberman, mutt, doggy, spaniel, bulldog, sheepdog, weimaraner, corgi, cocker, greyhound, retriever, brindle, hound, whippet, husky", "word_idx": 51286, "sentence_idx": 768, "label": "unlabeled"}, {"type": "text", "expr": "doberman, mutt, doggy, spaniel, bulldog, sheepdog, weimaraner, corgi, cocker, greyhound, retriever, brindle, hound, whippet, husky", "word_idx": 51416, "sentence_idx": 769, "label": "unlabeled"}, {"type": "text", "expr": "doberman, mutt, doggy, spaniel, bulldog, sheepdog, weimaraner, corgi, cocker, greyhound, retriever, brindle, hound, whippet, husky", "word_idx": 51546, "sentence_idx": 770, "label": "unlabeled"}, {"type": "text", "expr": "horse, colt, pony, racehorse, stallion, equine, mare, foal, palomino, mustang, clydesdale, bronc, bronco", "word_idx": 51676, "sentence_idx": 771, "label": "unlabeled"}, {"type": "text", "expr": "horse", "word_idx": 51780, "sentence_idx": 772, "label": "unlabeled"}, {"type": "text", "expr": "horse, colt, pony, racehorse, stallion, equine, mare, foal, palomino, mustang, clydesdale, bronc, bronco", "word_idx": 51785, "sentence_idx": 773, "label": "unlabeled"}, {"type": "text", "expr": "horse, colt, pony, racehorse, stallion, equine, mare, foal, palomino, mustang, clydesdale, bronc, bronco", "word_idx": 51889, "sentence_idx": 774, "label": "unlabeled"}, {"type": "text", "expr": "sheep, lamb, goat, ram, cattle, ewe", "word_idx": 51993, "sentence_idx": 775, "label": "unlabeled"}, {"type": "text", "expr": "sheep", "word_idx": 52028, "sentence_idx": 776, "label": "unlabeled"}, {"type": "text", "expr": "sheep, lamb, goat, ram, cattle, ewe", "word_idx": 52033, "sentence_idx": 777, "label": "unlabeled"}, {"type": "text", "expr": "sheep, lamb, goat, ram, cattle, ewe", "word_idx": 52068, "sentence_idx": 778, "label": "unlabeled"}, {"type": "text", "expr": "cow, cattle, oxen, ox, calf, ewe, holstein, heifer, buffalo, bull, zebu, bison", "word_idx": 52103, "sentence_idx": 779, "label": "unlabeled"}, {"type": "text", "expr": "cow, cattle, oxen, ox, calf, ewe, holstein, heifer, buffalo, bull, zebu, bison", "word_idx": 52181, "sentence_idx": 780, "label": "unlabeled"}, {"type": "text", "expr": "cow, cattle, oxen, ox, calf, ewe, holstein, heifer, buffalo, bull, zebu, bison", "word_idx": 52259, "sentence_idx": 781, "label": "unlabeled"}, {"type": "text", "expr": "elephant", "word_idx": 52337, "sentence_idx": 782, "label": "unlabeled"}, {"type": "text", "expr": "elephant", "word_idx": 52345, "sentence_idx": 783, "label": "unlabeled"}, {"type": "text", "expr": "elephant", "word_idx": 52353, "sentence_idx": 784, "label": "unlabeled"}, {"type": "text", "expr": "elephant", "word_idx": 52361, "sentence_idx": 785, "label": "unlabeled"}, {"type": "text", "expr": "bear, panda", "word_idx": 52369, "sentence_idx": 786, "label": "unlabeled"}, {"type": "text", "expr": "bear, panda", "word_idx": 52380, "sentence_idx": 787, "label": "unlabeled"}, {"type": "text", "expr": "bear, panda", "word_idx": 52391, "sentence_idx": 788, "label": "unlabeled"}, {"type": "text", "expr": "zebra", "word_idx": 52402, "sentence_idx": 789, "label": "unlabeled"}, {"type": "text", "expr": "zebra", "word_idx": 52407, "sentence_idx": 790, "label": "unlabeled"}, {"type": "text", "expr": "zebra", "word_idx": 52412, "sentence_idx": 791, "label": "unlabeled"}, {"type": "text", "expr": "zebra", "word_idx": 52417, "sentence_idx": 792, "label": "unlabeled"}, {"type": "text", "expr": "giraffe", "word_idx": 52422, "sentence_idx": 793, "label": "unlabeled"}, {"type": "text", "expr": "giraffe", "word_idx": 52429, "sentence_idx": 794, "label": "unlabeled"}, {"type": "text", "expr": "giraffe", "word_idx": 52436, "sentence_idx": 795, "label": "unlabeled"}, {"type": "text", "expr": "giraffe", "word_idx": 52443, "sentence_idx": 796, "label": "unlabeled"}, {"type": "text", "expr": "backpack, knapsack", "word_idx": 52450, "sentence_idx": 797, "label": "unlabeled"}, {"type": "text", "expr": "backpack", "word_idx": 52468, "sentence_idx": 798, "label": "unlabeled"}, {"type": "text", "expr": "backpack, knapsack", "word_idx": 52476, "sentence_idx": 799, "label": "unlabeled"}, {"type": "text", "expr": "backpack, knapsack", "word_idx": 52494, "sentence_idx": 800, "label": "unlabeled"}, {"type": "text", "expr": "umbrella", "word_idx": 52512, "sentence_idx": 801, "label": "unlabeled"}, {"type": "text", "expr": "umbrella", "word_idx": 52520, "sentence_idx": 802, "label": "unlabeled"}, {"type": "text", "expr": "umbrella", "word_idx": 52528, "sentence_idx": 803, "label": "unlabeled"}, {"type": "text", "expr": "umbrella", "word_idx": 52536, "sentence_idx": 804, "label": "unlabeled"}, {"type": "text", "expr": "handbag, handbag, wallet, purse, briefcase", "word_idx": 52544, "sentence_idx": 805, "label": "unlabeled"}, {"type": "text", "expr": "handbag", "word_idx": 52586, "sentence_idx": 806, "label": "unlabeled"}, {"type": "text", "expr": "handbag, handbag, wallet, purse, briefcase", "word_idx": 52593, "sentence_idx": 807, "label": "unlabeled"}, {"type": "text", "expr": "handbag, handbag, wallet, purse, briefcase", "word_idx": 52635, "sentence_idx": 808, "label": "unlabeled"}, {"type": "text", "expr": "suitcase, suit case, luggage", "word_idx": 52677, "sentence_idx": 809, "label": "unlabeled"}, {"type": "text", "expr": "suitcase", "word_idx": 52705, "sentence_idx": 810, "label": "unlabeled"}, {"type": "text", "expr": "suitcase, suit case, luggage", "word_idx": 52713, "sentence_idx": 811, "label": "unlabeled"}, {"type": "text", "expr": "suitcase, suit case, luggage", "word_idx": 52741, "sentence_idx": 812, "label": "unlabeled"}, {"type": "text", "expr": "frisbee", "word_idx": 52769, "sentence_idx": 813, "label": "unlabeled"}, {"type": "text", "expr": "frisbee", "word_idx": 52776, "sentence_idx": 814, "label": "unlabeled"}, {"type": "text", "expr": "frisbee", "word_idx": 52783, "sentence_idx": 815, "label": "unlabeled"}, {"type": "text", "expr": "frisbee", "word_idx": 52790, "sentence_idx": 816, "label": "unlabeled"}, {"type": "text", "expr": "skis, ski", "word_idx": 52797, "sentence_idx": 817, "label": "unlabeled"}, {"type": "text", "expr": "skis, ski", "word_idx": 52806, "sentence_idx": 818, "label": "unlabeled"}, {"type": "text", "expr": "skis, ski", "word_idx": 52815, "sentence_idx": 819, "label": "unlabeled"}, {"type": "text", "expr": "snowboard", "word_idx": 52824, "sentence_idx": 820, "label": "unlabeled"}, {"type": "text", "expr": "snowboard", "word_idx": 52833, "sentence_idx": 821, "label": "unlabeled"}, {"type": "text", "expr": "snowboard", "word_idx": 52842, "sentence_idx": 822, "label": "unlabeled"}, {"type": "text", "expr": "snowboard", "word_idx": 52851, "sentence_idx": 823, "label": "unlabeled"}, {"type": "text", "expr": "sports ball, baseball, ball, football, soccer, basketball, softball, volleyball, pinball, fastball, racquetball", "word_idx": 52860, "sentence_idx": 824, "label": "unlabeled"}, {"type": "text", "expr": "sports ball", "word_idx": 52971, "sentence_idx": 825, "label": "unlabeled"}, {"type": "text", "expr": "sports ball, baseball, ball, football, soccer, basketball, softball, volleyball, pinball, fastball, racquetball", "word_idx": 52982, "sentence_idx": 826, "label": "unlabeled"}, {"type": "text", "expr": "sports ball, baseball, ball, football, soccer, basketball, softball, volleyball, pinball, fastball, racquetball", "word_idx": 53093, "sentence_idx": 827, "label": "unlabeled"}, {"type": "text", "expr": "baseball bat", "word_idx": 53204, "sentence_idx": 828, "label": "unlabeled"}, {"type": "text", "expr": "baseball bat", "word_idx": 53216, "sentence_idx": 829, "label": "unlabeled"}, {"type": "text", "expr": "baseball bat", "word_idx": 53228, "sentence_idx": 830, "label": "unlabeled"}, {"type": "text", "expr": "baseball bat", "word_idx": 53240, "sentence_idx": 831, "label": "unlabeled"}, {"type": "text", "expr": "baseball glove", "word_idx": 53252, "sentence_idx": 832, "label": "unlabeled"}, {"type": "text", "expr": "baseball glove", "word_idx": 53266, "sentence_idx": 833, "label": "unlabeled"}, {"type": "text", "expr": "baseball glove", "word_idx": 53280, "sentence_idx": 834, "label": "unlabeled"}, {"type": "text", "expr": "baseball glove", "word_idx": 53294, "sentence_idx": 835, "label": "unlabeled"}, {"type": "text", "expr": "skateboard", "word_idx": 53308, "sentence_idx": 836, "label": "unlabeled"}, {"type": "text", "expr": "skateboard", "word_idx": 53318, "sentence_idx": 837, "label": "unlabeled"}, {"type": "text", "expr": "skateboard", "word_idx": 53328, "sentence_idx": 838, "label": "unlabeled"}, {"type": "text", "expr": "skateboard", "word_idx": 53338, "sentence_idx": 839, "label": "unlabeled"}, {"type": "text", "expr": "surfboard, longboard, skimboard, shortboard, wakeboard", "word_idx": 53348, "sentence_idx": 840, "label": "unlabeled"}, {"type": "text", "expr": "surfboard", "word_idx": 53402, "sentence_idx": 841, "label": "unlabeled"}, {"type": "text", "expr": "surfboard, longboard, skimboard, shortboard, wakeboard", "word_idx": 53411, "sentence_idx": 842, "label": "unlabeled"}, {"type": "text", "expr": "surfboard, longboard, skimboard, shortboard, wakeboard", "word_idx": 53465, "sentence_idx": 843, "label": "unlabeled"}, {"type": "text", "expr": "tennis racket", "word_idx": 53519, "sentence_idx": 844, "label": "unlabeled"}, {"type": "text", "expr": "tennis racket", "word_idx": 53532, "sentence_idx": 845, "label": "unlabeled"}, {"type": "text", "expr": "tennis racket", "word_idx": 53545, "sentence_idx": 846, "label": "unlabeled"}, {"type": "text", "expr": "tennis racket", "word_idx": 53558, "sentence_idx": 847, "label": "unlabeled"}, {"type": "text", "expr": "bottle", "word_idx": 53571, "sentence_idx": 848, "label": "unlabeled"}, {"type": "text", "expr": "bottle", "word_idx": 53577, "sentence_idx": 849, "label": "unlabeled"}, {"type": "text", "expr": "bottle", "word_idx": 53583, "sentence_idx": 850, "label": "unlabeled"}, {"type": "text", "expr": "bottle", "word_idx": 53589, "sentence_idx": 851, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a05:  COCO category mapping list for visual words", "word_idx": 53595, "sentence_idx": 852, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a05:", "word_idx": 53648, "sentence_idx": 853, "label": "unlabeled"}, {"type": "text", "expr": "Object category Fine-grained class", "word_idx": 53656, "sentence_idx": 854, "label": "unlabeled"}, {"type": "text", "expr": "Object category", "word_idx": 53690, "sentence_idx": 855, "label": "unlabeled"}, {"type": "text", "expr": "Fine-grained class", "word_idx": 53705, "sentence_idx": 856, "label": "unlabeled"}, {"type": "text", "expr": "wine glass", "word_idx": 53723, "sentence_idx": 857, "label": "unlabeled"}, {"type": "text", "expr": "wine glass", "word_idx": 53733, "sentence_idx": 858, "label": "unlabeled"}, {"type": "text", "expr": "wine glass", "word_idx": 53743, "sentence_idx": 859, "label": "unlabeled"}, {"type": "text", "expr": "wine glass", "word_idx": 53753, "sentence_idx": 860, "label": "unlabeled"}, {"type": "text", "expr": "knife, pocketknife, knive", "word_idx": 53763, "sentence_idx": 861, "label": "unlabeled"}, {"type": "text", "expr": "knife", "word_idx": 53788, "sentence_idx": 862, "label": "unlabeled"}, {"type": "text", "expr": "knife, pocketknife, knive", "word_idx": 53793, "sentence_idx": 863, "label": "unlabeled"}, {"type": "text", "expr": "knife, pocketknife, knive", "word_idx": 53818, "sentence_idx": 864, "label": "unlabeled"}, {"type": "text", "expr": "spoon", "word_idx": 53843, "sentence_idx": 865, "label": "unlabeled"}, {"type": "text", "expr": "spoon", "word_idx": 53848, "sentence_idx": 866, "label": "unlabeled"}, {"type": "text", "expr": "spoon", "word_idx": 53853, "sentence_idx": 867, "label": "unlabeled"}, {"type": "text", "expr": "spoon", "word_idx": 53858, "sentence_idx": 868, "label": "unlabeled"}, {"type": "text", "expr": "bowl, container, plate", "word_idx": 53863, "sentence_idx": 869, "label": "unlabeled"}, {"type": "text", "expr": "bowl, container, plate", "word_idx": 53885, "sentence_idx": 870, "label": "unlabeled"}, {"type": "text", "expr": "bowl, container, plate", "word_idx": 53907, "sentence_idx": 871, "label": "unlabeled"}, {"type": "text", "expr": "banana", "word_idx": 53929, "sentence_idx": 872, "label": "unlabeled"}, {"type": "text", "expr": "banana", "word_idx": 53935, "sentence_idx": 873, "label": "unlabeled"}, {"type": "text", "expr": "banana", "word_idx": 53941, "sentence_idx": 874, "label": "unlabeled"}, {"type": "text", "expr": "banana", "word_idx": 53947, "sentence_idx": 875, "label": "unlabeled"}, {"type": "text", "expr": "apple", "word_idx": 53953, "sentence_idx": 876, "label": "unlabeled"}, {"type": "text", "expr": "apple", "word_idx": 53958, "sentence_idx": 877, "label": "unlabeled"}, {"type": "text", "expr": "apple", "word_idx": 53963, "sentence_idx": 878, "label": "unlabeled"}, {"type": "text", "expr": "apple", "word_idx": 53968, "sentence_idx": 879, "label": "unlabeled"}, {"type": "text", "expr": "sandwich, burger, sub, cheeseburger, hamburger", "word_idx": 53973, "sentence_idx": 880, "label": "unlabeled"}, {"type": "text", "expr": "sandwich", "word_idx": 54019, "sentence_idx": 881, "label": "unlabeled"}, {"type": "text", "expr": "sandwich, burger, sub, cheeseburger, hamburger", "word_idx": 54027, "sentence_idx": 882, "label": "unlabeled"}, {"type": "text", "expr": "sandwich, burger, sub, cheeseburger, hamburger", "word_idx": 54073, "sentence_idx": 883, "label": "unlabeled"}, {"type": "text", "expr": "orange, lemons", "word_idx": 54119, "sentence_idx": 884, "label": "unlabeled"}, {"type": "text", "expr": "orange", "word_idx": 54133, "sentence_idx": 885, "label": "unlabeled"}, {"type": "text", "expr": "orange, lemons", "word_idx": 54139, "sentence_idx": 886, "label": "unlabeled"}, {"type": "text", "expr": "orange, lemons", "word_idx": 54153, "sentence_idx": 887, "label": "unlabeled"}, {"type": "text", "expr": "broccoli", "word_idx": 54167, "sentence_idx": 888, "label": "unlabeled"}, {"type": "text", "expr": "broccoli", "word_idx": 54175, "sentence_idx": 889, "label": "unlabeled"}, {"type": "text", "expr": "broccoli", "word_idx": 54183, "sentence_idx": 890, "label": "unlabeled"}, {"type": "text", "expr": "broccoli", "word_idx": 54191, "sentence_idx": 891, "label": "unlabeled"}, {"type": "text", "expr": "carrot", "word_idx": 54199, "sentence_idx": 892, "label": "unlabeled"}, {"type": "text", "expr": "carrot", "word_idx": 54205, "sentence_idx": 893, "label": "unlabeled"}, {"type": "text", "expr": "carrot", "word_idx": 54211, "sentence_idx": 894, "label": "unlabeled"}, {"type": "text", "expr": "carrot", "word_idx": 54217, "sentence_idx": 895, "label": "unlabeled"}, {"type": "text", "expr": "hot dog", "word_idx": 54223, "sentence_idx": 896, "label": "unlabeled"}, {"type": "text", "expr": "hot dog", "word_idx": 54230, "sentence_idx": 897, "label": "unlabeled"}, {"type": "text", "expr": "hot dog", "word_idx": 54237, "sentence_idx": 898, "label": "unlabeled"}, {"type": "text", "expr": "hot dog", "word_idx": 54244, "sentence_idx": 899, "label": "unlabeled"}, {"type": "text", "expr": "pizza", "word_idx": 54251, "sentence_idx": 900, "label": "unlabeled"}, {"type": "text", "expr": "pizza", "word_idx": 54256, "sentence_idx": 901, "label": "unlabeled"}, {"type": "text", "expr": "pizza", "word_idx": 54261, "sentence_idx": 902, "label": "unlabeled"}, {"type": "text", "expr": "pizza", "word_idx": 54266, "sentence_idx": 903, "label": "unlabeled"}, {"type": "text", "expr": "donut, doughnut, bagel", "word_idx": 54271, "sentence_idx": 904, "label": "unlabeled"}, {"type": "text", "expr": "donut", "word_idx": 54293, "sentence_idx": 905, "label": "unlabeled"}, {"type": "text", "expr": "donut, doughnut, bagel", "word_idx": 54298, "sentence_idx": 906, "label": "unlabeled"}, {"type": "text", "expr": "donut, doughnut, bagel", "word_idx": 54320, "sentence_idx": 907, "label": "unlabeled"}, {"type": "text", "expr": "cake, cheesecake, cupcake, shortcake, coffeecake, pancake", "word_idx": 54342, "sentence_idx": 908, "label": "unlabeled"}, {"type": "text", "expr": "cake, cheesecake, cupcake, shortcake, coffeecake, pancake", "word_idx": 54399, "sentence_idx": 909, "label": "unlabeled"}, {"type": "text", "expr": "cake, cheesecake, cupcake, shortcake, coffeecake, pancake", "word_idx": 54456, "sentence_idx": 910, "label": "unlabeled"}, {"type": "text", "expr": "bird, ostrich, owl, seagull, goose, duck, parakeet, falcon, robin, pelican, waterfowl, heron, hummingbird, mallard, finch, pigeon, sparrow,", "word_idx": 54513, "sentence_idx": 911, "label": "unlabeled"}, {"type": "text", "expr": "bird, ostrich, owl, seagull, goose, duck, parakeet, falcon, robin, pelican, waterfowl, heron, hummingbird, mallard, finch, pigeon, sparrow,", "word_idx": 54652, "sentence_idx": 912, "label": "unlabeled"}, {"type": "text", "expr": "bird, ostrich, owl, seagull, goose, duck, parakeet, falcon, robin, pelican, waterfowl, heron, hummingbird, mallard, finch, pigeon, sparrow,", "word_idx": 54791, "sentence_idx": 913, "label": "unlabeled"}, {"type": "text", "expr": "seabird, osprey, blackbird, fowl, shorebird, woodpecker, egret, chickadee, quail, bluebird, kingfisher, buzzard, willet, gull, swan, bluejay,", "word_idx": 54930, "sentence_idx": 914, "label": "unlabeled"}, {"type": "text", "expr": "seabird, osprey, blackbird, fowl, shorebird, woodpecker, egret, chickadee, quail, bluebird, kingfisher, buzzard, willet, gull, swan, bluejay,", "word_idx": 55071, "sentence_idx": 915, "label": "unlabeled"}, {"type": "text", "expr": "seabird, osprey, blackbird, fowl, shorebird, woodpecker, egret, chickadee, quail, bluebird, kingfisher, buzzard, willet, gull, swan, bluejay,", "word_idx": 55212, "sentence_idx": 916, "label": "unlabeled"}, {"type": "text", "expr": "flamingo, cormorant, parrot, loon, gosling, waterbird, pheasant, rooster, sandpiper, crow, raven, turkey, oriole, cowbird, warbler, magpie,", "word_idx": 55353, "sentence_idx": 917, "label": "unlabeled"}, {"type": "text", "expr": "flamingo, cormorant, parrot, loon, gosling, waterbird, pheasant, rooster, sandpiper, crow, raven, turkey, oriole, cowbird, warbler, magpie,", "word_idx": 55492, "sentence_idx": 918, "label": "unlabeled"}, {"type": "text", "expr": "flamingo, cormorant, parrot, loon, gosling, waterbird, pheasant, rooster, sandpiper, crow, raven, turkey, oriole, cowbird, warbler, magpie,", "word_idx": 55631, "sentence_idx": 919, "label": "unlabeled"}, {"type": "text", "expr": "peacock, cockatiel, lorikeet, puffin, vulture, condor, macaw, peafowl, cockatoo, songbird", "word_idx": 55770, "sentence_idx": 920, "label": "unlabeled"}, {"type": "text", "expr": "peacock, cockatiel, lorikeet, puffin, vulture, condor, macaw, peafowl, cockatoo, songbird", "word_idx": 55859, "sentence_idx": 921, "label": "unlabeled"}, {"type": "text", "expr": "peacock, cockatiel, lorikeet, puffin, vulture, condor, macaw, peafowl, cockatoo, songbird", "word_idx": 55948, "sentence_idx": 922, "label": "unlabeled"}, {"type": "text", "expr": "chair, seat, recliner, stool", "word_idx": 56037, "sentence_idx": 923, "label": "unlabeled"}, {"type": "text", "expr": "chair", "word_idx": 56065, "sentence_idx": 924, "label": "unlabeled"}, {"type": "text", "expr": "chair, seat, recliner, stool", "word_idx": 56070, "sentence_idx": 925, "label": "unlabeled"}, {"type": "text", "expr": "chair, seat, recliner, stool", "word_idx": 56098, "sentence_idx": 926, "label": "unlabeled"}, {"type": "text", "expr": "couch, sofa, recliner, futon, loveseat, settee, chesterfield", "word_idx": 56126, "sentence_idx": 927, "label": "unlabeled"}, {"type": "text", "expr": "couch", "word_idx": 56186, "sentence_idx": 928, "label": "unlabeled"}, {"type": "text", "expr": "couch, sofa, recliner, futon, loveseat, settee, chesterfield", "word_idx": 56191, "sentence_idx": 929, "label": "unlabeled"}, {"type": "text", "expr": "couch, sofa, recliner, futon, loveseat, settee, chesterfield", "word_idx": 56251, "sentence_idx": 930, "label": "unlabeled"}, {"type": "text", "expr": "potted plant, houseplant", "word_idx": 56311, "sentence_idx": 931, "label": "unlabeled"}, {"type": "text", "expr": "potted plant", "word_idx": 56335, "sentence_idx": 932, "label": "unlabeled"}, {"type": "text", "expr": "potted plant, houseplant", "word_idx": 56347, "sentence_idx": 933, "label": "unlabeled"}, {"type": "text", "expr": "potted plant, houseplant", "word_idx": 56371, "sentence_idx": 934, "label": "unlabeled"}, {"type": "text", "expr": "dining table, table", "word_idx": 56395, "sentence_idx": 935, "label": "unlabeled"}, {"type": "text", "expr": "dining table", "word_idx": 56414, "sentence_idx": 936, "label": "unlabeled"}, {"type": "text", "expr": "dining table, table", "word_idx": 56426, "sentence_idx": 937, "label": "unlabeled"}, {"type": "text", "expr": "dining table, table", "word_idx": 56445, "sentence_idx": 938, "label": "unlabeled"}, {"type": "text", "expr": "toilet, urinal, commode, lavatory, potty", "word_idx": 56464, "sentence_idx": 939, "label": "unlabeled"}, {"type": "text", "expr": "toilet", "word_idx": 56504, "sentence_idx": 940, "label": "unlabeled"}, {"type": "text", "expr": "toilet, urinal, commode, lavatory, potty", "word_idx": 56510, "sentence_idx": 941, "label": "unlabeled"}, {"type": "text", "expr": "toilet, urinal, commode, lavatory, potty", "word_idx": 56550, "sentence_idx": 942, "label": "unlabeled"}, {"type": "text", "expr": "tv, monitor, televison, television", "word_idx": 56590, "sentence_idx": 943, "label": "unlabeled"}, {"type": "text", "expr": "tv, monitor, televison, television", "word_idx": 56624, "sentence_idx": 944, "label": "unlabeled"}, {"type": "text", "expr": "tv, monitor, televison, television", "word_idx": 56658, "sentence_idx": 945, "label": "unlabeled"}, {"type": "text", "expr": "laptop, computer, notebook, netbook, lenovo, macbook", "word_idx": 56692, "sentence_idx": 946, "label": "unlabeled"}, {"type": "text", "expr": "laptop", "word_idx": 56744, "sentence_idx": 947, "label": "unlabeled"}, {"type": "text", "expr": "laptop, computer, notebook, netbook, lenovo, macbook", "word_idx": 56750, "sentence_idx": 948, "label": "unlabeled"}, {"type": "text", "expr": "laptop, computer, notebook, netbook, lenovo, macbook", "word_idx": 56802, "sentence_idx": 949, "label": "unlabeled"}, {"type": "text", "expr": "mouse", "word_idx": 56854, "sentence_idx": 950, "label": "unlabeled"}, {"type": "text", "expr": "mouse", "word_idx": 56859, "sentence_idx": 951, "label": "unlabeled"}, {"type": "text", "expr": "mouse", "word_idx": 56864, "sentence_idx": 952, "label": "unlabeled"}, {"type": "text", "expr": "mouse", "word_idx": 56869, "sentence_idx": 953, "label": "unlabeled"}, {"type": "text", "expr": "remote", "word_idx": 56874, "sentence_idx": 954, "label": "unlabeled"}, {"type": "text", "expr": "remote", "word_idx": 56880, "sentence_idx": 955, "label": "unlabeled"}, {"type": "text", "expr": "remote", "word_idx": 56886, "sentence_idx": 956, "label": "unlabeled"}, {"type": "text", "expr": "remote", "word_idx": 56892, "sentence_idx": 957, "label": "unlabeled"}, {"type": "text", "expr": "keyboard", "word_idx": 56898, "sentence_idx": 958, "label": "unlabeled"}, {"type": "text", "expr": "keyboard", "word_idx": 56906, "sentence_idx": 959, "label": "unlabeled"}, {"type": "text", "expr": "keyboard", "word_idx": 56914, "sentence_idx": 960, "label": "unlabeled"}, {"type": "text", "expr": "keyboard", "word_idx": 56922, "sentence_idx": 961, "label": "unlabeled"}, {"type": "text", "expr": "cell phone, mobile phone, phone, cellphone, cellphone, telephone, phon, smartphone, iPhone", "word_idx": 56930, "sentence_idx": 962, "label": "unlabeled"}, {"type": "text", "expr": "cell phone", "word_idx": 57020, "sentence_idx": 963, "label": "unlabeled"}, {"type": "text", "expr": "cell phone, mobile phone, phone, cellphone, cellphone, telephone, phon, smartphone, iPhone", "word_idx": 57030, "sentence_idx": 964, "label": "unlabeled"}, {"type": "text", "expr": "cell phone, mobile phone, phone, cellphone, cellphone, telephone, phon, smartphone, iPhone", "word_idx": 57120, "sentence_idx": 965, "label": "unlabeled"}, {"type": "text", "expr": "refrigerator, fridge, refrigerator, fridge, freezer, refridgerator, frig", "word_idx": 57210, "sentence_idx": 966, "label": "unlabeled"}, {"type": "text", "expr": "refrigerator", "word_idx": 57282, "sentence_idx": 967, "label": "unlabeled"}, {"type": "text", "expr": "refrigerator, fridge, refrigerator, fridge, freezer, refridgerator, frig", "word_idx": 57294, "sentence_idx": 968, "label": "unlabeled"}, {"type": "text", "expr": "refrigerator, fridge, refrigerator, fridge, freezer, refridgerator, frig", "word_idx": 57366, "sentence_idx": 969, "label": "unlabeled"}, {"type": "text", "expr": "clock", "word_idx": 57438, "sentence_idx": 970, "label": "unlabeled"}, {"type": "text", "expr": "clock", "word_idx": 57443, "sentence_idx": 971, "label": "unlabeled"}, {"type": "text", "expr": "clock", "word_idx": 57448, "sentence_idx": 972, "label": "unlabeled"}, {"type": "text", "expr": "clock", "word_idx": 57453, "sentence_idx": 973, "label": "unlabeled"}, {"type": "text", "expr": "scissors", "word_idx": 57458, "sentence_idx": 974, "label": "unlabeled"}, {"type": "text", "expr": "scissors", "word_idx": 57466, "sentence_idx": 975, "label": "unlabeled"}, {"type": "text", "expr": "scissors", "word_idx": 57474, "sentence_idx": 976, "label": "unlabeled"}, {"type": "text", "expr": "scissors", "word_idx": 57482, "sentence_idx": 977, "label": "unlabeled"}, {"type": "text", "expr": "teddy bear, teddybear", "word_idx": 57490, "sentence_idx": 978, "label": "unlabeled"}, {"type": "text", "expr": "teddy bear", "word_idx": 57511, "sentence_idx": 979, "label": "unlabeled"}, {"type": "text", "expr": "teddy bear, teddybear", "word_idx": 57521, "sentence_idx": 980, "label": "unlabeled"}, {"type": "text", "expr": "teddy bear, teddybear", "word_idx": 57542, "sentence_idx": 981, "label": "unlabeled"}, {"type": "text", "expr": "hair drier, hairdryer", "word_idx": 57563, "sentence_idx": 982, "label": "unlabeled"}, {"type": "text", "expr": "hair drier", "word_idx": 57584, "sentence_idx": 983, "label": "unlabeled"}, {"type": "text", "expr": "hair drier, hairdryer", "word_idx": 57594, "sentence_idx": 984, "label": "unlabeled"}, {"type": "text", "expr": "hair drier, hairdryer", "word_idx": 57615, "sentence_idx": 985, "label": "unlabeled"}, {"type": "text", "expr": "toothbrush", "word_idx": 57636, "sentence_idx": 986, "label": "unlabeled"}, {"type": "text", "expr": "toothbrush", "word_idx": 57646, "sentence_idx": 987, "label": "unlabeled"}, {"type": "text", "expr": "toothbrush", "word_idx": 57656, "sentence_idx": 988, "label": "unlabeled"}, {"type": "text", "expr": "toothbrush", "word_idx": 57666, "sentence_idx": 989, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a06:  COCO category mapping list for visual words (continued)", "word_idx": 57676, "sentence_idx": 990, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a06:", "word_idx": 57741, "sentence_idx": 991, "label": "unlabeled"}, {"type": "text", "expr": "References", "word_idx": 57749, "sentence_idx": 992, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Anderson, B", "word_idx": 57759, "sentence_idx": 993, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fernando, M", "word_idx": 57771, "sentence_idx": 994, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Johnson, and S", "word_idx": 57783, "sentence_idx": 995, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gould", "word_idx": 57798, "sentence_idx": 996, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Anderson, B", "word_idx": 57804, "sentence_idx": 997, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fernando, M", "word_idx": 57816, "sentence_idx": 998, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Johnson, and S", "word_idx": 57828, "sentence_idx": 999, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gould", "word_idx": 57843, "sentence_idx": 1000, "label": "unlabeled"}, {"type": "text", "expr": "Spice: Semantic propositional image caption evaluation", "word_idx": 57849, "sentence_idx": 1001, "label": "unlabeled"}, {"type": "text", "expr": "Spice: Semantic propositional image caption evaluation", "word_idx": 57903, "sentence_idx": 1002, "label": "unlabeled"}, {"type": "text", "expr": "In  ECCV , 2016", "word_idx": 57957, "sentence_idx": 1003, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 57972, "sentence_idx": 1004, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Anderson, B", "word_idx": 57978, "sentence_idx": 1005, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fernando, M", "word_idx": 57990, "sentence_idx": 1006, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Johnson, and S", "word_idx": 58002, "sentence_idx": 1007, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gould", "word_idx": 58017, "sentence_idx": 1008, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Anderson, B", "word_idx": 58023, "sentence_idx": 1009, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fernando, M", "word_idx": 58035, "sentence_idx": 1010, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Johnson, and S", "word_idx": 58047, "sentence_idx": 1011, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gould", "word_idx": 58062, "sentence_idx": 1012, "label": "unlabeled"}, {"type": "text", "expr": "Guided open vocabulary image captioning with constrained beam search", "word_idx": 58068, "sentence_idx": 1013, "label": "unlabeled"}, {"type": "text", "expr": "Guided open vocabulary image captioning with constrained beam search", "word_idx": 58136, "sentence_idx": 1014, "label": "unlabeled"}, {"type": "text", "expr": "EMNLP , 2017", "word_idx": 58204, "sentence_idx": 1015, "label": "unlabeled"}, {"type": "text", "expr": "EMNLP", "word_idx": 58216, "sentence_idx": 1016, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 58221, "sentence_idx": 1017, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Anderson, X", "word_idx": 58227, "sentence_idx": 1018, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, C", "word_idx": 58239, "sentence_idx": 1019, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Buehler, D", "word_idx": 58245, "sentence_idx": 1020, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Teney, M", "word_idx": 58256, "sentence_idx": 1021, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Johnson, S", "word_idx": 58265, "sentence_idx": 1022, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gould, and L", "word_idx": 58276, "sentence_idx": 1023, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang", "word_idx": 58289, "sentence_idx": 1024, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Anderson, X", "word_idx": 58295, "sentence_idx": 1025, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, C", "word_idx": 58307, "sentence_idx": 1026, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Buehler, D", "word_idx": 58313, "sentence_idx": 1027, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Teney, M", "word_idx": 58324, "sentence_idx": 1028, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Johnson, S", "word_idx": 58333, "sentence_idx": 1029, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gould, and L", "word_idx": 58344, "sentence_idx": 1030, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang", "word_idx": 58357, "sentence_idx": 1031, "label": "unlabeled"}, {"type": "text", "expr": "Bottom-up and top-down attention for image captioning and visual\nquestion answering", "word_idx": 58363, "sentence_idx": 1032, "label": "unlabeled"}, {"type": "text", "expr": "Bottom-up and top-down attention for image captioning and visual\nquestion answering", "word_idx": 58446, "sentence_idx": 1033, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2018", "word_idx": 58529, "sentence_idx": 1034, "label": "unlabeled"}, {"type": "text", "expr": ", 2018", "word_idx": 58544, "sentence_idx": 1035, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Anne\u00a0Hendricks, S", "word_idx": 58550, "sentence_idx": 1036, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Venugopalan, M", "word_idx": 58568, "sentence_idx": 1037, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rohrbach, R", "word_idx": 58583, "sentence_idx": 1038, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mooney, K", "word_idx": 58595, "sentence_idx": 1039, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Saenko, and\nT", "word_idx": 58605, "sentence_idx": 1040, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Darrell", "word_idx": 58619, "sentence_idx": 1041, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Anne\u00a0Hendricks, S", "word_idx": 58627, "sentence_idx": 1042, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Venugopalan, M", "word_idx": 58645, "sentence_idx": 1043, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rohrbach, R", "word_idx": 58660, "sentence_idx": 1044, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mooney, K", "word_idx": 58672, "sentence_idx": 1045, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Saenko, and\nT", "word_idx": 58682, "sentence_idx": 1046, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Darrell", "word_idx": 58696, "sentence_idx": 1047, "label": "unlabeled"}, {"type": "text", "expr": "Deep compositional captioning: Describing novel object categories\nwithout paired training data", "word_idx": 58704, "sentence_idx": 1048, "label": "unlabeled"}, {"type": "text", "expr": "Deep compositional captioning: Describing novel object categories\nwithout paired training data", "word_idx": 58798, "sentence_idx": 1049, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2016", "word_idx": 58892, "sentence_idx": 1050, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 58907, "sentence_idx": 1051, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Antol, A", "word_idx": 58913, "sentence_idx": 1052, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Agrawal, J", "word_idx": 58922, "sentence_idx": 1053, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lu, M", "word_idx": 58933, "sentence_idx": 1054, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mitchell, D", "word_idx": 58939, "sentence_idx": 1055, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Batra, C", "word_idx": 58951, "sentence_idx": 1056, "label": "unlabeled"}, {"type": "text", "expr": " Zitnick, and\nD", "word_idx": 58960, "sentence_idx": 1057, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Parikh", "word_idx": 58975, "sentence_idx": 1058, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Antol, A", "word_idx": 58982, "sentence_idx": 1059, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Agrawal, J", "word_idx": 58991, "sentence_idx": 1060, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lu, M", "word_idx": 59002, "sentence_idx": 1061, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mitchell, D", "word_idx": 59008, "sentence_idx": 1062, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Batra, C", "word_idx": 59020, "sentence_idx": 1063, "label": "unlabeled"}, {"type": "text", "expr": " Zitnick, and\nD", "word_idx": 59029, "sentence_idx": 1064, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Parikh", "word_idx": 59044, "sentence_idx": 1065, "label": "unlabeled"}, {"type": "text", "expr": "VQA: Visual Question Answering", "word_idx": 59051, "sentence_idx": 1066, "label": "unlabeled"}, {"type": "text", "expr": "VQA: Visual Question Answering", "word_idx": 59081, "sentence_idx": 1067, "label": "unlabeled"}, {"type": "text", "expr": "In  ICCV , 2015", "word_idx": 59111, "sentence_idx": 1068, "label": "unlabeled"}, {"type": "text", "expr": ", 2015", "word_idx": 59126, "sentence_idx": 1069, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chen and C", "word_idx": 59132, "sentence_idx": 1070, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Manning", "word_idx": 59143, "sentence_idx": 1071, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chen and C", "word_idx": 59151, "sentence_idx": 1072, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Manning", "word_idx": 59162, "sentence_idx": 1073, "label": "unlabeled"}, {"type": "text", "expr": "A fast and accurate dependency parser using neural networks", "word_idx": 59170, "sentence_idx": 1074, "label": "unlabeled"}, {"type": "text", "expr": "A fast and accurate dependency parser using neural networks", "word_idx": 59229, "sentence_idx": 1075, "label": "unlabeled"}, {"type": "text", "expr": "In  EMNLP , 2014", "word_idx": 59288, "sentence_idx": 1076, "label": "unlabeled"}, {"type": "text", "expr": "EMNLP", "word_idx": 59304, "sentence_idx": 1077, "label": "unlabeled"}, {"type": "text", "expr": ", 2014", "word_idx": 59309, "sentence_idx": 1078, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chen and C", "word_idx": 59315, "sentence_idx": 1079, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lawrence\u00a0Zitnick", "word_idx": 59326, "sentence_idx": 1080, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chen and C", "word_idx": 59343, "sentence_idx": 1081, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lawrence\u00a0Zitnick", "word_idx": 59354, "sentence_idx": 1082, "label": "unlabeled"}, {"type": "text", "expr": "Mind\u2019s eye: A recurrent visual representation for image caption\ngeneration", "word_idx": 59371, "sentence_idx": 1083, "label": "unlabeled"}, {"type": "text", "expr": "Mind\u2019s eye: A recurrent visual representation for image caption\ngeneration", "word_idx": 59445, "sentence_idx": 1084, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2015", "word_idx": 59519, "sentence_idx": 1085, "label": "unlabeled"}, {"type": "text", "expr": ", 2015", "word_idx": 59534, "sentence_idx": 1086, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Das, H", "word_idx": 59540, "sentence_idx": 1087, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Agrawal, C", "word_idx": 59547, "sentence_idx": 1088, "label": "unlabeled"}, {"type": "text", "expr": " Zitnick, D", "word_idx": 59558, "sentence_idx": 1089, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Parikh, and D", "word_idx": 59569, "sentence_idx": 1090, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Batra", "word_idx": 59583, "sentence_idx": 1091, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Das, H", "word_idx": 59589, "sentence_idx": 1092, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Agrawal, C", "word_idx": 59596, "sentence_idx": 1093, "label": "unlabeled"}, {"type": "text", "expr": " Zitnick, D", "word_idx": 59607, "sentence_idx": 1094, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Parikh, and D", "word_idx": 59618, "sentence_idx": 1095, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Batra", "word_idx": 59632, "sentence_idx": 1096, "label": "unlabeled"}, {"type": "text", "expr": "Human Attention in Visual Question Answering: Do Humans and Deep\nNetworks Look at the Same Regions?", "word_idx": 59638, "sentence_idx": 1097, "label": "unlabeled"}, {"type": "text", "expr": "Human Attention in Visual Question Answering: Do Humans and Deep\nNetworks Look at the Same Regions?", "word_idx": 59737, "sentence_idx": 1098, "label": "unlabeled"}, {"type": "text", "expr": "In  EMNLP , 2016", "word_idx": 59836, "sentence_idx": 1099, "label": "unlabeled"}, {"type": "text", "expr": "EMNLP", "word_idx": 59852, "sentence_idx": 1100, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 59857, "sentence_idx": 1101, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Das, S", "word_idx": 59863, "sentence_idx": 1102, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kottur, K", "word_idx": 59870, "sentence_idx": 1103, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gupta, A", "word_idx": 59880, "sentence_idx": 1104, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Singh, D", "word_idx": 59889, "sentence_idx": 1105, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yadav, J", "word_idx": 59898, "sentence_idx": 1106, "label": "unlabeled"}, {"type": "text", "expr": " Moura, D", "word_idx": 59907, "sentence_idx": 1107, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Parikh, and\nD", "word_idx": 59916, "sentence_idx": 1108, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Batra", "word_idx": 59930, "sentence_idx": 1109, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Das, S", "word_idx": 59936, "sentence_idx": 1110, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kottur, K", "word_idx": 59943, "sentence_idx": 1111, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gupta, A", "word_idx": 59953, "sentence_idx": 1112, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Singh, D", "word_idx": 59962, "sentence_idx": 1113, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yadav, J", "word_idx": 59971, "sentence_idx": 1114, "label": "unlabeled"}, {"type": "text", "expr": " Moura, D", "word_idx": 59980, "sentence_idx": 1115, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Parikh, and\nD", "word_idx": 59989, "sentence_idx": 1116, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Batra", "word_idx": 60003, "sentence_idx": 1117, "label": "unlabeled"}, {"type": "text", "expr": "Visual Dialog", "word_idx": 60009, "sentence_idx": 1118, "label": "unlabeled"}, {"type": "text", "expr": "Visual Dialog", "word_idx": 60022, "sentence_idx": 1119, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2017", "word_idx": 60035, "sentence_idx": 1120, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 60050, "sentence_idx": 1121, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Denkowski and A", "word_idx": 60056, "sentence_idx": 1122, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lavie", "word_idx": 60072, "sentence_idx": 1123, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Denkowski and A", "word_idx": 60078, "sentence_idx": 1124, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lavie", "word_idx": 60094, "sentence_idx": 1125, "label": "unlabeled"}, {"type": "text", "expr": "Meteor universal: Language specific translation evaluation for any\ntarget language", "word_idx": 60100, "sentence_idx": 1126, "label": "unlabeled"}, {"type": "text", "expr": "Meteor universal: Language specific translation evaluation for any\ntarget language", "word_idx": 60182, "sentence_idx": 1127, "label": "unlabeled"}, {"type": "text", "expr": "In  EACL 2014 Workshop on Statistical Machine Translation , 2014", "word_idx": 60264, "sentence_idx": 1128, "label": "unlabeled"}, {"type": "text", "expr": "EACL 2014 Workshop on Statistical Machine Translation", "word_idx": 60328, "sentence_idx": 1129, "label": "unlabeled"}, {"type": "text", "expr": ", 2014", "word_idx": 60381, "sentence_idx": 1130, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Donahue, L", "word_idx": 60387, "sentence_idx": 1131, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Anne\u00a0Hendricks, S", "word_idx": 60398, "sentence_idx": 1132, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Guadarrama, M", "word_idx": 60416, "sentence_idx": 1133, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rohrbach, S", "word_idx": 60430, "sentence_idx": 1134, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Venugopalan,\nK", "word_idx": 60442, "sentence_idx": 1135, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Saenko, and T", "word_idx": 60457, "sentence_idx": 1136, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Darrell", "word_idx": 60471, "sentence_idx": 1137, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Donahue, L", "word_idx": 60479, "sentence_idx": 1138, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Anne\u00a0Hendricks, S", "word_idx": 60490, "sentence_idx": 1139, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Guadarrama, M", "word_idx": 60508, "sentence_idx": 1140, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rohrbach, S", "word_idx": 60522, "sentence_idx": 1141, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Venugopalan,\nK", "word_idx": 60534, "sentence_idx": 1142, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Saenko, and T", "word_idx": 60549, "sentence_idx": 1143, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Darrell", "word_idx": 60563, "sentence_idx": 1144, "label": "unlabeled"}, {"type": "text", "expr": "Long-term recurrent convolutional networks for visual recognition and\ndescription", "word_idx": 60571, "sentence_idx": 1145, "label": "unlabeled"}, {"type": "text", "expr": "Long-term recurrent convolutional networks for visual recognition and\ndescription", "word_idx": 60652, "sentence_idx": 1146, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2015", "word_idx": 60733, "sentence_idx": 1147, "label": "unlabeled"}, {"type": "text", "expr": ", 2015", "word_idx": 60748, "sentence_idx": 1148, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fang, S", "word_idx": 60754, "sentence_idx": 1149, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gupta, F", "word_idx": 60762, "sentence_idx": 1150, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Iandola, R", "word_idx": 60771, "sentence_idx": 1151, "label": "unlabeled"}, {"type": "text", "expr": " Srivastava, L", "word_idx": 60782, "sentence_idx": 1152, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Deng, P", "word_idx": 60796, "sentence_idx": 1153, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Doll\u00e1r,\nJ", "word_idx": 60804, "sentence_idx": 1154, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gao, X", "word_idx": 60814, "sentence_idx": 1155, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, M", "word_idx": 60821, "sentence_idx": 1156, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mitchell, J", "word_idx": 60827, "sentence_idx": 1157, "label": "unlabeled"}, {"type": "text", "expr": " Platt, et\u00a0al", "word_idx": 60839, "sentence_idx": 1158, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fang, S", "word_idx": 60852, "sentence_idx": 1159, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gupta, F", "word_idx": 60860, "sentence_idx": 1160, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Iandola, R", "word_idx": 60869, "sentence_idx": 1161, "label": "unlabeled"}, {"type": "text", "expr": " Srivastava, L", "word_idx": 60880, "sentence_idx": 1162, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Deng, P", "word_idx": 60894, "sentence_idx": 1163, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Doll\u00e1r,\nJ", "word_idx": 60902, "sentence_idx": 1164, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gao, X", "word_idx": 60912, "sentence_idx": 1165, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, M", "word_idx": 60919, "sentence_idx": 1166, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mitchell, J", "word_idx": 60925, "sentence_idx": 1167, "label": "unlabeled"}, {"type": "text", "expr": " Platt, et\u00a0al", "word_idx": 60937, "sentence_idx": 1168, "label": "unlabeled"}, {"type": "text", "expr": "From captions to visual concepts and back", "word_idx": 60950, "sentence_idx": 1169, "label": "unlabeled"}, {"type": "text", "expr": "From captions to visual concepts and back", "word_idx": 60991, "sentence_idx": 1170, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2015", "word_idx": 61032, "sentence_idx": 1171, "label": "unlabeled"}, {"type": "text", "expr": ", 2015", "word_idx": 61047, "sentence_idx": 1172, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Farhadi, M", "word_idx": 61053, "sentence_idx": 1173, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hejrati, M", "word_idx": 61064, "sentence_idx": 1174, "label": "unlabeled"}, {"type": "text", "expr": " Sadeghi, P", "word_idx": 61075, "sentence_idx": 1175, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Young, C", "word_idx": 61086, "sentence_idx": 1176, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rashtchian, J", "word_idx": 61095, "sentence_idx": 1177, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hockenmaier,\nand D", "word_idx": 61109, "sentence_idx": 1178, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Forsyth", "word_idx": 61128, "sentence_idx": 1179, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Farhadi, M", "word_idx": 61136, "sentence_idx": 1180, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hejrati, M", "word_idx": 61147, "sentence_idx": 1181, "label": "unlabeled"}, {"type": "text", "expr": " Sadeghi, P", "word_idx": 61158, "sentence_idx": 1182, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Young, C", "word_idx": 61169, "sentence_idx": 1183, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rashtchian, J", "word_idx": 61178, "sentence_idx": 1184, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hockenmaier,\nand D", "word_idx": 61192, "sentence_idx": 1185, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Forsyth", "word_idx": 61211, "sentence_idx": 1186, "label": "unlabeled"}, {"type": "text", "expr": "Every picture tells a story: Generating sentences from images", "word_idx": 61219, "sentence_idx": 1187, "label": "unlabeled"}, {"type": "text", "expr": "Every picture tells a story: Generating sentences from images", "word_idx": 61280, "sentence_idx": 1188, "label": "unlabeled"}, {"type": "text", "expr": "In  ECCV , 2010", "word_idx": 61341, "sentence_idx": 1189, "label": "unlabeled"}, {"type": "text", "expr": ", 2010", "word_idx": 61356, "sentence_idx": 1190, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, G", "word_idx": 61362, "sentence_idx": 1191, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gkioxari, P", "word_idx": 61368, "sentence_idx": 1192, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Doll\u00e1r, and R", "word_idx": 61380, "sentence_idx": 1193, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Girshick", "word_idx": 61394, "sentence_idx": 1194, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, G", "word_idx": 61403, "sentence_idx": 1195, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gkioxari, P", "word_idx": 61409, "sentence_idx": 1196, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Doll\u00e1r, and R", "word_idx": 61421, "sentence_idx": 1197, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Girshick", "word_idx": 61435, "sentence_idx": 1198, "label": "unlabeled"}, {"type": "text", "expr": "Mask r-cnn", "word_idx": 61444, "sentence_idx": 1199, "label": "unlabeled"}, {"type": "text", "expr": "Mask r-cnn", "word_idx": 61454, "sentence_idx": 1200, "label": "unlabeled"}, {"type": "text", "expr": "ICCV , 2017", "word_idx": 61464, "sentence_idx": 1201, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 61475, "sentence_idx": 1202, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, X", "word_idx": 61481, "sentence_idx": 1203, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang, S", "word_idx": 61487, "sentence_idx": 1204, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ren, and J", "word_idx": 61496, "sentence_idx": 1205, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, X", "word_idx": 61507, "sentence_idx": 1206, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang, S", "word_idx": 61513, "sentence_idx": 1207, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ren, and J", "word_idx": 61522, "sentence_idx": 1208, "label": "unlabeled"}, {"type": "text", "expr": "Deep residual learning for image recognition", "word_idx": 61533, "sentence_idx": 1209, "label": "unlabeled"}, {"type": "text", "expr": "Deep residual learning for image recognition", "word_idx": 61577, "sentence_idx": 1210, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2016", "word_idx": 61621, "sentence_idx": 1211, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 61636, "sentence_idx": 1212, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hochreiter and J", "word_idx": 61642, "sentence_idx": 1213, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Schmidhuber", "word_idx": 61659, "sentence_idx": 1214, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hochreiter and J", "word_idx": 61671, "sentence_idx": 1215, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Schmidhuber", "word_idx": 61688, "sentence_idx": 1216, "label": "unlabeled"}, {"type": "text", "expr": "Long short-term memory", "word_idx": 61700, "sentence_idx": 1217, "label": "unlabeled"}, {"type": "text", "expr": "Long short-term memory", "word_idx": 61722, "sentence_idx": 1218, "label": "unlabeled"}, {"type": "text", "expr": "Neural computation , 9(8):1735\u20131780, 1997", "word_idx": 61744, "sentence_idx": 1219, "label": "unlabeled"}, {"type": "text", "expr": "Neural computation", "word_idx": 61785, "sentence_idx": 1220, "label": "unlabeled"}, {"type": "text", "expr": ", 9(8):1735\u20131780, 1997", "word_idx": 61803, "sentence_idx": 1221, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hu, M", "word_idx": 61825, "sentence_idx": 1222, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rohrbach, J", "word_idx": 61831, "sentence_idx": 1223, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Andreas, T", "word_idx": 61843, "sentence_idx": 1224, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Darrell, and K", "word_idx": 61854, "sentence_idx": 1225, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Saenko", "word_idx": 61869, "sentence_idx": 1226, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hu, M", "word_idx": 61876, "sentence_idx": 1227, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rohrbach, J", "word_idx": 61882, "sentence_idx": 1228, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Andreas, T", "word_idx": 61894, "sentence_idx": 1229, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Darrell, and K", "word_idx": 61905, "sentence_idx": 1230, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Saenko", "word_idx": 61920, "sentence_idx": 1231, "label": "unlabeled"}, {"type": "text", "expr": "Modeling relationships in referential expressions with compositional\nmodular networks", "word_idx": 61927, "sentence_idx": 1232, "label": "unlabeled"}, {"type": "text", "expr": "Modeling relationships in referential expressions with compositional\nmodular networks", "word_idx": 62012, "sentence_idx": 1233, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1611", "word_idx": 62097, "sentence_idx": 1234, "label": "unlabeled"}, {"type": "text", "expr": "09978 , 2016", "word_idx": 62122, "sentence_idx": 1235, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1611", "word_idx": 62134, "sentence_idx": 1236, "label": "unlabeled"}, {"type": "text", "expr": "09978", "word_idx": 62159, "sentence_idx": 1237, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 62164, "sentence_idx": 1238, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hu, H", "word_idx": 62170, "sentence_idx": 1239, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xu, M", "word_idx": 62176, "sentence_idx": 1240, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rohrbach, J", "word_idx": 62182, "sentence_idx": 1241, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Feng, K", "word_idx": 62194, "sentence_idx": 1242, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Saenko, and T", "word_idx": 62202, "sentence_idx": 1243, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Darrell", "word_idx": 62216, "sentence_idx": 1244, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hu, H", "word_idx": 62224, "sentence_idx": 1245, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xu, M", "word_idx": 62230, "sentence_idx": 1246, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rohrbach, J", "word_idx": 62236, "sentence_idx": 1247, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Feng, K", "word_idx": 62248, "sentence_idx": 1248, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Saenko, and T", "word_idx": 62256, "sentence_idx": 1249, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Darrell", "word_idx": 62270, "sentence_idx": 1250, "label": "unlabeled"}, {"type": "text", "expr": "Natural language object retrieval", "word_idx": 62278, "sentence_idx": 1251, "label": "unlabeled"}, {"type": "text", "expr": "Natural language object retrieval", "word_idx": 62311, "sentence_idx": 1252, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2016", "word_idx": 62344, "sentence_idx": 1253, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 62359, "sentence_idx": 1254, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Johnson, A", "word_idx": 62365, "sentence_idx": 1255, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Karpathy, and L", "word_idx": 62376, "sentence_idx": 1256, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fei-Fei", "word_idx": 62392, "sentence_idx": 1257, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Johnson, A", "word_idx": 62400, "sentence_idx": 1258, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Karpathy, and L", "word_idx": 62411, "sentence_idx": 1259, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fei-Fei", "word_idx": 62427, "sentence_idx": 1260, "label": "unlabeled"}, {"type": "text", "expr": "Densecap: Fully convolutional localization networks for dense\ncaptioning", "word_idx": 62435, "sentence_idx": 1261, "label": "unlabeled"}, {"type": "text", "expr": "Densecap: Fully convolutional localization networks for dense\ncaptioning", "word_idx": 62507, "sentence_idx": 1262, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2016", "word_idx": 62579, "sentence_idx": 1263, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 62594, "sentence_idx": 1264, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Karpathy and L", "word_idx": 62600, "sentence_idx": 1265, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fei-Fei", "word_idx": 62615, "sentence_idx": 1266, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Karpathy and L", "word_idx": 62623, "sentence_idx": 1267, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fei-Fei", "word_idx": 62638, "sentence_idx": 1268, "label": "unlabeled"}, {"type": "text", "expr": "Deep visual-semantic alignments for generating image descriptions", "word_idx": 62646, "sentence_idx": 1269, "label": "unlabeled"}, {"type": "text", "expr": "Deep visual-semantic alignments for generating image descriptions", "word_idx": 62711, "sentence_idx": 1270, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2015", "word_idx": 62776, "sentence_idx": 1271, "label": "unlabeled"}, {"type": "text", "expr": ", 2015", "word_idx": 62791, "sentence_idx": 1272, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kazemzadeh, V", "word_idx": 62797, "sentence_idx": 1273, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ordonez, M", "word_idx": 62811, "sentence_idx": 1274, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Matten, and T", "word_idx": 62822, "sentence_idx": 1275, "label": "unlabeled"}, {"type": "text", "expr": " Berg", "word_idx": 62836, "sentence_idx": 1276, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kazemzadeh, V", "word_idx": 62841, "sentence_idx": 1277, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ordonez, M", "word_idx": 62855, "sentence_idx": 1278, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Matten, and T", "word_idx": 62866, "sentence_idx": 1279, "label": "unlabeled"}, {"type": "text", "expr": " Berg", "word_idx": 62880, "sentence_idx": 1280, "label": "unlabeled"}, {"type": "text", "expr": "Referitgame: Referring to objects in photographs of natural scenes", "word_idx": 62885, "sentence_idx": 1281, "label": "unlabeled"}, {"type": "text", "expr": "Referitgame: Referring to objects in photographs of natural scenes", "word_idx": 62951, "sentence_idx": 1282, "label": "unlabeled"}, {"type": "text", "expr": "In  EMNLP , 2014", "word_idx": 63017, "sentence_idx": 1283, "label": "unlabeled"}, {"type": "text", "expr": "EMNLP", "word_idx": 63033, "sentence_idx": 1284, "label": "unlabeled"}, {"type": "text", "expr": ", 2014", "word_idx": 63038, "sentence_idx": 1285, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kingma and J", "word_idx": 63044, "sentence_idx": 1286, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kingma and J", "word_idx": 63057, "sentence_idx": 1287, "label": "unlabeled"}, {"type": "text", "expr": "Adam: A method for stochastic optimization", "word_idx": 63070, "sentence_idx": 1288, "label": "unlabeled"}, {"type": "text", "expr": "Adam: A method for stochastic optimization", "word_idx": 63112, "sentence_idx": 1289, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1412", "word_idx": 63154, "sentence_idx": 1290, "label": "unlabeled"}, {"type": "text", "expr": "6980 , 2014", "word_idx": 63179, "sentence_idx": 1291, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1412", "word_idx": 63190, "sentence_idx": 1292, "label": "unlabeled"}, {"type": "text", "expr": ", 2014", "word_idx": 63215, "sentence_idx": 1293, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kiros, R", "word_idx": 63221, "sentence_idx": 1294, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Salakhutdinov, and R", "word_idx": 63230, "sentence_idx": 1295, "label": "unlabeled"}, {"type": "text", "expr": " Zemel", "word_idx": 63251, "sentence_idx": 1296, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kiros, R", "word_idx": 63257, "sentence_idx": 1297, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Salakhutdinov, and R", "word_idx": 63266, "sentence_idx": 1298, "label": "unlabeled"}, {"type": "text", "expr": " Zemel", "word_idx": 63287, "sentence_idx": 1299, "label": "unlabeled"}, {"type": "text", "expr": "Multimodal neural language models", "word_idx": 63293, "sentence_idx": 1300, "label": "unlabeled"}, {"type": "text", "expr": "Multimodal neural language models", "word_idx": 63326, "sentence_idx": 1301, "label": "unlabeled"}, {"type": "text", "expr": "In  ICML , 2014", "word_idx": 63359, "sentence_idx": 1302, "label": "unlabeled"}, {"type": "text", "expr": ", 2014", "word_idx": 63374, "sentence_idx": 1303, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kulkarni, V", "word_idx": 63380, "sentence_idx": 1304, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Premraj, V", "word_idx": 63392, "sentence_idx": 1305, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ordonez, S", "word_idx": 63403, "sentence_idx": 1306, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dhar, S", "word_idx": 63414, "sentence_idx": 1307, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Li, Y", "word_idx": 63422, "sentence_idx": 1308, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Choi, A", "word_idx": 63428, "sentence_idx": 1309, "label": "unlabeled"}, {"type": "text", "expr": " Berg, and\nT", "word_idx": 63436, "sentence_idx": 1310, "label": "unlabeled"}, {"type": "text", "expr": " Berg", "word_idx": 63448, "sentence_idx": 1311, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kulkarni, V", "word_idx": 63453, "sentence_idx": 1312, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Premraj, V", "word_idx": 63465, "sentence_idx": 1313, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ordonez, S", "word_idx": 63476, "sentence_idx": 1314, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dhar, S", "word_idx": 63487, "sentence_idx": 1315, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Li, Y", "word_idx": 63495, "sentence_idx": 1316, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Choi, A", "word_idx": 63501, "sentence_idx": 1317, "label": "unlabeled"}, {"type": "text", "expr": " Berg, and\nT", "word_idx": 63509, "sentence_idx": 1318, "label": "unlabeled"}, {"type": "text", "expr": " Berg", "word_idx": 63521, "sentence_idx": 1319, "label": "unlabeled"}, {"type": "text", "expr": "Babytalk: Understanding and generating simple image descriptions", "word_idx": 63526, "sentence_idx": 1320, "label": "unlabeled"}, {"type": "text", "expr": "Babytalk: Understanding and generating simple image descriptions", "word_idx": 63590, "sentence_idx": 1321, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2011", "word_idx": 63654, "sentence_idx": 1322, "label": "unlabeled"}, {"type": "text", "expr": ", 2011", "word_idx": 63669, "sentence_idx": 1323, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kuznetsova, V", "word_idx": 63675, "sentence_idx": 1324, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ordonez, A", "word_idx": 63689, "sentence_idx": 1325, "label": "unlabeled"}, {"type": "text", "expr": " Berg, T", "word_idx": 63700, "sentence_idx": 1326, "label": "unlabeled"}, {"type": "text", "expr": " Berg, and Y", "word_idx": 63708, "sentence_idx": 1327, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Choi", "word_idx": 63720, "sentence_idx": 1328, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kuznetsova, V", "word_idx": 63725, "sentence_idx": 1329, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ordonez, A", "word_idx": 63739, "sentence_idx": 1330, "label": "unlabeled"}, {"type": "text", "expr": " Berg, T", "word_idx": 63750, "sentence_idx": 1331, "label": "unlabeled"}, {"type": "text", "expr": " Berg, and Y", "word_idx": 63758, "sentence_idx": 1332, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Choi", "word_idx": 63770, "sentence_idx": 1333, "label": "unlabeled"}, {"type": "text", "expr": "Collective generation of natural image descriptions", "word_idx": 63775, "sentence_idx": 1334, "label": "unlabeled"}, {"type": "text", "expr": "Collective generation of natural image descriptions", "word_idx": 63826, "sentence_idx": 1335, "label": "unlabeled"}, {"type": "text", "expr": "In  ACL , 2012", "word_idx": 63877, "sentence_idx": 1336, "label": "unlabeled"}, {"type": "text", "expr": ", 2012", "word_idx": 63891, "sentence_idx": 1337, "label": "unlabeled"}, {"type": "text", "expr": " Lin, M", "word_idx": 63897, "sentence_idx": 1338, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Maire, S", "word_idx": 63904, "sentence_idx": 1339, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Belongie, J", "word_idx": 63913, "sentence_idx": 1340, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hays, P", "word_idx": 63925, "sentence_idx": 1341, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Perona, D", "word_idx": 63933, "sentence_idx": 1342, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ramanan,\nP", "word_idx": 63943, "sentence_idx": 1343, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Doll\u00e1r, and C", "word_idx": 63954, "sentence_idx": 1344, "label": "unlabeled"}, {"type": "text", "expr": " Zitnick", "word_idx": 63968, "sentence_idx": 1345, "label": "unlabeled"}, {"type": "text", "expr": " Lin, M", "word_idx": 63976, "sentence_idx": 1346, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Maire, S", "word_idx": 63983, "sentence_idx": 1347, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Belongie, J", "word_idx": 63992, "sentence_idx": 1348, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hays, P", "word_idx": 64004, "sentence_idx": 1349, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Perona, D", "word_idx": 64012, "sentence_idx": 1350, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ramanan,\nP", "word_idx": 64022, "sentence_idx": 1351, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Doll\u00e1r, and C", "word_idx": 64033, "sentence_idx": 1352, "label": "unlabeled"}, {"type": "text", "expr": " Zitnick", "word_idx": 64047, "sentence_idx": 1353, "label": "unlabeled"}, {"type": "text", "expr": "Microsoft coco: Common objects in context", "word_idx": 64055, "sentence_idx": 1354, "label": "unlabeled"}, {"type": "text", "expr": "Microsoft coco: Common objects in context", "word_idx": 64096, "sentence_idx": 1355, "label": "unlabeled"}, {"type": "text", "expr": "In  ECCV , 2014", "word_idx": 64137, "sentence_idx": 1356, "label": "unlabeled"}, {"type": "text", "expr": ", 2014", "word_idx": 64152, "sentence_idx": 1357, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lu, C", "word_idx": 64158, "sentence_idx": 1358, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xiong, D", "word_idx": 64164, "sentence_idx": 1359, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Parikh, and R", "word_idx": 64173, "sentence_idx": 1360, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Socher", "word_idx": 64187, "sentence_idx": 1361, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lu, C", "word_idx": 64194, "sentence_idx": 1362, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xiong, D", "word_idx": 64200, "sentence_idx": 1363, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Parikh, and R", "word_idx": 64209, "sentence_idx": 1364, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Socher", "word_idx": 64223, "sentence_idx": 1365, "label": "unlabeled"}, {"type": "text", "expr": "Knowing when to look: Adaptive attention via a visual sentinel for\nimage captioning", "word_idx": 64230, "sentence_idx": 1366, "label": "unlabeled"}, {"type": "text", "expr": "Knowing when to look: Adaptive attention via a visual sentinel for\nimage captioning", "word_idx": 64313, "sentence_idx": 1367, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2017", "word_idx": 64396, "sentence_idx": 1368, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 64411, "sentence_idx": 1369, "label": "unlabeled"}, {"type": "text", "expr": "Unofficial pytorch implementation for self-critical sequence training\nfor image captioning", "word_idx": 64417, "sentence_idx": 1370, "label": "unlabeled"}, {"type": "text", "expr": "Unofficial pytorch implementation for self-critical sequence training\nfor image captioning", "word_idx": 64507, "sentence_idx": 1371, "label": "unlabeled"}, {"type": "text", "expr": "https://github", "word_idx": 64597, "sentence_idx": 1372, "label": "unlabeled"}, {"type": "text", "expr": "com/ruotianluo/self-critical", "word_idx": 64611, "sentence_idx": 1373, "label": "unlabeled"}, {"type": "text", "expr": "pytorch , 2017", "word_idx": 64639, "sentence_idx": 1374, "label": "unlabeled"}, {"type": "text", "expr": "https://github", "word_idx": 64653, "sentence_idx": 1375, "label": "unlabeled"}, {"type": "text", "expr": "com/ruotianluo/self-critical", "word_idx": 64667, "sentence_idx": 1376, "label": "unlabeled"}, {"type": "text", "expr": "pytorch", "word_idx": 64695, "sentence_idx": 1377, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 64702, "sentence_idx": 1378, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Luo and G", "word_idx": 64708, "sentence_idx": 1379, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Shakhnarovich", "word_idx": 64718, "sentence_idx": 1380, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Luo and G", "word_idx": 64732, "sentence_idx": 1381, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Shakhnarovich", "word_idx": 64742, "sentence_idx": 1382, "label": "unlabeled"}, {"type": "text", "expr": "Comprehension-guided referring expressions", "word_idx": 64756, "sentence_idx": 1383, "label": "unlabeled"}, {"type": "text", "expr": "Comprehension-guided referring expressions", "word_idx": 64798, "sentence_idx": 1384, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2017", "word_idx": 64840, "sentence_idx": 1385, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 64855, "sentence_idx": 1386, "label": "unlabeled"}, {"type": "text", "expr": " Manning, M", "word_idx": 64861, "sentence_idx": 1387, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Surdeanu, J", "word_idx": 64872, "sentence_idx": 1388, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bauer, J", "word_idx": 64884, "sentence_idx": 1389, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Finkel, S", "word_idx": 64893, "sentence_idx": 1390, "label": "unlabeled"}, {"type": "text", "expr": " Bethard, and\nD", "word_idx": 64903, "sentence_idx": 1391, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0McClosky", "word_idx": 64918, "sentence_idx": 1392, "label": "unlabeled"}, {"type": "text", "expr": " Manning, M", "word_idx": 64927, "sentence_idx": 1393, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Surdeanu, J", "word_idx": 64938, "sentence_idx": 1394, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bauer, J", "word_idx": 64950, "sentence_idx": 1395, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Finkel, S", "word_idx": 64959, "sentence_idx": 1396, "label": "unlabeled"}, {"type": "text", "expr": " Bethard, and\nD", "word_idx": 64969, "sentence_idx": 1397, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0McClosky", "word_idx": 64984, "sentence_idx": 1398, "label": "unlabeled"}, {"type": "text", "expr": "The Stanford CoreNLP natural language processing toolkit", "word_idx": 64993, "sentence_idx": 1399, "label": "unlabeled"}, {"type": "text", "expr": "The Stanford CoreNLP natural language processing toolkit", "word_idx": 65049, "sentence_idx": 1400, "label": "unlabeled"}, {"type": "text", "expr": "In  ACL , 2014", "word_idx": 65105, "sentence_idx": 1401, "label": "unlabeled"}, {"type": "text", "expr": ", 2014", "word_idx": 65119, "sentence_idx": 1402, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mao, J", "word_idx": 65125, "sentence_idx": 1403, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Huang, A", "word_idx": 65132, "sentence_idx": 1404, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Toshev, O", "word_idx": 65141, "sentence_idx": 1405, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Camburu, A", "word_idx": 65151, "sentence_idx": 1406, "label": "unlabeled"}, {"type": "text", "expr": " Yuille, and K", "word_idx": 65162, "sentence_idx": 1407, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Murphy", "word_idx": 65176, "sentence_idx": 1408, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mao, J", "word_idx": 65183, "sentence_idx": 1409, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Huang, A", "word_idx": 65190, "sentence_idx": 1410, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Toshev, O", "word_idx": 65199, "sentence_idx": 1411, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Camburu, A", "word_idx": 65209, "sentence_idx": 1412, "label": "unlabeled"}, {"type": "text", "expr": " Yuille, and K", "word_idx": 65220, "sentence_idx": 1413, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Murphy", "word_idx": 65234, "sentence_idx": 1414, "label": "unlabeled"}, {"type": "text", "expr": "Generation and comprehension of unambiguous object descriptions", "word_idx": 65241, "sentence_idx": 1415, "label": "unlabeled"}, {"type": "text", "expr": "Generation and comprehension of unambiguous object descriptions", "word_idx": 65304, "sentence_idx": 1416, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2016", "word_idx": 65367, "sentence_idx": 1417, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 65382, "sentence_idx": 1418, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mao, W", "word_idx": 65388, "sentence_idx": 1419, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xu, Y", "word_idx": 65395, "sentence_idx": 1420, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yang, J", "word_idx": 65401, "sentence_idx": 1421, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wang, Z", "word_idx": 65409, "sentence_idx": 1422, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Huang, and A", "word_idx": 65417, "sentence_idx": 1423, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yuille", "word_idx": 65430, "sentence_idx": 1424, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mao, W", "word_idx": 65437, "sentence_idx": 1425, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xu, Y", "word_idx": 65444, "sentence_idx": 1426, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yang, J", "word_idx": 65450, "sentence_idx": 1427, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wang, Z", "word_idx": 65458, "sentence_idx": 1428, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Huang, and A", "word_idx": 65466, "sentence_idx": 1429, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yuille", "word_idx": 65479, "sentence_idx": 1430, "label": "unlabeled"}, {"type": "text", "expr": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "word_idx": 65486, "sentence_idx": 1431, "label": "unlabeled"}, {"type": "text", "expr": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "word_idx": 65551, "sentence_idx": 1432, "label": "unlabeled"}, {"type": "text", "expr": "In  ICLR , 2015", "word_idx": 65616, "sentence_idx": 1433, "label": "unlabeled"}, {"type": "text", "expr": ", 2015", "word_idx": 65631, "sentence_idx": 1434, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mikolov, K", "word_idx": 65637, "sentence_idx": 1435, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chen, G", "word_idx": 65648, "sentence_idx": 1436, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Corrado, and J", "word_idx": 65656, "sentence_idx": 1437, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dean", "word_idx": 65671, "sentence_idx": 1438, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mikolov, K", "word_idx": 65676, "sentence_idx": 1439, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chen, G", "word_idx": 65687, "sentence_idx": 1440, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Corrado, and J", "word_idx": 65695, "sentence_idx": 1441, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dean", "word_idx": 65710, "sentence_idx": 1442, "label": "unlabeled"}, {"type": "text", "expr": "Efficient estimation of word representations in vector space", "word_idx": 65715, "sentence_idx": 1443, "label": "unlabeled"}, {"type": "text", "expr": "Efficient estimation of word representations in vector space", "word_idx": 65775, "sentence_idx": 1444, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1301", "word_idx": 65835, "sentence_idx": 1445, "label": "unlabeled"}, {"type": "text", "expr": "3781 , 2013", "word_idx": 65860, "sentence_idx": 1446, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1301", "word_idx": 65871, "sentence_idx": 1447, "label": "unlabeled"}, {"type": "text", "expr": ", 2013", "word_idx": 65896, "sentence_idx": 1448, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mitchell, X", "word_idx": 65902, "sentence_idx": 1449, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Han, J", "word_idx": 65914, "sentence_idx": 1450, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dodge, A", "word_idx": 65921, "sentence_idx": 1451, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mensch, A", "word_idx": 65930, "sentence_idx": 1452, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Goyal, A", "word_idx": 65940, "sentence_idx": 1453, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Berg, K", "word_idx": 65949, "sentence_idx": 1454, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yamaguchi,\nT", "word_idx": 65957, "sentence_idx": 1455, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Berg, K", "word_idx": 65970, "sentence_idx": 1456, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Stratos, and H", "word_idx": 65978, "sentence_idx": 1457, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Daum\u00e9\u00a0III", "word_idx": 65993, "sentence_idx": 1458, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mitchell, X", "word_idx": 66003, "sentence_idx": 1459, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Han, J", "word_idx": 66015, "sentence_idx": 1460, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dodge, A", "word_idx": 66022, "sentence_idx": 1461, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mensch, A", "word_idx": 66031, "sentence_idx": 1462, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Goyal, A", "word_idx": 66041, "sentence_idx": 1463, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Berg, K", "word_idx": 66050, "sentence_idx": 1464, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yamaguchi,\nT", "word_idx": 66058, "sentence_idx": 1465, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Berg, K", "word_idx": 66071, "sentence_idx": 1466, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Stratos, and H", "word_idx": 66079, "sentence_idx": 1467, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Daum\u00e9\u00a0III", "word_idx": 66094, "sentence_idx": 1468, "label": "unlabeled"}, {"type": "text", "expr": "Midge: Generating image descriptions from computer vision detections", "word_idx": 66104, "sentence_idx": 1469, "label": "unlabeled"}, {"type": "text", "expr": "Midge: Generating image descriptions from computer vision detections", "word_idx": 66172, "sentence_idx": 1470, "label": "unlabeled"}, {"type": "text", "expr": "In  EACL , 2012", "word_idx": 66240, "sentence_idx": 1471, "label": "unlabeled"}, {"type": "text", "expr": ", 2012", "word_idx": 66255, "sentence_idx": 1472, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Papineni, S", "word_idx": 66261, "sentence_idx": 1473, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Roukos, T", "word_idx": 66273, "sentence_idx": 1474, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ward, and W", "word_idx": 66283, "sentence_idx": 1475, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Papineni, S", "word_idx": 66295, "sentence_idx": 1476, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Roukos, T", "word_idx": 66307, "sentence_idx": 1477, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ward, and W", "word_idx": 66317, "sentence_idx": 1478, "label": "unlabeled"}, {"type": "text", "expr": "Bleu: a method for automatic evaluation of machine translation", "word_idx": 66329, "sentence_idx": 1479, "label": "unlabeled"}, {"type": "text", "expr": "Bleu: a method for automatic evaluation of machine translation", "word_idx": 66391, "sentence_idx": 1480, "label": "unlabeled"}, {"type": "text", "expr": "In  ACL , 2002", "word_idx": 66453, "sentence_idx": 1481, "label": "unlabeled"}, {"type": "text", "expr": ", 2002", "word_idx": 66467, "sentence_idx": 1482, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Pennington, R", "word_idx": 66473, "sentence_idx": 1483, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Socher, and C", "word_idx": 66487, "sentence_idx": 1484, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Manning", "word_idx": 66501, "sentence_idx": 1485, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Pennington, R", "word_idx": 66509, "sentence_idx": 1486, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Socher, and C", "word_idx": 66523, "sentence_idx": 1487, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Manning", "word_idx": 66537, "sentence_idx": 1488, "label": "unlabeled"}, {"type": "text", "expr": "Glove: Global vectors for word representation", "word_idx": 66545, "sentence_idx": 1489, "label": "unlabeled"}, {"type": "text", "expr": "Glove: Global vectors for word representation", "word_idx": 66590, "sentence_idx": 1490, "label": "unlabeled"}, {"type": "text", "expr": "In  EMNLP , 2014", "word_idx": 66635, "sentence_idx": 1491, "label": "unlabeled"}, {"type": "text", "expr": "EMNLP", "word_idx": 66651, "sentence_idx": 1492, "label": "unlabeled"}, {"type": "text", "expr": ", 2014", "word_idx": 66656, "sentence_idx": 1493, "label": "unlabeled"}, {"type": "text", "expr": " Plummer, L", "word_idx": 66662, "sentence_idx": 1494, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wang, C", "word_idx": 66673, "sentence_idx": 1495, "label": "unlabeled"}, {"type": "text", "expr": " Cervantes, J", "word_idx": 66681, "sentence_idx": 1496, "label": "unlabeled"}, {"type": "text", "expr": " Caicedo, J", "word_idx": 66694, "sentence_idx": 1497, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hockenmaier, and\nS", "word_idx": 66705, "sentence_idx": 1498, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lazebnik", "word_idx": 66724, "sentence_idx": 1499, "label": "unlabeled"}, {"type": "text", "expr": " Plummer, L", "word_idx": 66733, "sentence_idx": 1500, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wang, C", "word_idx": 66744, "sentence_idx": 1501, "label": "unlabeled"}, {"type": "text", "expr": " Cervantes, J", "word_idx": 66752, "sentence_idx": 1502, "label": "unlabeled"}, {"type": "text", "expr": " Caicedo, J", "word_idx": 66765, "sentence_idx": 1503, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hockenmaier, and\nS", "word_idx": 66776, "sentence_idx": 1504, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lazebnik", "word_idx": 66795, "sentence_idx": 1505, "label": "unlabeled"}, {"type": "text", "expr": "Flickr30k entities: Collecting region-to-phrase correspondences for\nricher image-to-sentence models", "word_idx": 66804, "sentence_idx": 1506, "label": "unlabeled"}, {"type": "text", "expr": "Flickr30k entities: Collecting region-to-phrase correspondences for\nricher image-to-sentence models", "word_idx": 66903, "sentence_idx": 1507, "label": "unlabeled"}, {"type": "text", "expr": "In  ICCV , 2015", "word_idx": 67002, "sentence_idx": 1508, "label": "unlabeled"}, {"type": "text", "expr": ", 2015", "word_idx": 67017, "sentence_idx": 1509, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ren, K", "word_idx": 67023, "sentence_idx": 1510, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, R", "word_idx": 67030, "sentence_idx": 1511, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Girshick, and J", "word_idx": 67036, "sentence_idx": 1512, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ren, K", "word_idx": 67052, "sentence_idx": 1513, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, R", "word_idx": 67059, "sentence_idx": 1514, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Girshick, and J", "word_idx": 67065, "sentence_idx": 1515, "label": "unlabeled"}, {"type": "text", "expr": "Faster r-cnn: Towards real-time object detection with region proposal\nnetworks", "word_idx": 67081, "sentence_idx": 1516, "label": "unlabeled"}, {"type": "text", "expr": "Faster r-cnn: Towards real-time object detection with region proposal\nnetworks", "word_idx": 67159, "sentence_idx": 1517, "label": "unlabeled"}, {"type": "text", "expr": "In  NIPS , 2015", "word_idx": 67237, "sentence_idx": 1518, "label": "unlabeled"}, {"type": "text", "expr": ", 2015", "word_idx": 67252, "sentence_idx": 1519, "label": "unlabeled"}, {"type": "text", "expr": " Rennie, E", "word_idx": 67258, "sentence_idx": 1520, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Marcheret, Y", "word_idx": 67268, "sentence_idx": 1521, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mroueh, J", "word_idx": 67281, "sentence_idx": 1522, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ross, and V", "word_idx": 67291, "sentence_idx": 1523, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Goel", "word_idx": 67303, "sentence_idx": 1524, "label": "unlabeled"}, {"type": "text", "expr": " Rennie, E", "word_idx": 67308, "sentence_idx": 1525, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Marcheret, Y", "word_idx": 67318, "sentence_idx": 1526, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mroueh, J", "word_idx": 67331, "sentence_idx": 1527, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ross, and V", "word_idx": 67341, "sentence_idx": 1528, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Goel", "word_idx": 67353, "sentence_idx": 1529, "label": "unlabeled"}, {"type": "text", "expr": "Self-critical sequence training for image captioning", "word_idx": 67358, "sentence_idx": 1530, "label": "unlabeled"}, {"type": "text", "expr": "Self-critical sequence training for image captioning", "word_idx": 67410, "sentence_idx": 1531, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2017", "word_idx": 67462, "sentence_idx": 1532, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 67477, "sentence_idx": 1533, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rohrbach, M", "word_idx": 67483, "sentence_idx": 1534, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rohrbach, R", "word_idx": 67495, "sentence_idx": 1535, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hu, T", "word_idx": 67507, "sentence_idx": 1536, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Darrell, and B", "word_idx": 67513, "sentence_idx": 1537, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Schiele", "word_idx": 67528, "sentence_idx": 1538, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rohrbach, M", "word_idx": 67536, "sentence_idx": 1539, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rohrbach, R", "word_idx": 67548, "sentence_idx": 1540, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hu, T", "word_idx": 67560, "sentence_idx": 1541, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Darrell, and B", "word_idx": 67566, "sentence_idx": 1542, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Schiele", "word_idx": 67581, "sentence_idx": 1543, "label": "unlabeled"}, {"type": "text", "expr": "Grounding of textual phrases in images by reconstruction", "word_idx": 67589, "sentence_idx": 1544, "label": "unlabeled"}, {"type": "text", "expr": "Grounding of textual phrases in images by reconstruction", "word_idx": 67645, "sentence_idx": 1545, "label": "unlabeled"}, {"type": "text", "expr": "In  ECCV , 2016", "word_idx": 67701, "sentence_idx": 1546, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 67716, "sentence_idx": 1547, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Simonyan and A", "word_idx": 67722, "sentence_idx": 1548, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zisserman", "word_idx": 67737, "sentence_idx": 1549, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Simonyan and A", "word_idx": 67747, "sentence_idx": 1550, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zisserman", "word_idx": 67762, "sentence_idx": 1551, "label": "unlabeled"}, {"type": "text", "expr": "Very deep convolutional networks for large-scale image recognition", "word_idx": 67772, "sentence_idx": 1552, "label": "unlabeled"}, {"type": "text", "expr": "Very deep convolutional networks for large-scale image recognition", "word_idx": 67838, "sentence_idx": 1553, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1409", "word_idx": 67904, "sentence_idx": 1554, "label": "unlabeled"}, {"type": "text", "expr": "1556 , 2014", "word_idx": 67929, "sentence_idx": 1555, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1409", "word_idx": 67940, "sentence_idx": 1556, "label": "unlabeled"}, {"type": "text", "expr": ", 2014", "word_idx": 67965, "sentence_idx": 1557, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Vedantam, C", "word_idx": 67971, "sentence_idx": 1558, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lawrence\u00a0Zitnick, and D", "word_idx": 67983, "sentence_idx": 1559, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Parikh", "word_idx": 68007, "sentence_idx": 1560, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Vedantam, C", "word_idx": 68014, "sentence_idx": 1561, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lawrence\u00a0Zitnick, and D", "word_idx": 68026, "sentence_idx": 1562, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Parikh", "word_idx": 68050, "sentence_idx": 1563, "label": "unlabeled"}, {"type": "text", "expr": "Cider: Consensus-based image description evaluation", "word_idx": 68057, "sentence_idx": 1564, "label": "unlabeled"}, {"type": "text", "expr": "Cider: Consensus-based image description evaluation", "word_idx": 68108, "sentence_idx": 1565, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2015", "word_idx": 68159, "sentence_idx": 1566, "label": "unlabeled"}, {"type": "text", "expr": ", 2015", "word_idx": 68174, "sentence_idx": 1567, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Venugopalan, L", "word_idx": 68180, "sentence_idx": 1568, "label": "unlabeled"}, {"type": "text", "expr": " Hendricks, M", "word_idx": 68195, "sentence_idx": 1569, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rohrbach, R", "word_idx": 68208, "sentence_idx": 1570, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mooney, T", "word_idx": 68220, "sentence_idx": 1571, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Darrell, and\nK", "word_idx": 68230, "sentence_idx": 1572, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Saenko", "word_idx": 68245, "sentence_idx": 1573, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Venugopalan, L", "word_idx": 68252, "sentence_idx": 1574, "label": "unlabeled"}, {"type": "text", "expr": " Hendricks, M", "word_idx": 68267, "sentence_idx": 1575, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rohrbach, R", "word_idx": 68280, "sentence_idx": 1576, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mooney, T", "word_idx": 68292, "sentence_idx": 1577, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Darrell, and\nK", "word_idx": 68302, "sentence_idx": 1578, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Saenko", "word_idx": 68317, "sentence_idx": 1579, "label": "unlabeled"}, {"type": "text", "expr": "Captioning images with diverse objects", "word_idx": 68324, "sentence_idx": 1580, "label": "unlabeled"}, {"type": "text", "expr": "Captioning images with diverse objects", "word_idx": 68362, "sentence_idx": 1581, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2017", "word_idx": 68400, "sentence_idx": 1582, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 68415, "sentence_idx": 1583, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Vinyals, M", "word_idx": 68421, "sentence_idx": 1584, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fortunato, and N", "word_idx": 68432, "sentence_idx": 1585, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Jaitly", "word_idx": 68449, "sentence_idx": 1586, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Vinyals, M", "word_idx": 68456, "sentence_idx": 1587, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fortunato, and N", "word_idx": 68467, "sentence_idx": 1588, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Jaitly", "word_idx": 68484, "sentence_idx": 1589, "label": "unlabeled"}, {"type": "text", "expr": "Pointer networks", "word_idx": 68491, "sentence_idx": 1590, "label": "unlabeled"}, {"type": "text", "expr": "Pointer networks", "word_idx": 68507, "sentence_idx": 1591, "label": "unlabeled"}, {"type": "text", "expr": "In  NIPS , 2015", "word_idx": 68523, "sentence_idx": 1592, "label": "unlabeled"}, {"type": "text", "expr": ", 2015", "word_idx": 68538, "sentence_idx": 1593, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Vinyals, A", "word_idx": 68544, "sentence_idx": 1594, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Toshev, S", "word_idx": 68555, "sentence_idx": 1595, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bengio, and D", "word_idx": 68565, "sentence_idx": 1596, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Erhan", "word_idx": 68579, "sentence_idx": 1597, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Vinyals, A", "word_idx": 68585, "sentence_idx": 1598, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Toshev, S", "word_idx": 68596, "sentence_idx": 1599, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bengio, and D", "word_idx": 68606, "sentence_idx": 1600, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Erhan", "word_idx": 68620, "sentence_idx": 1601, "label": "unlabeled"}, {"type": "text", "expr": "Show and tell: A neural image caption generator", "word_idx": 68626, "sentence_idx": 1602, "label": "unlabeled"}, {"type": "text", "expr": "Show and tell: A neural image caption generator", "word_idx": 68673, "sentence_idx": 1603, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2015", "word_idx": 68720, "sentence_idx": 1604, "label": "unlabeled"}, {"type": "text", "expr": ", 2015", "word_idx": 68735, "sentence_idx": 1605, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xu, J", "word_idx": 68741, "sentence_idx": 1606, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ba, R", "word_idx": 68747, "sentence_idx": 1607, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kiros, K", "word_idx": 68753, "sentence_idx": 1608, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Cho, A", "word_idx": 68762, "sentence_idx": 1609, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Courville, R", "word_idx": 68769, "sentence_idx": 1610, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Salakhutdinov, R", "word_idx": 68782, "sentence_idx": 1611, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zemel, and\nY", "word_idx": 68799, "sentence_idx": 1612, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bengio", "word_idx": 68812, "sentence_idx": 1613, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xu, J", "word_idx": 68819, "sentence_idx": 1614, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ba, R", "word_idx": 68825, "sentence_idx": 1615, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kiros, K", "word_idx": 68831, "sentence_idx": 1616, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Cho, A", "word_idx": 68840, "sentence_idx": 1617, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Courville, R", "word_idx": 68847, "sentence_idx": 1618, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Salakhutdinov, R", "word_idx": 68860, "sentence_idx": 1619, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zemel, and\nY", "word_idx": 68877, "sentence_idx": 1620, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bengio", "word_idx": 68890, "sentence_idx": 1621, "label": "unlabeled"}, {"type": "text", "expr": "Show, attend and tell: Neural image caption generation with visual\nattention", "word_idx": 68897, "sentence_idx": 1622, "label": "unlabeled"}, {"type": "text", "expr": "Show, attend and tell: Neural image caption generation with visual\nattention", "word_idx": 68973, "sentence_idx": 1623, "label": "unlabeled"}, {"type": "text", "expr": "In  ICML , 2015", "word_idx": 69049, "sentence_idx": 1624, "label": "unlabeled"}, {"type": "text", "expr": ", 2015", "word_idx": 69064, "sentence_idx": 1625, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yang, J", "word_idx": 69070, "sentence_idx": 1626, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lu, D", "word_idx": 69078, "sentence_idx": 1627, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Batra, and D", "word_idx": 69084, "sentence_idx": 1628, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Parikh", "word_idx": 69097, "sentence_idx": 1629, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yang, J", "word_idx": 69104, "sentence_idx": 1630, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lu, D", "word_idx": 69112, "sentence_idx": 1631, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Batra, and D", "word_idx": 69118, "sentence_idx": 1632, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Parikh", "word_idx": 69131, "sentence_idx": 1633, "label": "unlabeled"}, {"type": "text", "expr": "A faster pytorch implementation of faster r-cnn", "word_idx": 69138, "sentence_idx": 1634, "label": "unlabeled"}, {"type": "text", "expr": "A faster pytorch implementation of faster r-cnn", "word_idx": 69185, "sentence_idx": 1635, "label": "unlabeled"}, {"type": "text", "expr": "https://github", "word_idx": 69232, "sentence_idx": 1636, "label": "unlabeled"}, {"type": "text", "expr": "com/jwyang/faster-rcnn", "word_idx": 69246, "sentence_idx": 1637, "label": "unlabeled"}, {"type": "text", "expr": "pytorch , 2017", "word_idx": 69268, "sentence_idx": 1638, "label": "unlabeled"}, {"type": "text", "expr": "https://github", "word_idx": 69282, "sentence_idx": 1639, "label": "unlabeled"}, {"type": "text", "expr": "com/jwyang/faster-rcnn", "word_idx": 69296, "sentence_idx": 1640, "label": "unlabeled"}, {"type": "text", "expr": "pytorch", "word_idx": 69318, "sentence_idx": 1641, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 69325, "sentence_idx": 1642, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yang, Y", "word_idx": 69331, "sentence_idx": 1643, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yuan, Y", "word_idx": 69339, "sentence_idx": 1644, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wu, R", "word_idx": 69347, "sentence_idx": 1645, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Salakhutdinov, and W", "word_idx": 69353, "sentence_idx": 1646, "label": "unlabeled"}, {"type": "text", "expr": " Cohen", "word_idx": 69374, "sentence_idx": 1647, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yang, Y", "word_idx": 69380, "sentence_idx": 1648, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yuan, Y", "word_idx": 69388, "sentence_idx": 1649, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wu, R", "word_idx": 69396, "sentence_idx": 1650, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Salakhutdinov, and W", "word_idx": 69402, "sentence_idx": 1651, "label": "unlabeled"}, {"type": "text", "expr": " Cohen", "word_idx": 69423, "sentence_idx": 1652, "label": "unlabeled"}, {"type": "text", "expr": "Encode, review, and decode: Reviewer module for caption generation", "word_idx": 69429, "sentence_idx": 1653, "label": "unlabeled"}, {"type": "text", "expr": "Encode, review, and decode: Reviewer module for caption generation", "word_idx": 69495, "sentence_idx": 1654, "label": "unlabeled"}, {"type": "text", "expr": "In  NIPS , 2016", "word_idx": 69561, "sentence_idx": 1655, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 69576, "sentence_idx": 1656, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yao, Y", "word_idx": 69582, "sentence_idx": 1657, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Pan, Y", "word_idx": 69589, "sentence_idx": 1658, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Li, and T", "word_idx": 69596, "sentence_idx": 1659, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yao, Y", "word_idx": 69606, "sentence_idx": 1660, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Pan, Y", "word_idx": 69613, "sentence_idx": 1661, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Li, and T", "word_idx": 69620, "sentence_idx": 1662, "label": "unlabeled"}, {"type": "text", "expr": "Incorporating copying mechanism in image captioning for learning\nnovel objects", "word_idx": 69630, "sentence_idx": 1663, "label": "unlabeled"}, {"type": "text", "expr": "Incorporating copying mechanism in image captioning for learning\nnovel objects", "word_idx": 69708, "sentence_idx": 1664, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2017", "word_idx": 69786, "sentence_idx": 1665, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 69801, "sentence_idx": 1666, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0You, H", "word_idx": 69807, "sentence_idx": 1667, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Jin, Z", "word_idx": 69814, "sentence_idx": 1668, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wang, C", "word_idx": 69821, "sentence_idx": 1669, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fang, and J", "word_idx": 69829, "sentence_idx": 1670, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0You, H", "word_idx": 69841, "sentence_idx": 1671, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Jin, Z", "word_idx": 69848, "sentence_idx": 1672, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wang, C", "word_idx": 69855, "sentence_idx": 1673, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fang, and J", "word_idx": 69863, "sentence_idx": 1674, "label": "unlabeled"}, {"type": "text", "expr": "Image captioning with semantic attention", "word_idx": 69875, "sentence_idx": 1675, "label": "unlabeled"}, {"type": "text", "expr": "Image captioning with semantic attention", "word_idx": 69915, "sentence_idx": 1676, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2016", "word_idx": 69955, "sentence_idx": 1677, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 69970, "sentence_idx": 1678, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yu, P", "word_idx": 69976, "sentence_idx": 1679, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Poirson, S", "word_idx": 69982, "sentence_idx": 1680, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yang, A", "word_idx": 69993, "sentence_idx": 1681, "label": "unlabeled"}, {"type": "text", "expr": " Berg, and T", "word_idx": 70001, "sentence_idx": 1682, "label": "unlabeled"}, {"type": "text", "expr": " Berg", "word_idx": 70013, "sentence_idx": 1683, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yu, P", "word_idx": 70018, "sentence_idx": 1684, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Poirson, S", "word_idx": 70024, "sentence_idx": 1685, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yang, A", "word_idx": 70035, "sentence_idx": 1686, "label": "unlabeled"}, {"type": "text", "expr": " Berg, and T", "word_idx": 70043, "sentence_idx": 1687, "label": "unlabeled"}, {"type": "text", "expr": " Berg", "word_idx": 70055, "sentence_idx": 1688, "label": "unlabeled"}, {"type": "text", "expr": "Modeling context in referring expressions", "word_idx": 70060, "sentence_idx": 1689, "label": "unlabeled"}, {"type": "text", "expr": "Modeling context in referring expressions", "word_idx": 70101, "sentence_idx": 1690, "label": "unlabeled"}, {"type": "text", "expr": "In  ECCV , 2016", "word_idx": 70142, "sentence_idx": 1691, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 70157, "sentence_idx": 1692, "label": "unlabeled"}, {"type": "text", "expr": "Generated  on Tue Apr  3 11:28:05 2018 by", "word_idx": 70163, "sentence_idx": 1693, "label": "unlabeled"}, {"type": "text", "expr": "LaTeXML", "word_idx": 70204, "sentence_idx": 1694, "label": "unlabeled"}], "Fast_Parametric_Learning_with_Activation_Memorization": [{"type": "text", "expr": "Untitled Document", "word_idx": 0, "sentence_idx": 0, "label": "unlabeled"}, {"type": "text", "expr": "Abstract", "word_idx": 17, "sentence_idx": 1, "label": "unlabeled"}, {"type": "text", "expr": "Abstract", "word_idx": 25, "sentence_idx": 2, "label": "unlabeled"}, {"type": "text", "expr": "Neural networks trained with backpropagation often struggle to identify classes that have been observed a small number of times", "word_idx": 33, "sentence_idx": 3, "label": "unlabeled"}, {"type": "text", "expr": " In applications where most class labels are rare, such as language modelling, this can become a performance bottleneck", "word_idx": 160, "sentence_idx": 4, "label": "unlabeled"}, {"type": "text", "expr": " One potential remedy is to augment the network with a fast-learning non-parametric model which stores recent activations and class labels into an external memory", "word_idx": 279, "sentence_idx": 5, "label": "unlabeled"}, {"type": "text", "expr": " We explore a simplified architecture where we treat a subset of the model parameters as fast memory stores", "word_idx": 441, "sentence_idx": 6, "label": "unlabeled"}, {"type": "text", "expr": " This can help retain information over longer time intervals than a traditional memory, and does not require additional space or compute", "word_idx": 548, "sentence_idx": 7, "label": "unlabeled"}, {"type": "text", "expr": "\nIn the case of image classification, we display faster binding of novel classes on an Omniglot image curriculum task", "word_idx": 684, "sentence_idx": 8, "label": "unlabeled"}, {"type": "text", "expr": "\nWe also show improved performance for word-based language models on news reports (GigaWord), books (Project Gutenberg) and Wikipedia articles (WikiText-103) \u2014 the latter achieving a state-of-the-art perplexity of  $292$ ", "word_idx": 801, "sentence_idx": 9, "label": "unlabeled"}, {"type": "math", "expr": "$$29.2$$", "word_idx": 1022, "sentence_idx": 10, "label": "unlabeled"}, {"type": "text", "expr": "\\icmltitlerunning", "word_idx": 1026, "sentence_idx": 11, "label": "unlabeled"}, {"type": "text", "expr": "FastParametricLearningwithActivationMemorization", "word_idx": 1043, "sentence_idx": 12, "label": "unlabeled"}, {"type": "text", "expr": "FastParametricLearningwithActivationMemorization", "word_idx": 1091, "sentence_idx": 13, "label": "unlabeled"}, {"type": "text", "expr": "\\printAffiliationsAndNotice", "word_idx": 1139, "sentence_idx": 14, "label": "unlabeled"}, {"type": "text", "expr": "1  Introduction", "word_idx": 1166, "sentence_idx": 15, "label": "unlabeled"}, {"type": "text", "expr": "Neural networks can be trained to classify discrete outputs by appending a softmax output layer", "word_idx": 1181, "sentence_idx": 16, "label": "unlabeled"}, {"type": "text", "expr": " This is a linear map projecting the  $d$ -dimensional hidden output of the network to  $m$  outputs, where  $m$  is the number of distinct classes", "word_idx": 1276, "sentence_idx": 17, "label": "unlabeled"}, {"type": "text", "expr": " A softmax operator  \\citep bridle1990training is then applied to produce a probability distribution over classes", "word_idx": 1423, "sentence_idx": 18, "label": "unlabeled"}, {"type": "text", "expr": " The parameters in this softmax layer are typically optimized with the network\u2019s parameters by gradient descent", "word_idx": 1536, "sentence_idx": 19, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 1647, "sentence_idx": 20, "label": "unlabeled"}, {"type": "text", "expr": "We can think of the weights in the softmax layer  $\\theta\\in\\mathbb{R}^{m\\times d}$  as a set of  $m$  vectors  $\\theta[i];\\;i=1,\\ldots,m$  that each correspond to a given class", "word_idx": 1653, "sentence_idx": 21, "label": "unlabeled"}, {"type": "text", "expr": " When trained with a supervised loss, such as cross-entropy, each step of gradient descent pulls the parameter  $\\theta[y]$ , corresponding to the class label  $y$ , towards having a greater inner product with the network output  $h$ , and pushes all other parameters  $\\theta[j]\\;,\\,j\\neq y$  towards having a smaller inner product with  $h$ ", "word_idx": 1830, "sentence_idx": 22, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta\\in\\mathbb{R}^{m\\times d}$$", "word_idx": 2173, "sentence_idx": 23, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta[i];\\;i=1,\\ldots,m$$", "word_idx": 2204, "sentence_idx": 24, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta[y]$$", "word_idx": 2228, "sentence_idx": 25, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta[j]\\;,\\,j\\neq y$$", "word_idx": 2237, "sentence_idx": 26, "label": "unlabeled"}, {"type": "text", "expr": "One shortcoming of neural network classifiers trained with backpropagation is that they require many input examples for a given class in order to predict it with reasonable accuracy", "word_idx": 2258, "sentence_idx": 27, "label": "unlabeled"}, {"type": "text", "expr": " That is, many positive class examples and optimization steps are required to pull  $\\theta[i]$  towards a point in space where class  $i$  can then be recognized", "word_idx": 2439, "sentence_idx": 28, "label": "unlabeled"}, {"type": "text", "expr": " While the learner will have many opportunities to organize  $\\theta[i]$  parameters associated with frequent classes, infrequent class parameters will be poorly estimated", "word_idx": 2601, "sentence_idx": 29, "label": "unlabeled"}, {"type": "text", "expr": " In domains where new classes are frequently introduced, or large-scale classification problems where some classes are very infrequently observed, this estimation problem is potentially quite serious", "word_idx": 2772, "sentence_idx": 30, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta[i]$$", "word_idx": 2971, "sentence_idx": 31, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta[i]$$", "word_idx": 2980, "sentence_idx": 32, "label": "unlabeled"}, {"type": "text", "expr": "One approach to speed up learning, which has received revived interest, is meta-learning", "word_idx": 2989, "sentence_idx": 33, "label": "unlabeled"}, {"type": "text", "expr": " Here, meta-learning refers to algorithms which learn to produce or manipulate learning algorithms  \\citep thrun1998lifelong, hochreiter2001learning, and it operates by learning over a distribution of tasks or datasets", "word_idx": 3077, "sentence_idx": 34, "label": "unlabeled"}, {"type": "text", "expr": " A meta-learner applies knowledge from the global distribution of tasks to produce or optimize algorithms which specialize to a given task instance", "word_idx": 3295, "sentence_idx": 35, "label": "unlabeled"}, {"type": "text", "expr": " Meta-learning of neural networks has seen promising results for applications such as parameter optimization  \\citep andrychowicz2016learning, finn2017model and classification  \\citep santoro2016one, vinyals2016matching, zhou2018deep", "word_idx": 3442, "sentence_idx": 36, "label": "unlabeled"}, {"type": "text", "expr": " For classification, the networks are augmented with a differentiable external memory, and are trained with many rounds of data \u2014 with class labels permuted between episodes", "word_idx": 3675, "sentence_idx": 37, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 3848, "sentence_idx": 38, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 3854, "sentence_idx": 39, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 3860, "sentence_idx": 40, "label": "unlabeled"}, {"type": "text", "expr": "Meta-learning can be very powerful for few-shot learning in cases where there is a set of similar prior data to meta-learn over, however it may not be practical for standalone datasets", "word_idx": 3866, "sentence_idx": 41, "label": "unlabeled"}, {"type": "text", "expr": " For example, if one wants to model the grammar of computer code, it is unclear that a meta-learning system trained over natural language will be useful", "word_idx": 4050, "sentence_idx": 42, "label": "unlabeled"}, {"type": "text", "expr": " Also memory-based meta-learning requires backpropagating from the read time to the original write time, which is not well suited to applications where writes and reads are separated by long time steps of conditional computation", "word_idx": 4202, "sentence_idx": 43, "label": "unlabeled"}, {"type": "text", "expr": " In the case of modelling language, for example, infrequent words will not occur for large time intervals \u2014 rendering memory-based meta-learning challenging", "word_idx": 4430, "sentence_idx": 44, "label": "unlabeled"}, {"type": "text", "expr": "Meta-learning can be very powerful for few-shot learning in cases where there is a set of similar prior data to meta-learn over, however it may not be practical for standalone datasets", "word_idx": 4586, "sentence_idx": 45, "label": "unlabeled"}, {"type": "text", "expr": " For example, if one wants to model the grammar of computer code, it is unclear that a meta-learning system trained over natural language will be useful", "word_idx": 4770, "sentence_idx": 46, "label": "unlabeled"}, {"type": "text", "expr": " Also memory-based meta-learning requires backpropagating from the read time to the original write time, which is not well suited to applications where writes and reads are separated by long time steps of conditional computation", "word_idx": 4922, "sentence_idx": 47, "label": "unlabeled"}, {"type": "text", "expr": " In the case of modelling language, for example, infrequent words will not occur for large time intervals \u2014 rendering memory-based meta-learning challenging", "word_idx": 5150, "sentence_idx": 48, "label": "unlabeled"}, {"type": "text", "expr": "The task of statistical language modelling itself is interesting to investigate issues of binding new or infrequent classes, because most classes (words) are infrequent  \\citep zipf1935psychology and new classes naturally emerge over time", "word_idx": 5306, "sentence_idx": 49, "label": "unlabeled"}, {"type": "text", "expr": " Recent approaches to improve neural language models have involved augmenting the network with a non-parametric cache, which stores past hidden activations  $h_{t-n},\\ldots,h_{t-1}$  and corresponding labels,  $y_{t-n},\\ldots,y_{t-1}$   \\citep vinyals2015pointer, merity2016pointer, grave2016improving, kawakami2017learning, grave2017unbounded", "word_idx": 5544, "sentence_idx": 50, "label": "unlabeled"}, {"type": "text", "expr": " Attention over this cache provides better modelling of infrequent words that occur in a recent context, including previously unknown words  \\citep gulcehre2016pointing", "word_idx": 5887, "sentence_idx": 51, "label": "unlabeled"}, {"type": "text", "expr": " However there is a diminishing return to increasing the cache size  \\citep grave2016improving, and once rare words fall outside the recent context the boost in predictive performance expires", "word_idx": 6055, "sentence_idx": 52, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 6246, "sentence_idx": 53, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{t-n},\\ldots,h_{t-1}$$", "word_idx": 6252, "sentence_idx": 54, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t-n},\\ldots,y_{t-1}$$", "word_idx": 6274, "sentence_idx": 55, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 6296, "sentence_idx": 56, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 6302, "sentence_idx": 57, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 6308, "sentence_idx": 58, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:  Mixture model of parametric and non-parametric classifiers connected to a recurrent language model", "word_idx": 6314, "sentence_idx": 59, "label": "unlabeled"}, {"type": "text", "expr": " The non-parametric model (right hand side) stores a history of past activations and associated labels as key, value pairs", "word_idx": 6423, "sentence_idx": 60, "label": "unlabeled"}, {"type": "text", "expr": " The parametric model (left hand side) contains learnable parameters  $\\theta$  for each class in the output vocabulary  $V$ ", "word_idx": 6545, "sentence_idx": 61, "label": "unlabeled"}, {"type": "text", "expr": " We can view both components as key, value memories \u2014 one slow-moving, optimized with gradient descent, and one rapidly updating but ephemeral", "word_idx": 6670, "sentence_idx": 62, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:", "word_idx": 6812, "sentence_idx": 63, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta$$", "word_idx": 6821, "sentence_idx": 64, "label": "unlabeled"}, {"type": "text", "expr": "Motivated from these memory systems, we explore a very simple optimization procedure where the network accumulates activations  $h_{t}$  directly into the softmax layer weights  $\\theta[y_{t}]$  when a class  $y_{t}$  has been seen a small number of times, and uses gradient descent otherwise", "word_idx": 6827, "sentence_idx": 65, "label": "unlabeled"}, {"type": "text", "expr": " Accumulating or smoothing network activations into the weights actually corresponds to the well-known Hebbian learning update rule  $W[i,j]\\leftarrow\\frac{1}{n}\\sum_{t=1}^{n}x_{t}^{i}x_{t}^{j}$   \\citep hebb1949organization in the special case of classification on the output layer, where  $W,x_{t}^{i},x_{t}^{j}$  correspond to  $\\theta,h_{t},y_{t}$  respectively", "word_idx": 7119, "sentence_idx": 66, "label": "unlabeled"}, {"type": "text", "expr": " We see that mixing the two rules provides better initial representations and can also preserve these representations for much longer time spans", "word_idx": 7484, "sentence_idx": 67, "label": "unlabeled"}, {"type": "text", "expr": " This is because memorized activations for one class are not competing for space with activations from other (more frequent, say) classes \u2014 unlike a conventional external memory", "word_idx": 7628, "sentence_idx": 68, "label": "unlabeled"}, {"type": "text", "expr": " In this sense, the parameters become an instance of a quickly updated compressed memory, we explore this idea in Section  3", "word_idx": 7805, "sentence_idx": 69, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{t}$$", "word_idx": 7929, "sentence_idx": 70, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta[y_{t}]$$", "word_idx": 7934, "sentence_idx": 71, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}$$", "word_idx": 7947, "sentence_idx": 72, "label": "unlabeled"}, {"type": "math", "expr": "$$W[i,j]\\leftarrow\\frac{1}{n}\\sum_{t=1}^{n}x_{t}^{i}x_{t}^{j}$$", "word_idx": 7952, "sentence_idx": 73, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 8011, "sentence_idx": 74, "label": "unlabeled"}, {"type": "math", "expr": "$$W,x_{t}^{i},x_{t}^{j}$$", "word_idx": 8017, "sentence_idx": 75, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta,h_{t},y_{t}$$", "word_idx": 8038, "sentence_idx": 76, "label": "unlabeled"}, {"type": "text", "expr": "We demonstrate this model adapts quickly to novel classes in a simple image classification task using handwritten characters from Omniglot  ", "word_idx": 8056, "sentence_idx": 77, "label": "unlabeled"}, {"type": "text", "expr": " We then show it improves overall test perplexity for two medium-scale language modelling corpora, WikiText103 (wikipedia articles)  \\citep merity2016pointer and Project Gutenberg (books)   alongside a large-scale corpus GigaWord v5 (news articles)  \\citep parker2011english", "word_idx": 8196, "sentence_idx": 78, "label": "unlabeled"}, {"type": "text", "expr": " By splitting accuracy over word frequency buckets, we see improved perplexity for less frequent words", "word_idx": 8470, "sentence_idx": 79, "label": "unlabeled"}, {"type": "text", "expr": "Lake et\u00a0al", "word_idx": 8572, "sentence_idx": 80, "label": "unlabeled"}, {"type": "text", "expr": "(2015)Lake, Salakhutdinov, and Tenenbaum", "word_idx": 8582, "sentence_idx": 81, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 8622, "sentence_idx": 82, "label": "unlabeled"}, {"type": "text", "expr": "1 Project Gutenberg", "word_idx": 8628, "sentence_idx": 83, "label": "unlabeled"}, {"type": "text", "expr": " Retrieved January 2, 2018, from www", "word_idx": 8647, "sentence_idx": 84, "label": "unlabeled"}, {"type": "text", "expr": "gutenberg", "word_idx": 8683, "sentence_idx": 85, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 8692, "sentence_idx": 86, "label": "unlabeled"}, {"type": "text", "expr": "2  Background", "word_idx": 8698, "sentence_idx": 87, "label": "unlabeled"}, {"type": "text", "expr": "1  Memory", "word_idx": 8711, "sentence_idx": 88, "label": "unlabeled"}, {"type": "text", "expr": "There has been recent interest in models which store past hidden activations through time  $h_{1},h_{2},\\ldots,h_{t-1}$  into a memory matrix and query the contents with a differentiable attention mechanism", "word_idx": 8720, "sentence_idx": 89, "label": "unlabeled"}, {"type": "text", "expr": " This has been applied to machine translation  \\citep bahdanau2014neural, program induction  \\citep graves2014neural, graves2016hybrid, and question answering  \\citep sukhbaatar2015end", "word_idx": 8926, "sentence_idx": 90, "label": "unlabeled"}, {"type": "text", "expr": " Memory-augmented neural networks have also been successfully applied to language modelling  \\citep vinyals2015pointer, kawakami2017learning, merity2016pointer, grave2016improving, grave2017unbounded to facilitate the learning of unknown words, capture the tendency for globally rare words to be repeated in close proximity, and to quickly adapt the network to contextually relevant prior text  \\citep sprechmann2018memorybased", "word_idx": 9110, "sentence_idx": 91, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{1},h_{2},\\ldots,h_{t-1}$$", "word_idx": 9537, "sentence_idx": 92, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 9563, "sentence_idx": 93, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 9569, "sentence_idx": 94, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 9575, "sentence_idx": 95, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 9581, "sentence_idx": 96, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 9587, "sentence_idx": 97, "label": "unlabeled"}, {"type": "text", "expr": "There are many variants of how to read from memory and mix this information with the network computations", "word_idx": 9593, "sentence_idx": 98, "label": "unlabeled"}, {"type": "text", "expr": " One approach is to retrieve hidden activations and mix these with network activations in latent space  \\citep gulcehre2016pointing", "word_idx": 9698, "sentence_idx": 99, "label": "unlabeled"}, {"type": "text", "expr": " Another approach is a classic mixture model, as shown in Figure  1 ; the output probability distribution can be obtained by interpolating the probabilities  $p_{p},p_{np}$  from the parametric model and memory respectively", "word_idx": 9829, "sentence_idx": 100, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 10052, "sentence_idx": 101, "label": "unlabeled"}, {"type": "math", "expr": "$$p_{p},p_{np}$$", "word_idx": 10058, "sentence_idx": 102, "label": "unlabeled"}, {"type": "text", "expr": "For intuition we briefly explain a particular architecture, the Neural Cache  \\citep grave2016improving, whose operation is related to our model", "word_idx": 10070, "sentence_idx": 103, "label": "unlabeled"}, {"type": "text", "expr": " The cache is a store of the last  $n$  hidden activations along with their corresponding target output (next word) from a trained parametric language model, such as the Long Short Term Memory (LSTM)  \\citep hochreiter1997long", "word_idx": 10214, "sentence_idx": 104, "label": "unlabeled"}, {"type": "text", "expr": " The conditional probability of a word  $w$  occurring is proportional to the sum over kernalized inner product similarities between the current hidden state  $h_{t}$  and past hidden states when word  $w$  occurred", "word_idx": 10440, "sentence_idx": 105, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 10655, "sentence_idx": 106, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 10661, "sentence_idx": 107, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{t}$$", "word_idx": 10667, "sentence_idx": 108, "label": "unlabeled"}, {"type": "text", "expr": "$p_{c}(w\\mid h_{t})\\propto\\sum_{i=t-n}^{t-1}e^{h_{t}^{T}h_{i}}\\,\\mathbb{I}\\{y_{%\ni}=w\\}$", "word_idx": 10672, "sentence_idx": 109, "label": "unlabeled"}, {"type": "math", "expr": "$$p_{c}(w\\mid h_{t})\\propto\\sum_{i=t-n}^{t-1}e^{h_{t}^{T}h_{i}}\\,\\mathbb{I}\\{y_{%\ni}=w\\}$$", "word_idx": 10760, "sentence_idx": 110, "label": "unlabeled"}, {"type": "text", "expr": "Where  $\\mathbb{I}\\{p\\}=1$  if  $p$  is true,  $0$  otherwise", "word_idx": 10846, "sentence_idx": 111, "label": "unlabeled"}, {"type": "text", "expr": " This is then interpolated with the parametric language model using a fixed hyper-parameter, swept over during validation", "word_idx": 10907, "sentence_idx": 112, "label": "unlabeled"}, {"type": "text", "expr": " Although the cache is of fixed size  $n$ , it can be defined to be very large with sparse attention and efficient data-structures  ", "word_idx": 11028, "sentence_idx": 113, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbb{I}\\{p\\}=1$$", "word_idx": 11160, "sentence_idx": 114, "label": "unlabeled"}, {"type": "text", "expr": "Rae et\u00a0al", "word_idx": 11177, "sentence_idx": 115, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Rae, Hunt, Danihelka, Harley, Senior, Wayne, Graves,\nand Lillicrap", "word_idx": 11186, "sentence_idx": 116, "label": "unlabeled"}, {"type": "text", "expr": "Kaiser et\u00a0al", "word_idx": 11258, "sentence_idx": 117, "label": "unlabeled"}, {"type": "text", "expr": "(2017)Kaiser, Nachum, Roy, and\nBengio", "word_idx": 11270, "sentence_idx": 118, "label": "unlabeled"}, {"type": "text", "expr": "Grave et\u00a0al", "word_idx": 11307, "sentence_idx": 119, "label": "unlabeled"}, {"type": "text", "expr": "(2017)Grave, Cisse, and Joulin", "word_idx": 11318, "sentence_idx": 120, "label": "unlabeled"}, {"type": "text", "expr": "2  Language modelling", "word_idx": 11348, "sentence_idx": 121, "label": "unlabeled"}, {"type": "text", "expr": "We can model a sequence of text as the product of conditional word probabilities,", "word_idx": 11369, "sentence_idx": 122, "label": "unlabeled"}, {"type": "text", "expr": "We can model a sequence of text as the product of conditional word probabilities,", "word_idx": 11450, "sentence_idx": 123, "label": "unlabeled"}, {"type": "text", "expr": "$p(w_{1},w_{2},\\ldots,w_{t})=\\prod_{i=1}^{t}p(w_{i}\\mid w_{1},w_{2},\\ldots,w_{i%\n-1})$", "word_idx": 11531, "sentence_idx": 124, "label": "unlabeled"}, {"type": "math", "expr": "$$p(w_{1},w_{2},\\ldots,w_{t})=\\prod_{i=1}^{t}p(w_{i}\\mid w_{1},w_{2},\\ldots,w_{i%\n-1})$$", "word_idx": 11617, "sentence_idx": 125, "label": "unlabeled"}, {"type": "text", "expr": "which are estimated separately", "word_idx": 11701, "sentence_idx": 126, "label": "unlabeled"}, {"type": "text", "expr": " Traditional  $n$ -gram models take frequency-based estimates of these conditional probabilities with truncated contexts  $p_{n}=p(w_{i}\\mid w_{i-n},\\ldots,w_{i-1})$  and smooth between them to estimate the full conditional probability,  $p(w_{i}\\mid w_{1},\\ldots,w_{i-1})=\\sum_{j=1}^{n}\\lambda_{j}p_{j}$ ", "word_idx": 11731, "sentence_idx": 127, "label": "unlabeled"}, {"type": "text", "expr": " A popular approach is Kneser-Ney smoothing  \\citep kneser1995improved", "word_idx": 12036, "sentence_idx": 128, "label": "unlabeled"}, {"type": "text", "expr": " More recently, neural language models such as LSTMs and convolutional neural networks directly model the conditional probabilities through sequence-to-sequence training and achieve state-of-the-art performance in many established benchmarks  \\citep collobert2008unified, sundermeyer2012lstm, kalchbrenner2014convolutional, jozefowicz2016exploring, dauphin2016language, melis2017state", "word_idx": 12106, "sentence_idx": 129, "label": "unlabeled"}, {"type": "math", "expr": "$$p_{n}=p(w_{i}\\mid w_{i-n},\\ldots,w_{i-1})$$", "word_idx": 12490, "sentence_idx": 130, "label": "unlabeled"}, {"type": "math", "expr": "$$p(w_{i}\\mid w_{1},\\ldots,w_{i-1})=\\sum_{j=1}^{n}\\lambda_{j}p_{j}$$", "word_idx": 12531, "sentence_idx": 131, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 12595, "sentence_idx": 132, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 12601, "sentence_idx": 133, "label": "unlabeled"}, {"type": "text", "expr": "3  Model", "word_idx": 12607, "sentence_idx": 134, "label": "unlabeled"}, {"type": "text", "expr": "We propose Hebbian Softmax , a modification of the traditional softmax layer with an updated learning rule", "word_idx": 12615, "sentence_idx": 135, "label": "unlabeled"}, {"type": "text", "expr": " Hebbian Softmax contains the same linear map from the hidden state to the output vocabulary, but learns by smoothing hidden activations into the weight parameters for novel classes whilst concurrently applying gradient descent", "word_idx": 12721, "sentence_idx": 136, "label": "unlabeled"}, {"type": "text", "expr": " This is to facilitate faster binding of novel classes, and improve learning of infrequent classes", "word_idx": 12948, "sentence_idx": 137, "label": "unlabeled"}, {"type": "text", "expr": " We note this corresponds to a learning rule that transitions from Hebbian learning to gradient descent, and we will show that the combination of the two learning rules works better than either one in isolation", "word_idx": 13046, "sentence_idx": 138, "label": "unlabeled"}, {"type": "text", "expr": "We propose Hebbian Softmax , a modification of the traditional softmax layer with an updated learning rule", "word_idx": 13256, "sentence_idx": 139, "label": "unlabeled"}, {"type": "text", "expr": " Hebbian Softmax contains the same linear map from the hidden state to the output vocabulary, but learns by smoothing hidden activations into the weight parameters for novel classes whilst concurrently applying gradient descent", "word_idx": 13362, "sentence_idx": 140, "label": "unlabeled"}, {"type": "text", "expr": " This is to facilitate faster binding of novel classes, and improve learning of infrequent classes", "word_idx": 13589, "sentence_idx": 141, "label": "unlabeled"}, {"type": "text", "expr": " We note this corresponds to a learning rule that transitions from Hebbian learning to gradient descent, and we will show that the combination of the two learning rules works better than either one in isolation", "word_idx": 13687, "sentence_idx": 142, "label": "unlabeled"}, {"type": "text", "expr": "Many of the features of Hebbian Softmax are motivated from memory systems, and the theory of complementary learning systems in the brain  ", "word_idx": 13897, "sentence_idx": 143, "label": "unlabeled"}, {"type": "text", "expr": " During training, the weights corresponding to a given class will initially correspond to a compressed  episodic memory store \u2014 with new activations memorized and older activations eventually forgotten", "word_idx": 14035, "sentence_idx": 144, "label": "unlabeled"}, {"type": "text", "expr": "McClelland et\u00a0al", "word_idx": 14236, "sentence_idx": 145, "label": "unlabeled"}, {"type": "text", "expr": "(1995)McClelland, McNaughton, and\nO\u2019reilly", "word_idx": 14252, "sentence_idx": 146, "label": "unlabeled"}, {"type": "text", "expr": "2 The memory is compressed because multiple activations corresponding to the same class are smoothed into one vector, instead of being stored separately", "word_idx": 14294, "sentence_idx": 147, "label": "unlabeled"}, {"type": "text", "expr": "The parameters of the softmax layer are treated both as regular slow-adapting network parameters through which gradients flow to the rest of the network, and fast-adapting memory slots which are updated sparsely without altering the rest of the network", "word_idx": 14446, "sentence_idx": 148, "label": "unlabeled"}, {"type": "text", "expr": " In comparison to an external memory, the advantage of Hebbian Softmax is that it is simple to implement and requires almost no additional space or computation", "word_idx": 14698, "sentence_idx": 149, "label": "unlabeled"}, {"type": "text", "expr": "The parameters of the softmax layer are treated both as regular slow-adapting network parameters through which gradients flow to the rest of the network, and fast-adapting memory slots which are updated sparsely without altering the rest of the network", "word_idx": 14857, "sentence_idx": 150, "label": "unlabeled"}, {"type": "text", "expr": " In comparison to an external memory, the advantage of Hebbian Softmax is that it is simple to implement and requires almost no additional space or computation", "word_idx": 15109, "sentence_idx": 151, "label": "unlabeled"}, {"type": "text", "expr": "We will describe the learning rule in detail, and contrast the conditional probabilities from Hebbian Softmax to those generated by a non-parametric cache", "word_idx": 15268, "sentence_idx": 152, "label": "unlabeled"}, {"type": "text", "expr": " We also generalize the memorization procedure in Section  3", "word_idx": 15422, "sentence_idx": 153, "label": "unlabeled"}, {"type": "text", "expr": "3  as an instance of a secondary fast-learning overfitting procedure with respect to a euclidean objective, and explore several promising variant objective functions", "word_idx": 15482, "sentence_idx": 154, "label": "unlabeled"}, {"type": "text", "expr": "1  Update Rule", "word_idx": 15647, "sentence_idx": 155, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:  Update rule", "word_idx": 15661, "sentence_idx": 156, "label": "unlabeled"}, {"type": "text", "expr": " Here the vector  $\\hat{\\theta}_{t+05}$  denotes the parameters  $\\theta_{t}[y_{t}]$  of the final layer softmax corresponding to the active class  $y_{t}$  after one step of gradient descent", "word_idx": 15683, "sentence_idx": 157, "label": "unlabeled"}, {"type": "text", "expr": " This is interpolated with the hidden activation at the time of class occurrence,  $h_{t}$ ", "word_idx": 15874, "sentence_idx": 158, "label": "unlabeled"}, {"type": "text", "expr": " The remaining parameters are optimized with gradient descent", "word_idx": 15965, "sentence_idx": 159, "label": "unlabeled"}, {"type": "text", "expr": " Here,  $\\mathbb{I}\\{y_{t}\\}$  is the one-hot target vector,  $V$  denotes the vocabulary of classes, and  $c_{t}$  is defined to be a counter of class occurrences during training \u2014 which is used to anneal  $\\lambda_{t}$  as described in ( 4 )", "word_idx": 16026, "sentence_idx": 160, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:", "word_idx": 16269, "sentence_idx": 161, "label": "unlabeled"}, {"type": "math", "expr": "$$\\hat{\\theta}_{t+0.5}$$", "word_idx": 16278, "sentence_idx": 162, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta_{t}[y_{t}]$$", "word_idx": 16298, "sentence_idx": 163, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}$$", "word_idx": 16315, "sentence_idx": 164, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{t}$$", "word_idx": 16320, "sentence_idx": 165, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbb{I}\\{y_{t}\\}$$", "word_idx": 16325, "sentence_idx": 166, "label": "unlabeled"}, {"type": "math", "expr": "$$c_{t}$$", "word_idx": 16344, "sentence_idx": 167, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda_{t}$$", "word_idx": 16349, "sentence_idx": 168, "label": "unlabeled"}, {"type": "text", "expr": "Given the weights of a linear projection  $\\theta\\in\\mathbb{R}^{d\\times m}$  in the final softmax layer of a network, we calculate the gradient descent update with respect to a cross-entropy loss,", "word_idx": 16360, "sentence_idx": 169, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta\\in\\mathbb{R}^{d\\times m}$$", "word_idx": 16556, "sentence_idx": 170, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\hat{\\theta}_{t+05}[i]\\leftarrow\\begin{cases}\\theta_{t}[i]-%\n\\alpha\\,(p_{i}-1)\\,h_{t}&i=y_{t}\\\\\n\\theta_{t}[i]-\\alpha\\,p_{i}\\,h_{t}&i\\neq y_{t}\\end{cases}$", "word_idx": 16587, "sentence_idx": 171, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\hat{\\theta}_{t+0.5}[i]\\leftarrow\\begin{cases}\\theta_{t}[i]-%\n\\alpha\\,(p_{i}-1)\\,h_{t}&i=y_{t}\\\\\n\\theta_{t}[i]-\\alpha\\,p_{i}\\,h_{t}&i\\neq y_{t}\\end{cases}$$", "word_idx": 16755, "sentence_idx": 172, "label": "unlabeled"}, {"type": "text", "expr": "where  $p_{i}=e^{h_{t}^{T}\\theta_{i}}/\\sum_{j=1}^{n}e^{h_{t}^{T}\\theta_{j}}$  is the probability output from the softmax, and  $\\alpha$  is the learning rate", "word_idx": 16922, "sentence_idx": 173, "label": "unlabeled"}, {"type": "text", "expr": " In practice the gradient descent update  $\\hat{\\theta}_{t+05}$  can be calculated with adaptive optimizers, such as RMSProp  \\citep tieleman2012lecture", "word_idx": 17079, "sentence_idx": 174, "label": "unlabeled"}, {"type": "text", "expr": " This is interpolated with the previous layer\u2019s hidden activation  $h_{t}$  for the active class  $y_{t}$ ,", "word_idx": 17231, "sentence_idx": 175, "label": "unlabeled"}, {"type": "math", "expr": "$$p_{i}=e^{h_{t}^{T}\\theta_{i}}/\\sum_{j=1}^{n}e^{h_{t}^{T}\\theta_{j}}$$", "word_idx": 17338, "sentence_idx": 176, "label": "unlabeled"}, {"type": "math", "expr": "$$\\alpha$$", "word_idx": 17405, "sentence_idx": 177, "label": "unlabeled"}, {"type": "math", "expr": "$$\\hat{\\theta}_{t+0.5}$$", "word_idx": 17411, "sentence_idx": 178, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 17431, "sentence_idx": 179, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{t}$$", "word_idx": 17437, "sentence_idx": 180, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}$$", "word_idx": 17442, "sentence_idx": 181, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\theta_{t+1}[i]\\leftarrow\\begin{cases}\\lambda_{t}\\,h_{t}+(1-%\n\\lambda_{t})\\,\\hat{\\theta}_{t+05}[i]&i=y_{t}\\\\\n\\hat{\\theta}_{t+05}[i]&i\\neq y_{t}\\;,\\end{cases}$", "word_idx": 17447, "sentence_idx": 182, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\theta_{t+1}[i]\\leftarrow\\begin{cases}\\lambda_{t}\\,h_{t}+(1-%\n\\lambda_{t})\\,\\hat{\\theta}_{t+0.5}[i]&i=y_{t}\\\\\n\\hat{\\theta}_{t+0.5}[i]&i\\neq y_{t}\\;,\\end{cases}$$", "word_idx": 17619, "sentence_idx": 183, "label": "unlabeled"}, {"type": "text", "expr": "as illustrated in Figure  2 ", "word_idx": 17791, "sentence_idx": 184, "label": "unlabeled"}, {"type": "text", "expr": " When  $\\lambda_{t}=1$  this corresponds to the rule  $\\theta_{t+1}\\leftarrow h_{t}\\cdot\\mathbb{I}\\{y_{t}\\}$  where  $\\mathbb{I}\\{y_{t}\\}\\in[0,1]^{m}$  is a one-hot target vector", "word_idx": 17819, "sentence_idx": 185, "label": "unlabeled"}, {"type": "text", "expr": " In this case Hebbian update rule,  $W_{ij}\\leftarrow x_{i}x_{j}$  for  $x_{i}=h_{t}$  the hidden output and  $x_{j}=\\mathbb{I}\\{y_{t}\\}$  the target", "word_idx": 17997, "sentence_idx": 186, "label": "unlabeled"}, {"type": "text", "expr": " Naturally when  $\\lambda=0$  this is gradient descent, and so we see Hebbian Softmax is mixture of the two learning rules", "word_idx": 18146, "sentence_idx": 187, "label": "unlabeled"}, {"type": "text", "expr": " All remaining parameters in the model are optimized with gradient descent as usual", "word_idx": 18268, "sentence_idx": 188, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda_{t}=1$$", "word_idx": 18351, "sentence_idx": 189, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta_{t+1}\\leftarrow h_{t}\\cdot\\mathbb{I}\\{y_{t}\\}$$", "word_idx": 18364, "sentence_idx": 190, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbb{I}\\{y_{t}\\}\\in[0,1]^{m}$$", "word_idx": 18416, "sentence_idx": 191, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{ij}\\leftarrow x_{i}x_{j}$$", "word_idx": 18447, "sentence_idx": 192, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{i}=h_{t}$$", "word_idx": 18474, "sentence_idx": 193, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{j}=\\mathbb{I}\\{y_{t}\\}$$", "word_idx": 18485, "sentence_idx": 194, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda=0$$", "word_idx": 18510, "sentence_idx": 195, "label": "unlabeled"}, {"type": "text", "expr": "When mixing the two learning rules, we would like to benefit from fast initial learning of classes that have not been seen many times, along with stable consolidation of frequently seen classes", "word_idx": 18519, "sentence_idx": 196, "label": "unlabeled"}, {"type": "text", "expr": " As such we do not want  $\\lambda_{t}$  to be constant, but instead something that is eventually annealed to zero", "word_idx": 18712, "sentence_idx": 197, "label": "unlabeled"}, {"type": "text", "expr": " We add an additional counter array  $\\mathbf{c}\\in\\mathbb{Z}^{m}$  which counts class occurrences, and propose an annealing function of", "word_idx": 18825, "sentence_idx": 198, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda_{t}$$", "word_idx": 18961, "sentence_idx": 199, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{c}\\in\\mathbb{Z}^{m}$$", "word_idx": 18972, "sentence_idx": 200, "label": "unlabeled"}, {"type": "text", "expr": "$\\lambda_{t}=\\max(1\\,/\\,\\mathbf{c}[y_{t}],\\;\\gamma)\\,\\cdot\\,\\mathbb{I}\\{\\mathbf%\n{c}[y_{t}]<T\\}$", "word_idx": 18999, "sentence_idx": 201, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda_{t}=\\max(1\\,/\\,\\mathbf{c}[y_{t}],\\;\\gamma)\\,\\cdot\\,\\mathbb{I}\\{\\mathbf%\n{c}[y_{t}]<T\\}$$", "word_idx": 19095, "sentence_idx": 202, "label": "unlabeled"}, {"type": "text", "expr": "where  $\\gamma,\\,T$  are tuning parameters", "word_idx": 19189, "sentence_idx": 203, "label": "unlabeled"}, {"type": "text", "expr": "  $T$  is the number of class occurrences before switching completely to gradient descent and  $\\gamma$  is the minimum activation mixing parameter", "word_idx": 19231, "sentence_idx": 204, "label": "unlabeled"}, {"type": "text", "expr": " Although heuristic, we found this worked well in practice vs", "word_idx": 19378, "sentence_idx": 205, "label": "unlabeled"}, {"type": "text", "expr": " a constant  $\\lambda$  or pure annealing  $\\lambda_{t}=1/c[y_{t}]$ ", "word_idx": 19439, "sentence_idx": 206, "label": "unlabeled"}, {"type": "text", "expr": " If training from scratch, we suggest setting  $\\gamma=1/N_{min}$  and  $T=N_{min}\\times(\\hbox{\\# epochs until convergence})$  where  $N_{min}$  is the minimum number of occurrences of any class in a training epoch", "word_idx": 19507, "sentence_idx": 207, "label": "unlabeled"}, {"type": "text", "expr": " This is to ensure we smooth over many class examples in a given epoch, and the memorization of activations continues until the representation of  $h_{t}$  stabilizes", "word_idx": 19721, "sentence_idx": 208, "label": "unlabeled"}, {"type": "text", "expr": " We describe the full algorithm in Algorithm  4 , including details for training with minibatches", "word_idx": 19887, "sentence_idx": 209, "label": "unlabeled"}, {"type": "text", "expr": "\n {algorithm} [tb]\n Hebbian Softmax batched update {algorithmic} \\STATE \u2014 At iteration  $0$ \\STATE $\\gamma\\leftarrow$  min", "word_idx": 19984, "sentence_idx": 210, "label": "unlabeled"}, {"type": "text", "expr": " discount (hyper-parameter)\n \\STATE $T\\leftarrow$  smoothing limit (hyper-parameter)\n \\STATE $M\\leftarrow$  num", "word_idx": 20106, "sentence_idx": 211, "label": "unlabeled"}, {"type": "text", "expr": " classes\n \\STATE $B\\leftarrow$  batch size\n \\STATE $\\mathbf{c}_{0}[i]\\leftarrow 0;\\quad i=1,\\ldots,M$ \\STATE \u2014 At iteration  $t$ \\STATE $\\mathbf{h}_{t,1:B}\\leftarrow$  softmax inputs\n \\STATE $\\mathbf{p}_{t,1:B}\\leftarrow$  softmax outputs\n \\STATE $\\mathbf{y}_{t,1:B}\\leftarrow$  target labels\n \\STATE $\\hat{\\theta}_{t+05}\\leftarrow$ \\FOR $i=1,\\ldots,M$ \\STATE $n_{t,i}\\leftarrow\\sum_{j=1}^{B}\\,\\mathbb{I}\\{y_{t,j}=i\\}$ \\IF $n_{t,i}>0$ \\STATE $\\lambda_{t,i}\\leftarrow\\max(1/\\mathbf{c}_{t}[i],\\gamma)\\,\\mathbb{I}\\{\\mathbf{c%\n}_{t}[i]<T\\}$ \\STATE $\\bar{h}_{t,i}\\leftarrow\\frac{1}{n_{t,i}}\\sum_{j=1}^{B}h_{t,j}\\mathbb{I}\\{y_{t,%\nj}=i\\}$ \\STATE $\\theta_{t+1}\\leftarrow\\lambda_{t,i}\\bar{h}_{t,i}+(1-\\lambda_{t,i})\\hat{\\theta}%\n_{t+05}[i]$ \\ELSE \\STATE $\\theta_{t+1}\\leftarrow\\hat{\\theta}_{t+05}[i]$ \\ENDIF \\STATE $\\mathbf{c}_{t+1}[i]\\leftarrow\\mathbf{c}_{t}[i]+n_{t,i}$ \\ENDFOR", "word_idx": 20217, "sentence_idx": 212, "label": "unlabeled"}, {"type": "math", "expr": "$$\\gamma,\\,T$$", "word_idx": 21088, "sentence_idx": 213, "label": "unlabeled"}, {"type": "math", "expr": "$$\\gamma$$", "word_idx": 21098, "sentence_idx": 214, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda$$", "word_idx": 21104, "sentence_idx": 215, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda_{t}=1/c[y_{t}]$$", "word_idx": 21111, "sentence_idx": 216, "label": "unlabeled"}, {"type": "math", "expr": "$$\\gamma=1/N_{min}$$", "word_idx": 21133, "sentence_idx": 217, "label": "unlabeled"}, {"type": "math", "expr": "$$T=N_{min}\\times(\\hbox{\\# epochs until convergence})$$", "word_idx": 21149, "sentence_idx": 218, "label": "unlabeled"}, {"type": "math", "expr": "$$N_{min}$$", "word_idx": 21200, "sentence_idx": 219, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{t}$$", "word_idx": 21207, "sentence_idx": 220, "label": "unlabeled"}, {"type": "text", "expr": "{algorithm}", "word_idx": 21212, "sentence_idx": 221, "label": "unlabeled"}, {"type": "text", "expr": "Hebbian Softmax batched update", "word_idx": 21223, "sentence_idx": 222, "label": "unlabeled"}, {"type": "text", "expr": "{algorithmic}", "word_idx": 21253, "sentence_idx": 223, "label": "unlabeled"}, {"type": "text", "expr": "\\STATE", "word_idx": 21266, "sentence_idx": 224, "label": "unlabeled"}, {"type": "text", "expr": "\\STATE", "word_idx": 21272, "sentence_idx": 225, "label": "unlabeled"}, {"type": "math", "expr": "$$\\gamma\\leftarrow$$", "word_idx": 21278, "sentence_idx": 226, "label": "unlabeled"}, {"type": "text", "expr": "\\STATE", "word_idx": 21294, "sentence_idx": 227, "label": "unlabeled"}, {"type": "math", "expr": "$$T\\leftarrow$$", "word_idx": 21300, "sentence_idx": 228, "label": "unlabeled"}, {"type": "text", "expr": "\\STATE", "word_idx": 21311, "sentence_idx": 229, "label": "unlabeled"}, {"type": "math", "expr": "$$M\\leftarrow$$", "word_idx": 21317, "sentence_idx": 230, "label": "unlabeled"}, {"type": "text", "expr": "\\STATE", "word_idx": 21328, "sentence_idx": 231, "label": "unlabeled"}, {"type": "math", "expr": "$$B\\leftarrow$$", "word_idx": 21334, "sentence_idx": 232, "label": "unlabeled"}, {"type": "text", "expr": "\\STATE", "word_idx": 21345, "sentence_idx": 233, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{c}_{0}[i]\\leftarrow 0;\\quad i=1,\\ldots,M$$", "word_idx": 21351, "sentence_idx": 234, "label": "unlabeled"}, {"type": "text", "expr": "\\STATE", "word_idx": 21399, "sentence_idx": 235, "label": "unlabeled"}, {"type": "text", "expr": "\\STATE", "word_idx": 21405, "sentence_idx": 236, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{h}_{t,1:B}\\leftarrow$$", "word_idx": 21411, "sentence_idx": 237, "label": "unlabeled"}, {"type": "text", "expr": "\\STATE", "word_idx": 21439, "sentence_idx": 238, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{p}_{t,1:B}\\leftarrow$$", "word_idx": 21445, "sentence_idx": 239, "label": "unlabeled"}, {"type": "text", "expr": "\\STATE", "word_idx": 21473, "sentence_idx": 240, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{y}_{t,1:B}\\leftarrow$$", "word_idx": 21479, "sentence_idx": 241, "label": "unlabeled"}, {"type": "text", "expr": "\\STATE", "word_idx": 21507, "sentence_idx": 242, "label": "unlabeled"}, {"type": "math", "expr": "$$\\hat{\\theta}_{t+0.5}\\leftarrow$$", "word_idx": 21513, "sentence_idx": 243, "label": "unlabeled"}, {"type": "text", "expr": "SGD $(\\theta_{t},\\mathbf{h}_{t,1:B},\\mathbf{p}_{t,1:B},\\mathbf{y}_{1:B})$", "word_idx": 21543, "sentence_idx": 244, "label": "unlabeled"}, {"type": "math", "expr": "$$(\\theta_{t},\\mathbf{h}_{t,1:B},\\mathbf{p}_{t,1:B},\\mathbf{y}_{1:B})$$", "word_idx": 21616, "sentence_idx": 245, "label": "unlabeled"}, {"type": "math", "expr": "$$i=1,\\ldots,M$$", "word_idx": 21683, "sentence_idx": 246, "label": "unlabeled"}, {"type": "text", "expr": "\\STATE", "word_idx": 21695, "sentence_idx": 247, "label": "unlabeled"}, {"type": "math", "expr": "$$n_{t,i}\\leftarrow\\sum_{j=1}^{B}\\,\\mathbb{I}\\{y_{t,j}=i\\}$$", "word_idx": 21701, "sentence_idx": 248, "label": "unlabeled"}, {"type": "math", "expr": "$$n_{t,i}>0$$", "word_idx": 21757, "sentence_idx": 249, "label": "unlabeled"}, {"type": "text", "expr": "\\STATE", "word_idx": 21766, "sentence_idx": 250, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda_{t,i}\\leftarrow\\max(1/\\mathbf{c}_{t}[i],\\gamma)\\,\\mathbb{I}\\{\\mathbf{c%\n}_{t}[i]<T\\}$$", "word_idx": 21772, "sentence_idx": 251, "label": "unlabeled"}, {"type": "text", "expr": "\\STATE", "word_idx": 21864, "sentence_idx": 252, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{h}_{t,i}\\leftarrow\\frac{1}{n_{t,i}}\\sum_{j=1}^{B}h_{t,j}\\mathbb{I}\\{y_{t,%\nj}=i\\}$$", "word_idx": 21870, "sentence_idx": 253, "label": "unlabeled"}, {"type": "text", "expr": "\\STATE", "word_idx": 21956, "sentence_idx": 254, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta_{t+1}\\leftarrow\\lambda_{t,i}\\bar{h}_{t,i}+(1-\\lambda_{t,i})\\hat{\\theta}%\n_{t+0.5}[i]$$", "word_idx": 21962, "sentence_idx": 255, "label": "unlabeled"}, {"type": "text", "expr": "\\ELSE", "word_idx": 22053, "sentence_idx": 256, "label": "unlabeled"}, {"type": "text", "expr": "\\STATE", "word_idx": 22058, "sentence_idx": 257, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta_{t+1}\\leftarrow\\hat{\\theta}_{t+0.5}[i]$$", "word_idx": 22064, "sentence_idx": 258, "label": "unlabeled"}, {"type": "text", "expr": "\\ENDIF", "word_idx": 22109, "sentence_idx": 259, "label": "unlabeled"}, {"type": "text", "expr": "\\STATE", "word_idx": 22115, "sentence_idx": 260, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{c}_{t+1}[i]\\leftarrow\\mathbf{c}_{t}[i]+n_{t,i}$$", "word_idx": 22121, "sentence_idx": 261, "label": "unlabeled"}, {"type": "text", "expr": "\\ENDFOR", "word_idx": 22175, "sentence_idx": 262, "label": "unlabeled"}, {"type": "text", "expr": "The final layer trains with a two-speed dynamic", "word_idx": 22182, "sentence_idx": 263, "label": "unlabeled"}, {"type": "text", "expr": " For some training steps the full network will be optimized slowly via gradient descent as usual (when frequently-encountered classes are observed), and for other time steps a sparse subset of parameters will rapidly change", "word_idx": 22229, "sentence_idx": 264, "label": "unlabeled"}, {"type": "text", "expr": " The remaining network parameters are optimized with gradient descent", "word_idx": 22452, "sentence_idx": 265, "label": "unlabeled"}, {"type": "text", "expr": "The final layer trains with a two-speed dynamic", "word_idx": 22521, "sentence_idx": 266, "label": "unlabeled"}, {"type": "text", "expr": " For some training steps the full network will be optimized slowly via gradient descent as usual (when frequently-encountered classes are observed), and for other time steps a sparse subset of parameters will rapidly change", "word_idx": 22568, "sentence_idx": 267, "label": "unlabeled"}, {"type": "text", "expr": " The remaining network parameters are optimized with gradient descent", "word_idx": 22791, "sentence_idx": 268, "label": "unlabeled"}, {"type": "text", "expr": "It is worth noting that simply increasing the learning rate of the softmax layer, or running multiple steps of optimization on rare class inputs, would not achieve the same effect", "word_idx": 22860, "sentence_idx": 269, "label": "unlabeled"}, {"type": "text", "expr": " The value  $\\theta[y_{t}]$  would indeed be pulled towards a large inner product with  $h_{t}$ , however neighbouring parameters  $\\theta[i];\\;i\\neq y_{t}$  would be pushed towards a large negative inner product with  $h_{t}$  and this could lead to catastrophic forgetting of previously consolidated classes", "word_idx": 23039, "sentence_idx": 270, "label": "unlabeled"}, {"type": "text", "expr": " Instead we allow gradient descent to slowly push neighbouring parameters away, and thus disambiguate similar classes in a gradual fashion", "word_idx": 23348, "sentence_idx": 271, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta[y_{t}]$$", "word_idx": 23486, "sentence_idx": 272, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{t}$$", "word_idx": 23499, "sentence_idx": 273, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta[i];\\;i\\neq y_{t}$$", "word_idx": 23504, "sentence_idx": 274, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{t}$$", "word_idx": 23527, "sentence_idx": 275, "label": "unlabeled"}, {"type": "text", "expr": "2  Relation to cache models", "word_idx": 23532, "sentence_idx": 276, "label": "unlabeled"}, {"type": "text", "expr": "We can consider the weights constructed from the above optimization procedure as a compressed memory storing historic activations", "word_idx": 23559, "sentence_idx": 277, "label": "unlabeled"}, {"type": "text", "expr": " We contrast the output probabilities of Hebbian Softmax with those produced from a non-parametric cache model", "word_idx": 23688, "sentence_idx": 278, "label": "unlabeled"}, {"type": "text", "expr": "We can consider the weights constructed from the above optimization procedure as a compressed memory storing historic activations", "word_idx": 23798, "sentence_idx": 279, "label": "unlabeled"}, {"type": "text", "expr": " We contrast the output probabilities of Hebbian Softmax with those produced from a non-parametric cache model", "word_idx": 23927, "sentence_idx": 280, "label": "unlabeled"}, {"type": "text", "expr": "Recall the conditional probability of a class,  $w$ , given a cache of previous activations ( 1 )", "word_idx": 24037, "sentence_idx": 281, "label": "unlabeled"}, {"type": "text", "expr": " If we set  $I_{w}(j)$  to be the time step of j-th most recent occurrence of  $w$ , then we can re-write the cache probability,", "word_idx": 24134, "sentence_idx": 282, "label": "unlabeled"}, {"type": "math", "expr": "$$I_{w}(j)$$", "word_idx": 24262, "sentence_idx": 283, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle p_{c}(w\\mid h_{t})$", "word_idx": 24270, "sentence_idx": 284, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle p_{c}(w\\mid h_{t})$$", "word_idx": 24304, "sentence_idx": 285, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\propto\\displaystyle\\sum_{i=t-n}^{t-1}e^{h_{t}^{T}h_{i}}\\mathbb{I%\n}\\{y_{i}=w\\}$", "word_idx": 24336, "sentence_idx": 286, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\propto\\displaystyle\\sum_{i=t-n}^{t-1}e^{h_{t}^{T}h_{i}}\\mathbb{I%\n}\\{y_{i}=w\\}$$", "word_idx": 24430, "sentence_idx": 287, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\displaystyle\\sum_{j=1}^{N_{w}}e^{g(j)\\,h_{t}^{T}h_{I_{w}(j)}}$", "word_idx": 24522, "sentence_idx": 288, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\displaystyle\\sum_{j=1}^{N_{w}}e^{g(j)\\,h_{t}^{T}h_{I_{w}(j)}}$$", "word_idx": 24600, "sentence_idx": 289, "label": "unlabeled"}, {"type": "text", "expr": "where  $g(j)=-\\infty\\hbox{ if }j<t-n\\hbox{ and }1$  otherwise, is a weighting function which places uniform weight to the attention over classes in the past  $n$  time steps", "word_idx": 24676, "sentence_idx": 290, "label": "unlabeled"}, {"type": "text", "expr": " However if we wish to characterize infrequent classes, we may want a weighting scheme with a larger time horizon that has a smooth decay", "word_idx": 24849, "sentence_idx": 291, "label": "unlabeled"}, {"type": "math", "expr": "$$g(j)=-\\infty\\hbox{ if }j<t-n\\hbox{ and }1$$", "word_idx": 24986, "sentence_idx": 292, "label": "unlabeled"}, {"type": "text", "expr": "plugging this into our softmax conditional probability,", "word_idx": 25027, "sentence_idx": 293, "label": "unlabeled"}, {"type": "text", "expr": "If we modified the cache to have infinite memory capacity and used a geometric weighting scheme to decay the contribution of the  $j$ -th most recent activation corresponding to the given class, e", "word_idx": 25082, "sentence_idx": 294, "label": "unlabeled"}, {"type": "text", "expr": "  $g(j)=\\lambda\\,(1-\\lambda)^{j-1}$ , then the resulting conditional probability is,", "word_idx": 25278, "sentence_idx": 295, "label": "unlabeled"}, {"type": "math", "expr": "$$g(j)=\\lambda\\,(1-\\lambda)^{j-1}$$", "word_idx": 25362, "sentence_idx": 296, "label": "unlabeled"}, {"type": "text", "expr": "$\\tilde{p}_{c}(w\\mid h_{t})\\propto\\sum_{j=1}^{N_{w}}e^{\\lambda\\,(1-\\lambda)^{j-%\n1}\\,h_{t}^{T}h_{I_{w}(j)}}$", "word_idx": 25393, "sentence_idx": 297, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{p}_{c}(w\\mid h_{t})\\propto\\sum_{j=1}^{N_{w}}e^{\\lambda\\,(1-\\lambda)^{j-%\n1}\\,h_{t}^{T}h_{I_{w}(j)}}$$", "word_idx": 25501, "sentence_idx": 298, "label": "unlabeled"}, {"type": "text", "expr": "where  $N_{w}$  is the total number of occurrences of class  $w$ ", "word_idx": 25607, "sentence_idx": 299, "label": "unlabeled"}, {"type": "text", "expr": " Let us now consider the conditional probability from Hebbian Softmax for class  $w$ , where  $w$  has been observed less than  $T$  times", "word_idx": 25672, "sentence_idx": 300, "label": "unlabeled"}, {"type": "text", "expr": " If  $\\theta$  has not received large gradients from the occurrence of nearby neighboring classes, and we fix  $\\lambda_{t}=\\lambda$  over time, then ( 3 ) gives", "word_idx": 25810, "sentence_idx": 301, "label": "unlabeled"}, {"type": "math", "expr": "$$N_{w}$$", "word_idx": 25971, "sentence_idx": 302, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta$$", "word_idx": 25976, "sentence_idx": 303, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda_{t}=\\lambda$$", "word_idx": 25982, "sentence_idx": 304, "label": "unlabeled"}, {"type": "text", "expr": "$\\theta_{i}\\approx\\sum_{j=1}^{N_{w}}\\lambda\\,(1-\\lambda)^{j-1}h_{I_{w}(j)}\\;,$", "word_idx": 26001, "sentence_idx": 305, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta_{i}\\approx\\sum_{j=1}^{N_{w}}\\lambda\\,(1-\\lambda)^{j-1}h_{I_{w}(j)}\\;,$$", "word_idx": 26079, "sentence_idx": 306, "label": "unlabeled"}, {"type": "text", "expr": "plugging this into our softmax conditional probability,", "word_idx": 26155, "sentence_idx": 307, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle p_{\\theta}(w\\mid h_{t})\\propto e^{h_{t}^{T}\\theta_{w}}$", "word_idx": 26210, "sentence_idx": 308, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle p_{\\theta}(w\\mid h_{t})\\propto e^{h_{t}^{T}\\theta_{w}}$$", "word_idx": 26280, "sentence_idx": 309, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\approx e^{h_{t}^{T}\\sum_{j=1}^{N_{w}}\\lambda\\,(1-\\lambda)^{j-1}h%\n_{I_{w}(j)}}$", "word_idx": 26348, "sentence_idx": 310, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\approx e^{h_{t}^{T}\\sum_{j=1}^{N_{w}}\\lambda\\,(1-\\lambda)^{j-1}h%\n_{I_{w}(j)}}$$", "word_idx": 26442, "sentence_idx": 311, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\prod_{j=1}^{N_{w}}e^{\\lambda\\,(1-\\lambda)^{j-1}h_{t}^{T}h_{I_{w%\n}(j)}}\\,$", "word_idx": 26534, "sentence_idx": 312, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\prod_{j=1}^{N_{w}}e^{\\lambda\\,(1-\\lambda)^{j-1}h_{t}^{T}h_{I_{w%\n}(j)}}\\,.$$", "word_idx": 26624, "sentence_idx": 313, "label": "unlabeled"}, {"type": "text", "expr": "we see the parametric Hebbian Softmax actually becomes a proxy for the conditional probability output by the non-parametric infinite cache model  $\\tilde{p}_{c}$ ", "word_idx": 26713, "sentence_idx": 314, "label": "unlabeled"}, {"type": "text", "expr": " Past activations now have a geometric contribution to the probability, versus the cache\u2019s arithmetic reduction ( 6 )", "word_idx": 26875, "sentence_idx": 315, "label": "unlabeled"}, {"type": "text", "expr": " This form is useful because we can compute  $p_{sm}$  much more efficiently than  $\\tilde{p}_{c}$  and it does not require storing the entire history of past activations", "word_idx": 26992, "sentence_idx": 316, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{p}_{c}$$", "word_idx": 27162, "sentence_idx": 317, "label": "unlabeled"}, {"type": "math", "expr": "$$p_{sm}$$", "word_idx": 27175, "sentence_idx": 318, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{p}_{c}$$", "word_idx": 27181, "sentence_idx": 319, "label": "unlabeled"}, {"type": "text", "expr": "3  Alternate Objective Functions", "word_idx": 27194, "sentence_idx": 320, "label": "unlabeled"}, {"type": "text", "expr": "We briefly discuss a generalization of the Hebbian Softmax update by casting it as an overfitting procedure to an inner objective function", "word_idx": 27226, "sentence_idx": 321, "label": "unlabeled"}, {"type": "text", "expr": " Recall equation ( 3 ) for parameters corresponding to the active class,", "word_idx": 27364, "sentence_idx": 322, "label": "unlabeled"}, {"type": "text", "expr": "$\\theta_{t+1}[i]\\leftarrow\\lambda_{t}\\,h_{t}+(1-\\lambda_{t})\\,\\hat{\\theta}_{t+0%\n5}[i]$", "word_idx": 27436, "sentence_idx": 323, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta_{t+1}[i]\\leftarrow\\lambda_{t}\\,h_{t}+(1-\\lambda_{t})\\,\\hat{\\theta}_{t+0%\n.5}[i].$$", "word_idx": 27523, "sentence_idx": 324, "label": "unlabeled"}, {"type": "text", "expr": "We can re-phrase this as smoothing  $\\hat{\\theta}_{t+05}[i]$  with the trivial solution to a euclidean objective function, which we overfit to", "word_idx": 27610, "sentence_idx": 325, "label": "unlabeled"}, {"type": "math", "expr": "$$\\hat{\\theta}_{t+0.5}[i]$$", "word_idx": 27752, "sentence_idx": 326, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\theta_{t+1}[i]\\leftarrow\\lambda w^{*}+(1-\\lambda)\\,\\hat{\\theta}_%\n{t+05}[i]$", "word_idx": 27775, "sentence_idx": 327, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\theta_{t+1}[i]\\leftarrow\\lambda w^{*}+(1-\\lambda)\\,\\hat{\\theta}_%\n{t+0.5}[i]$$", "word_idx": 27866, "sentence_idx": 328, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle w^{*}\\leftarrow\\operatornamewithlimits{arg\\,max}_{w}-||w-h_{t}||%\n_{2}$", "word_idx": 27956, "sentence_idx": 329, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle w^{*}\\leftarrow\\operatornamewithlimits{arg\\,max}_{w}-||w-h_{t}||%\n_{2}$$", "word_idx": 28042, "sentence_idx": 330, "label": "unlabeled"}, {"type": "text", "expr": "From this perspective we are performing a two-level optimization procedure", "word_idx": 28126, "sentence_idx": 331, "label": "unlabeled"}, {"type": "text", "expr": " The outer optimization loop is the mixture of gradient descent and exponential smoothing, and the inner optimization loop determines a good value for  $w^{*}$  based on the activation  $h_{t}$  and the current parameters", "word_idx": 28200, "sentence_idx": 332, "label": "unlabeled"}, {"type": "math", "expr": "$$w^{*}$$", "word_idx": 28421, "sentence_idx": 333, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{t}$$", "word_idx": 28426, "sentence_idx": 334, "label": "unlabeled"}, {"type": "text", "expr": "We consider several other objective functions that are more expensive to compute, but may be preferable to a simple Euclidean distance", "word_idx": 28431, "sentence_idx": 335, "label": "unlabeled"}, {"type": "text", "expr": " Notably, switching to inner product similarity ( IP ), and also incorporating a cost to parameter similarity ( SVM, Smax ) to push  $w^{*}$  towards  $h_{t}$  but away from neighbouring parameters \u2014 to avoid confusion or interference with other classes", "word_idx": 28565, "sentence_idx": 336, "label": "unlabeled"}, {"type": "text", "expr": " As we keep neighbouring parameters fixed, we hope to avoid the catastrophic forgetting typically associated with model overfitting", "word_idx": 28818, "sentence_idx": 337, "label": "unlabeled"}, {"type": "text", "expr": " We list the set of objectives considered,", "word_idx": 28949, "sentence_idx": 338, "label": "unlabeled"}, {"type": "text", "expr": "SVM, Smax", "word_idx": 28991, "sentence_idx": 339, "label": "unlabeled"}, {"type": "math", "expr": "$$w^{*}$$", "word_idx": 29000, "sentence_idx": 340, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{t}$$", "word_idx": 29005, "sentence_idx": 341, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle w^{*}\\leftarrow\\operatornamewithlimits{arg\\,max}_{w}\\;g(w)$", "word_idx": 29010, "sentence_idx": 342, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle w^{*}\\leftarrow\\operatornamewithlimits{arg\\,max}_{w}\\;g(w)$$", "word_idx": 29084, "sentence_idx": 343, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle g_{\\tiny\\hbox{L2}}(w)=-||w-h_{t}||_{2}$", "word_idx": 29156, "sentence_idx": 344, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle g_{\\tiny\\hbox{L2}}(w)=-||w-h_{t}||_{2}$$", "word_idx": 29210, "sentence_idx": 345, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle g_{\\tiny\\hbox{IP}}(w)=w^{T}h_{t}$", "word_idx": 29262, "sentence_idx": 346, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle g_{\\tiny\\hbox{IP}}(w)=w^{T}h_{t}$$", "word_idx": 29310, "sentence_idx": 347, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle g_{\\tiny\\hbox{SVM}}(w)=w^{T}h_{t}-\\kern-100pt\\displaystyle\\sum_%\n{\\theta_{j}\\in\\mathcal{N}_{k}(h_{t})}\\kern-100pt\\xi\\,w^{T}\\theta_{j}\\,\\cdot\\,%\n\\mathbb{I}(w^{T}\\theta_{j}>\\epsilon)$", "word_idx": 29356, "sentence_idx": 348, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle g_{\\tiny\\hbox{SVM}}(w)=w^{T}h_{t}-\\kern-10.0pt\\displaystyle\\sum_%\n{\\theta_{j}\\in\\mathcal{N}_{k}(h_{t})}\\kern-10.0pt\\xi\\,w^{T}\\theta_{j}\\,\\cdot\\,%\n\\mathbb{I}(w^{T}\\theta_{j}>\\epsilon)$$", "word_idx": 29552, "sentence_idx": 349, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle g_{\\tiny\\hbox{Smax}}(w)=e^{w^{T}h_{t}}/\\displaystyle\\sum_{\\theta%\n_{j}\\in\\mathcal{N}_{k}(h_{t})}e^{w^{T}\\theta_{j}}$", "word_idx": 29748, "sentence_idx": 350, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle g_{\\tiny\\hbox{Smax}}(w)=e^{w^{T}h_{t}}/\\displaystyle\\sum_{\\theta%\n_{j}\\in\\mathcal{N}_{k}(h_{t})}e^{w^{T}\\theta_{j}}$$", "word_idx": 29879, "sentence_idx": 351, "label": "unlabeled"}, {"type": "text", "expr": "where  $\\mathcal{N}_{k}(h_{t})$  refers to the  $k$  nearest parameters to the activation  $h_{t}$  that do not correspond to  $y_{t}$ , the class label", "word_idx": 30008, "sentence_idx": 352, "label": "unlabeled"}, {"type": "text", "expr": " Including all  $M$  parameters in  $\\theta_{t}$  would make the inner optimization loop very slow, so we choose a sparse subset  $k\\ll M$ ", "word_idx": 30160, "sentence_idx": 353, "label": "unlabeled"}, {"type": "text", "expr": " These are all optimized under the hard norm constraint  $||w||_{2}<10$  with gradient descent for multiple steps, typically  $20$ , at a given point in training", "word_idx": 30299, "sentence_idx": 354, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{N}_{k}(h_{t})$$", "word_idx": 30460, "sentence_idx": 355, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{t}$$", "word_idx": 30482, "sentence_idx": 356, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}$$", "word_idx": 30487, "sentence_idx": 357, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta_{t}$$", "word_idx": 30492, "sentence_idx": 358, "label": "unlabeled"}, {"type": "math", "expr": "$$k\\ll M$$", "word_idx": 30502, "sentence_idx": 359, "label": "unlabeled"}, {"type": "math", "expr": "$$||w||_{2}<10$$", "word_idx": 30508, "sentence_idx": 360, "label": "unlabeled"}, {"type": "text", "expr": "4  Results", "word_idx": 30520, "sentence_idx": 361, "label": "unlabeled"}, {"type": "text", "expr": "1  Image Curriculum", "word_idx": 30530, "sentence_idx": 362, "label": "unlabeled"}, {"type": "text", "expr": "We apply Hebbian Softmax to the problem of image classification", "word_idx": 30549, "sentence_idx": 363, "label": "unlabeled"}, {"type": "text", "expr": " We create a simple curriculum task using Omniglot data  , where a subset of classes ( $30$ ) are initially provided, and  $5$  new classes are added when test performance exceeds a threshold ( $60\\%$ )", "word_idx": 30612, "sentence_idx": 364, "label": "unlabeled"}, {"type": "text", "expr": " Although this is a toy setup, it allows us to investigate the basic properties of fast class binding without other confounding factors, found in real-world problems", "word_idx": 30814, "sentence_idx": 365, "label": "unlabeled"}, {"type": "text", "expr": "Lake et\u00a0al", "word_idx": 30979, "sentence_idx": 366, "label": "unlabeled"}, {"type": "text", "expr": "(2015)Lake, Salakhutdinov, and Tenenbaum", "word_idx": 30989, "sentence_idx": 367, "label": "unlabeled"}, {"type": "math", "expr": "$$60\\%$$", "word_idx": 31029, "sentence_idx": 368, "label": "unlabeled"}, {"type": "text", "expr": "Omniglot contains handwritten characters from  $50$  alphabets, totalling  $1,623$  unique character classes", "word_idx": 31033, "sentence_idx": 369, "label": "unlabeled"}, {"type": "text", "expr": " There are  $20$  examples per class", "word_idx": 31141, "sentence_idx": 370, "label": "unlabeled"}, {"type": "text", "expr": " We partition the first  $5$  examples per class to a test set, and assign the rest for training", "word_idx": 31177, "sentence_idx": 371, "label": "unlabeled"}, {"type": "math", "expr": "$$1,623$$", "word_idx": 31273, "sentence_idx": 372, "label": "unlabeled"}, {"type": "text", "expr": "We use the same architectural setup as  Matching Networks   \\citep vinyals2016matching where the images are re-sized to  $28\\times 28$  and a  $4$  layer convolutional neural network is used", "word_idx": 31278, "sentence_idx": 373, "label": "unlabeled"}, {"type": "text", "expr": " Each layer has  $64$  filters,  $3\\times 3$  convolutions, batch normalization, ReLU activations, and  $2\\times 2$  max pooling", "word_idx": 31468, "sentence_idx": 374, "label": "unlabeled"}, {"type": "text", "expr": " Each channel maps the input to a scalar, so the resulting hidden size is  $64$ ", "word_idx": 31596, "sentence_idx": 375, "label": "unlabeled"}, {"type": "text", "expr": " All weight parameter in the softmax are initialized with Glorot initialization  \\citep glorot2010understanding", "word_idx": 31676, "sentence_idx": 376, "label": "unlabeled"}, {"type": "text", "expr": " Models were trained with  $20$ % dropout on the final layer and a small amount of data augmentation was applied to training examples (rotation  $\\in[-30,30]$ , translation) to avoid over-fitting", "word_idx": 31787, "sentence_idx": 377, "label": "unlabeled"}, {"type": "text", "expr": " Otherwise the models quickly plateau on a low level", "word_idx": 31982, "sentence_idx": 378, "label": "unlabeled"}, {"type": "text", "expr": " For the Hebbian Softmax update, we store the pristine hidden activation pre-dropout", "word_idx": 32034, "sentence_idx": 379, "label": "unlabeled"}, {"type": "text", "expr": " Unlike many one-shot Omniglot papers, we do not train in a meta-learning setup \u2014 namely, labels are not shuffled between episodes", "word_idx": 32118, "sentence_idx": 380, "label": "unlabeled"}, {"type": "text", "expr": "Matching Networks", "word_idx": 32248, "sentence_idx": 381, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 32265, "sentence_idx": 382, "label": "unlabeled"}, {"type": "math", "expr": "$$28\\times 28$$", "word_idx": 32271, "sentence_idx": 383, "label": "unlabeled"}, {"type": "math", "expr": "$$3\\times 3$$", "word_idx": 32282, "sentence_idx": 384, "label": "unlabeled"}, {"type": "math", "expr": "$$2\\times 2$$", "word_idx": 32291, "sentence_idx": 385, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 32300, "sentence_idx": 386, "label": "unlabeled"}, {"type": "math", "expr": "$$\\in[-30,30]$$", "word_idx": 32306, "sentence_idx": 387, "label": "unlabeled"}, {"type": "text", "expr": "We trained the convnet classifier with RMSProp and swept over learning rates  $\\alpha\\in[1e-4,4e-2]$  to find the fastest-learning baseline softmax model", "word_idx": 32317, "sentence_idx": 388, "label": "unlabeled"}, {"type": "text", "expr": " A value of  $\\alpha=8e-3$  was the largest learning rate to provide stable learning (see Figure  8  in Appendix  B )", "word_idx": 32470, "sentence_idx": 389, "label": "unlabeled"}, {"type": "text", "expr": " We then compared the regular softmax layer with Hebbian Softmax , both placed on the deep convnet", "word_idx": 32587, "sentence_idx": 390, "label": "unlabeled"}, {"type": "math", "expr": "$$\\alpha\\in[1e-4,4e-2]$$", "word_idx": 32685, "sentence_idx": 391, "label": "unlabeled"}, {"type": "math", "expr": "$$\\alpha=8e-3$$", "word_idx": 32705, "sentence_idx": 392, "label": "unlabeled"}, {"type": "text", "expr": "If we inspect the number of steps spent on each level averaged over  $10$  seeds, we see in Figure  3  that the model is noticeably more data efficient after  $80$  total classes", "word_idx": 32716, "sentence_idx": 393, "label": "unlabeled"}, {"type": "text", "expr": " Although it is still far from one-shot, there is a  $1-2X$  data efficiency gain on average", "word_idx": 32894, "sentence_idx": 394, "label": "unlabeled"}, {"type": "text", "expr": " In Appendix  B , Figure  9  we show the progression of the curriculum in terms of the number of classes shown versus training steps", "word_idx": 32986, "sentence_idx": 395, "label": "unlabeled"}, {"type": "text", "expr": " Hebbian Softmax progresses through the curriculum at identical speed to the softmax until around  $1,000$  steps of training, from then on it begins to bind new classes and complete the each level faster", "word_idx": 33118, "sentence_idx": 396, "label": "unlabeled"}, {"type": "math", "expr": "$$1-2X$$", "word_idx": 33322, "sentence_idx": 397, "label": "unlabeled"}, {"type": "math", "expr": "$$1,000$$", "word_idx": 33326, "sentence_idx": 398, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:  Number of training steps taken to complete each level on the Omniglot curriculum task", "word_idx": 33331, "sentence_idx": 399, "label": "unlabeled"}, {"type": "text", "expr": " Comparisons between the Hebbian Softmax and softmax baseline are averaged over  $10$  independent seeds", "word_idx": 33427, "sentence_idx": 400, "label": "unlabeled"}, {"type": "text", "expr": " As classes are sampled uniformly, we expect the number of steps taken to level completion to rise linearly with the number of classes", "word_idx": 33531, "sentence_idx": 401, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:", "word_idx": 33665, "sentence_idx": 402, "label": "unlabeled"}, {"type": "text", "expr": "2  Language Modelling", "word_idx": 33674, "sentence_idx": 403, "label": "unlabeled"}, {"type": "text", "expr": "We would like to evaluate Hebbian Softmax in the context of a large-scale classification task, where some classes are infrequently observed", "word_idx": 33695, "sentence_idx": 404, "label": "unlabeled"}, {"type": "text", "expr": " Word-level language modelling is an ideal fit because it satisfies both criteria, and there are established performance benchmarks", "word_idx": 33834, "sentence_idx": 405, "label": "unlabeled"}, {"type": "text", "expr": " Some large-scale language modelling corpora require the use of efficient softmax approximations, such as the adaptive softmax  \\citep grave2016efficient or hierarchical softmax  \\citep goodman2001classes due to the very large vocabulary size", "word_idx": 33965, "sentence_idx": 406, "label": "unlabeled"}, {"type": "text", "expr": " To reduce confounding factors, we restrict ourselves to applications where the full softmax can be used", "word_idx": 34207, "sentence_idx": 407, "label": "unlabeled"}, {"type": "text", "expr": " We investigate two medium-sized corpora, WikiText-103 which contains just over  $100M$  tokens derived from Wikipedia articles  \\citep merity2016pointer, and Gutenberg which contains a subset of open-access texts from Project Gutenberg listed in Appendix  A", "word_idx": 34311, "sentence_idx": 408, "label": "unlabeled"}, {"type": "text", "expr": " The idea is that Wikipedia articles should cover factual information, where the style of writing is somewhat consistent and named entities may appear across many articles; whereas books should be more self-contained (unique named entities) and stylistically different", "word_idx": 34569, "sentence_idx": 409, "label": "unlabeled"}, {"type": "text", "expr": " We also consider a very large corpus, GigaWord v5, which is a collection of articles from eight press associations exceeding a decade\u2019s worth of global news", "word_idx": 34837, "sentence_idx": 410, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 34994, "sentence_idx": 411, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 35000, "sentence_idx": 412, "label": "unlabeled"}, {"type": "math", "expr": "$$100M$$", "word_idx": 35006, "sentence_idx": 413, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 35010, "sentence_idx": 414, "label": "unlabeled"}, {"type": "text", "expr": "1  Model details", "word_idx": 35016, "sentence_idx": 415, "label": "unlabeled"}, {"type": "text", "expr": "For WikiText-103 we swept over LSTM hidden sizes  $\\{1024,2048,4096\\}$ , no", "word_idx": 35032, "sentence_idx": 416, "label": "unlabeled"}, {"type": "text", "expr": " LSTM layers  $\\{1,2\\}$ , embedding dropout  $\\{0,01,02,03\\}$ , use of layer norm  \\citep ba2016layer  $\\{\\textit{True},\\textit{False}\\}$ , and whether to share the input/output embedding parameters  $\\{\\textit{True},\\textit{False}\\}$  totalling  $96$  parameters", "word_idx": 35107, "sentence_idx": 417, "label": "unlabeled"}, {"type": "text", "expr": " A single-layer LSTM with  $2048$  hidden units with tied embedding parameters and an input dropout rate of  $03$  was selected, and we used this same model configuration for the other language corpora", "word_idx": 35370, "sentence_idx": 418, "label": "unlabeled"}, {"type": "text", "expr": " We trained the models on  $8$  P100 Nvidia GPUs by splitting the batch size into  $8$  sub-batches, sending them to each GPU and summing the resulting gradients", "word_idx": 35571, "sentence_idx": 419, "label": "unlabeled"}, {"type": "text", "expr": " The total batch size used was  $512$  and a sequence length of  $100$  was chosen", "word_idx": 35732, "sentence_idx": 420, "label": "unlabeled"}, {"type": "text", "expr": " We did not pass the state of the LSTM between sequences during training, however the state is passed during evaluation", "word_idx": 35814, "sentence_idx": 421, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{1024,2048,4096\\}$$", "word_idx": 35933, "sentence_idx": 422, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{1,2\\}$$", "word_idx": 35951, "sentence_idx": 423, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{0,0.1,0.2,0.3\\}$$", "word_idx": 35958, "sentence_idx": 424, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 35975, "sentence_idx": 425, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{\\textit{True},\\textit{False}\\}$$", "word_idx": 35981, "sentence_idx": 426, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{\\textit{True},\\textit{False}\\}$$", "word_idx": 36013, "sentence_idx": 427, "label": "unlabeled"}, {"type": "math", "expr": "$$2048$$", "word_idx": 36045, "sentence_idx": 428, "label": "unlabeled"}, {"type": "math", "expr": "$$0.3$$", "word_idx": 36049, "sentence_idx": 429, "label": "unlabeled"}, {"type": "math", "expr": "$$512$$", "word_idx": 36052, "sentence_idx": 430, "label": "unlabeled"}, {"type": "math", "expr": "$$100$$", "word_idx": 36055, "sentence_idx": 431, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:  Validation perplexity for WikiText-103 over  $9$  billion words of training ( $\\approx 90$  epochs)", "word_idx": 36058, "sentence_idx": 432, "label": "unlabeled"}, {"type": "text", "expr": "\nThe LSTM drops to a perplexity of  $364$  with a regular softmax layer, and  $343$  with the Hebbian Softmax ,  $T=500$ , when representations from the LSTM begin to settle", "word_idx": 36168, "sentence_idx": 433, "label": "unlabeled"}, {"type": "text", "expr": " For tuning parameter  $T$ ;  $T=100$  converges quicker, but begins to overfit after  $55$ B training words (coinciding when all classes have been observed at least  $100$  times)", "word_idx": 36341, "sentence_idx": 434, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:", "word_idx": 36521, "sentence_idx": 435, "label": "unlabeled"}, {"type": "math", "expr": "$$\\approx 90$$", "word_idx": 36530, "sentence_idx": 436, "label": "unlabeled"}, {"type": "math", "expr": "$$36.4$$", "word_idx": 36540, "sentence_idx": 437, "label": "unlabeled"}, {"type": "math", "expr": "$$34.3$$", "word_idx": 36544, "sentence_idx": 438, "label": "unlabeled"}, {"type": "math", "expr": "$$T=500$$", "word_idx": 36548, "sentence_idx": 439, "label": "unlabeled"}, {"type": "math", "expr": "$$T=100$$", "word_idx": 36553, "sentence_idx": 440, "label": "unlabeled"}, {"type": "math", "expr": "$$5.5$$", "word_idx": 36558, "sentence_idx": 441, "label": "unlabeled"}, {"type": "math", "expr": "$$100$$", "word_idx": 36561, "sentence_idx": 442, "label": "unlabeled"}, {"type": "text", "expr": "2  WikiText-103", "word_idx": 36564, "sentence_idx": 443, "label": "unlabeled"}, {"type": "text", "expr": "The WikiText-103 corpus contains  $267,735$  unique words and each word occurs at least three times in the training set", "word_idx": 36579, "sentence_idx": 444, "label": "unlabeled"}, {"type": "text", "expr": " We take the best LSTM parameter configuration (described above) as a baseline, and compare it to an identical model where the final layer is replaced with Hebbian Softmax ", "word_idx": 36698, "sentence_idx": 445, "label": "unlabeled"}, {"type": "text", "expr": " We swept over the insertion limit parameter  $T\\in\\{100,500,1000\\}$  and discount factor  $\\gamma\\in\\{005,01,025\\}$  using the validation set", "word_idx": 36870, "sentence_idx": 446, "label": "unlabeled"}, {"type": "text", "expr": " We found  $T=500,\\,\\gamma=025$  worked best, achieving a test perplexity of  $343$  on this dataset (Table  1 )", "word_idx": 37012, "sentence_idx": 447, "label": "unlabeled"}, {"type": "text", "expr": " Inspecting the validation curves in Figure  4  we see the Hebbian Softmax initially hampers validation performance, until around  $2$ \u2013 $3$ B training tokens have been consumed", "word_idx": 37124, "sentence_idx": 448, "label": "unlabeled"}, {"type": "text", "expr": " This makes sense, as storing activations from prior layers of the network is only an effective strategy once the network has rich intermediate representations of its inputs", "word_idx": 37301, "sentence_idx": 449, "label": "unlabeled"}, {"type": "text", "expr": " Inspecting Table  2  we see the test perplexity broken down by word frequency, the gain in overall performance is obtained from less frequent vocabulary", "word_idx": 37474, "sentence_idx": 450, "label": "unlabeled"}, {"type": "math", "expr": "$$267,735$$", "word_idx": 37627, "sentence_idx": 451, "label": "unlabeled"}, {"type": "math", "expr": "$$T\\in\\{100,500,1000\\}$$", "word_idx": 37634, "sentence_idx": 452, "label": "unlabeled"}, {"type": "math", "expr": "$$\\gamma\\in\\{0.05,0.1,0.25\\}$$", "word_idx": 37654, "sentence_idx": 453, "label": "unlabeled"}, {"type": "math", "expr": "$$T=500,\\,\\gamma=0.25$$", "word_idx": 37680, "sentence_idx": 454, "label": "unlabeled"}, {"type": "math", "expr": "$$34.3$$", "word_idx": 37699, "sentence_idx": 455, "label": "unlabeled"}, {"type": "text", "expr": "We also investigate the model evaluated dynamically on the test using (a) a Neural Cache  \\citep grave2016improving and (b) Memory-based Parameter Adaptation (MbPA)  \\citep sprechmann2018memorybased", "word_idx": 37703, "sentence_idx": 456, "label": "unlabeled"}, {"type": "text", "expr": " Hyper-parameter details for these models are detailed in Appendix  A", "word_idx": 37901, "sentence_idx": 457, "label": "unlabeled"}, {"type": "text", "expr": " The cache reduces the test perplexity by  $16$  for the LSTM and  $44$  for LSTM + Hebbian Softmax ", "word_idx": 37970, "sentence_idx": 458, "label": "unlabeled"}, {"type": "text", "expr": " The addition of MbPA reaches a test perplexity of  $292$  which is, to the authors\u2019 knowledge, state-of-the-art at time of writing", "word_idx": 38070, "sentence_idx": 459, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 38201, "sentence_idx": 460, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 38207, "sentence_idx": 461, "label": "unlabeled"}, {"type": "math", "expr": "$$1.6$$", "word_idx": 38213, "sentence_idx": 462, "label": "unlabeled"}, {"type": "math", "expr": "$$4.4$$", "word_idx": 38216, "sentence_idx": 463, "label": "unlabeled"}, {"type": "math", "expr": "$$29.2$$", "word_idx": 38219, "sentence_idx": 464, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:  Validation and test perplexities on WikiText-103", "word_idx": 38223, "sentence_idx": 465, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:", "word_idx": 38281, "sentence_idx": 466, "label": "unlabeled"}, {"type": "text", "expr": "Valid", "word_idx": 38289, "sentence_idx": 467, "label": "unlabeled"}, {"type": "text", "expr": " Test", "word_idx": 38294, "sentence_idx": 468, "label": "unlabeled"}, {"type": "text", "expr": "Graves et\u00a0al", "word_idx": 38299, "sentence_idx": 469, "label": "unlabeled"}, {"type": "text", "expr": "(2014)Graves, Wayne, and Danihelka", "word_idx": 38311, "sentence_idx": 470, "label": "unlabeled"}, {"type": "text", "expr": "Bai et\u00a0al", "word_idx": 38345, "sentence_idx": 471, "label": "unlabeled"}, {"type": "text", "expr": "(2018)Bai, Kolter, and Koltun", "word_idx": 38354, "sentence_idx": 472, "label": "unlabeled"}, {"type": "text", "expr": "Dauphin et\u00a0al", "word_idx": 38383, "sentence_idx": 473, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Dauphin, Fan, Auli, and\nGrangier", "word_idx": 38396, "sentence_idx": 474, "label": "unlabeled"}, {"type": "text", "expr": "LSTM (ours) 36", "word_idx": 38434, "sentence_idx": 475, "label": "unlabeled"}, {"type": "text", "expr": "LSTM + Cache 34", "word_idx": 38448, "sentence_idx": 476, "label": "unlabeled"}, {"type": "text", "expr": "LSTM + Hebbian 34", "word_idx": 38463, "sentence_idx": 477, "label": "unlabeled"}, {"type": "text", "expr": "LSTM + Hebbian + Cache 29", "word_idx": 38480, "sentence_idx": 478, "label": "unlabeled"}, {"type": "text", "expr": "LSTM + Hebbian + Cache + MbPA 29", "word_idx": 38505, "sentence_idx": 479, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:  Test perplexity versus training word frequency", "word_idx": 38537, "sentence_idx": 480, "label": "unlabeled"}, {"type": "text", "expr": " Hebbian Softmax models less frequent words with better accuracy", "word_idx": 38593, "sentence_idx": 481, "label": "unlabeled"}, {"type": "text", "expr": " Note the training set size of WikiText is smaller than Gutenberg, which is itself much smaller than GigaWord; so the  $>10$ K bucket includes an increasing number of unique words", "word_idx": 38657, "sentence_idx": 482, "label": "unlabeled"}, {"type": "text", "expr": " This explains GigaWord\u2019s larger perplexity in this bucket", "word_idx": 38836, "sentence_idx": 483, "label": "unlabeled"}, {"type": "text", "expr": " Furthermore there were no words observed  $<100$  times within the GigaWord  $250$ K vocabulary", "word_idx": 38894, "sentence_idx": 484, "label": "unlabeled"}, {"type": "text", "expr": " A random model would have a perplexity of  $|V|\\approx 25e5$  for all frequency buckets", "word_idx": 38990, "sentence_idx": 485, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:", "word_idx": 39078, "sentence_idx": 486, "label": "unlabeled"}, {"type": "math", "expr": "$$>10$$", "word_idx": 39086, "sentence_idx": 487, "label": "unlabeled"}, {"type": "math", "expr": "$$<100$$", "word_idx": 39089, "sentence_idx": 488, "label": "unlabeled"}, {"type": "math", "expr": "$$250$$", "word_idx": 39093, "sentence_idx": 489, "label": "unlabeled"}, {"type": "math", "expr": "$$|V|\\approx 2.5e5$$", "word_idx": 39096, "sentence_idx": 490, "label": "unlabeled"}, {"type": "math", "expr": "$$>10$$", "word_idx": 39112, "sentence_idx": 491, "label": "unlabeled"}, {"type": "math", "expr": "$$-10$$", "word_idx": 39115, "sentence_idx": 492, "label": "unlabeled"}, {"type": "math", "expr": "$$100-1$$", "word_idx": 39118, "sentence_idx": 493, "label": "unlabeled"}, {"type": "math", "expr": "$$<100$$", "word_idx": 39123, "sentence_idx": 494, "label": "unlabeled"}, {"type": "text", "expr": "WikiText-103", "word_idx": 39127, "sentence_idx": 495, "label": "unlabeled"}, {"type": "text", "expr": "WikiText-103", "word_idx": 39139, "sentence_idx": 496, "label": "unlabeled"}, {"type": "text", "expr": "softmax 12", "word_idx": 39151, "sentence_idx": 497, "label": "unlabeled"}, {"type": "text", "expr": "2e2 1", "word_idx": 39161, "sentence_idx": 498, "label": "unlabeled"}, {"type": "text", "expr": "2e3 9", "word_idx": 39166, "sentence_idx": 499, "label": "unlabeled"}, {"type": "text", "expr": "7e3 36", "word_idx": 39171, "sentence_idx": 500, "label": "unlabeled"}, {"type": "text", "expr": "softmax", "word_idx": 39177, "sentence_idx": 501, "label": "unlabeled"}, {"type": "text", "expr": "Hebbian Softmax 12", "word_idx": 39184, "sentence_idx": 502, "label": "unlabeled"}, {"type": "text", "expr": "8e2 7", "word_idx": 39202, "sentence_idx": 503, "label": "unlabeled"}, {"type": "text", "expr": "6e2 5", "word_idx": 39207, "sentence_idx": 504, "label": "unlabeled"}, {"type": "text", "expr": "2e3 34", "word_idx": 39212, "sentence_idx": 505, "label": "unlabeled"}, {"type": "text", "expr": "Hebbian Softmax", "word_idx": 39218, "sentence_idx": 506, "label": "unlabeled"}, {"type": "text", "expr": "Gutenberg", "word_idx": 39233, "sentence_idx": 507, "label": "unlabeled"}, {"type": "text", "expr": "Gutenberg", "word_idx": 39242, "sentence_idx": 508, "label": "unlabeled"}, {"type": "text", "expr": "softmax 19", "word_idx": 39251, "sentence_idx": 509, "label": "unlabeled"}, {"type": "text", "expr": "8e2 6", "word_idx": 39261, "sentence_idx": 510, "label": "unlabeled"}, {"type": "text", "expr": "9e3 8", "word_idx": 39266, "sentence_idx": 511, "label": "unlabeled"}, {"type": "text", "expr": "6e4 47", "word_idx": 39271, "sentence_idx": 512, "label": "unlabeled"}, {"type": "text", "expr": "softmax", "word_idx": 39277, "sentence_idx": 513, "label": "unlabeled"}, {"type": "text", "expr": "Hebbian Softmax 18", "word_idx": 39284, "sentence_idx": 514, "label": "unlabeled"}, {"type": "text", "expr": "4e2 6", "word_idx": 39302, "sentence_idx": 515, "label": "unlabeled"}, {"type": "text", "expr": "6e3 5", "word_idx": 39307, "sentence_idx": 516, "label": "unlabeled"}, {"type": "text", "expr": "9e4 45", "word_idx": 39312, "sentence_idx": 517, "label": "unlabeled"}, {"type": "text", "expr": "Hebbian Softmax", "word_idx": 39318, "sentence_idx": 518, "label": "unlabeled"}, {"type": "text", "expr": "GigaWord", "word_idx": 39333, "sentence_idx": 519, "label": "unlabeled"}, {"type": "text", "expr": "GigaWord", "word_idx": 39341, "sentence_idx": 520, "label": "unlabeled"}, {"type": "text", "expr": "softmax 39", "word_idx": 39349, "sentence_idx": 521, "label": "unlabeled"}, {"type": "text", "expr": "5e3 3", "word_idx": 39359, "sentence_idx": 522, "label": "unlabeled"}, {"type": "text", "expr": "7e4 - 53", "word_idx": 39364, "sentence_idx": 523, "label": "unlabeled"}, {"type": "text", "expr": "softmax", "word_idx": 39372, "sentence_idx": 524, "label": "unlabeled"}, {"type": "text", "expr": "Hebbian Softmax 33", "word_idx": 39379, "sentence_idx": 525, "label": "unlabeled"}, {"type": "text", "expr": "2e3 1", "word_idx": 39397, "sentence_idx": 526, "label": "unlabeled"}, {"type": "text", "expr": "6e4 - 43", "word_idx": 39402, "sentence_idx": 527, "label": "unlabeled"}, {"type": "text", "expr": "Hebbian Softmax", "word_idx": 39410, "sentence_idx": 528, "label": "unlabeled"}, {"type": "text", "expr": "3  Gutenberg", "word_idx": 39425, "sentence_idx": 529, "label": "unlabeled"}, {"type": "text", "expr": "Books provide several different linguistic challenges to articles", "word_idx": 39437, "sentence_idx": 530, "label": "unlabeled"}, {"type": "text", "expr": " The style of writing is intentionally varied between authors, and named entities can be wholly fictional \u2014 confined to a single text", "word_idx": 39502, "sentence_idx": 531, "label": "unlabeled"}, {"type": "text", "expr": " We extract a subset of English-language books from the corpus, strip the Gutenberg headers and tokenize the text (Appendix  A", "word_idx": 39635, "sentence_idx": 532, "label": "unlabeled"}, {"type": "text", "expr": " We select a dataset of comparable size to WikiText-103;  $2042$  books in total with  $2017$  training books ( $175,181,505$  tokens),  $12$  validation books ( $609,545$  tokens), and  $13$  test books ( $526,646$  tokens) \u2014 see Appendix  A", "word_idx": 39761, "sentence_idx": 533, "label": "unlabeled"}, {"type": "text", "expr": "2  for full details", "word_idx": 40003, "sentence_idx": 534, "label": "unlabeled"}, {"type": "text", "expr": " We select all words that occur at least five times in the training set, a total vocabulary of  $242,621$  and map the remainder to an unk token", "word_idx": 40022, "sentence_idx": 535, "label": "unlabeled"}, {"type": "math", "expr": "$$2042$$", "word_idx": 40166, "sentence_idx": 536, "label": "unlabeled"}, {"type": "math", "expr": "$$2017$$", "word_idx": 40170, "sentence_idx": 537, "label": "unlabeled"}, {"type": "math", "expr": "$$175,181,505$$", "word_idx": 40174, "sentence_idx": 538, "label": "unlabeled"}, {"type": "math", "expr": "$$609,545$$", "word_idx": 40185, "sentence_idx": 539, "label": "unlabeled"}, {"type": "math", "expr": "$$526,646$$", "word_idx": 40192, "sentence_idx": 540, "label": "unlabeled"}, {"type": "math", "expr": "$$242,621$$", "word_idx": 40199, "sentence_idx": 541, "label": "unlabeled"}, {"type": "text", "expr": "We use the same LSTM hyper-parameters as those chosen from the wikipedia sweep, and compare against Hebbian Softmax with  $T=100,T=500$  and  $\\gamma=01$ ", "word_idx": 40206, "sentence_idx": 542, "label": "unlabeled"}, {"type": "text", "expr": " Figure  5  in Appendix  A", "word_idx": 40360, "sentence_idx": 543, "label": "unlabeled"}, {"type": "text", "expr": "2  shows the validation performance after  $15$ B steps of training, equating to roughly  $80$  epochs and  $6$  days of training with  $8$  P $100$ s training synchronously", "word_idx": 40386, "sentence_idx": 544, "label": "unlabeled"}, {"type": "text", "expr": " After approximately  $4$ B steps of training the softmax performance is surpassed, and this gap widens even up to  $15$ B steps to a gap of  $2-3$  points in perplexity", "word_idx": 40559, "sentence_idx": 545, "label": "unlabeled"}, {"type": "text", "expr": " Similar to WikiText-103, we see in Table  2  the gain in perplexity is more pronounced over less frequent words", "word_idx": 40728, "sentence_idx": 546, "label": "unlabeled"}, {"type": "math", "expr": "$$T=100,T=500$$", "word_idx": 40840, "sentence_idx": 547, "label": "unlabeled"}, {"type": "math", "expr": "$$\\gamma=0.1$$", "word_idx": 40851, "sentence_idx": 548, "label": "unlabeled"}, {"type": "math", "expr": "$$100$$", "word_idx": 40861, "sentence_idx": 549, "label": "unlabeled"}, {"type": "math", "expr": "$$2-3$$", "word_idx": 40864, "sentence_idx": 550, "label": "unlabeled"}, {"type": "text", "expr": "3  GigaWord v5", "word_idx": 40867, "sentence_idx": 551, "label": "unlabeled"}, {"type": "text", "expr": "We evaluate Hebbian Softmax on a large-scale language modelling corpus", "word_idx": 40881, "sentence_idx": 552, "label": "unlabeled"}, {"type": "text", "expr": " GigaWord is interesting because it is a vast collection of news articles, and there is a natural temporal order", "word_idx": 40951, "sentence_idx": 553, "label": "unlabeled"}, {"type": "text", "expr": " We pre-process the dataset (Appendix  A", "word_idx": 41063, "sentence_idx": 554, "label": "unlabeled"}, {"type": "text", "expr": "5 ), select all articles from  $2000$ \u2013 $2009$  for the training set, and test on all articles from  $2010$ ", "word_idx": 41103, "sentence_idx": 555, "label": "unlabeled"}, {"type": "text", "expr": " The total number of training tokens is  $40$ B and the total number of test tokens is  $260$ M", "word_idx": 41211, "sentence_idx": 556, "label": "unlabeled"}, {"type": "text", "expr": " The total unique tokens (after pre-processing) for the training set reaches  $6$ M, however for parity with the other experiments we choose a vocabulary size of  $250$ K", "word_idx": 41306, "sentence_idx": 557, "label": "unlabeled"}, {"type": "text", "expr": " We use the same LSTM hyper-parameters and Hebbian Softmax hyper-parameters, and train the model for  $6$ B steps, after which the models plateau in evaluation performance", "word_idx": 41476, "sentence_idx": 558, "label": "unlabeled"}, {"type": "text", "expr": " We observe a  $98$ -point drop in perplexity, from  $535$  to  $437$ , illustrated in Table\u00a0 2 ", "word_idx": 41647, "sentence_idx": 559, "label": "unlabeled"}, {"type": "math", "expr": "$$2000$$", "word_idx": 41743, "sentence_idx": 560, "label": "unlabeled"}, {"type": "math", "expr": "$$2009$$", "word_idx": 41747, "sentence_idx": 561, "label": "unlabeled"}, {"type": "math", "expr": "$$2010$$", "word_idx": 41751, "sentence_idx": 562, "label": "unlabeled"}, {"type": "math", "expr": "$$4.0$$", "word_idx": 41755, "sentence_idx": 563, "label": "unlabeled"}, {"type": "math", "expr": "$$260$$", "word_idx": 41758, "sentence_idx": 564, "label": "unlabeled"}, {"type": "math", "expr": "$$250$$", "word_idx": 41761, "sentence_idx": 565, "label": "unlabeled"}, {"type": "math", "expr": "$$9.8$$", "word_idx": 41764, "sentence_idx": 566, "label": "unlabeled"}, {"type": "math", "expr": "$$53.5$$", "word_idx": 41767, "sentence_idx": 567, "label": "unlabeled"}, {"type": "math", "expr": "$$43.7$$", "word_idx": 41771, "sentence_idx": 568, "label": "unlabeled"}, {"type": "text", "expr": "4  Alternate Objective Functions", "word_idx": 41775, "sentence_idx": 569, "label": "unlabeled"}, {"type": "text", "expr": "We test out some of the alternate inner objective functions described in Section  3", "word_idx": 41807, "sentence_idx": 570, "label": "unlabeled"}, {"type": "text", "expr": " These are described in ( 3", "word_idx": 41890, "sentence_idx": 571, "label": "unlabeled"}, {"type": "text", "expr": "3 ), and the inner objective functions include  Euclidean, Inner Product, SVM, (sparse) Softmax ", "word_idx": 41917, "sentence_idx": 572, "label": "unlabeled"}, {"type": "text", "expr": " These could be applied to any of the described experiments, we chose the WikiText-103 language modelling task because it is more comparable to prior work", "word_idx": 42013, "sentence_idx": 573, "label": "unlabeled"}, {"type": "text", "expr": "Euclidean, Inner Product, SVM, (sparse) Softmax", "word_idx": 42167, "sentence_idx": 574, "label": "unlabeled"}, {"type": "text", "expr": "Although more expressive objective functions appear promising, in practice we see (Figure  7  in Appendix  A", "word_idx": 42214, "sentence_idx": 575, "label": "unlabeled"}, {"type": "text", "expr": "4 ) that validation performance is roughly equivalent between all inner objective functions", "word_idx": 42322, "sentence_idx": 576, "label": "unlabeled"}, {"type": "text", "expr": " This suggests the network activation  $h_{t}$  naturally do not land too close to other class parameters, and the norm of activations is not too large or small, in comparison to the model parameters  $\\theta$ ", "word_idx": 42413, "sentence_idx": 577, "label": "unlabeled"}, {"type": "text", "expr": " The latter may be due to the use of layer normalization from the LSTM", "word_idx": 42623, "sentence_idx": 578, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{t}$$", "word_idx": 42693, "sentence_idx": 579, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta$$", "word_idx": 42698, "sentence_idx": 580, "label": "unlabeled"}, {"type": "text", "expr": "5  Related Work", "word_idx": 42704, "sentence_idx": 581, "label": "unlabeled"}, {"type": "text", "expr": "Few-shot classification has been investigated in a meta-learning setup with a mixture model of a parametric neural network and a non-parametric memory  \\citep santoro2016one, vinyals2016matching", "word_idx": 42719, "sentence_idx": 582, "label": "unlabeled"}, {"type": "text", "expr": " Here, a subset of classes are used with permuted labels per episode, activations are stored to memory, and gradients are passed through the memory", "word_idx": 42913, "sentence_idx": 583, "label": "unlabeled"}, {"type": "text", "expr": " This allows the network to shape its activations to be conducive to accurate retrieval and classification", "word_idx": 43060, "sentence_idx": 584, "label": "unlabeled"}, {"type": "text", "expr": " Here, we do not meta-learn the activations stored into network parameters and instead rely on their representation being rich enough from regular parametric training", "word_idx": 43166, "sentence_idx": 585, "label": "unlabeled"}, {"type": "text", "expr": " We do this to avoid backpropagating through time to the point of writing to memory, as the parameters may contain memories stored millions of time steps ago in the case of rare words", "word_idx": 43332, "sentence_idx": 586, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 43515, "sentence_idx": 587, "label": "unlabeled"}, {"type": "text", "expr": "In natural language processing memory-augmented models have been shown to improve the modelling of unknown words and adaptation to new domains  \\citep grave2016improving,merity2016pointer,kawakami2017learning", "word_idx": 43521, "sentence_idx": 588, "label": "unlabeled"}, {"type": "text", "expr": " However in these works the memory is typically small and models the recent past", "word_idx": 43729, "sentence_idx": 589, "label": "unlabeled"}, {"type": "text", "expr": " During evaluation the test activations and corresponding labels are stored in memory, and the model is evaluated dynamically \u2014 adapting to the test data on the fly", "word_idx": 43809, "sentence_idx": 590, "label": "unlabeled"}, {"type": "text", "expr": " Whilst dynamic evaluation provides insights into domain transfer, it is limited in applicability as the model may not receive ground-truth labels when launched into production", "word_idx": 43973, "sentence_idx": 591, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 44149, "sentence_idx": 592, "label": "unlabeled"}, {"type": "text", "expr": "More recent work has investigated methods of memorizing and searching over the training set to enhance performance  \\citep kaiser2017learning, grave2017unbounded, gu2017search", "word_idx": 44155, "sentence_idx": 593, "label": "unlabeled"}, {"type": "text", "expr": " These approaches typically require complex engineering to efficiently index this memory store", "word_idx": 44330, "sentence_idx": 594, "label": "unlabeled"}, {"type": "text", "expr": " Part of the benefit of Hebbian Softmax is implementation simplicity", "word_idx": 44424, "sentence_idx": 595, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 44492, "sentence_idx": 596, "label": "unlabeled"}, {"type": "text", "expr": "Prior literature on the softmax operator for language modelling computational efficiency   or tricks such as smoothing across many softmax layers  ", "word_idx": 44498, "sentence_idx": 597, "label": "unlabeled"}, {"type": "text", "expr": " However these do not focus on increasing the data-efficiency or faster learning of infrequent classes", "word_idx": 44645, "sentence_idx": 598, "label": "unlabeled"}, {"type": "text", "expr": "Chen et\u00a0al", "word_idx": 44747, "sentence_idx": 599, "label": "unlabeled"}, {"type": "text", "expr": "(2015)Chen, Grangier, and Auli", "word_idx": 44757, "sentence_idx": 600, "label": "unlabeled"}, {"type": "text", "expr": "Grave et\u00a0al", "word_idx": 44787, "sentence_idx": 601, "label": "unlabeled"}, {"type": "text", "expr": "(2016a)Grave, Joulin, Ciss\u00e9, Grangier,\nand J\u00e9gou", "word_idx": 44798, "sentence_idx": 602, "label": "unlabeled"}, {"type": "text", "expr": "Yang et\u00a0al", "word_idx": 44846, "sentence_idx": 603, "label": "unlabeled"}, {"type": "text", "expr": "(2017)Yang, Dai, Salakhutdinov, and\nCohen", "word_idx": 44856, "sentence_idx": 604, "label": "unlabeled"}, {"type": "text", "expr": "Other architectures have been considered for fast learning, such as the \u2018fast weights\u2019 auto-associative memory  \\citep ba2016using", "word_idx": 44897, "sentence_idx": 605, "label": "unlabeled"}, {"type": "text", "expr": " This focuses on fast adaptation to recent information that persists over a short window of time", "word_idx": 45027, "sentence_idx": 606, "label": "unlabeled"}, {"type": "text", "expr": " The LEABRA architecture  \\citep o1996leabra contains a mixture of contrastive Hebbian learning (GENEREC)  \\citep o1996biologically and gradient descent for fast and slow learning, however this cognitively-inspired model has not been shown to scale to large-scale classification problems", "word_idx": 45123, "sentence_idx": 607, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 45410, "sentence_idx": 608, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 45416, "sentence_idx": 609, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 45422, "sentence_idx": 610, "label": "unlabeled"}, {"type": "text", "expr": "6  Discussion", "word_idx": 45428, "sentence_idx": 611, "label": "unlabeled"}, {"type": "text", "expr": "This paper explores one way in which we can achieve fast parametric learning in neural networks, and preserve this knowledge over time", "word_idx": 45441, "sentence_idx": 612, "label": "unlabeled"}, {"type": "text", "expr": " We show that activation memorization is useful for vision in the binding of newly introduced classes, beating a well tuned adaptive learning rate optimizer, RMSProp", "word_idx": 45575, "sentence_idx": 613, "label": "unlabeled"}, {"type": "text", "expr": "This paper explores one way in which we can achieve fast parametric learning in neural networks, and preserve this knowledge over time", "word_idx": 45740, "sentence_idx": 614, "label": "unlabeled"}, {"type": "text", "expr": " We show that activation memorization is useful for vision in the binding of newly introduced classes, beating a well tuned adaptive learning rate optimizer, RMSProp", "word_idx": 45874, "sentence_idx": 615, "label": "unlabeled"}, {"type": "text", "expr": "For language we show improvement in the modelling of text with an extensive vocabulary", "word_idx": 46039, "sentence_idx": 616, "label": "unlabeled"}, {"type": "text", "expr": " In the latter we show the model beats a very strong LSTM benchmark on three stylistically different corpora, and achieves state of the art on WikiText-103", "word_idx": 46125, "sentence_idx": 617, "label": "unlabeled"}, {"type": "text", "expr": " This is achieved with effectively no additional compute or memory resources", "word_idx": 46280, "sentence_idx": 618, "label": "unlabeled"}, {"type": "text", "expr": " Breaking down perplexity over word frequency bucket, we see that less frequent words are better modelled, as hypothesized", "word_idx": 46356, "sentence_idx": 619, "label": "unlabeled"}, {"type": "text", "expr": " We suggest that Hebbian Softmax could be applied to any classification domain with infrequent classes, or non-stationary data", "word_idx": 46478, "sentence_idx": 620, "label": "unlabeled"}, {"type": "text", "expr": " It may also be useful in quickly adapting a pre-trained classifier to a new task / set of classes \u2014 however this is beyond the scope of our initial investigation", "word_idx": 46604, "sentence_idx": 621, "label": "unlabeled"}, {"type": "text", "expr": "For language we show improvement in the modelling of text with an extensive vocabulary", "word_idx": 46766, "sentence_idx": 622, "label": "unlabeled"}, {"type": "text", "expr": " In the latter we show the model beats a very strong LSTM benchmark on three stylistically different corpora, and achieves state of the art on WikiText-103", "word_idx": 46852, "sentence_idx": 623, "label": "unlabeled"}, {"type": "text", "expr": " This is achieved with effectively no additional compute or memory resources", "word_idx": 47007, "sentence_idx": 624, "label": "unlabeled"}, {"type": "text", "expr": " Breaking down perplexity over word frequency bucket, we see that less frequent words are better modelled, as hypothesized", "word_idx": 47083, "sentence_idx": 625, "label": "unlabeled"}, {"type": "text", "expr": " We suggest that Hebbian Softmax could be applied to any classification domain with infrequent classes, or non-stationary data", "word_idx": 47205, "sentence_idx": 626, "label": "unlabeled"}, {"type": "text", "expr": " It may also be useful in quickly adapting a pre-trained classifier to a new task / set of classes \u2014 however this is beyond the scope of our initial investigation", "word_idx": 47331, "sentence_idx": 627, "label": "unlabeled"}, {"type": "text", "expr": "It would also be interesting to explore activation memorization deeper within the network, and thus in more general scenarios to classification", "word_idx": 47493, "sentence_idx": 628, "label": "unlabeled"}, {"type": "text", "expr": " In this case, there is no direct feedback from a ground-truth class label and the update rule would not necessarily be an instance of Hebbian learning", "word_idx": 47636, "sentence_idx": 629, "label": "unlabeled"}, {"type": "text", "expr": " A natural first step would be to generalize the ideas to large-scale softmax operators that are internal to the network \u2014 such as attention over a large memory", "word_idx": 47787, "sentence_idx": 630, "label": "unlabeled"}, {"type": "text", "expr": "It would also be interesting to explore activation memorization deeper within the network, and thus in more general scenarios to classification", "word_idx": 47947, "sentence_idx": 631, "label": "unlabeled"}, {"type": "text", "expr": " In this case, there is no direct feedback from a ground-truth class label and the update rule would not necessarily be an instance of Hebbian learning", "word_idx": 48090, "sentence_idx": 632, "label": "unlabeled"}, {"type": "text", "expr": " A natural first step would be to generalize the ideas to large-scale softmax operators that are internal to the network \u2014 such as attention over a large memory", "word_idx": 48241, "sentence_idx": 633, "label": "unlabeled"}, {"type": "text", "expr": "Acknowledgements", "word_idx": 48401, "sentence_idx": 634, "label": "unlabeled"}, {"type": "text", "expr": "The authors would like to thank Gabor Melis, Greg Wayne, Oriol Vinyals, Pablo Sprechmann, Siddhant Jayakumar, Charles Blundell, Koray Kavukcuoglu, Shakir Mohamed, Adam Santoro, Phil Blunsom, Felix Hill, Angeliki Lazaridou, James Martens, Ozlem Aslan, Guillaume Desjardins, and Chloe Hillier for their comments and assistance during the course of this project", "word_idx": 48417, "sentence_idx": 635, "label": "unlabeled"}, {"type": "text", "expr": " Peter Dayan is currently on a leave of absence at Uber Technologies; Uber was not involved in this study", "word_idx": 48775, "sentence_idx": 636, "label": "unlabeled"}, {"type": "text", "expr": "The authors would like to thank Gabor Melis, Greg Wayne, Oriol Vinyals, Pablo Sprechmann, Siddhant Jayakumar, Charles Blundell, Koray Kavukcuoglu, Shakir Mohamed, Adam Santoro, Phil Blunsom, Felix Hill, Angeliki Lazaridou, James Martens, Ozlem Aslan, Guillaume Desjardins, and Chloe Hillier for their comments and assistance during the course of this project", "word_idx": 48880, "sentence_idx": 637, "label": "unlabeled"}, {"type": "text", "expr": " Peter Dayan is currently on a leave of absence at Uber Technologies; Uber was not involved in this study", "word_idx": 49238, "sentence_idx": 638, "label": "unlabeled"}, {"type": "text", "expr": "References", "word_idx": 49343, "sentence_idx": 639, "label": "unlabeled"}, {"type": "text", "expr": "Andrychowicz et\u00a0al", "word_idx": 49353, "sentence_idx": 640, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Andrychowicz, Denil, Gomez, Hoffman, Pfau,\nSchaul, and de\u00a0Freitas \nAndrychowicz, Marcin, Denil, Misha, Gomez, Sergio, Hoffman, Matthew\u00a0W, Pfau,\nDavid, Schaul, Tom, and de\u00a0Freitas, Nando", "word_idx": 49371, "sentence_idx": 641, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Learning to learn by gradient descent by gradient descent", "word_idx": 49562, "sentence_idx": 642, "label": "unlabeled"}, {"type": "text", "expr": "Andrychowicz et\u00a0al", "word_idx": 49622, "sentence_idx": 643, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Andrychowicz, Denil, Gomez, Hoffman, Pfau,\nSchaul, and de\u00a0Freitas", "word_idx": 49640, "sentence_idx": 644, "label": "unlabeled"}, {"type": "text", "expr": "Andrychowicz, Marcin, Denil, Misha, Gomez, Sergio, Hoffman, Matthew\u00a0W, Pfau,\nDavid, Schaul, Tom, and de\u00a0Freitas, Nando", "word_idx": 49711, "sentence_idx": 645, "label": "unlabeled"}, {"type": "text", "expr": "Learning to learn by gradient descent by gradient descent", "word_idx": 49829, "sentence_idx": 646, "label": "unlabeled"}, {"type": "text", "expr": "In  Advances in Neural Information Processing Systems , pp", "word_idx": 49886, "sentence_idx": 647, "label": "unlabeled"}, {"type": "text", "expr": "\u00a03981\u20133989, 2016", "word_idx": 49944, "sentence_idx": 648, "label": "unlabeled"}, {"type": "text", "expr": "Advances in Neural Information Processing Systems", "word_idx": 49960, "sentence_idx": 649, "label": "unlabeled"}, {"type": "text", "expr": "Ba et\u00a0al", "word_idx": 50009, "sentence_idx": 650, "label": "unlabeled"}, {"type": "text", "expr": "(2016a)Ba, Hinton, Mnih, Leibo, and\nIonescu \nBa, Jimmy, Hinton, Geoffrey\u00a0E, Mnih, Volodymyr, Leibo, Joel\u00a0Z, and Ionescu,\nCatalin", "word_idx": 50017, "sentence_idx": 651, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Using fast weights to attend to the recent past", "word_idx": 50145, "sentence_idx": 652, "label": "unlabeled"}, {"type": "text", "expr": "Ba et\u00a0al", "word_idx": 50195, "sentence_idx": 653, "label": "unlabeled"}, {"type": "text", "expr": "(2016a)Ba, Hinton, Mnih, Leibo, and\nIonescu", "word_idx": 50203, "sentence_idx": 654, "label": "unlabeled"}, {"type": "text", "expr": "Ba, Jimmy, Hinton, Geoffrey\u00a0E, Mnih, Volodymyr, Leibo, Joel\u00a0Z, and Ionescu,\nCatalin", "word_idx": 50246, "sentence_idx": 655, "label": "unlabeled"}, {"type": "text", "expr": "Using fast weights to attend to the recent past", "word_idx": 50329, "sentence_idx": 656, "label": "unlabeled"}, {"type": "text", "expr": "In  Advances in Neural Information Processing Systems , pp", "word_idx": 50376, "sentence_idx": 657, "label": "unlabeled"}, {"type": "text", "expr": "\u00a04331\u20134339, 2016a", "word_idx": 50434, "sentence_idx": 658, "label": "unlabeled"}, {"type": "text", "expr": "Advances in Neural Information Processing Systems", "word_idx": 50451, "sentence_idx": 659, "label": "unlabeled"}, {"type": "text", "expr": "Ba et\u00a0al", "word_idx": 50500, "sentence_idx": 660, "label": "unlabeled"}, {"type": "text", "expr": "(2016b)Ba, Kiros, and Hinton \nBa, Jimmy\u00a0Lei, Kiros, Jamie\u00a0Ryan, and Hinton, Geoffrey\u00a0E", "word_idx": 50508, "sentence_idx": 661, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Layer normalization", "word_idx": 50594, "sentence_idx": 662, "label": "unlabeled"}, {"type": "text", "expr": "Ba et\u00a0al", "word_idx": 50616, "sentence_idx": 663, "label": "unlabeled"}, {"type": "text", "expr": "(2016b)Ba, Kiros, and Hinton", "word_idx": 50624, "sentence_idx": 664, "label": "unlabeled"}, {"type": "text", "expr": "Ba, Jimmy\u00a0Lei, Kiros, Jamie\u00a0Ryan, and Hinton, Geoffrey\u00a0E", "word_idx": 50652, "sentence_idx": 665, "label": "unlabeled"}, {"type": "text", "expr": "Layer normalization", "word_idx": 50708, "sentence_idx": 666, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1607", "word_idx": 50727, "sentence_idx": 667, "label": "unlabeled"}, {"type": "text", "expr": "06450 , 2016b", "word_idx": 50752, "sentence_idx": 668, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1607", "word_idx": 50765, "sentence_idx": 669, "label": "unlabeled"}, {"type": "text", "expr": "06450", "word_idx": 50790, "sentence_idx": 670, "label": "unlabeled"}, {"type": "text", "expr": "Bahdanau et\u00a0al", "word_idx": 50795, "sentence_idx": 671, "label": "unlabeled"}, {"type": "text", "expr": "(2014)Bahdanau, Cho, and Bengio \nBahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua", "word_idx": 50809, "sentence_idx": 672, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Neural machine translation by jointly learning to align and\ntranslate", "word_idx": 50895, "sentence_idx": 673, "label": "unlabeled"}, {"type": "text", "expr": "Bahdanau et\u00a0al", "word_idx": 50967, "sentence_idx": 674, "label": "unlabeled"}, {"type": "text", "expr": "(2014)Bahdanau, Cho, and Bengio", "word_idx": 50981, "sentence_idx": 675, "label": "unlabeled"}, {"type": "text", "expr": "Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua", "word_idx": 51012, "sentence_idx": 676, "label": "unlabeled"}, {"type": "text", "expr": "Neural machine translation by jointly learning to align and\ntranslate", "word_idx": 51065, "sentence_idx": 677, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1409", "word_idx": 51134, "sentence_idx": 678, "label": "unlabeled"}, {"type": "text", "expr": "0473 , 2014", "word_idx": 51159, "sentence_idx": 679, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1409", "word_idx": 51170, "sentence_idx": 680, "label": "unlabeled"}, {"type": "text", "expr": "Bai et\u00a0al", "word_idx": 51195, "sentence_idx": 681, "label": "unlabeled"}, {"type": "text", "expr": "(2018)Bai, Kolter, and Koltun \nBai, Shaojie, Kolter, J", "word_idx": 51204, "sentence_idx": 682, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zico, and Koltun, Vladlen", "word_idx": 51258, "sentence_idx": 683, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Convolutional sequence modeling revisited, 2018", "word_idx": 51284, "sentence_idx": 684, "label": "unlabeled"}, {"type": "text", "expr": "Bai et\u00a0al", "word_idx": 51334, "sentence_idx": 685, "label": "unlabeled"}, {"type": "text", "expr": "(2018)Bai, Kolter, and Koltun", "word_idx": 51343, "sentence_idx": 686, "label": "unlabeled"}, {"type": "text", "expr": "Bai, Shaojie, Kolter, J", "word_idx": 51372, "sentence_idx": 687, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zico, and Koltun, Vladlen", "word_idx": 51395, "sentence_idx": 688, "label": "unlabeled"}, {"type": "text", "expr": "Convolutional sequence modeling revisited, 2018", "word_idx": 51421, "sentence_idx": 689, "label": "unlabeled"}, {"type": "text", "expr": "URL  https://openreview", "word_idx": 51468, "sentence_idx": 690, "label": "unlabeled"}, {"type": "text", "expr": "net/forum?id=rk8wKk-R- ", "word_idx": 51491, "sentence_idx": 691, "label": "unlabeled"}, {"type": "text", "expr": "https://openreview", "word_idx": 51514, "sentence_idx": 692, "label": "unlabeled"}, {"type": "text", "expr": "net/forum?id=rk8wKk-R-", "word_idx": 51532, "sentence_idx": 693, "label": "unlabeled"}, {"type": "text", "expr": "Bridle(1990) \nBridle, John\u00a0S", "word_idx": 51554, "sentence_idx": 694, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Training stochastic model recognition algorithms as networks can lead\nto maximum mutual information estimation of parameters", "word_idx": 51582, "sentence_idx": 695, "label": "unlabeled"}, {"type": "text", "expr": "Bridle(1990)", "word_idx": 51709, "sentence_idx": 696, "label": "unlabeled"}, {"type": "text", "expr": "Bridle, John\u00a0S", "word_idx": 51721, "sentence_idx": 697, "label": "unlabeled"}, {"type": "text", "expr": "Training stochastic model recognition algorithms as networks can lead\nto maximum mutual information estimation of parameters", "word_idx": 51735, "sentence_idx": 698, "label": "unlabeled"}, {"type": "text", "expr": "In  Advances in neural information processing systems , pp", "word_idx": 51859, "sentence_idx": 699, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0211\u2013217, 1990", "word_idx": 51917, "sentence_idx": 700, "label": "unlabeled"}, {"type": "text", "expr": "Advances in neural information processing systems", "word_idx": 51931, "sentence_idx": 701, "label": "unlabeled"}, {"type": "text", "expr": "Chen et\u00a0al", "word_idx": 51980, "sentence_idx": 702, "label": "unlabeled"}, {"type": "text", "expr": "(2015)Chen, Grangier, and Auli \nChen, Welin, Grangier, David, and Auli, Michael", "word_idx": 51990, "sentence_idx": 703, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Strategies for training large vocabulary neural language models", "word_idx": 52069, "sentence_idx": 704, "label": "unlabeled"}, {"type": "text", "expr": "Chen et\u00a0al", "word_idx": 52135, "sentence_idx": 705, "label": "unlabeled"}, {"type": "text", "expr": "(2015)Chen, Grangier, and Auli", "word_idx": 52145, "sentence_idx": 706, "label": "unlabeled"}, {"type": "text", "expr": "Chen, Welin, Grangier, David, and Auli, Michael", "word_idx": 52175, "sentence_idx": 707, "label": "unlabeled"}, {"type": "text", "expr": "Strategies for training large vocabulary neural language models", "word_idx": 52222, "sentence_idx": 708, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1512", "word_idx": 52285, "sentence_idx": 709, "label": "unlabeled"}, {"type": "text", "expr": "04906 , 2015", "word_idx": 52310, "sentence_idx": 710, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1512", "word_idx": 52322, "sentence_idx": 711, "label": "unlabeled"}, {"type": "text", "expr": "04906", "word_idx": 52347, "sentence_idx": 712, "label": "unlabeled"}, {"type": "text", "expr": "Collobert & Weston(2008)Collobert and Weston \nCollobert, Ronan and Weston, Jason", "word_idx": 52352, "sentence_idx": 713, "label": "unlabeled"}, {"type": "text", "expr": "\n\n A unified architecture for natural language processing: Deep neural\nnetworks with multitask learning", "word_idx": 52432, "sentence_idx": 714, "label": "unlabeled"}, {"type": "text", "expr": "Collobert & Weston(2008)Collobert and Weston", "word_idx": 52535, "sentence_idx": 715, "label": "unlabeled"}, {"type": "text", "expr": "Collobert, Ronan and Weston, Jason", "word_idx": 52579, "sentence_idx": 716, "label": "unlabeled"}, {"type": "text", "expr": "A unified architecture for natural language processing: Deep neural\nnetworks with multitask learning", "word_idx": 52613, "sentence_idx": 717, "label": "unlabeled"}, {"type": "text", "expr": "In  Proceedings of the 25th international conference on Machine\nlearning , pp", "word_idx": 52713, "sentence_idx": 718, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0160\u2013167", "word_idx": 52790, "sentence_idx": 719, "label": "unlabeled"}, {"type": "text", "expr": " ACM, 2008", "word_idx": 52798, "sentence_idx": 720, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the 25th international conference on Machine\nlearning", "word_idx": 52808, "sentence_idx": 721, "label": "unlabeled"}, {"type": "text", "expr": "Dauphin et\u00a0al", "word_idx": 52876, "sentence_idx": 722, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Dauphin, Fan, Auli, and\nGrangier \nDauphin, Yann\u00a0N, Fan, Angela, Auli, Michael, and Grangier, David", "word_idx": 52889, "sentence_idx": 723, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Language modeling with gated convolutional networks", "word_idx": 52993, "sentence_idx": 724, "label": "unlabeled"}, {"type": "text", "expr": "Dauphin et\u00a0al", "word_idx": 53047, "sentence_idx": 725, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Dauphin, Fan, Auli, and\nGrangier", "word_idx": 53060, "sentence_idx": 726, "label": "unlabeled"}, {"type": "text", "expr": "Dauphin, Yann\u00a0N, Fan, Angela, Auli, Michael, and Grangier, David", "word_idx": 53098, "sentence_idx": 727, "label": "unlabeled"}, {"type": "text", "expr": "Language modeling with gated convolutional networks", "word_idx": 53162, "sentence_idx": 728, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1612", "word_idx": 53213, "sentence_idx": 729, "label": "unlabeled"}, {"type": "text", "expr": "08083 , 2016", "word_idx": 53238, "sentence_idx": 730, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1612", "word_idx": 53250, "sentence_idx": 731, "label": "unlabeled"}, {"type": "text", "expr": "08083", "word_idx": 53275, "sentence_idx": 732, "label": "unlabeled"}, {"type": "text", "expr": "Finn et\u00a0al", "word_idx": 53280, "sentence_idx": 733, "label": "unlabeled"}, {"type": "text", "expr": "(2017)Finn, Abbeel, and Levine \nFinn, Chelsea, Abbeel, Pieter, and Levine, Sergey", "word_idx": 53290, "sentence_idx": 734, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Model-agnostic meta-learning for fast adaptation of deep networks", "word_idx": 53371, "sentence_idx": 735, "label": "unlabeled"}, {"type": "text", "expr": "Finn et\u00a0al", "word_idx": 53439, "sentence_idx": 736, "label": "unlabeled"}, {"type": "text", "expr": "(2017)Finn, Abbeel, and Levine", "word_idx": 53449, "sentence_idx": 737, "label": "unlabeled"}, {"type": "text", "expr": "Finn, Chelsea, Abbeel, Pieter, and Levine, Sergey", "word_idx": 53479, "sentence_idx": 738, "label": "unlabeled"}, {"type": "text", "expr": "Model-agnostic meta-learning for fast adaptation of deep networks", "word_idx": 53528, "sentence_idx": 739, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1703", "word_idx": 53593, "sentence_idx": 740, "label": "unlabeled"}, {"type": "text", "expr": "03400 , 2017", "word_idx": 53618, "sentence_idx": 741, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1703", "word_idx": 53630, "sentence_idx": 742, "label": "unlabeled"}, {"type": "text", "expr": "03400", "word_idx": 53655, "sentence_idx": 743, "label": "unlabeled"}, {"type": "text", "expr": "Glorot & Bengio(2010)Glorot and Bengio \nGlorot, Xavier and Bengio, Yoshua", "word_idx": 53660, "sentence_idx": 744, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Understanding the difficulty of training deep feedforward neural\nnetworks", "word_idx": 53733, "sentence_idx": 745, "label": "unlabeled"}, {"type": "text", "expr": "Glorot & Bengio(2010)Glorot and Bengio", "word_idx": 53809, "sentence_idx": 746, "label": "unlabeled"}, {"type": "text", "expr": "Glorot, Xavier and Bengio, Yoshua", "word_idx": 53847, "sentence_idx": 747, "label": "unlabeled"}, {"type": "text", "expr": "Understanding the difficulty of training deep feedforward neural\nnetworks", "word_idx": 53880, "sentence_idx": 748, "label": "unlabeled"}, {"type": "text", "expr": "In  Proceedings of the Thirteenth International Conference on\nArtificial Intelligence and Statistics , pp", "word_idx": 53953, "sentence_idx": 749, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0249\u2013256, 2010", "word_idx": 54058, "sentence_idx": 750, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the Thirteenth International Conference on\nArtificial Intelligence and Statistics", "word_idx": 54072, "sentence_idx": 751, "label": "unlabeled"}, {"type": "text", "expr": "Goodman(2001) \nGoodman, Joshua", "word_idx": 54168, "sentence_idx": 752, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Classes for fast maximum entropy training", "word_idx": 54198, "sentence_idx": 753, "label": "unlabeled"}, {"type": "text", "expr": "Goodman(2001)", "word_idx": 54242, "sentence_idx": 754, "label": "unlabeled"}, {"type": "text", "expr": "Goodman, Joshua", "word_idx": 54255, "sentence_idx": 755, "label": "unlabeled"}, {"type": "text", "expr": "Classes for fast maximum entropy training", "word_idx": 54270, "sentence_idx": 756, "label": "unlabeled"}, {"type": "text", "expr": "In  Acoustics, Speech, and Signal Processing, 2001", "word_idx": 54311, "sentence_idx": 757, "label": "unlabeled"}, {"type": "text", "expr": "\nProceedings", "word_idx": 54361, "sentence_idx": 758, "label": "unlabeled"}, {"type": "text", "expr": "(ICASSP\u201901)", "word_idx": 54373, "sentence_idx": 759, "label": "unlabeled"}, {"type": "text", "expr": " 2001 IEEE International Conference on , volume\u00a01,\npp", "word_idx": 54384, "sentence_idx": 760, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0561\u2013564", "word_idx": 54437, "sentence_idx": 761, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 2001", "word_idx": 54445, "sentence_idx": 762, "label": "unlabeled"}, {"type": "text", "expr": "Acoustics, Speech, and Signal Processing, 2001", "word_idx": 54456, "sentence_idx": 763, "label": "unlabeled"}, {"type": "text", "expr": "\nProceedings", "word_idx": 54502, "sentence_idx": 764, "label": "unlabeled"}, {"type": "text", "expr": "(ICASSP\u201901)", "word_idx": 54514, "sentence_idx": 765, "label": "unlabeled"}, {"type": "text", "expr": " 2001 IEEE International Conference on", "word_idx": 54525, "sentence_idx": 766, "label": "unlabeled"}, {"type": "text", "expr": "Grave et\u00a0al", "word_idx": 54563, "sentence_idx": 767, "label": "unlabeled"}, {"type": "text", "expr": "(2016a)Grave, Joulin, Ciss\u00e9, Grangier,\nand J\u00e9gou \nGrave, Edouard, Joulin, Armand, Ciss\u00e9, Moustapha, Grangier, David, and\nJ\u00e9gou, Herv\u00e9", "word_idx": 54574, "sentence_idx": 768, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Efficient softmax approximation for gpus", "word_idx": 54707, "sentence_idx": 769, "label": "unlabeled"}, {"type": "text", "expr": "Grave et\u00a0al", "word_idx": 54750, "sentence_idx": 770, "label": "unlabeled"}, {"type": "text", "expr": "(2016a)Grave, Joulin, Ciss\u00e9, Grangier,\nand J\u00e9gou", "word_idx": 54761, "sentence_idx": 771, "label": "unlabeled"}, {"type": "text", "expr": "Grave, Edouard, Joulin, Armand, Ciss\u00e9, Moustapha, Grangier, David, and\nJ\u00e9gou, Herv\u00e9", "word_idx": 54809, "sentence_idx": 772, "label": "unlabeled"}, {"type": "text", "expr": "Efficient softmax approximation for gpus", "word_idx": 54892, "sentence_idx": 773, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1609", "word_idx": 54932, "sentence_idx": 774, "label": "unlabeled"}, {"type": "text", "expr": "04309 , 2016a", "word_idx": 54957, "sentence_idx": 775, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1609", "word_idx": 54970, "sentence_idx": 776, "label": "unlabeled"}, {"type": "text", "expr": "04309", "word_idx": 54995, "sentence_idx": 777, "label": "unlabeled"}, {"type": "text", "expr": "Grave et\u00a0al", "word_idx": 55000, "sentence_idx": 778, "label": "unlabeled"}, {"type": "text", "expr": "(2016b)Grave, Joulin, and\nUsunier \nGrave, Edouard, Joulin, Armand, and Usunier, Nicolas", "word_idx": 55011, "sentence_idx": 779, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Improving neural language models with a continuous cache", "word_idx": 55098, "sentence_idx": 780, "label": "unlabeled"}, {"type": "text", "expr": "Grave et\u00a0al", "word_idx": 55157, "sentence_idx": 781, "label": "unlabeled"}, {"type": "text", "expr": "(2016b)Grave, Joulin, and\nUsunier", "word_idx": 55168, "sentence_idx": 782, "label": "unlabeled"}, {"type": "text", "expr": "Grave, Edouard, Joulin, Armand, and Usunier, Nicolas", "word_idx": 55201, "sentence_idx": 783, "label": "unlabeled"}, {"type": "text", "expr": "Improving neural language models with a continuous cache", "word_idx": 55253, "sentence_idx": 784, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1612", "word_idx": 55309, "sentence_idx": 785, "label": "unlabeled"}, {"type": "text", "expr": "04426 , 2016b", "word_idx": 55334, "sentence_idx": 786, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1612", "word_idx": 55347, "sentence_idx": 787, "label": "unlabeled"}, {"type": "text", "expr": "04426", "word_idx": 55372, "sentence_idx": 788, "label": "unlabeled"}, {"type": "text", "expr": "Grave et\u00a0al", "word_idx": 55377, "sentence_idx": 789, "label": "unlabeled"}, {"type": "text", "expr": "(2017)Grave, Cisse, and Joulin \nGrave, Edouard, Cisse, Moustapha\u00a0M, and Joulin, Armand", "word_idx": 55388, "sentence_idx": 790, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Unbounded cache model for online language modeling with open\nvocabulary", "word_idx": 55474, "sentence_idx": 791, "label": "unlabeled"}, {"type": "text", "expr": "Grave et\u00a0al", "word_idx": 55548, "sentence_idx": 792, "label": "unlabeled"}, {"type": "text", "expr": "(2017)Grave, Cisse, and Joulin", "word_idx": 55559, "sentence_idx": 793, "label": "unlabeled"}, {"type": "text", "expr": "Grave, Edouard, Cisse, Moustapha\u00a0M, and Joulin, Armand", "word_idx": 55589, "sentence_idx": 794, "label": "unlabeled"}, {"type": "text", "expr": "Unbounded cache model for online language modeling with open\nvocabulary", "word_idx": 55643, "sentence_idx": 795, "label": "unlabeled"}, {"type": "text", "expr": "In  Advances in Neural Information Processing Systems , pp", "word_idx": 55714, "sentence_idx": 796, "label": "unlabeled"}, {"type": "text", "expr": "\u00a06044\u20136054, 2017", "word_idx": 55772, "sentence_idx": 797, "label": "unlabeled"}, {"type": "text", "expr": "Advances in Neural Information Processing Systems", "word_idx": 55788, "sentence_idx": 798, "label": "unlabeled"}, {"type": "text", "expr": "Graves et\u00a0al", "word_idx": 55837, "sentence_idx": 799, "label": "unlabeled"}, {"type": "text", "expr": "(2014)Graves, Wayne, and Danihelka \nGraves, Alex, Wayne, Greg, and Danihelka, Ivo", "word_idx": 55849, "sentence_idx": 800, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Neural turing machines", "word_idx": 55930, "sentence_idx": 801, "label": "unlabeled"}, {"type": "text", "expr": "Graves et\u00a0al", "word_idx": 55955, "sentence_idx": 802, "label": "unlabeled"}, {"type": "text", "expr": "(2014)Graves, Wayne, and Danihelka", "word_idx": 55967, "sentence_idx": 803, "label": "unlabeled"}, {"type": "text", "expr": "Graves, Alex, Wayne, Greg, and Danihelka, Ivo", "word_idx": 56001, "sentence_idx": 804, "label": "unlabeled"}, {"type": "text", "expr": "Neural turing machines", "word_idx": 56046, "sentence_idx": 805, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1410", "word_idx": 56068, "sentence_idx": 806, "label": "unlabeled"}, {"type": "text", "expr": "5401 , 2014", "word_idx": 56093, "sentence_idx": 807, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1410", "word_idx": 56104, "sentence_idx": 808, "label": "unlabeled"}, {"type": "text", "expr": "Graves et\u00a0al", "word_idx": 56129, "sentence_idx": 809, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Graves, Wayne, Reynolds, Harley, Danihelka,\nGrabska-Barwi\u0144ska, Colmenarejo, Grefenstette, Ramalho, Agapiou,\net\u00a0al", "word_idx": 56141, "sentence_idx": 810, "label": "unlabeled"}, {"type": "text", "expr": " \nGraves, Alex, Wayne, Greg, Reynolds, Malcolm, Harley, Tim, Danihelka, Ivo,\nGrabska-Barwi\u0144ska, Agnieszka, Colmenarejo, Sergio\u00a0G\u00f3mez,\nGrefenstette, Edward, Ramalho, Tiago, Agapiou, John, et\u00a0al", "word_idx": 56260, "sentence_idx": 811, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Hybrid computing using a neural network with dynamic external memory", "word_idx": 56452, "sentence_idx": 812, "label": "unlabeled"}, {"type": "text", "expr": "Graves et\u00a0al", "word_idx": 56523, "sentence_idx": 813, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Graves, Wayne, Reynolds, Harley, Danihelka,\nGrabska-Barwi\u0144ska, Colmenarejo, Grefenstette, Ramalho, Agapiou,\net\u00a0al", "word_idx": 56535, "sentence_idx": 814, "label": "unlabeled"}, {"type": "text", "expr": "Graves, Alex, Wayne, Greg, Reynolds, Malcolm, Harley, Tim, Danihelka, Ivo,\nGrabska-Barwi\u0144ska, Agnieszka, Colmenarejo, Sergio\u00a0G\u00f3mez,\nGrefenstette, Edward, Ramalho, Tiago, Agapiou, John, et\u00a0al", "word_idx": 56654, "sentence_idx": 815, "label": "unlabeled"}, {"type": "text", "expr": "Hybrid computing using a neural network with dynamic external memory", "word_idx": 56844, "sentence_idx": 816, "label": "unlabeled"}, {"type": "text", "expr": "Nature , 538(7626):471, 2016", "word_idx": 56912, "sentence_idx": 817, "label": "unlabeled"}, {"type": "text", "expr": "Nature", "word_idx": 56940, "sentence_idx": 818, "label": "unlabeled"}, {"type": "text", "expr": "Gu et\u00a0al", "word_idx": 56946, "sentence_idx": 819, "label": "unlabeled"}, {"type": "text", "expr": "(2017)Gu, Wang, Cho, and Li \nGu, Jiatao, Wang, Yong, Cho, Kyunghyun, and Li, Victor\u00a0OK", "word_idx": 56954, "sentence_idx": 820, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Search engine guided non-parametric neural machine translation", "word_idx": 57040, "sentence_idx": 821, "label": "unlabeled"}, {"type": "text", "expr": "Gu et\u00a0al", "word_idx": 57105, "sentence_idx": 822, "label": "unlabeled"}, {"type": "text", "expr": "(2017)Gu, Wang, Cho, and Li", "word_idx": 57113, "sentence_idx": 823, "label": "unlabeled"}, {"type": "text", "expr": "Gu, Jiatao, Wang, Yong, Cho, Kyunghyun, and Li, Victor\u00a0OK", "word_idx": 57140, "sentence_idx": 824, "label": "unlabeled"}, {"type": "text", "expr": "Search engine guided non-parametric neural machine translation", "word_idx": 57197, "sentence_idx": 825, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1705", "word_idx": 57259, "sentence_idx": 826, "label": "unlabeled"}, {"type": "text", "expr": "07267 , 2017", "word_idx": 57284, "sentence_idx": 827, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1705", "word_idx": 57296, "sentence_idx": 828, "label": "unlabeled"}, {"type": "text", "expr": "07267", "word_idx": 57321, "sentence_idx": 829, "label": "unlabeled"}, {"type": "text", "expr": "Gulcehre et\u00a0al", "word_idx": 57326, "sentence_idx": 830, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Gulcehre, Ahn, Nallapati, Zhou, and\nBengio \nGulcehre, Caglar, Ahn, Sungjin, Nallapati, Ramesh, Zhou, Bowen, and Bengio,\nYoshua", "word_idx": 57340, "sentence_idx": 831, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Pointing the unknown words", "word_idx": 57472, "sentence_idx": 832, "label": "unlabeled"}, {"type": "text", "expr": "Gulcehre et\u00a0al", "word_idx": 57501, "sentence_idx": 833, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Gulcehre, Ahn, Nallapati, Zhou, and\nBengio", "word_idx": 57515, "sentence_idx": 834, "label": "unlabeled"}, {"type": "text", "expr": "Gulcehre, Caglar, Ahn, Sungjin, Nallapati, Ramesh, Zhou, Bowen, and Bengio,\nYoshua", "word_idx": 57563, "sentence_idx": 835, "label": "unlabeled"}, {"type": "text", "expr": "Pointing the unknown words", "word_idx": 57645, "sentence_idx": 836, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1603", "word_idx": 57671, "sentence_idx": 837, "label": "unlabeled"}, {"type": "text", "expr": "08148 , 2016", "word_idx": 57696, "sentence_idx": 838, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1603", "word_idx": 57708, "sentence_idx": 839, "label": "unlabeled"}, {"type": "text", "expr": "08148", "word_idx": 57733, "sentence_idx": 840, "label": "unlabeled"}, {"type": "text", "expr": "Hebb(1949) \nHebb, Donald\u00a0O", "word_idx": 57738, "sentence_idx": 841, "label": "unlabeled"}, {"type": "text", "expr": "\n\n The organization of behavior: A neurophysiological approach, 1949", "word_idx": 57764, "sentence_idx": 842, "label": "unlabeled"}, {"type": "text", "expr": "Hebb(1949)", "word_idx": 57832, "sentence_idx": 843, "label": "unlabeled"}, {"type": "text", "expr": "Hebb, Donald\u00a0O", "word_idx": 57842, "sentence_idx": 844, "label": "unlabeled"}, {"type": "text", "expr": "The organization of behavior: A neurophysiological approach, 1949", "word_idx": 57856, "sentence_idx": 845, "label": "unlabeled"}, {"type": "text", "expr": "Hochreiter & Schmidhuber(1997)Hochreiter and\nSchmidhuber \nHochreiter, Sepp and Schmidhuber, J\u00fcrgen", "word_idx": 57921, "sentence_idx": 846, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Long short-term memory", "word_idx": 58019, "sentence_idx": 847, "label": "unlabeled"}, {"type": "text", "expr": "Hochreiter & Schmidhuber(1997)Hochreiter and\nSchmidhuber", "word_idx": 58044, "sentence_idx": 848, "label": "unlabeled"}, {"type": "text", "expr": "Hochreiter, Sepp and Schmidhuber, J\u00fcrgen", "word_idx": 58100, "sentence_idx": 849, "label": "unlabeled"}, {"type": "text", "expr": "Long short-term memory", "word_idx": 58140, "sentence_idx": 850, "label": "unlabeled"}, {"type": "text", "expr": "Neural computation , 9(8):1735\u20131780, 1997", "word_idx": 58162, "sentence_idx": 851, "label": "unlabeled"}, {"type": "text", "expr": "Neural computation", "word_idx": 58203, "sentence_idx": 852, "label": "unlabeled"}, {"type": "text", "expr": "Hochreiter et\u00a0al", "word_idx": 58221, "sentence_idx": 853, "label": "unlabeled"}, {"type": "text", "expr": "(2001)Hochreiter, Younger, and\nConwell \nHochreiter, Sepp, Younger, A\u00a0Steven, and Conwell, Peter\u00a0R", "word_idx": 58237, "sentence_idx": 854, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Learning to learn using gradient descent", "word_idx": 58334, "sentence_idx": 855, "label": "unlabeled"}, {"type": "text", "expr": "Hochreiter et\u00a0al", "word_idx": 58377, "sentence_idx": 856, "label": "unlabeled"}, {"type": "text", "expr": "(2001)Hochreiter, Younger, and\nConwell", "word_idx": 58393, "sentence_idx": 857, "label": "unlabeled"}, {"type": "text", "expr": "Hochreiter, Sepp, Younger, A\u00a0Steven, and Conwell, Peter\u00a0R", "word_idx": 58431, "sentence_idx": 858, "label": "unlabeled"}, {"type": "text", "expr": "Learning to learn using gradient descent", "word_idx": 58488, "sentence_idx": 859, "label": "unlabeled"}, {"type": "text", "expr": "In  International Conference on Artificial Neural Networks ,\npp", "word_idx": 58528, "sentence_idx": 860, "label": "unlabeled"}, {"type": "text", "expr": "\u00a087\u201394", "word_idx": 58591, "sentence_idx": 861, "label": "unlabeled"}, {"type": "text", "expr": " Springer, 2001", "word_idx": 58597, "sentence_idx": 862, "label": "unlabeled"}, {"type": "text", "expr": "International Conference on Artificial Neural Networks", "word_idx": 58612, "sentence_idx": 863, "label": "unlabeled"}, {"type": "text", "expr": "Jozefowicz et\u00a0al", "word_idx": 58666, "sentence_idx": 864, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Jozefowicz, Vinyals, Schuster, Shazeer, and\nWu \nJozefowicz, Rafal, Vinyals, Oriol, Schuster, Mike, Shazeer, Noam, and Wu,\nYonghui", "word_idx": 58682, "sentence_idx": 865, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Exploring the limits of language modeling", "word_idx": 58817, "sentence_idx": 866, "label": "unlabeled"}, {"type": "text", "expr": "Jozefowicz et\u00a0al", "word_idx": 58861, "sentence_idx": 867, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Jozefowicz, Vinyals, Schuster, Shazeer, and\nWu", "word_idx": 58877, "sentence_idx": 868, "label": "unlabeled"}, {"type": "text", "expr": "Jozefowicz, Rafal, Vinyals, Oriol, Schuster, Mike, Shazeer, Noam, and Wu,\nYonghui", "word_idx": 58929, "sentence_idx": 869, "label": "unlabeled"}, {"type": "text", "expr": "Exploring the limits of language modeling", "word_idx": 59010, "sentence_idx": 870, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1602", "word_idx": 59051, "sentence_idx": 871, "label": "unlabeled"}, {"type": "text", "expr": "02410 , 2016", "word_idx": 59076, "sentence_idx": 872, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1602", "word_idx": 59088, "sentence_idx": 873, "label": "unlabeled"}, {"type": "text", "expr": "02410", "word_idx": 59113, "sentence_idx": 874, "label": "unlabeled"}, {"type": "text", "expr": "Kaiser et\u00a0al", "word_idx": 59118, "sentence_idx": 875, "label": "unlabeled"}, {"type": "text", "expr": "(2017)Kaiser, Nachum, Roy, and\nBengio \nKaiser, Lukasz, Nachum, Ofir, Roy, Aurko, and Bengio, Samy", "word_idx": 59130, "sentence_idx": 876, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Learning to remember rare events", "word_idx": 59227, "sentence_idx": 877, "label": "unlabeled"}, {"type": "text", "expr": "Kaiser et\u00a0al", "word_idx": 59262, "sentence_idx": 878, "label": "unlabeled"}, {"type": "text", "expr": "(2017)Kaiser, Nachum, Roy, and\nBengio", "word_idx": 59274, "sentence_idx": 879, "label": "unlabeled"}, {"type": "text", "expr": "Kaiser, Lukasz, Nachum, Ofir, Roy, Aurko, and Bengio, Samy", "word_idx": 59311, "sentence_idx": 880, "label": "unlabeled"}, {"type": "text", "expr": "Learning to remember rare events", "word_idx": 59369, "sentence_idx": 881, "label": "unlabeled"}, {"type": "text", "expr": "International Conference on Learning Representations , 2017", "word_idx": 59401, "sentence_idx": 882, "label": "unlabeled"}, {"type": "text", "expr": "International Conference on Learning Representations", "word_idx": 59460, "sentence_idx": 883, "label": "unlabeled"}, {"type": "text", "expr": "Kalchbrenner et\u00a0al", "word_idx": 59512, "sentence_idx": 884, "label": "unlabeled"}, {"type": "text", "expr": "(2014)Kalchbrenner, Grefenstette, and\nBlunsom \nKalchbrenner, Nal, Grefenstette, Edward, and Blunsom, Phil", "word_idx": 59530, "sentence_idx": 885, "label": "unlabeled"}, {"type": "text", "expr": "\n\n A convolutional neural network for modelling sentences", "word_idx": 59635, "sentence_idx": 886, "label": "unlabeled"}, {"type": "text", "expr": "Kalchbrenner et\u00a0al", "word_idx": 59692, "sentence_idx": 887, "label": "unlabeled"}, {"type": "text", "expr": "(2014)Kalchbrenner, Grefenstette, and\nBlunsom", "word_idx": 59710, "sentence_idx": 888, "label": "unlabeled"}, {"type": "text", "expr": "Kalchbrenner, Nal, Grefenstette, Edward, and Blunsom, Phil", "word_idx": 59755, "sentence_idx": 889, "label": "unlabeled"}, {"type": "text", "expr": "A convolutional neural network for modelling sentences", "word_idx": 59813, "sentence_idx": 890, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1404", "word_idx": 59867, "sentence_idx": 891, "label": "unlabeled"}, {"type": "text", "expr": "2188 , 2014", "word_idx": 59892, "sentence_idx": 892, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1404", "word_idx": 59903, "sentence_idx": 893, "label": "unlabeled"}, {"type": "text", "expr": "Kawakami et\u00a0al", "word_idx": 59928, "sentence_idx": 894, "label": "unlabeled"}, {"type": "text", "expr": "(2017)Kawakami, Dyer, and\nBlunsom \nKawakami, Kazuya, Dyer, Chris, and Blunsom, Phil", "word_idx": 59942, "sentence_idx": 895, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Learning to create and reuse words in open-vocabulary neural language\nmodeling", "word_idx": 60025, "sentence_idx": 896, "label": "unlabeled"}, {"type": "text", "expr": "Kawakami et\u00a0al", "word_idx": 60106, "sentence_idx": 897, "label": "unlabeled"}, {"type": "text", "expr": "(2017)Kawakami, Dyer, and\nBlunsom", "word_idx": 60120, "sentence_idx": 898, "label": "unlabeled"}, {"type": "text", "expr": "Kawakami, Kazuya, Dyer, Chris, and Blunsom, Phil", "word_idx": 60153, "sentence_idx": 899, "label": "unlabeled"}, {"type": "text", "expr": "Learning to create and reuse words in open-vocabulary neural language\nmodeling", "word_idx": 60201, "sentence_idx": 900, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1704", "word_idx": 60279, "sentence_idx": 901, "label": "unlabeled"}, {"type": "text", "expr": "06986 , 2017", "word_idx": 60304, "sentence_idx": 902, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1704", "word_idx": 60316, "sentence_idx": 903, "label": "unlabeled"}, {"type": "text", "expr": "06986", "word_idx": 60341, "sentence_idx": 904, "label": "unlabeled"}, {"type": "text", "expr": "Kneser & Ney(1995)Kneser and Ney \nKneser, Reinhard and Ney, Hermann", "word_idx": 60346, "sentence_idx": 905, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Improved backing-off for m-gram language modeling", "word_idx": 60413, "sentence_idx": 906, "label": "unlabeled"}, {"type": "text", "expr": "Kneser & Ney(1995)Kneser and Ney", "word_idx": 60465, "sentence_idx": 907, "label": "unlabeled"}, {"type": "text", "expr": "Kneser, Reinhard and Ney, Hermann", "word_idx": 60497, "sentence_idx": 908, "label": "unlabeled"}, {"type": "text", "expr": "Improved backing-off for m-gram language modeling", "word_idx": 60530, "sentence_idx": 909, "label": "unlabeled"}, {"type": "text", "expr": "In  Acoustics, Speech, and Signal Processing, 1995", "word_idx": 60579, "sentence_idx": 910, "label": "unlabeled"}, {"type": "text", "expr": " ICASSP-95", "word_idx": 60629, "sentence_idx": 911, "label": "unlabeled"}, {"type": "text", "expr": ",\n1995 International Conference on , volume\u00a01, pp", "word_idx": 60639, "sentence_idx": 912, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0181\u2013184", "word_idx": 60688, "sentence_idx": 913, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 1995", "word_idx": 60696, "sentence_idx": 914, "label": "unlabeled"}, {"type": "text", "expr": "Acoustics, Speech, and Signal Processing, 1995", "word_idx": 60707, "sentence_idx": 915, "label": "unlabeled"}, {"type": "text", "expr": " ICASSP-95", "word_idx": 60753, "sentence_idx": 916, "label": "unlabeled"}, {"type": "text", "expr": ",\n1995 International Conference on", "word_idx": 60763, "sentence_idx": 917, "label": "unlabeled"}, {"type": "text", "expr": "Lake et\u00a0al", "word_idx": 60797, "sentence_idx": 918, "label": "unlabeled"}, {"type": "text", "expr": "(2015)Lake, Salakhutdinov, and Tenenbaum \nLake, Brenden\u00a0M, Salakhutdinov, Ruslan, and Tenenbaum, Joshua\u00a0B", "word_idx": 60807, "sentence_idx": 919, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Human-level concept learning through probabilistic program induction", "word_idx": 60912, "sentence_idx": 920, "label": "unlabeled"}, {"type": "text", "expr": "Lake et\u00a0al", "word_idx": 60983, "sentence_idx": 921, "label": "unlabeled"}, {"type": "text", "expr": "(2015)Lake, Salakhutdinov, and Tenenbaum", "word_idx": 60993, "sentence_idx": 922, "label": "unlabeled"}, {"type": "text", "expr": "Lake, Brenden\u00a0M, Salakhutdinov, Ruslan, and Tenenbaum, Joshua\u00a0B", "word_idx": 61033, "sentence_idx": 923, "label": "unlabeled"}, {"type": "text", "expr": "Human-level concept learning through probabilistic program induction", "word_idx": 61096, "sentence_idx": 924, "label": "unlabeled"}, {"type": "text", "expr": "Science , 350(6266):1332\u20131338, 2015", "word_idx": 61164, "sentence_idx": 925, "label": "unlabeled"}, {"type": "text", "expr": "Science", "word_idx": 61199, "sentence_idx": 926, "label": "unlabeled"}, {"type": "text", "expr": "McClelland et\u00a0al", "word_idx": 61206, "sentence_idx": 927, "label": "unlabeled"}, {"type": "text", "expr": "(1995)McClelland, McNaughton, and\nO\u2019reilly \nMcClelland, James\u00a0L, McNaughton, Bruce\u00a0L, and O\u2019reilly, Randall\u00a0C", "word_idx": 61222, "sentence_idx": 928, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Why there are complementary learning systems in the hippocampus and\nneocortex: insights from the successes and failures of connectionist models\nof learning and memory", "word_idx": 61331, "sentence_idx": 929, "label": "unlabeled"}, {"type": "text", "expr": "McClelland et\u00a0al", "word_idx": 61500, "sentence_idx": 930, "label": "unlabeled"}, {"type": "text", "expr": "(1995)McClelland, McNaughton, and\nO\u2019reilly", "word_idx": 61516, "sentence_idx": 931, "label": "unlabeled"}, {"type": "text", "expr": "McClelland, James\u00a0L, McNaughton, Bruce\u00a0L, and O\u2019reilly, Randall\u00a0C", "word_idx": 61558, "sentence_idx": 932, "label": "unlabeled"}, {"type": "text", "expr": "Why there are complementary learning systems in the hippocampus and\nneocortex: insights from the successes and failures of connectionist models\nof learning and memory", "word_idx": 61623, "sentence_idx": 933, "label": "unlabeled"}, {"type": "text", "expr": "Psychological review , 102(3):419, 1995", "word_idx": 61789, "sentence_idx": 934, "label": "unlabeled"}, {"type": "text", "expr": "Psychological review", "word_idx": 61828, "sentence_idx": 935, "label": "unlabeled"}, {"type": "text", "expr": "Melis et\u00a0al", "word_idx": 61848, "sentence_idx": 936, "label": "unlabeled"}, {"type": "text", "expr": "(2017)Melis, Dyer, and Blunsom \nMelis, G\u00e1bor, Dyer, Chris, and Blunsom, Phil", "word_idx": 61859, "sentence_idx": 937, "label": "unlabeled"}, {"type": "text", "expr": "\n\n On the state of the art of evaluation in neural language models", "word_idx": 61935, "sentence_idx": 938, "label": "unlabeled"}, {"type": "text", "expr": "Melis et\u00a0al", "word_idx": 62001, "sentence_idx": 939, "label": "unlabeled"}, {"type": "text", "expr": "(2017)Melis, Dyer, and Blunsom", "word_idx": 62012, "sentence_idx": 940, "label": "unlabeled"}, {"type": "text", "expr": "Melis, G\u00e1bor, Dyer, Chris, and Blunsom, Phil", "word_idx": 62042, "sentence_idx": 941, "label": "unlabeled"}, {"type": "text", "expr": "On the state of the art of evaluation in neural language models", "word_idx": 62086, "sentence_idx": 942, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1707", "word_idx": 62149, "sentence_idx": 943, "label": "unlabeled"}, {"type": "text", "expr": "05589 , 2017", "word_idx": 62174, "sentence_idx": 944, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1707", "word_idx": 62186, "sentence_idx": 945, "label": "unlabeled"}, {"type": "text", "expr": "05589", "word_idx": 62211, "sentence_idx": 946, "label": "unlabeled"}, {"type": "text", "expr": "Merity et\u00a0al", "word_idx": 62216, "sentence_idx": 947, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Merity, Xiong, Bradbury, and\nSocher \nMerity, Stephen, Xiong, Caiming, Bradbury, James, and Socher, Richard", "word_idx": 62228, "sentence_idx": 948, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Pointer sentinel mixture models", "word_idx": 62340, "sentence_idx": 949, "label": "unlabeled"}, {"type": "text", "expr": "Merity et\u00a0al", "word_idx": 62374, "sentence_idx": 950, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Merity, Xiong, Bradbury, and\nSocher", "word_idx": 62386, "sentence_idx": 951, "label": "unlabeled"}, {"type": "text", "expr": "Merity, Stephen, Xiong, Caiming, Bradbury, James, and Socher, Richard", "word_idx": 62427, "sentence_idx": 952, "label": "unlabeled"}, {"type": "text", "expr": "Pointer sentinel mixture models", "word_idx": 62496, "sentence_idx": 953, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1609", "word_idx": 62527, "sentence_idx": 954, "label": "unlabeled"}, {"type": "text", "expr": "07843 , 2016", "word_idx": 62552, "sentence_idx": 955, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1609", "word_idx": 62564, "sentence_idx": 956, "label": "unlabeled"}, {"type": "text", "expr": "07843", "word_idx": 62589, "sentence_idx": 957, "label": "unlabeled"}, {"type": "text", "expr": "O\u2019Reilly(1996b) \nO\u2019Reilly, Randall\u00a0C", "word_idx": 62594, "sentence_idx": 958, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Biologically plausible error-driven learning using local activation\ndifferences: The generalized recirculation algorithm", "word_idx": 62630, "sentence_idx": 959, "label": "unlabeled"}, {"type": "text", "expr": "O\u2019Reilly(1996b)", "word_idx": 62753, "sentence_idx": 960, "label": "unlabeled"}, {"type": "text", "expr": "O\u2019Reilly, Randall\u00a0C", "word_idx": 62768, "sentence_idx": 961, "label": "unlabeled"}, {"type": "text", "expr": "Biologically plausible error-driven learning using local activation\ndifferences: The generalized recirculation algorithm", "word_idx": 62787, "sentence_idx": 962, "label": "unlabeled"}, {"type": "text", "expr": "Neural computation , 8(5):895\u2013938, 1996b", "word_idx": 62907, "sentence_idx": 963, "label": "unlabeled"}, {"type": "text", "expr": "Neural computation", "word_idx": 62947, "sentence_idx": 964, "label": "unlabeled"}, {"type": "text", "expr": "O\u2019Reilly(1996a) \nO\u2019Reilly, Randall\u00a0C", "word_idx": 62965, "sentence_idx": 965, "label": "unlabeled"}, {"type": "text", "expr": "\n\n PhD thesis, PhD thesis, Carnegie Mellon University, Pittsburgh, PA,\nUSA, 1996a", "word_idx": 63001, "sentence_idx": 966, "label": "unlabeled"}, {"type": "text", "expr": "O\u2019Reilly(1996a)", "word_idx": 63082, "sentence_idx": 967, "label": "unlabeled"}, {"type": "text", "expr": "O\u2019Reilly, Randall\u00a0C", "word_idx": 63097, "sentence_idx": 968, "label": "unlabeled"}, {"type": "text", "expr": "The Leabra model of neural interactions and learning in the\nneocortex ", "word_idx": 63116, "sentence_idx": 969, "label": "unlabeled"}, {"type": "text", "expr": "The Leabra model of neural interactions and learning in the\nneocortex", "word_idx": 63186, "sentence_idx": 970, "label": "unlabeled"}, {"type": "text", "expr": "PhD thesis, PhD thesis, Carnegie Mellon University, Pittsburgh, PA,\nUSA, 1996a", "word_idx": 63255, "sentence_idx": 971, "label": "unlabeled"}, {"type": "text", "expr": "Parker et\u00a0al", "word_idx": 63333, "sentence_idx": 972, "label": "unlabeled"}, {"type": "text", "expr": "(2011)Parker, Graff, Kong, Chen, and\nMaeda \nParker, Robert, Graff, David, Kong, Junbo, Chen, Ke, and Maeda, Kazuaki", "word_idx": 63345, "sentence_idx": 973, "label": "unlabeled"}, {"type": "text", "expr": "\n\n English gigaword fifth edition ldc2011t07", "word_idx": 63460, "sentence_idx": 974, "label": "unlabeled"}, {"type": "text", "expr": "Parker et\u00a0al", "word_idx": 63504, "sentence_idx": 975, "label": "unlabeled"}, {"type": "text", "expr": "(2011)Parker, Graff, Kong, Chen, and\nMaeda", "word_idx": 63516, "sentence_idx": 976, "label": "unlabeled"}, {"type": "text", "expr": "Parker, Robert, Graff, David, Kong, Junbo, Chen, Ke, and Maeda, Kazuaki", "word_idx": 63558, "sentence_idx": 977, "label": "unlabeled"}, {"type": "text", "expr": "English gigaword fifth edition ldc2011t07", "word_idx": 63629, "sentence_idx": 978, "label": "unlabeled"}, {"type": "text", "expr": "Philadelphia: Linguistic Data Consortium , 2011", "word_idx": 63670, "sentence_idx": 979, "label": "unlabeled"}, {"type": "text", "expr": "Philadelphia: Linguistic Data Consortium", "word_idx": 63717, "sentence_idx": 980, "label": "unlabeled"}, {"type": "text", "expr": "Rae et\u00a0al", "word_idx": 63757, "sentence_idx": 981, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Rae, Hunt, Danihelka, Harley, Senior, Wayne, Graves,\nand Lillicrap \nRae, Jack, Hunt, Jonathan\u00a0J, Danihelka, Ivo, Harley, Timothy, Senior, Andrew\u00a0W,\nWayne, Gregory, Graves, Alex, and Lillicrap, Tim", "word_idx": 63766, "sentence_idx": 982, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Scaling memory-augmented neural networks with sparse reads and\nwrites", "word_idx": 63968, "sentence_idx": 983, "label": "unlabeled"}, {"type": "text", "expr": "Rae et\u00a0al", "word_idx": 64040, "sentence_idx": 984, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Rae, Hunt, Danihelka, Harley, Senior, Wayne, Graves,\nand Lillicrap", "word_idx": 64049, "sentence_idx": 985, "label": "unlabeled"}, {"type": "text", "expr": "Rae, Jack, Hunt, Jonathan\u00a0J, Danihelka, Ivo, Harley, Timothy, Senior, Andrew\u00a0W,\nWayne, Gregory, Graves, Alex, and Lillicrap, Tim", "word_idx": 64121, "sentence_idx": 986, "label": "unlabeled"}, {"type": "text", "expr": "Scaling memory-augmented neural networks with sparse reads and\nwrites", "word_idx": 64249, "sentence_idx": 987, "label": "unlabeled"}, {"type": "text", "expr": "In  Advances in Neural Information Processing Systems , pp", "word_idx": 64318, "sentence_idx": 988, "label": "unlabeled"}, {"type": "text", "expr": "\u00a03621\u20133629, 2016", "word_idx": 64376, "sentence_idx": 989, "label": "unlabeled"}, {"type": "text", "expr": "Advances in Neural Information Processing Systems", "word_idx": 64392, "sentence_idx": 990, "label": "unlabeled"}, {"type": "text", "expr": "Santoro et\u00a0al", "word_idx": 64441, "sentence_idx": 991, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Santoro, Bartunov, Botvinick, Wierstra, and\nLillicrap \nSantoro, Adam, Bartunov, Sergey, Botvinick, Matthew, Wierstra, Daan, and\nLillicrap, Timothy", "word_idx": 64454, "sentence_idx": 992, "label": "unlabeled"}, {"type": "text", "expr": "\n\n One-shot learning with memory-augmented neural networks", "word_idx": 64606, "sentence_idx": 993, "label": "unlabeled"}, {"type": "text", "expr": "Santoro et\u00a0al", "word_idx": 64664, "sentence_idx": 994, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Santoro, Bartunov, Botvinick, Wierstra, and\nLillicrap", "word_idx": 64677, "sentence_idx": 995, "label": "unlabeled"}, {"type": "text", "expr": "Santoro, Adam, Bartunov, Sergey, Botvinick, Matthew, Wierstra, Daan, and\nLillicrap, Timothy", "word_idx": 64736, "sentence_idx": 996, "label": "unlabeled"}, {"type": "text", "expr": "One-shot learning with memory-augmented neural networks", "word_idx": 64827, "sentence_idx": 997, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1605", "word_idx": 64882, "sentence_idx": 998, "label": "unlabeled"}, {"type": "text", "expr": "06065 , 2016", "word_idx": 64907, "sentence_idx": 999, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1605", "word_idx": 64919, "sentence_idx": 1000, "label": "unlabeled"}, {"type": "text", "expr": "06065", "word_idx": 64944, "sentence_idx": 1001, "label": "unlabeled"}, {"type": "text", "expr": "Sprechmann et\u00a0al", "word_idx": 64949, "sentence_idx": 1002, "label": "unlabeled"}, {"type": "text", "expr": "(2018)Sprechmann, Jayakumar, Rae, Pritzel,\nPuigdomenech, Uria, Vinyals, Hassabis, Pascanu, and\nBlundell \nSprechmann, Pablo, Jayakumar, Siddhant, Rae, W", "word_idx": 64965, "sentence_idx": 1003, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Jack, Pritzel, Alexander,\nPuigdomenech, Adria\u00a0Badia, Uria, Benigno, Vinyals, Oriol, Hassabis, Demis,\nPascanu, Razvan, and Blundell, Charles", "word_idx": 65116, "sentence_idx": 1004, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Memory-based parameter adaptation", "word_idx": 65256, "sentence_idx": 1005, "label": "unlabeled"}, {"type": "text", "expr": "Sprechmann et\u00a0al", "word_idx": 65292, "sentence_idx": 1006, "label": "unlabeled"}, {"type": "text", "expr": "(2018)Sprechmann, Jayakumar, Rae, Pritzel,\nPuigdomenech, Uria, Vinyals, Hassabis, Pascanu, and\nBlundell", "word_idx": 65308, "sentence_idx": 1007, "label": "unlabeled"}, {"type": "text", "expr": "Sprechmann, Pablo, Jayakumar, Siddhant, Rae, W", "word_idx": 65411, "sentence_idx": 1008, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Jack, Pritzel, Alexander,\nPuigdomenech, Adria\u00a0Badia, Uria, Benigno, Vinyals, Oriol, Hassabis, Demis,\nPascanu, Razvan, and Blundell, Charles", "word_idx": 65457, "sentence_idx": 1009, "label": "unlabeled"}, {"type": "text", "expr": "Memory-based parameter adaptation", "word_idx": 65597, "sentence_idx": 1010, "label": "unlabeled"}, {"type": "text", "expr": "International Conference on Learning Representations , 2018", "word_idx": 65630, "sentence_idx": 1011, "label": "unlabeled"}, {"type": "text", "expr": "International Conference on Learning Representations", "word_idx": 65689, "sentence_idx": 1012, "label": "unlabeled"}, {"type": "text", "expr": "Sukhbaatar et\u00a0al", "word_idx": 65741, "sentence_idx": 1013, "label": "unlabeled"}, {"type": "text", "expr": "(2015)Sukhbaatar, Weston, Fergus,\net\u00a0al", "word_idx": 65757, "sentence_idx": 1014, "label": "unlabeled"}, {"type": "text", "expr": " \nSukhbaatar, Sainbayar, Weston, Jason, Fergus, Rob, et\u00a0al", "word_idx": 65796, "sentence_idx": 1015, "label": "unlabeled"}, {"type": "text", "expr": "\n\n End-to-end memory networks", "word_idx": 65854, "sentence_idx": 1016, "label": "unlabeled"}, {"type": "text", "expr": "Sukhbaatar et\u00a0al", "word_idx": 65883, "sentence_idx": 1017, "label": "unlabeled"}, {"type": "text", "expr": "(2015)Sukhbaatar, Weston, Fergus,\net\u00a0al", "word_idx": 65899, "sentence_idx": 1018, "label": "unlabeled"}, {"type": "text", "expr": "Sukhbaatar, Sainbayar, Weston, Jason, Fergus, Rob, et\u00a0al", "word_idx": 65938, "sentence_idx": 1019, "label": "unlabeled"}, {"type": "text", "expr": "End-to-end memory networks", "word_idx": 65994, "sentence_idx": 1020, "label": "unlabeled"}, {"type": "text", "expr": "In  Advances in neural information processing systems , pp", "word_idx": 66020, "sentence_idx": 1021, "label": "unlabeled"}, {"type": "text", "expr": "\u00a02440\u20132448, 2015", "word_idx": 66078, "sentence_idx": 1022, "label": "unlabeled"}, {"type": "text", "expr": "Advances in neural information processing systems", "word_idx": 66094, "sentence_idx": 1023, "label": "unlabeled"}, {"type": "text", "expr": "Sundermeyer et\u00a0al", "word_idx": 66143, "sentence_idx": 1024, "label": "unlabeled"}, {"type": "text", "expr": "(2012)Sundermeyer, Schl\u00fcter, and\nNey \nSundermeyer, Martin, Schl\u00fcter, Ralf, and Ney, Hermann", "word_idx": 66160, "sentence_idx": 1025, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Lstm neural networks for language modeling", "word_idx": 66251, "sentence_idx": 1026, "label": "unlabeled"}, {"type": "text", "expr": "Sundermeyer et\u00a0al", "word_idx": 66296, "sentence_idx": 1027, "label": "unlabeled"}, {"type": "text", "expr": "(2012)Sundermeyer, Schl\u00fcter, and\nNey", "word_idx": 66313, "sentence_idx": 1028, "label": "unlabeled"}, {"type": "text", "expr": "Sundermeyer, Martin, Schl\u00fcter, Ralf, and Ney, Hermann", "word_idx": 66349, "sentence_idx": 1029, "label": "unlabeled"}, {"type": "text", "expr": "Lstm neural networks for language modeling", "word_idx": 66402, "sentence_idx": 1030, "label": "unlabeled"}, {"type": "text", "expr": "In  Thirteenth Annual Conference of the International Speech\nCommunication Association , 2012", "word_idx": 66444, "sentence_idx": 1031, "label": "unlabeled"}, {"type": "text", "expr": "Thirteenth Annual Conference of the International Speech\nCommunication Association", "word_idx": 66537, "sentence_idx": 1032, "label": "unlabeled"}, {"type": "text", "expr": "Thrun(1998) \nThrun, Sebastian", "word_idx": 66619, "sentence_idx": 1033, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Lifelong learning algorithms", "word_idx": 66648, "sentence_idx": 1034, "label": "unlabeled"}, {"type": "text", "expr": "Thrun(1998)", "word_idx": 66679, "sentence_idx": 1035, "label": "unlabeled"}, {"type": "text", "expr": "Thrun, Sebastian", "word_idx": 66690, "sentence_idx": 1036, "label": "unlabeled"}, {"type": "text", "expr": "Lifelong learning algorithms", "word_idx": 66706, "sentence_idx": 1037, "label": "unlabeled"}, {"type": "text", "expr": "In  Learning to learn , pp", "word_idx": 66734, "sentence_idx": 1038, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0181\u2013209", "word_idx": 66760, "sentence_idx": 1039, "label": "unlabeled"}, {"type": "text", "expr": " Springer, 1998", "word_idx": 66768, "sentence_idx": 1040, "label": "unlabeled"}, {"type": "text", "expr": "Learning to learn", "word_idx": 66783, "sentence_idx": 1041, "label": "unlabeled"}, {"type": "text", "expr": "Tieleman & Hinton(2012)Tieleman and Hinton \nTieleman, Tijmen and Hinton, Geoffrey", "word_idx": 66800, "sentence_idx": 1042, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Lecture 6", "word_idx": 66881, "sentence_idx": 1043, "label": "unlabeled"}, {"type": "text", "expr": "5-rmsprop: Divide the gradient by a running average of its\nrecent magnitude", "word_idx": 66893, "sentence_idx": 1044, "label": "unlabeled"}, {"type": "text", "expr": "Tieleman & Hinton(2012)Tieleman and Hinton", "word_idx": 66968, "sentence_idx": 1045, "label": "unlabeled"}, {"type": "text", "expr": "Tieleman, Tijmen and Hinton, Geoffrey", "word_idx": 67010, "sentence_idx": 1046, "label": "unlabeled"}, {"type": "text", "expr": "Lecture 6", "word_idx": 67047, "sentence_idx": 1047, "label": "unlabeled"}, {"type": "text", "expr": "5-rmsprop: Divide the gradient by a running average of its\nrecent magnitude", "word_idx": 67056, "sentence_idx": 1048, "label": "unlabeled"}, {"type": "text", "expr": "COURSERA: Neural networks for machine learning , 4(2):26\u201331, 2012", "word_idx": 67131, "sentence_idx": 1049, "label": "unlabeled"}, {"type": "text", "expr": "COURSERA: Neural networks for machine learning", "word_idx": 67196, "sentence_idx": 1050, "label": "unlabeled"}, {"type": "text", "expr": "Vinyals et\u00a0al", "word_idx": 67242, "sentence_idx": 1051, "label": "unlabeled"}, {"type": "text", "expr": "(2015)Vinyals, Fortunato, and\nJaitly \nVinyals, Oriol, Fortunato, Meire, and Jaitly, Navdeep", "word_idx": 67255, "sentence_idx": 1052, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Pointer networks", "word_idx": 67346, "sentence_idx": 1053, "label": "unlabeled"}, {"type": "text", "expr": "Vinyals et\u00a0al", "word_idx": 67365, "sentence_idx": 1054, "label": "unlabeled"}, {"type": "text", "expr": "(2015)Vinyals, Fortunato, and\nJaitly", "word_idx": 67378, "sentence_idx": 1055, "label": "unlabeled"}, {"type": "text", "expr": "Vinyals, Oriol, Fortunato, Meire, and Jaitly, Navdeep", "word_idx": 67414, "sentence_idx": 1056, "label": "unlabeled"}, {"type": "text", "expr": "Pointer networks", "word_idx": 67467, "sentence_idx": 1057, "label": "unlabeled"}, {"type": "text", "expr": "In  Advances in Neural Information Processing Systems , pp", "word_idx": 67483, "sentence_idx": 1058, "label": "unlabeled"}, {"type": "text", "expr": "\u00a02692\u20132700, 2015", "word_idx": 67541, "sentence_idx": 1059, "label": "unlabeled"}, {"type": "text", "expr": "Advances in Neural Information Processing Systems", "word_idx": 67557, "sentence_idx": 1060, "label": "unlabeled"}, {"type": "text", "expr": "Vinyals et\u00a0al", "word_idx": 67606, "sentence_idx": 1061, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Vinyals, Blundell, Lillicrap, Wierstra,\net\u00a0al", "word_idx": 67619, "sentence_idx": 1062, "label": "unlabeled"}, {"type": "text", "expr": " \nVinyals, Oriol, Blundell, Charles, Lillicrap, Tim, Wierstra, Daan, et\u00a0al", "word_idx": 67670, "sentence_idx": 1063, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Matching networks for one shot learning", "word_idx": 67744, "sentence_idx": 1064, "label": "unlabeled"}, {"type": "text", "expr": "Vinyals et\u00a0al", "word_idx": 67786, "sentence_idx": 1065, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Vinyals, Blundell, Lillicrap, Wierstra,\net\u00a0al", "word_idx": 67799, "sentence_idx": 1066, "label": "unlabeled"}, {"type": "text", "expr": "Vinyals, Oriol, Blundell, Charles, Lillicrap, Tim, Wierstra, Daan, et\u00a0al", "word_idx": 67850, "sentence_idx": 1067, "label": "unlabeled"}, {"type": "text", "expr": "Matching networks for one shot learning", "word_idx": 67922, "sentence_idx": 1068, "label": "unlabeled"}, {"type": "text", "expr": "In  Advances in Neural Information Processing Systems , pp", "word_idx": 67961, "sentence_idx": 1069, "label": "unlabeled"}, {"type": "text", "expr": "\u00a03630\u20133638, 2016", "word_idx": 68019, "sentence_idx": 1070, "label": "unlabeled"}, {"type": "text", "expr": "Advances in Neural Information Processing Systems", "word_idx": 68035, "sentence_idx": 1071, "label": "unlabeled"}, {"type": "text", "expr": "Yang et\u00a0al", "word_idx": 68084, "sentence_idx": 1072, "label": "unlabeled"}, {"type": "text", "expr": "(2017)Yang, Dai, Salakhutdinov, and\nCohen \nYang, Zhilin, Dai, Zihang, Salakhutdinov, Ruslan, and Cohen, William\u00a0W", "word_idx": 68094, "sentence_idx": 1073, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Breaking the softmax bottleneck: a high-rank rnn language model", "word_idx": 68207, "sentence_idx": 1074, "label": "unlabeled"}, {"type": "text", "expr": "Yang et\u00a0al", "word_idx": 68273, "sentence_idx": 1075, "label": "unlabeled"}, {"type": "text", "expr": "(2017)Yang, Dai, Salakhutdinov, and\nCohen", "word_idx": 68283, "sentence_idx": 1076, "label": "unlabeled"}, {"type": "text", "expr": "Yang, Zhilin, Dai, Zihang, Salakhutdinov, Ruslan, and Cohen, William\u00a0W", "word_idx": 68324, "sentence_idx": 1077, "label": "unlabeled"}, {"type": "text", "expr": "Breaking the softmax bottleneck: a high-rank rnn language model", "word_idx": 68394, "sentence_idx": 1078, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1711", "word_idx": 68457, "sentence_idx": 1079, "label": "unlabeled"}, {"type": "text", "expr": "03953 , 2017", "word_idx": 68482, "sentence_idx": 1080, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1711", "word_idx": 68494, "sentence_idx": 1081, "label": "unlabeled"}, {"type": "text", "expr": "03953", "word_idx": 68519, "sentence_idx": 1082, "label": "unlabeled"}, {"type": "text", "expr": "Zhou et\u00a0al", "word_idx": 68524, "sentence_idx": 1083, "label": "unlabeled"}, {"type": "text", "expr": "(2018)Zhou, Wu, and Li \nZhou, Fengwei, Wu, Bin, and Li, Zhenguo", "word_idx": 68534, "sentence_idx": 1084, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Deep meta-learning: Learning to learn in the concept space", "word_idx": 68597, "sentence_idx": 1085, "label": "unlabeled"}, {"type": "text", "expr": "Zhou et\u00a0al", "word_idx": 68658, "sentence_idx": 1086, "label": "unlabeled"}, {"type": "text", "expr": "(2018)Zhou, Wu, and Li", "word_idx": 68668, "sentence_idx": 1087, "label": "unlabeled"}, {"type": "text", "expr": "Zhou, Fengwei, Wu, Bin, and Li, Zhenguo", "word_idx": 68690, "sentence_idx": 1088, "label": "unlabeled"}, {"type": "text", "expr": "Deep meta-learning: Learning to learn in the concept space", "word_idx": 68729, "sentence_idx": 1089, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1802", "word_idx": 68787, "sentence_idx": 1090, "label": "unlabeled"}, {"type": "text", "expr": "03596 , 2018", "word_idx": 68812, "sentence_idx": 1091, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1802", "word_idx": 68824, "sentence_idx": 1092, "label": "unlabeled"}, {"type": "text", "expr": "03596", "word_idx": 68849, "sentence_idx": 1093, "label": "unlabeled"}, {"type": "text", "expr": "Zipf(1935) \nZipf, George\u00a0K", "word_idx": 68854, "sentence_idx": 1094, "label": "unlabeled"}, {"type": "text", "expr": "\n\n The psychology of language", "word_idx": 68880, "sentence_idx": 1095, "label": "unlabeled"}, {"type": "text", "expr": "Zipf(1935)", "word_idx": 68909, "sentence_idx": 1096, "label": "unlabeled"}, {"type": "text", "expr": "Zipf, George\u00a0K", "word_idx": 68919, "sentence_idx": 1097, "label": "unlabeled"}, {"type": "text", "expr": "The psychology of language", "word_idx": 68933, "sentence_idx": 1098, "label": "unlabeled"}, {"type": "text", "expr": "NY Houghton-Mifflin , 1935", "word_idx": 68959, "sentence_idx": 1099, "label": "unlabeled"}, {"type": "text", "expr": "NY Houghton-Mifflin", "word_idx": 68985, "sentence_idx": 1100, "label": "unlabeled"}, {"type": "text", "expr": "Appendix", "word_idx": 69004, "sentence_idx": 1101, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0A  Language Modelling", "word_idx": 69012, "sentence_idx": 1102, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0A", "word_idx": 69042, "sentence_idx": 1103, "label": "unlabeled"}, {"type": "text", "expr": "1  WikiText-103", "word_idx": 69052, "sentence_idx": 1104, "label": "unlabeled"}, {"type": "text", "expr": "1  Dynamic Evaluation Parameters", "word_idx": 69067, "sentence_idx": 1105, "label": "unlabeled"}, {"type": "text", "expr": "For the Neural Cache, we swept over the hyper-parameters:", "word_idx": 69099, "sentence_idx": 1106, "label": "unlabeled"}, {"type": "text", "expr": "For the Neural Cache, we swept over the hyper-parameters:", "word_idx": 69156, "sentence_idx": 1107, "label": "unlabeled"}, {"type": "text", "expr": "Softmax inverse temperature:  $\\theta_{cache}\\in\\{$ 0", "word_idx": 69213, "sentence_idx": 1108, "label": "unlabeled"}, {"type": "text", "expr": "3 $\\}$", "word_idx": 69266, "sentence_idx": 1109, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta_{cache}\\in\\{$$", "word_idx": 69272, "sentence_idx": 1110, "label": "unlabeled"}, {"type": "text", "expr": "Cache output interpolation:  $\\lambda_{cache}\\in\\{$ 0", "word_idx": 69291, "sentence_idx": 1111, "label": "unlabeled"}, {"type": "text", "expr": "05, 0", "word_idx": 69344, "sentence_idx": 1112, "label": "unlabeled"}, {"type": "text", "expr": "15, 0", "word_idx": 69349, "sentence_idx": 1113, "label": "unlabeled"}, {"type": "text", "expr": "25, 0", "word_idx": 69354, "sentence_idx": 1114, "label": "unlabeled"}, {"type": "text", "expr": "35 $\\}$", "word_idx": 69359, "sentence_idx": 1115, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda_{cache}\\in\\{$$", "word_idx": 69366, "sentence_idx": 1116, "label": "unlabeled"}, {"type": "text", "expr": "Cache size  $n_{cache}\\in\\{$ 1000, 5000, 8000, 9000, 10000 $\\}$", "word_idx": 69386, "sentence_idx": 1117, "label": "unlabeled"}, {"type": "math", "expr": "$$n_{cache}\\in\\{$$", "word_idx": 69449, "sentence_idx": 1118, "label": "unlabeled"}, {"type": "text", "expr": "and chose  $\\theta_{cache}=03,\\,\\lambda_{cache}=01,\\,n_{cache}=10000$  by sweeping over the validation set", "word_idx": 69463, "sentence_idx": 1119, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta_{cache}=0.3,\\,\\lambda_{cache}=0.1,\\,n_{cache}=10000$$", "word_idx": 69569, "sentence_idx": 1120, "label": "unlabeled"}, {"type": "text", "expr": "For the mixture of Neural Cache and MbPA we swept over the same cache parameters, alongside:", "word_idx": 69627, "sentence_idx": 1121, "label": "unlabeled"}, {"type": "text", "expr": "For the mixture of Neural Cache and MbPA we swept over the same cache parameters, alongside:", "word_idx": 69719, "sentence_idx": 1122, "label": "unlabeled"}, {"type": "text", "expr": "MbPA output interpolation:  $\\lambda_{mbpa}\\in\\{$ 0", "word_idx": 69811, "sentence_idx": 1123, "label": "unlabeled"}, {"type": "text", "expr": "02, 0", "word_idx": 69862, "sentence_idx": 1124, "label": "unlabeled"}, {"type": "text", "expr": "04, 0", "word_idx": 69867, "sentence_idx": 1125, "label": "unlabeled"}, {"type": "text", "expr": "06, 0", "word_idx": 69872, "sentence_idx": 1126, "label": "unlabeled"}, {"type": "text", "expr": "08, 0", "word_idx": 69877, "sentence_idx": 1127, "label": "unlabeled"}, {"type": "text", "expr": "10 $\\}$ ,", "word_idx": 69882, "sentence_idx": 1128, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda_{mbpa}\\in\\{$$", "word_idx": 69891, "sentence_idx": 1129, "label": "unlabeled"}, {"type": "text", "expr": "Number of neighbours retrieved from memory:  $K\\in\\{512,1024\\}$ ,", "word_idx": 69910, "sentence_idx": 1130, "label": "unlabeled"}, {"type": "math", "expr": "$$K\\in\\{512,1024\\}$$", "word_idx": 69975, "sentence_idx": 1131, "label": "unlabeled"}, {"type": "text", "expr": "Number of MbPA steps:  $n_{mbpa}\\in\\{1,2\\}$", "word_idx": 69991, "sentence_idx": 1132, "label": "unlabeled"}, {"type": "math", "expr": "$$n_{mbpa}\\in\\{1,2\\}$$", "word_idx": 70034, "sentence_idx": 1133, "label": "unlabeled"}, {"type": "text", "expr": "and selected  $\\lambda_{mbpa}=004,\\lambda_{cache}=01,\\theta_{cache}=03,K=1024,n_{mbpa}=1,n%\n_{cache}=10000$ ", "word_idx": 70052, "sentence_idx": 1134, "label": "unlabeled"}, {"type": "text", "expr": " We also selected the MbPA learning rate  $\\alpha_{lr}=03$ , and the L2-regularization  $\\beta_{mbpa}=05$  on the MbPA-modified parameters", "word_idx": 70160, "sentence_idx": 1135, "label": "unlabeled"}, {"type": "text", "expr": " The memory size for MbPA was chosen to be equal to the cache size", "word_idx": 70298, "sentence_idx": 1136, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda_{mbpa}=0.04,\\lambda_{cache}=0.1,\\theta_{cache}=0.3,K=1024,n_{mbpa}=1,n%\n_{cache}=10000$$", "word_idx": 70364, "sentence_idx": 1137, "label": "unlabeled"}, {"type": "math", "expr": "$$\\alpha_{lr}=0.3$$", "word_idx": 70458, "sentence_idx": 1138, "label": "unlabeled"}, {"type": "math", "expr": "$$\\beta_{mbpa}=0.5$$", "word_idx": 70473, "sentence_idx": 1139, "label": "unlabeled"}, {"type": "text", "expr": "2  Gutenberg", "word_idx": 70489, "sentence_idx": 1140, "label": "unlabeled"}, {"type": "text", "expr": "1  Splits", "word_idx": 70501, "sentence_idx": 1141, "label": "unlabeled"}, {"type": "text", "expr": "We downloaded a subset of books (listed below) from Project Gutenberg on January 2, 2018 from a mirror site ( https://www", "word_idx": 70510, "sentence_idx": 1142, "label": "unlabeled"}, {"type": "text", "expr": "gutenberg", "word_idx": 70631, "sentence_idx": 1143, "label": "unlabeled"}, {"type": "text", "expr": "org/MIRRORS", "word_idx": 70640, "sentence_idx": 1144, "label": "unlabeled"}, {"type": "text", "expr": "ALL )", "word_idx": 70651, "sentence_idx": 1145, "label": "unlabeled"}, {"type": "text", "expr": " We selected  $2042$  English-language books under the  $/1$  subdirectory", "word_idx": 70656, "sentence_idx": 1146, "label": "unlabeled"}, {"type": "text", "expr": " Each book has a unique id, we shuffled the books and split out a reasonably sized train, validation and test set", "word_idx": 70730, "sentence_idx": 1147, "label": "unlabeled"}, {"type": "text", "expr": " The book ids are listed below for these splits", "word_idx": 70843, "sentence_idx": 1148, "label": "unlabeled"}, {"type": "text", "expr": "https://www", "word_idx": 70890, "sentence_idx": 1149, "label": "unlabeled"}, {"type": "text", "expr": "gutenberg", "word_idx": 70901, "sentence_idx": 1150, "label": "unlabeled"}, {"type": "text", "expr": "org/MIRRORS", "word_idx": 70910, "sentence_idx": 1151, "label": "unlabeled"}, {"type": "math", "expr": "$$2042$$", "word_idx": 70921, "sentence_idx": 1152, "label": "unlabeled"}, {"type": "text", "expr": "Test  ( $13$  books,  $526,646$  tokens):", "word_idx": 70925, "sentence_idx": 1153, "label": "unlabeled"}, {"type": "math", "expr": "$$526,646$$", "word_idx": 70966, "sentence_idx": 1154, "label": "unlabeled"}, {"type": "text", "expr": "11959, 12211, 10912, 11015, 12585, 10827, 10268, 11670, 126, 1064, 11774, 12505, 11931", "word_idx": 70973, "sentence_idx": 1155, "label": "unlabeled"}, {"type": "text", "expr": "11959, 12211, 10912, 11015, 12585, 10827, 10268, 11670, 126, 1064, 11774, 12505, 11931", "word_idx": 71059, "sentence_idx": 1156, "label": "unlabeled"}, {"type": "text", "expr": "11959, 12211, 10912, 11015, 12585, 10827, 10268, 11670, 126, 1064, 11774, 12505, 11931", "word_idx": 71145, "sentence_idx": 1157, "label": "unlabeled"}, {"type": "text", "expr": "Validation  ( $12$  books,  $609,545$  tokens):", "word_idx": 71231, "sentence_idx": 1158, "label": "unlabeled"}, {"type": "text", "expr": "Validation", "word_idx": 71278, "sentence_idx": 1159, "label": "unlabeled"}, {"type": "math", "expr": "$$609,545$$", "word_idx": 71288, "sentence_idx": 1160, "label": "unlabeled"}, {"type": "text", "expr": "11399, 10003, 1202, 12213, 11177, 12856, 10516, 11635, 12315, 11804, 11249, 11163", "word_idx": 71295, "sentence_idx": 1161, "label": "unlabeled"}, {"type": "text", "expr": "11399, 10003, 1202, 12213, 11177, 12856, 10516, 11635, 12315, 11804, 11249, 11163", "word_idx": 71376, "sentence_idx": 1162, "label": "unlabeled"}, {"type": "text", "expr": "11399, 10003, 1202, 12213, 11177, 12856, 10516, 11635, 12315, 11804, 11249, 11163", "word_idx": 71457, "sentence_idx": 1163, "label": "unlabeled"}, {"type": "text", "expr": "Train  ( $2017$  books,  $175,181,505$  tokens)", "word_idx": 71538, "sentence_idx": 1164, "label": "unlabeled"}, {"type": "text", "expr": "Train", "word_idx": 71585, "sentence_idx": 1165, "label": "unlabeled"}, {"type": "math", "expr": "$$2017$$", "word_idx": 71590, "sentence_idx": 1166, "label": "unlabeled"}, {"type": "math", "expr": "$$175,181,505$$", "word_idx": 71594, "sentence_idx": 1167, "label": "unlabeled"}, {"type": "text", "expr": "10000, 10064, 1019, 10358, 10482, 10598, 10675, 10787, 10864, 10929, 10, 11112, 11190, 11257, 11363, 11449, 11526, 11612, 11715, 11825, 11920, 11987, 1204, 12118, 12184, 12249, 12326, 12405, 12478, 12582, 12691, 12806, 12895, 10001, 10065, 101, 1035, 10483, 10599, 10676, 10788, 10865, 10930, 11007, 11113, 11191, 11258, 11364, 11451, 11527, 11613, 11716, 11826, 11921, 11988, 12050, 1211, 12185, 12252, 12327, 12406, 1247, 12583, 12692, 12807, 12896, 10002, 10066, 10201, 10363, 10489, 1059, 1067, 10789, 10867, 10931, 11008, 11114, 11192, 11259, 11365, 11452, 11528, 11614, 1171, 11827, 11922, 11989, 12051, 12121, 12186, 12253, 12328, 12409, 12486, 12584, 12696, 12808, 12897, 10067, 10202, 10365, 1048, 105, 10684, 1078, 10868, 10932, 11009, 11115, 11193, 11260, 11366, 11454, 1152, 11615, 1172, 11828, 11923, 1198, 12052, 12122, 12187, 12254, 12329, 1240, 1248, 12697, 12809, 12898, 10004, 10068, 1020, 10366, 10490, 10600, 10687, 10790, 10869, 10933, 11010, 11119, 11194, 11263, 11367, 11455, 11530, 11623, 11734, 11829, 11924, 11990, 12054, 12123, 12188, 12256, 1232, 12412, 12490, 12589, 12699, 1280, 12899, 10005, 10069, 10210, 10367, 10491, 10601, 1068, 10791, 10870, 10934, 11012, 11120, 11195, 11264, 11368, 11456, 11531, 11624, 11735, 1182, 11926, 11991, 12055, 12124, 12189, 12257, 12330, 12413, 12491, 1258, 1269, 12810, 128, 10006, 1006, 10211, 10368, 10493, 10602, 10690, 10792, 10871, 10935, 11013, 11121, 11196, 11265, 11369, 11459, 11533, 11625, 11736, 11830, 11929, 11992, 12056, 12125, 1218, 12259, 12333, 12414, 12498, 12590, 12811, 12900, 10007, 10070, 10212, 10369, 1049, 10603, 10691, 10793, 10872, 10936, 11014, 11122, 11197, 11266, 11370, 1145, 11534, 11626, 11737, 11831, 11930, 11993, 12057, 12126, 12190, 1225, 12336, 12415, 124, 12591, 12700, 12813, 12901, 10008, 10071, 10213, 1036, 104, 10605, 10692, 10794, 10873, 10937, 11123, 11198, 11267, 11371, 11460, 11537, 11632, 11738, 11832, 11994, 12058, 12127, 12191, 12261, 12337, 12416, 12504, 12592, 1270, 12814, 12902, 10009, 10072, 10214, 10370, 1050, 10606, 10693, 10795, 10874, 10938, 11016, 11124, 111, 11268, 11372, 11461, 11538, 11633, 1173, 11833, 11932, 11995, 12059, 12128, 12192, 12262, 12340, 12417, 12593, 1271, 12815, 12904, 10010, 10073, 10216, 10371, 10510, 10607, 10694, 10796, 10875, 10939, 11017, 11125, 11200, 11269, 11373, 11462, 11539, 11634, 11740, 11834, 11933, 11996, 12060, 12129, 12193, 12263, 12341, 12418, 12506, 12594, 12732, 12816, 12905, 10011, 1007, 10217, 10372, 10609, 10698, 10797, 10876, 10940, 11018, 11127, 11201, 11270, 11376, 11464, 1153, 11741, 11835, 11934, 11997, 12061, 1212, 12194, 12264, 12342, 12419, 12507, 12595, 12736, 12817, 1290, 10012, 10084, 10219, 10373, 10517, 10610, 10699, 10798, 10877, 10942, 11019, 11128, 11202, 11271, 11377, 11468, 11540, 11636, 11742, 11836, 11935, 11998, 12062, 12130, 12195, 12265, 12343, 1241, 1250, 12596, 12737, 12819, 12915, 10013, 10085, 1021, 10374, 10518, 10611, 1069, 10799, 10878, 10943, 11020, 1112, 11203, 11272, 11378, 11469, 11541, 11637, 11743, 11837, 11936, 11999, 12063, 12131, 12196, 12269, 12344, 12420, 12511, 1259, 12738, 1281, 12916, 10014, 10090, 10222, 10375, 10519, 10612, 106, 1079, 10879, 10944, 11021, 11130, 11204, 11273, 11379, 1146, 11542, 11638, 11745, 11838, 11937, 119, 12064, 12132, 12197, 12270, 12345, 12421, 12512, 125, 12739, 12821, 12917, 10015, 10091, 10224, 10376, 1051, 10613, 10700, 10800, 1087, 10945, 11028, 11136, 11210, 11274, 11382, 11470, 11543, 11647, 11746, 11839, 11938, 11, 12066, 12133, 12198, 12272, 12346, 12422, 12513, 12600, 12740, 12823, 1291, 10016, 10092, 10225, 10377, 10520, 10615, 10707, 10801, 10880, 10946, 11029, 11137, 11211, 11275, 11383, 11471, 11544, 11648, 11749, 1183, 11939, 12001, 12067, 12134, 12199, 12277, 12349, 12423, 12514, 12601, 12741, 12825, 12922, 10017, 10095, 10226, 10378, 10523, 10616, 10708, 10803, 10881, 10947, 11030, 11138, 11212, 11276, 11385, 11472, 11545, 11649, 1174, 11840, 11941, 12002, 12068, 12135, 121, 12278, 12350, 12424, 12515, 12611, 12742, 12826, 12923, 10018, 10096, 1022, 10379, 1052, 10617, 10709, 10804, 10882, 10948, 11031, 11140, 11213, 11277, 11386, 11473, 11546, 1164, 11753, 11841, 11942, 12004, 12069, 12136, 12200, 12279, 12351, 12425, 12516, 12614, 12743, 12827, 12924, 10019, 10097, 10234, 10380, 10538, 10618, 10712, 10805, 10883, 10949, 11032, 11141, 11214, 11278, 11387, 11474, 11548, 11651, 11754, 11842, 11943, 12006, 1206, 12137, 12201, 1227, 12352, 12426, 12517, 12617, 12744, 12828, 12925, 1001, 10098, 1024, 10381, 10543, 10619, 10714, 10806, 10884, 1094, 11033, 11142, 11215, 11279, 11388, 11475, 11549, 11652, 1175, 11843, 11944, 12007, 12071, 12138, 12202, 12280, 12353, 12427, 1251, 12618, 12745, 1282, 12926, 10020, 10099, 10266, 10382, 10544, 1061, 10715, 10807, 10885, 10950, 11034, 11143, 11216, 11280, 11389, 11476, 1154, 11653, 11761, 11844, 11945, 1200, 12073, 12139, 12203, 12281, 12354, 12428, 12521, 12619, 12746, 12830, 12928, 10022, 100, 10267, 10383, 10545, 10620, 10716, 10808, 10886, 10954, 1103, 11144, 11217, 11281, 1138, 11477, 11550, 11654, 11762, 11845, 11946, 12010, 12074, 12140, 12204, 12282, 12357, 12429, 12522, 1261, 12747, 12832, 12929, 10023, 10100, 10386, 10546, 10621, 10717, 1080, 10887, 10955, 11045, 11145, 11218, 11282, 11390, 11478, 11551, 11655, 11763, 11846, 11947, 12013, 12077, 12141, 12205, 12283, 12358, 1242, 12523, 12622, 1274, 12833, 12933, 10024, 10101, 1027, 10388, 10550, 10622, 10720, 10811, 10888, 10956, 11047, 11146, 11219, 11283, 11391, 11479, 11552, 11656, 11764, 11847, 11948, 12014, 12078, 12142, 12206, 12285, 12359, 12430, 12524, 12628, 12750, 12834, 12934, 10025, 10102, 1028, 10389, 10551, 10623, 10721, 10812, 10889, 10957, 11050, 11147, 1121, 11284, 11392, 1147, 11553, 11658, 11765, 11848, 11949, 12015, 12079, 12143, 12207, 12286, 1235, 12431, 12525, 12629, 12753, 12835, 12935, 10029, 10103, 10291, 10392, 10554, 10624, 10722, 10813, 1088, 10958, 11051, 11148, 11221, 11289, 11395, 11480, 11554, 11659, 11768, 11849, 11950, 12016, 1207, 12144, 12208, 12287, 12360, 12433, 1252, 1262, 12754, 12836, 12936, 10030, 10104, 10292, 10393, 10555, 10625, 10734, 10814, 10890, 10959, 11052, 11149, 11222, 112, 11397, 11481, 11555, 11660, 1176, 1184, 11951, 12017, 12081, 12145, 12209, 12288, 12361, 12434, 12532, 12630, 12755, 12839, 12937, 10031, 10105, 10294, 10394, 10556, 10628, 10737, 10815, 10891, 1095, 11053, 11150, 11223, 11308, 11398, 11482, 11556, 11661, 11771, 11850, 11952, 12018, 12083, 12146, 1220, 1228, 12362, 12436, 12535, 12631, 12758, 1283, 12938, 10032, 10106, 1029, 10395, 10557, 10629, 10738, 10816, 10892, 10960, 11054, 11153, 11224, 11309, 11483, 11557, 11662, 11772, 11851, 11953, 12019, 12084, 12147, 12210, 12291, 12363, 12438, 12536, 12632, 12759, 12841, 12939, 10033, 10107, 102, 10396, 10560, 1062, 10739, 10817, 10893, 10961, 11055, 11156, 11225, 11310, 1139, 11485, 11558, 11664, 11852, 11954, 12020, 12085, 1214, 12292, 12366, 12439, 12537, 12633, 12760, 12843, 12940, 10034, 10108, 10314, 10399, 10561, 10630, 10740, 10818, 10894, 10962, 11056, 11157, 11226, 11311, 113, 11488, 11559, 11665, 1177, 11853, 11955, 12021, 12086, 12150, 12212, 12294, 1236, 12440, 12538, 12634, 12761, 12845, 12941, 10035, 10112, 10317, 1039, 10562, 10631, 10741, 1081, 10895, 10965, 11059, 11158, 11227, 11312, 11400, 11489, 1155, 11666, 1178, 11854, 11956, 12022, 12087, 12151, 12296, 12370, 12441, 12539, 12635, 12762, 12846, 12942, 10036, 10118, 10318, 103, 10563, 10632, 10743, 10826, 10896, 10966, 11060, 11159, 11228, 11313, 11401, 11490, 11565, 11667, 1179, 11855, 11957, 12023, 12088, 12152, 12214, 12297, 12371, 12442, 1253, 12638, 12763, 12847, 12943, 10037, 10119, 10319, 10401, 10564, 10633, 10744, 10897, 10967, 11067, 11160, 11229, 11314, 11402, 11491, 11566, 11668, 117, 11856, 11958, 12024, 12089, 12153, 12215, 12298, 12372, 12443, 12540, 12639, 12764, 12849, 12944, 10038, 10120, 10320, 10402, 10565, 10635, 10747, 10828, 10898, 10968, 11068, 11161, 11230, 11315, 11403, 11493, 11567, 11800, 1185, 12025, 1208, 12154, 12217, 12299, 12373, 12444, 12541, 1263, 12765, 1284, 12945, 10039, 10121, 10321, 10409, 10566, 10636, 10748, 10830, 10899, 10969, 11069, 11162, 11231, 11321, 11408, 11496, 11568, 11671, 11801, 11878, 11960, 12026, 12090, 12155, 12218, 122, 12374, 12445, 12542, 12645, 12766, 12851, 12946, 10040, 10125, 10322, 1040, 10567, 10637, 10760, 10831, 1089, 10970, 11074, 11232, 11322, 11409, 11498, 11569, 11672, 11802, 11880, 11961, 12027, 12091, 12156, 12219, 12300, 12375, 12450, 12545, 1264, 12767, 12852, 12947, 10041, 10127, 10323, 10410, 10568, 10638, 10761, 10832, 108, 10973, 11078, 11164, 11233, 11323, 1140, 11499, 1156, 11673, 11803, 11881, 11962, 12028, 12092, 12157, 12220, 12302, 12376, 12452, 12548, 12652, 12768, 12853, 12948, 10042, 10128, 10324, 10417, 10569, 10639, 10762, 10835, 10900, 10974, 11079, 11165, 11234, 11327, 11410, 114, 11570, 11674, 11882, 11963, 12029, 12093, 12158, 12221, 12304, 12378, 12453, 12549, 12653, 12769, 12854, 1294, 10043, 10129, 10327, 10418, 10570, 1063, 10763, 10837, 10901, 10976, 11080, 11166, 11235, 11328, 11411, 11503, 11571, 11675, 11805, 11883, 11965, 12094, 12159, 12222, 12305, 1237, 12454, 1254, 12654, 12770, 12855, 12951, 10044, 10130, 10328, 1041, 10571, 10642, 10765, 10840, 10902, 10979, 11082, 11167, 11236, 11329, 11416, 11504, 11575, 11676, 11806, 11885, 11966, 12030, 12096, 1215, 12223, 12306, 12380, 12455, 12550, 12655, 12779, 12955, 10045, 10131, 10329, 10422, 10572, 10643, 10766, 10842, 10904, 1097, 11083, 11168, 11237, 11330, 11417, 11505, 11576, 1167, 11807, 11886, 11969, 12031, 12097, 12160, 12224, 12307, 12381, 12456, 12551, 12658, 1277, 1285, 12956, 10046, 10132, 10330, 1043, 10573, 10767, 10843, 10905, 10981, 11084, 11169, 11238, 11331, 11418, 11506, 11577, 1168, 11808, 11887, 1196, 12032, 12098, 12161, 12225, 12308, 12383, 1245, 12552, 12659, 12781, 12860, 12957, 10047, 10133, 10331, 10451, 10576, 10655, 10769, 10844, 10908, 10983, 11085, 11170, 11239, 11332, 11419, 11507, 11578, 11690, 11809, 11888, 11970, 12033, 12099, 12162, 12226, 12309, 12384, 12460, 12553, 1265, 12784, 12863, 12958, 10048, 10134, 10332, 10452, 10577, 10656, 1076, 10847, 10909, 10984, 11088, 11171, 11240, 11333, 11420, 11508, 11579, 11691, 1180, 11889, 11971, 12034, 1209, 12163, 12227, 1230, 12387, 12461, 12554, 12664, 12785, 12864, 1297, 10049, 1013, 10333, 10453, 10578, 10657, 10770, 10848, 1090, 10985, 11089, 11172, 11241, 11334, 11421, 11509, 1157, 11692, 11810, 11890, 11972, 12035, 120, 12164, 12228, 12310, 12388, 12462, 12563, 12667, 12786, 12866, 1298, 10050, 10140, 10335, 10454, 10579, 10658, 10771, 10849, 10910, 10986, 11090, 11173, 11242, 11335, 11422, 1150, 11580, 11693, 11811, 11892, 11973, 12036, 12100, 12166, 12229, 12311, 12389, 12463, 12564, 12668, 12787, 12867, 1299, 10051, 10142, 10338, 10455, 10580, 10659, 10772, 10850, 10911, 10987, 11091, 11174, 11243, 11336, 11424, 11510, 11581, 11694, 11812, 11894, 11974, 12037, 12101, 12169, 1222, 12312, 1238, 12464, 12565, 12669, 12788, 12868, 129, 10052, 10143, 10339, 10458, 10581, 1065, 10773, 10851, 10988, 11092, 11244, 11339, 11426, 11512, 11582, 11695, 11813, 11895, 11975, 12038, 12102, 1216, 12231, 12313, 12390, 12465, 12567, 1266, 1278, 12870, 12, 10053, 10144, 1033, 10459, 10582, 10660, 10776, 10852, 10913, 10989, 11093, 11179, 11245, 11343, 11427, 11513, 1158, 11696, 11814, 11897, 11976, 12039, 12103, 12170, 12232, 12314, 12391, 12466, 12568, 12675, 12792, 12871, 13, 10054, 10147, 10340, 1045, 10583, 10661, 10777, 10853, 10916, 1098, 11095, 11180, 11246, 11344, 1142, 11514, 11599, 11697, 11815, 1189, 11977, 1203, 12104, 12171, 12233, 12392, 12467, 12569, 12676, 12793, 12872, 14, 10055, 10148, 10341, 10460, 10585, 10662, 10778, 10854, 10918, 10991, 11096, 11181, 11247, 11345, 11431, 11515, 1159, 11698, 11816, 118, 11978, 12040, 12106, 12172, 12235, 12317, 12393, 12468, 12570, 12677, 12794, 12874, 15, 10056, 1014, 10342, 10461, 10586, 10665, 10779, 10855, 10919, 10993, 11097, 11182, 11248, 11347, 11433, 11516, 115, 1169, 11817, 11901, 11979, 12041, 12107, 12173, 12236, 12318, 12394, 12469, 12571, 12678, 12797, 12880, 16, 10057, 10159, 10345, 10462, 10587, 10666, 1077, 10856, 1091, 10994, 11100, 11183, 11349, 11435, 11517, 11604, 11707, 11818, 11902, 1197, 12042, 12109, 12175, 12239, 12319, 12395, 1246, 12572, 1267, 12798, 12881, 17, 10058, 1015, 1034, 10463, 10588, 10667, 10780, 10857, 10920, 10995, 11101, 11184, 11250, 11350, 11436, 11518, 11605, 11708, 11819, 11904, 11980, 12043, 1210, 12176, 1223, 1231, 12396, 12470, 12573, 12684, 12799, 12882, 18, 10059, 10161, 10350, 10464, 10589, 10668, 10781, 10858, 10921, 10996, 11102, 11185, 11251, 11351, 11437, 11519, 11606, 11709, 1181, 11906, 11981, 12044, 12110, 12177, 12240, 12320, 12397, 12472, 12574, 12685, 1279, 12885, 19, 1005, 10162, 10351, 1046, 10590, 10670, 10782, 10859, 10922, 10997, 11105, 11186, 11252, 11352, 11438, 1151, 11607, 1170, 11820, 11912, 11982, 12045, 12111, 12179, 12241, 12321, 12398, 12473, 12575, 12686, 127, 12886, 2609, 10060, 10163, 10352, 10472, 10592, 10671, 10783, 1085, 10924, 10998, 11106, 11187, 11253, 11353, 11440, 11520, 11608, 11710, 11821, 11913, 11983, 12046, 12112, 1217, 12242, 12322, 1239, 12474, 12576, 12687, 12800, 12887, 10061, 10164, 10355, 10473, 10593, 10672, 10784, 10860, 10925, 10999, 11107, 11188, 11254, 11354, 11441, 11521, 11609, 11711, 11822, 11915, 11984, 12047, 12114, 12180, 12244, 12323, 123, 12475, 1257, 12689, 12801, 12888, 10062, 10166, 10356, 10474, 10596, 10673, 10785, 10861, 10926, 1099, 11110, 11189, 11255, 11356, 11442, 11524, 11610, 11712, 11823, 11917, 11985, 12048, 12115, 12181, 12245, 12324, 12400, 12476, 12580, 1268, 12803, 1288, 10063, 1016, 10357, 1047, 10597, 10674, 10786, 10862, 10928, 109, 11111, 1118, 11256, 11357, 11448, 11525, 11611, 11713, 11824, 11918, 11986, 12049, 12116, 12183, 12248, 12325, 12402, 12477, 12581, 12690, 12805, 12892", "word_idx": 71605, "sentence_idx": 1168, "label": "unlabeled"}, {"type": "text", "expr": "10000, 10064, 1019, 10358, 10482, 10598, 10675, 10787, 10864, 10929, 10, 11112, 11190, 11257, 11363, 11449, 11526, 11612, 11715, 11825, 11920, 11987, 1204, 12118, 12184, 12249, 12326, 12405, 12478, 12582, 12691, 12806, 12895, 10001, 10065, 101, 1035, 10483, 10599, 10676, 10788, 10865, 10930, 11007, 11113, 11191, 11258, 11364, 11451, 11527, 11613, 11716, 11826, 11921, 11988, 12050, 1211, 12185, 12252, 12327, 12406, 1247, 12583, 12692, 12807, 12896, 10002, 10066, 10201, 10363, 10489, 1059, 1067, 10789, 10867, 10931, 11008, 11114, 11192, 11259, 11365, 11452, 11528, 11614, 1171, 11827, 11922, 11989, 12051, 12121, 12186, 12253, 12328, 12409, 12486, 12584, 12696, 12808, 12897, 10067, 10202, 10365, 1048, 105, 10684, 1078, 10868, 10932, 11009, 11115, 11193, 11260, 11366, 11454, 1152, 11615, 1172, 11828, 11923, 1198, 12052, 12122, 12187, 12254, 12329, 1240, 1248, 12697, 12809, 12898, 10004, 10068, 1020, 10366, 10490, 10600, 10687, 10790, 10869, 10933, 11010, 11119, 11194, 11263, 11367, 11455, 11530, 11623, 11734, 11829, 11924, 11990, 12054, 12123, 12188, 12256, 1232, 12412, 12490, 12589, 12699, 1280, 12899, 10005, 10069, 10210, 10367, 10491, 10601, 1068, 10791, 10870, 10934, 11012, 11120, 11195, 11264, 11368, 11456, 11531, 11624, 11735, 1182, 11926, 11991, 12055, 12124, 12189, 12257, 12330, 12413, 12491, 1258, 1269, 12810, 128, 10006, 1006, 10211, 10368, 10493, 10602, 10690, 10792, 10871, 10935, 11013, 11121, 11196, 11265, 11369, 11459, 11533, 11625, 11736, 11830, 11929, 11992, 12056, 12125, 1218, 12259, 12333, 12414, 12498, 12590, 12811, 12900, 10007, 10070, 10212, 10369, 1049, 10603, 10691, 10793, 10872, 10936, 11014, 11122, 11197, 11266, 11370, 1145, 11534, 11626, 11737, 11831, 11930, 11993, 12057, 12126, 12190, 1225, 12336, 12415, 124, 12591, 12700, 12813, 12901, 10008, 10071, 10213, 1036, 104, 10605, 10692, 10794, 10873, 10937, 11123, 11198, 11267, 11371, 11460, 11537, 11632, 11738, 11832, 11994, 12058, 12127, 12191, 12261, 12337, 12416, 12504, 12592, 1270, 12814, 12902, 10009, 10072, 10214, 10370, 1050, 10606, 10693, 10795, 10874, 10938, 11016, 11124, 111, 11268, 11372, 11461, 11538, 11633, 1173, 11833, 11932, 11995, 12059, 12128, 12192, 12262, 12340, 12417, 12593, 1271, 12815, 12904, 10010, 10073, 10216, 10371, 10510, 10607, 10694, 10796, 10875, 10939, 11017, 11125, 11200, 11269, 11373, 11462, 11539, 11634, 11740, 11834, 11933, 11996, 12060, 12129, 12193, 12263, 12341, 12418, 12506, 12594, 12732, 12816, 12905, 10011, 1007, 10217, 10372, 10609, 10698, 10797, 10876, 10940, 11018, 11127, 11201, 11270, 11376, 11464, 1153, 11741, 11835, 11934, 11997, 12061, 1212, 12194, 12264, 12342, 12419, 12507, 12595, 12736, 12817, 1290, 10012, 10084, 10219, 10373, 10517, 10610, 10699, 10798, 10877, 10942, 11019, 11128, 11202, 11271, 11377, 11468, 11540, 11636, 11742, 11836, 11935, 11998, 12062, 12130, 12195, 12265, 12343, 1241, 1250, 12596, 12737, 12819, 12915, 10013, 10085, 1021, 10374, 10518, 10611, 1069, 10799, 10878, 10943, 11020, 1112, 11203, 11272, 11378, 11469, 11541, 11637, 11743, 11837, 11936, 11999, 12063, 12131, 12196, 12269, 12344, 12420, 12511, 1259, 12738, 1281, 12916, 10014, 10090, 10222, 10375, 10519, 10612, 106, 1079, 10879, 10944, 11021, 11130, 11204, 11273, 11379, 1146, 11542, 11638, 11745, 11838, 11937, 119, 12064, 12132, 12197, 12270, 12345, 12421, 12512, 125, 12739, 12821, 12917, 10015, 10091, 10224, 10376, 1051, 10613, 10700, 10800, 1087, 10945, 11028, 11136, 11210, 11274, 11382, 11470, 11543, 11647, 11746, 11839, 11938, 11, 12066, 12133, 12198, 12272, 12346, 12422, 12513, 12600, 12740, 12823, 1291, 10016, 10092, 10225, 10377, 10520, 10615, 10707, 10801, 10880, 10946, 11029, 11137, 11211, 11275, 11383, 11471, 11544, 11648, 11749, 1183, 11939, 12001, 12067, 12134, 12199, 12277, 12349, 12423, 12514, 12601, 12741, 12825, 12922, 10017, 10095, 10226, 10378, 10523, 10616, 10708, 10803, 10881, 10947, 11030, 11138, 11212, 11276, 11385, 11472, 11545, 11649, 1174, 11840, 11941, 12002, 12068, 12135, 121, 12278, 12350, 12424, 12515, 12611, 12742, 12826, 12923, 10018, 10096, 1022, 10379, 1052, 10617, 10709, 10804, 10882, 10948, 11031, 11140, 11213, 11277, 11386, 11473, 11546, 1164, 11753, 11841, 11942, 12004, 12069, 12136, 12200, 12279, 12351, 12425, 12516, 12614, 12743, 12827, 12924, 10019, 10097, 10234, 10380, 10538, 10618, 10712, 10805, 10883, 10949, 11032, 11141, 11214, 11278, 11387, 11474, 11548, 11651, 11754, 11842, 11943, 12006, 1206, 12137, 12201, 1227, 12352, 12426, 12517, 12617, 12744, 12828, 12925, 1001, 10098, 1024, 10381, 10543, 10619, 10714, 10806, 10884, 1094, 11033, 11142, 11215, 11279, 11388, 11475, 11549, 11652, 1175, 11843, 11944, 12007, 12071, 12138, 12202, 12280, 12353, 12427, 1251, 12618, 12745, 1282, 12926, 10020, 10099, 10266, 10382, 10544, 1061, 10715, 10807, 10885, 10950, 11034, 11143, 11216, 11280, 11389, 11476, 1154, 11653, 11761, 11844, 11945, 1200, 12073, 12139, 12203, 12281, 12354, 12428, 12521, 12619, 12746, 12830, 12928, 10022, 100, 10267, 10383, 10545, 10620, 10716, 10808, 10886, 10954, 1103, 11144, 11217, 11281, 1138, 11477, 11550, 11654, 11762, 11845, 11946, 12010, 12074, 12140, 12204, 12282, 12357, 12429, 12522, 1261, 12747, 12832, 12929, 10023, 10100, 10386, 10546, 10621, 10717, 1080, 10887, 10955, 11045, 11145, 11218, 11282, 11390, 11478, 11551, 11655, 11763, 11846, 11947, 12013, 12077, 12141, 12205, 12283, 12358, 1242, 12523, 12622, 1274, 12833, 12933, 10024, 10101, 1027, 10388, 10550, 10622, 10720, 10811, 10888, 10956, 11047, 11146, 11219, 11283, 11391, 11479, 11552, 11656, 11764, 11847, 11948, 12014, 12078, 12142, 12206, 12285, 12359, 12430, 12524, 12628, 12750, 12834, 12934, 10025, 10102, 1028, 10389, 10551, 10623, 10721, 10812, 10889, 10957, 11050, 11147, 1121, 11284, 11392, 1147, 11553, 11658, 11765, 11848, 11949, 12015, 12079, 12143, 12207, 12286, 1235, 12431, 12525, 12629, 12753, 12835, 12935, 10029, 10103, 10291, 10392, 10554, 10624, 10722, 10813, 1088, 10958, 11051, 11148, 11221, 11289, 11395, 11480, 11554, 11659, 11768, 11849, 11950, 12016, 1207, 12144, 12208, 12287, 12360, 12433, 1252, 1262, 12754, 12836, 12936, 10030, 10104, 10292, 10393, 10555, 10625, 10734, 10814, 10890, 10959, 11052, 11149, 11222, 112, 11397, 11481, 11555, 11660, 1176, 1184, 11951, 12017, 12081, 12145, 12209, 12288, 12361, 12434, 12532, 12630, 12755, 12839, 12937, 10031, 10105, 10294, 10394, 10556, 10628, 10737, 10815, 10891, 1095, 11053, 11150, 11223, 11308, 11398, 11482, 11556, 11661, 11771, 11850, 11952, 12018, 12083, 12146, 1220, 1228, 12362, 12436, 12535, 12631, 12758, 1283, 12938, 10032, 10106, 1029, 10395, 10557, 10629, 10738, 10816, 10892, 10960, 11054, 11153, 11224, 11309, 11483, 11557, 11662, 11772, 11851, 11953, 12019, 12084, 12147, 12210, 12291, 12363, 12438, 12536, 12632, 12759, 12841, 12939, 10033, 10107, 102, 10396, 10560, 1062, 10739, 10817, 10893, 10961, 11055, 11156, 11225, 11310, 1139, 11485, 11558, 11664, 11852, 11954, 12020, 12085, 1214, 12292, 12366, 12439, 12537, 12633, 12760, 12843, 12940, 10034, 10108, 10314, 10399, 10561, 10630, 10740, 10818, 10894, 10962, 11056, 11157, 11226, 11311, 113, 11488, 11559, 11665, 1177, 11853, 11955, 12021, 12086, 12150, 12212, 12294, 1236, 12440, 12538, 12634, 12761, 12845, 12941, 10035, 10112, 10317, 1039, 10562, 10631, 10741, 1081, 10895, 10965, 11059, 11158, 11227, 11312, 11400, 11489, 1155, 11666, 1178, 11854, 11956, 12022, 12087, 12151, 12296, 12370, 12441, 12539, 12635, 12762, 12846, 12942, 10036, 10118, 10318, 103, 10563, 10632, 10743, 10826, 10896, 10966, 11060, 11159, 11228, 11313, 11401, 11490, 11565, 11667, 1179, 11855, 11957, 12023, 12088, 12152, 12214, 12297, 12371, 12442, 1253, 12638, 12763, 12847, 12943, 10037, 10119, 10319, 10401, 10564, 10633, 10744, 10897, 10967, 11067, 11160, 11229, 11314, 11402, 11491, 11566, 11668, 117, 11856, 11958, 12024, 12089, 12153, 12215, 12298, 12372, 12443, 12540, 12639, 12764, 12849, 12944, 10038, 10120, 10320, 10402, 10565, 10635, 10747, 10828, 10898, 10968, 11068, 11161, 11230, 11315, 11403, 11493, 11567, 11800, 1185, 12025, 1208, 12154, 12217, 12299, 12373, 12444, 12541, 1263, 12765, 1284, 12945, 10039, 10121, 10321, 10409, 10566, 10636, 10748, 10830, 10899, 10969, 11069, 11162, 11231, 11321, 11408, 11496, 11568, 11671, 11801, 11878, 11960, 12026, 12090, 12155, 12218, 122, 12374, 12445, 12542, 12645, 12766, 12851, 12946, 10040, 10125, 10322, 1040, 10567, 10637, 10760, 10831, 1089, 10970, 11074, 11232, 11322, 11409, 11498, 11569, 11672, 11802, 11880, 11961, 12027, 12091, 12156, 12219, 12300, 12375, 12450, 12545, 1264, 12767, 12852, 12947, 10041, 10127, 10323, 10410, 10568, 10638, 10761, 10832, 108, 10973, 11078, 11164, 11233, 11323, 1140, 11499, 1156, 11673, 11803, 11881, 11962, 12028, 12092, 12157, 12220, 12302, 12376, 12452, 12548, 12652, 12768, 12853, 12948, 10042, 10128, 10324, 10417, 10569, 10639, 10762, 10835, 10900, 10974, 11079, 11165, 11234, 11327, 11410, 114, 11570, 11674, 11882, 11963, 12029, 12093, 12158, 12221, 12304, 12378, 12453, 12549, 12653, 12769, 12854, 1294, 10043, 10129, 10327, 10418, 10570, 1063, 10763, 10837, 10901, 10976, 11080, 11166, 11235, 11328, 11411, 11503, 11571, 11675, 11805, 11883, 11965, 12094, 12159, 12222, 12305, 1237, 12454, 1254, 12654, 12770, 12855, 12951, 10044, 10130, 10328, 1041, 10571, 10642, 10765, 10840, 10902, 10979, 11082, 11167, 11236, 11329, 11416, 11504, 11575, 11676, 11806, 11885, 11966, 12030, 12096, 1215, 12223, 12306, 12380, 12455, 12550, 12655, 12779, 12955, 10045, 10131, 10329, 10422, 10572, 10643, 10766, 10842, 10904, 1097, 11083, 11168, 11237, 11330, 11417, 11505, 11576, 1167, 11807, 11886, 11969, 12031, 12097, 12160, 12224, 12307, 12381, 12456, 12551, 12658, 1277, 1285, 12956, 10046, 10132, 10330, 1043, 10573, 10767, 10843, 10905, 10981, 11084, 11169, 11238, 11331, 11418, 11506, 11577, 1168, 11808, 11887, 1196, 12032, 12098, 12161, 12225, 12308, 12383, 1245, 12552, 12659, 12781, 12860, 12957, 10047, 10133, 10331, 10451, 10576, 10655, 10769, 10844, 10908, 10983, 11085, 11170, 11239, 11332, 11419, 11507, 11578, 11690, 11809, 11888, 11970, 12033, 12099, 12162, 12226, 12309, 12384, 12460, 12553, 1265, 12784, 12863, 12958, 10048, 10134, 10332, 10452, 10577, 10656, 1076, 10847, 10909, 10984, 11088, 11171, 11240, 11333, 11420, 11508, 11579, 11691, 1180, 11889, 11971, 12034, 1209, 12163, 12227, 1230, 12387, 12461, 12554, 12664, 12785, 12864, 1297, 10049, 1013, 10333, 10453, 10578, 10657, 10770, 10848, 1090, 10985, 11089, 11172, 11241, 11334, 11421, 11509, 1157, 11692, 11810, 11890, 11972, 12035, 120, 12164, 12228, 12310, 12388, 12462, 12563, 12667, 12786, 12866, 1298, 10050, 10140, 10335, 10454, 10579, 10658, 10771, 10849, 10910, 10986, 11090, 11173, 11242, 11335, 11422, 1150, 11580, 11693, 11811, 11892, 11973, 12036, 12100, 12166, 12229, 12311, 12389, 12463, 12564, 12668, 12787, 12867, 1299, 10051, 10142, 10338, 10455, 10580, 10659, 10772, 10850, 10911, 10987, 11091, 11174, 11243, 11336, 11424, 11510, 11581, 11694, 11812, 11894, 11974, 12037, 12101, 12169, 1222, 12312, 1238, 12464, 12565, 12669, 12788, 12868, 129, 10052, 10143, 10339, 10458, 10581, 1065, 10773, 10851, 10988, 11092, 11244, 11339, 11426, 11512, 11582, 11695, 11813, 11895, 11975, 12038, 12102, 1216, 12231, 12313, 12390, 12465, 12567, 1266, 1278, 12870, 12, 10053, 10144, 1033, 10459, 10582, 10660, 10776, 10852, 10913, 10989, 11093, 11179, 11245, 11343, 11427, 11513, 1158, 11696, 11814, 11897, 11976, 12039, 12103, 12170, 12232, 12314, 12391, 12466, 12568, 12675, 12792, 12871, 13, 10054, 10147, 10340, 1045, 10583, 10661, 10777, 10853, 10916, 1098, 11095, 11180, 11246, 11344, 1142, 11514, 11599, 11697, 11815, 1189, 11977, 1203, 12104, 12171, 12233, 12392, 12467, 12569, 12676, 12793, 12872, 14, 10055, 10148, 10341, 10460, 10585, 10662, 10778, 10854, 10918, 10991, 11096, 11181, 11247, 11345, 11431, 11515, 1159, 11698, 11816, 118, 11978, 12040, 12106, 12172, 12235, 12317, 12393, 12468, 12570, 12677, 12794, 12874, 15, 10056, 1014, 10342, 10461, 10586, 10665, 10779, 10855, 10919, 10993, 11097, 11182, 11248, 11347, 11433, 11516, 115, 1169, 11817, 11901, 11979, 12041, 12107, 12173, 12236, 12318, 12394, 12469, 12571, 12678, 12797, 12880, 16, 10057, 10159, 10345, 10462, 10587, 10666, 1077, 10856, 1091, 10994, 11100, 11183, 11349, 11435, 11517, 11604, 11707, 11818, 11902, 1197, 12042, 12109, 12175, 12239, 12319, 12395, 1246, 12572, 1267, 12798, 12881, 17, 10058, 1015, 1034, 10463, 10588, 10667, 10780, 10857, 10920, 10995, 11101, 11184, 11250, 11350, 11436, 11518, 11605, 11708, 11819, 11904, 11980, 12043, 1210, 12176, 1223, 1231, 12396, 12470, 12573, 12684, 12799, 12882, 18, 10059, 10161, 10350, 10464, 10589, 10668, 10781, 10858, 10921, 10996, 11102, 11185, 11251, 11351, 11437, 11519, 11606, 11709, 1181, 11906, 11981, 12044, 12110, 12177, 12240, 12320, 12397, 12472, 12574, 12685, 1279, 12885, 19, 1005, 10162, 10351, 1046, 10590, 10670, 10782, 10859, 10922, 10997, 11105, 11186, 11252, 11352, 11438, 1151, 11607, 1170, 11820, 11912, 11982, 12045, 12111, 12179, 12241, 12321, 12398, 12473, 12575, 12686, 127, 12886, 2609, 10060, 10163, 10352, 10472, 10592, 10671, 10783, 1085, 10924, 10998, 11106, 11187, 11253, 11353, 11440, 11520, 11608, 11710, 11821, 11913, 11983, 12046, 12112, 1217, 12242, 12322, 1239, 12474, 12576, 12687, 12800, 12887, 10061, 10164, 10355, 10473, 10593, 10672, 10784, 10860, 10925, 10999, 11107, 11188, 11254, 11354, 11441, 11521, 11609, 11711, 11822, 11915, 11984, 12047, 12114, 12180, 12244, 12323, 123, 12475, 1257, 12689, 12801, 12888, 10062, 10166, 10356, 10474, 10596, 10673, 10785, 10861, 10926, 1099, 11110, 11189, 11255, 11356, 11442, 11524, 11610, 11712, 11823, 11917, 11985, 12048, 12115, 12181, 12245, 12324, 12400, 12476, 12580, 1268, 12803, 1288, 10063, 1016, 10357, 1047, 10597, 10674, 10786, 10862, 10928, 109, 11111, 1118, 11256, 11357, 11448, 11525, 11611, 11713, 11824, 11918, 11986, 12049, 12116, 12183, 12248, 12325, 12402, 12477, 12581, 12690, 12805, 12892", "word_idx": 85465, "sentence_idx": 1169, "label": "unlabeled"}, {"type": "text", "expr": "10000, 10064, 1019, 10358, 10482, 10598, 10675, 10787, 10864, 10929, 10, 11112, 11190, 11257, 11363, 11449, 11526, 11612, 11715, 11825, 11920, 11987, 1204, 12118, 12184, 12249, 12326, 12405, 12478, 12582, 12691, 12806, 12895, 10001, 10065, 101, 1035, 10483, 10599, 10676, 10788, 10865, 10930, 11007, 11113, 11191, 11258, 11364, 11451, 11527, 11613, 11716, 11826, 11921, 11988, 12050, 1211, 12185, 12252, 12327, 12406, 1247, 12583, 12692, 12807, 12896, 10002, 10066, 10201, 10363, 10489, 1059, 1067, 10789, 10867, 10931, 11008, 11114, 11192, 11259, 11365, 11452, 11528, 11614, 1171, 11827, 11922, 11989, 12051, 12121, 12186, 12253, 12328, 12409, 12486, 12584, 12696, 12808, 12897, 10067, 10202, 10365, 1048, 105, 10684, 1078, 10868, 10932, 11009, 11115, 11193, 11260, 11366, 11454, 1152, 11615, 1172, 11828, 11923, 1198, 12052, 12122, 12187, 12254, 12329, 1240, 1248, 12697, 12809, 12898, 10004, 10068, 1020, 10366, 10490, 10600, 10687, 10790, 10869, 10933, 11010, 11119, 11194, 11263, 11367, 11455, 11530, 11623, 11734, 11829, 11924, 11990, 12054, 12123, 12188, 12256, 1232, 12412, 12490, 12589, 12699, 1280, 12899, 10005, 10069, 10210, 10367, 10491, 10601, 1068, 10791, 10870, 10934, 11012, 11120, 11195, 11264, 11368, 11456, 11531, 11624, 11735, 1182, 11926, 11991, 12055, 12124, 12189, 12257, 12330, 12413, 12491, 1258, 1269, 12810, 128, 10006, 1006, 10211, 10368, 10493, 10602, 10690, 10792, 10871, 10935, 11013, 11121, 11196, 11265, 11369, 11459, 11533, 11625, 11736, 11830, 11929, 11992, 12056, 12125, 1218, 12259, 12333, 12414, 12498, 12590, 12811, 12900, 10007, 10070, 10212, 10369, 1049, 10603, 10691, 10793, 10872, 10936, 11014, 11122, 11197, 11266, 11370, 1145, 11534, 11626, 11737, 11831, 11930, 11993, 12057, 12126, 12190, 1225, 12336, 12415, 124, 12591, 12700, 12813, 12901, 10008, 10071, 10213, 1036, 104, 10605, 10692, 10794, 10873, 10937, 11123, 11198, 11267, 11371, 11460, 11537, 11632, 11738, 11832, 11994, 12058, 12127, 12191, 12261, 12337, 12416, 12504, 12592, 1270, 12814, 12902, 10009, 10072, 10214, 10370, 1050, 10606, 10693, 10795, 10874, 10938, 11016, 11124, 111, 11268, 11372, 11461, 11538, 11633, 1173, 11833, 11932, 11995, 12059, 12128, 12192, 12262, 12340, 12417, 12593, 1271, 12815, 12904, 10010, 10073, 10216, 10371, 10510, 10607, 10694, 10796, 10875, 10939, 11017, 11125, 11200, 11269, 11373, 11462, 11539, 11634, 11740, 11834, 11933, 11996, 12060, 12129, 12193, 12263, 12341, 12418, 12506, 12594, 12732, 12816, 12905, 10011, 1007, 10217, 10372, 10609, 10698, 10797, 10876, 10940, 11018, 11127, 11201, 11270, 11376, 11464, 1153, 11741, 11835, 11934, 11997, 12061, 1212, 12194, 12264, 12342, 12419, 12507, 12595, 12736, 12817, 1290, 10012, 10084, 10219, 10373, 10517, 10610, 10699, 10798, 10877, 10942, 11019, 11128, 11202, 11271, 11377, 11468, 11540, 11636, 11742, 11836, 11935, 11998, 12062, 12130, 12195, 12265, 12343, 1241, 1250, 12596, 12737, 12819, 12915, 10013, 10085, 1021, 10374, 10518, 10611, 1069, 10799, 10878, 10943, 11020, 1112, 11203, 11272, 11378, 11469, 11541, 11637, 11743, 11837, 11936, 11999, 12063, 12131, 12196, 12269, 12344, 12420, 12511, 1259, 12738, 1281, 12916, 10014, 10090, 10222, 10375, 10519, 10612, 106, 1079, 10879, 10944, 11021, 11130, 11204, 11273, 11379, 1146, 11542, 11638, 11745, 11838, 11937, 119, 12064, 12132, 12197, 12270, 12345, 12421, 12512, 125, 12739, 12821, 12917, 10015, 10091, 10224, 10376, 1051, 10613, 10700, 10800, 1087, 10945, 11028, 11136, 11210, 11274, 11382, 11470, 11543, 11647, 11746, 11839, 11938, 11, 12066, 12133, 12198, 12272, 12346, 12422, 12513, 12600, 12740, 12823, 1291, 10016, 10092, 10225, 10377, 10520, 10615, 10707, 10801, 10880, 10946, 11029, 11137, 11211, 11275, 11383, 11471, 11544, 11648, 11749, 1183, 11939, 12001, 12067, 12134, 12199, 12277, 12349, 12423, 12514, 12601, 12741, 12825, 12922, 10017, 10095, 10226, 10378, 10523, 10616, 10708, 10803, 10881, 10947, 11030, 11138, 11212, 11276, 11385, 11472, 11545, 11649, 1174, 11840, 11941, 12002, 12068, 12135, 121, 12278, 12350, 12424, 12515, 12611, 12742, 12826, 12923, 10018, 10096, 1022, 10379, 1052, 10617, 10709, 10804, 10882, 10948, 11031, 11140, 11213, 11277, 11386, 11473, 11546, 1164, 11753, 11841, 11942, 12004, 12069, 12136, 12200, 12279, 12351, 12425, 12516, 12614, 12743, 12827, 12924, 10019, 10097, 10234, 10380, 10538, 10618, 10712, 10805, 10883, 10949, 11032, 11141, 11214, 11278, 11387, 11474, 11548, 11651, 11754, 11842, 11943, 12006, 1206, 12137, 12201, 1227, 12352, 12426, 12517, 12617, 12744, 12828, 12925, 1001, 10098, 1024, 10381, 10543, 10619, 10714, 10806, 10884, 1094, 11033, 11142, 11215, 11279, 11388, 11475, 11549, 11652, 1175, 11843, 11944, 12007, 12071, 12138, 12202, 12280, 12353, 12427, 1251, 12618, 12745, 1282, 12926, 10020, 10099, 10266, 10382, 10544, 1061, 10715, 10807, 10885, 10950, 11034, 11143, 11216, 11280, 11389, 11476, 1154, 11653, 11761, 11844, 11945, 1200, 12073, 12139, 12203, 12281, 12354, 12428, 12521, 12619, 12746, 12830, 12928, 10022, 100, 10267, 10383, 10545, 10620, 10716, 10808, 10886, 10954, 1103, 11144, 11217, 11281, 1138, 11477, 11550, 11654, 11762, 11845, 11946, 12010, 12074, 12140, 12204, 12282, 12357, 12429, 12522, 1261, 12747, 12832, 12929, 10023, 10100, 10386, 10546, 10621, 10717, 1080, 10887, 10955, 11045, 11145, 11218, 11282, 11390, 11478, 11551, 11655, 11763, 11846, 11947, 12013, 12077, 12141, 12205, 12283, 12358, 1242, 12523, 12622, 1274, 12833, 12933, 10024, 10101, 1027, 10388, 10550, 10622, 10720, 10811, 10888, 10956, 11047, 11146, 11219, 11283, 11391, 11479, 11552, 11656, 11764, 11847, 11948, 12014, 12078, 12142, 12206, 12285, 12359, 12430, 12524, 12628, 12750, 12834, 12934, 10025, 10102, 1028, 10389, 10551, 10623, 10721, 10812, 10889, 10957, 11050, 11147, 1121, 11284, 11392, 1147, 11553, 11658, 11765, 11848, 11949, 12015, 12079, 12143, 12207, 12286, 1235, 12431, 12525, 12629, 12753, 12835, 12935, 10029, 10103, 10291, 10392, 10554, 10624, 10722, 10813, 1088, 10958, 11051, 11148, 11221, 11289, 11395, 11480, 11554, 11659, 11768, 11849, 11950, 12016, 1207, 12144, 12208, 12287, 12360, 12433, 1252, 1262, 12754, 12836, 12936, 10030, 10104, 10292, 10393, 10555, 10625, 10734, 10814, 10890, 10959, 11052, 11149, 11222, 112, 11397, 11481, 11555, 11660, 1176, 1184, 11951, 12017, 12081, 12145, 12209, 12288, 12361, 12434, 12532, 12630, 12755, 12839, 12937, 10031, 10105, 10294, 10394, 10556, 10628, 10737, 10815, 10891, 1095, 11053, 11150, 11223, 11308, 11398, 11482, 11556, 11661, 11771, 11850, 11952, 12018, 12083, 12146, 1220, 1228, 12362, 12436, 12535, 12631, 12758, 1283, 12938, 10032, 10106, 1029, 10395, 10557, 10629, 10738, 10816, 10892, 10960, 11054, 11153, 11224, 11309, 11483, 11557, 11662, 11772, 11851, 11953, 12019, 12084, 12147, 12210, 12291, 12363, 12438, 12536, 12632, 12759, 12841, 12939, 10033, 10107, 102, 10396, 10560, 1062, 10739, 10817, 10893, 10961, 11055, 11156, 11225, 11310, 1139, 11485, 11558, 11664, 11852, 11954, 12020, 12085, 1214, 12292, 12366, 12439, 12537, 12633, 12760, 12843, 12940, 10034, 10108, 10314, 10399, 10561, 10630, 10740, 10818, 10894, 10962, 11056, 11157, 11226, 11311, 113, 11488, 11559, 11665, 1177, 11853, 11955, 12021, 12086, 12150, 12212, 12294, 1236, 12440, 12538, 12634, 12761, 12845, 12941, 10035, 10112, 10317, 1039, 10562, 10631, 10741, 1081, 10895, 10965, 11059, 11158, 11227, 11312, 11400, 11489, 1155, 11666, 1178, 11854, 11956, 12022, 12087, 12151, 12296, 12370, 12441, 12539, 12635, 12762, 12846, 12942, 10036, 10118, 10318, 103, 10563, 10632, 10743, 10826, 10896, 10966, 11060, 11159, 11228, 11313, 11401, 11490, 11565, 11667, 1179, 11855, 11957, 12023, 12088, 12152, 12214, 12297, 12371, 12442, 1253, 12638, 12763, 12847, 12943, 10037, 10119, 10319, 10401, 10564, 10633, 10744, 10897, 10967, 11067, 11160, 11229, 11314, 11402, 11491, 11566, 11668, 117, 11856, 11958, 12024, 12089, 12153, 12215, 12298, 12372, 12443, 12540, 12639, 12764, 12849, 12944, 10038, 10120, 10320, 10402, 10565, 10635, 10747, 10828, 10898, 10968, 11068, 11161, 11230, 11315, 11403, 11493, 11567, 11800, 1185, 12025, 1208, 12154, 12217, 12299, 12373, 12444, 12541, 1263, 12765, 1284, 12945, 10039, 10121, 10321, 10409, 10566, 10636, 10748, 10830, 10899, 10969, 11069, 11162, 11231, 11321, 11408, 11496, 11568, 11671, 11801, 11878, 11960, 12026, 12090, 12155, 12218, 122, 12374, 12445, 12542, 12645, 12766, 12851, 12946, 10040, 10125, 10322, 1040, 10567, 10637, 10760, 10831, 1089, 10970, 11074, 11232, 11322, 11409, 11498, 11569, 11672, 11802, 11880, 11961, 12027, 12091, 12156, 12219, 12300, 12375, 12450, 12545, 1264, 12767, 12852, 12947, 10041, 10127, 10323, 10410, 10568, 10638, 10761, 10832, 108, 10973, 11078, 11164, 11233, 11323, 1140, 11499, 1156, 11673, 11803, 11881, 11962, 12028, 12092, 12157, 12220, 12302, 12376, 12452, 12548, 12652, 12768, 12853, 12948, 10042, 10128, 10324, 10417, 10569, 10639, 10762, 10835, 10900, 10974, 11079, 11165, 11234, 11327, 11410, 114, 11570, 11674, 11882, 11963, 12029, 12093, 12158, 12221, 12304, 12378, 12453, 12549, 12653, 12769, 12854, 1294, 10043, 10129, 10327, 10418, 10570, 1063, 10763, 10837, 10901, 10976, 11080, 11166, 11235, 11328, 11411, 11503, 11571, 11675, 11805, 11883, 11965, 12094, 12159, 12222, 12305, 1237, 12454, 1254, 12654, 12770, 12855, 12951, 10044, 10130, 10328, 1041, 10571, 10642, 10765, 10840, 10902, 10979, 11082, 11167, 11236, 11329, 11416, 11504, 11575, 11676, 11806, 11885, 11966, 12030, 12096, 1215, 12223, 12306, 12380, 12455, 12550, 12655, 12779, 12955, 10045, 10131, 10329, 10422, 10572, 10643, 10766, 10842, 10904, 1097, 11083, 11168, 11237, 11330, 11417, 11505, 11576, 1167, 11807, 11886, 11969, 12031, 12097, 12160, 12224, 12307, 12381, 12456, 12551, 12658, 1277, 1285, 12956, 10046, 10132, 10330, 1043, 10573, 10767, 10843, 10905, 10981, 11084, 11169, 11238, 11331, 11418, 11506, 11577, 1168, 11808, 11887, 1196, 12032, 12098, 12161, 12225, 12308, 12383, 1245, 12552, 12659, 12781, 12860, 12957, 10047, 10133, 10331, 10451, 10576, 10655, 10769, 10844, 10908, 10983, 11085, 11170, 11239, 11332, 11419, 11507, 11578, 11690, 11809, 11888, 11970, 12033, 12099, 12162, 12226, 12309, 12384, 12460, 12553, 1265, 12784, 12863, 12958, 10048, 10134, 10332, 10452, 10577, 10656, 1076, 10847, 10909, 10984, 11088, 11171, 11240, 11333, 11420, 11508, 11579, 11691, 1180, 11889, 11971, 12034, 1209, 12163, 12227, 1230, 12387, 12461, 12554, 12664, 12785, 12864, 1297, 10049, 1013, 10333, 10453, 10578, 10657, 10770, 10848, 1090, 10985, 11089, 11172, 11241, 11334, 11421, 11509, 1157, 11692, 11810, 11890, 11972, 12035, 120, 12164, 12228, 12310, 12388, 12462, 12563, 12667, 12786, 12866, 1298, 10050, 10140, 10335, 10454, 10579, 10658, 10771, 10849, 10910, 10986, 11090, 11173, 11242, 11335, 11422, 1150, 11580, 11693, 11811, 11892, 11973, 12036, 12100, 12166, 12229, 12311, 12389, 12463, 12564, 12668, 12787, 12867, 1299, 10051, 10142, 10338, 10455, 10580, 10659, 10772, 10850, 10911, 10987, 11091, 11174, 11243, 11336, 11424, 11510, 11581, 11694, 11812, 11894, 11974, 12037, 12101, 12169, 1222, 12312, 1238, 12464, 12565, 12669, 12788, 12868, 129, 10052, 10143, 10339, 10458, 10581, 1065, 10773, 10851, 10988, 11092, 11244, 11339, 11426, 11512, 11582, 11695, 11813, 11895, 11975, 12038, 12102, 1216, 12231, 12313, 12390, 12465, 12567, 1266, 1278, 12870, 12, 10053, 10144, 1033, 10459, 10582, 10660, 10776, 10852, 10913, 10989, 11093, 11179, 11245, 11343, 11427, 11513, 1158, 11696, 11814, 11897, 11976, 12039, 12103, 12170, 12232, 12314, 12391, 12466, 12568, 12675, 12792, 12871, 13, 10054, 10147, 10340, 1045, 10583, 10661, 10777, 10853, 10916, 1098, 11095, 11180, 11246, 11344, 1142, 11514, 11599, 11697, 11815, 1189, 11977, 1203, 12104, 12171, 12233, 12392, 12467, 12569, 12676, 12793, 12872, 14, 10055, 10148, 10341, 10460, 10585, 10662, 10778, 10854, 10918, 10991, 11096, 11181, 11247, 11345, 11431, 11515, 1159, 11698, 11816, 118, 11978, 12040, 12106, 12172, 12235, 12317, 12393, 12468, 12570, 12677, 12794, 12874, 15, 10056, 1014, 10342, 10461, 10586, 10665, 10779, 10855, 10919, 10993, 11097, 11182, 11248, 11347, 11433, 11516, 115, 1169, 11817, 11901, 11979, 12041, 12107, 12173, 12236, 12318, 12394, 12469, 12571, 12678, 12797, 12880, 16, 10057, 10159, 10345, 10462, 10587, 10666, 1077, 10856, 1091, 10994, 11100, 11183, 11349, 11435, 11517, 11604, 11707, 11818, 11902, 1197, 12042, 12109, 12175, 12239, 12319, 12395, 1246, 12572, 1267, 12798, 12881, 17, 10058, 1015, 1034, 10463, 10588, 10667, 10780, 10857, 10920, 10995, 11101, 11184, 11250, 11350, 11436, 11518, 11605, 11708, 11819, 11904, 11980, 12043, 1210, 12176, 1223, 1231, 12396, 12470, 12573, 12684, 12799, 12882, 18, 10059, 10161, 10350, 10464, 10589, 10668, 10781, 10858, 10921, 10996, 11102, 11185, 11251, 11351, 11437, 11519, 11606, 11709, 1181, 11906, 11981, 12044, 12110, 12177, 12240, 12320, 12397, 12472, 12574, 12685, 1279, 12885, 19, 1005, 10162, 10351, 1046, 10590, 10670, 10782, 10859, 10922, 10997, 11105, 11186, 11252, 11352, 11438, 1151, 11607, 1170, 11820, 11912, 11982, 12045, 12111, 12179, 12241, 12321, 12398, 12473, 12575, 12686, 127, 12886, 2609, 10060, 10163, 10352, 10472, 10592, 10671, 10783, 1085, 10924, 10998, 11106, 11187, 11253, 11353, 11440, 11520, 11608, 11710, 11821, 11913, 11983, 12046, 12112, 1217, 12242, 12322, 1239, 12474, 12576, 12687, 12800, 12887, 10061, 10164, 10355, 10473, 10593, 10672, 10784, 10860, 10925, 10999, 11107, 11188, 11254, 11354, 11441, 11521, 11609, 11711, 11822, 11915, 11984, 12047, 12114, 12180, 12244, 12323, 123, 12475, 1257, 12689, 12801, 12888, 10062, 10166, 10356, 10474, 10596, 10673, 10785, 10861, 10926, 1099, 11110, 11189, 11255, 11356, 11442, 11524, 11610, 11712, 11823, 11917, 11985, 12048, 12115, 12181, 12245, 12324, 12400, 12476, 12580, 1268, 12803, 1288, 10063, 1016, 10357, 1047, 10597, 10674, 10786, 10862, 10928, 109, 11111, 1118, 11256, 11357, 11448, 11525, 11611, 11713, 11824, 11918, 11986, 12049, 12116, 12183, 12248, 12325, 12402, 12477, 12581, 12690, 12805, 12892", "word_idx": 99325, "sentence_idx": 1170, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a05:  Validation learning curves for the Gutenberg corpus", "word_idx": 113185, "sentence_idx": 1171, "label": "unlabeled"}, {"type": "text", "expr": " All word classes have been observed after around  $4$ B training tokens and we observe the performance of Hebbian Softmax return to that of the vanilla LSTM thereafter, as all parameters are optimized by gradient descent", "word_idx": 113247, "sentence_idx": 1172, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a05:", "word_idx": 113468, "sentence_idx": 1173, "label": "unlabeled"}, {"type": "text", "expr": "3  GigaWord", "word_idx": 113477, "sentence_idx": 1174, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a06:  Test perplexity on GigaWord v5 corpus", "word_idx": 113488, "sentence_idx": 1175, "label": "unlabeled"}, {"type": "text", "expr": " Each model is trained on all articles from  $200-2009$  and tested on  $2010$ ", "word_idx": 113536, "sentence_idx": 1176, "label": "unlabeled"}, {"type": "text", "expr": " Because the test set is very large, a random subsample of articles are used per evaluation cycle", "word_idx": 113615, "sentence_idx": 1177, "label": "unlabeled"}, {"type": "text", "expr": " For this reason, the measurements are more noisy", "word_idx": 113712, "sentence_idx": 1178, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a06:", "word_idx": 113761, "sentence_idx": 1179, "label": "unlabeled"}, {"type": "math", "expr": "$$200-2009$$", "word_idx": 113770, "sentence_idx": 1180, "label": "unlabeled"}, {"type": "math", "expr": "$$2010$$", "word_idx": 113778, "sentence_idx": 1181, "label": "unlabeled"}, {"type": "text", "expr": "4  Alternate Objective Functions", "word_idx": 113782, "sentence_idx": 1182, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a07:  Validation learning curves for WikiText-103 comparing different overfitting objectives", "word_idx": 113814, "sentence_idx": 1183, "label": "unlabeled"}, {"type": "text", "expr": " Surprisingly there is not a significant improvement in performance by choosing inner objectives which relate to the overall training objective, e", "word_idx": 113911, "sentence_idx": 1184, "label": "unlabeled"}, {"type": "text", "expr": " Softmax, vs  $L2$ ", "word_idx": 114057, "sentence_idx": 1185, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a07:", "word_idx": 114076, "sentence_idx": 1186, "label": "unlabeled"}, {"type": "text", "expr": "5  Data pre-processing", "word_idx": 114085, "sentence_idx": 1187, "label": "unlabeled"}, {"type": "text", "expr": "# Copyright 2018 Google LLC", "word_idx": 114107, "sentence_idx": 1188, "label": "unlabeled"}, {"type": "text", "expr": "\n#\n# Licensed under the Apache License, Version 2", "word_idx": 114134, "sentence_idx": 1189, "label": "unlabeled"}, {"type": "text", "expr": "0 (the \"License\");\n# you may not use this file except in compliance with the License", "word_idx": 114183, "sentence_idx": 1190, "label": "unlabeled"}, {"type": "text", "expr": "\n# You may obtain a copy of the License at\n#\n# https://www", "word_idx": 114267, "sentence_idx": 1191, "label": "unlabeled"}, {"type": "text", "expr": "apache", "word_idx": 114325, "sentence_idx": 1192, "label": "unlabeled"}, {"type": "text", "expr": "org/licenses/LICENSE-2", "word_idx": 114331, "sentence_idx": 1193, "label": "unlabeled"}, {"type": "text", "expr": "0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied", "word_idx": 114353, "sentence_idx": 1194, "label": "unlabeled"}, {"type": "text", "expr": "\n# See the License for the specific language governing permissions and\n# limitations under the License", "word_idx": 114568, "sentence_idx": 1195, "label": "unlabeled"}, {"type": "text", "expr": "\n\ndef process_text(text):\n  import nltk\n  start_text = \"START OF THIS PROJECT GUTENBERG EBOOK\"\n  start = text", "word_idx": 114670, "sentence_idx": 1196, "label": "unlabeled"}, {"type": "text", "expr": "find(start_text) + len(start_text)\n  end = text", "word_idx": 114779, "sentence_idx": 1197, "label": "unlabeled"}, {"type": "text", "expr": "find(\"END OF THIS PROJECT GUTENBERG EBOOK\")\n  text = text[start:end]\n  text = text", "word_idx": 114826, "sentence_idx": 1198, "label": "unlabeled"}, {"type": "text", "expr": "decode(\"utf-8\", \"ignore\")\n  text = text", "word_idx": 114908, "sentence_idx": 1199, "label": "unlabeled"}, {"type": "text", "expr": "replace(\"\\r\", \" \")\n  text = text", "word_idx": 114947, "sentence_idx": 1200, "label": "unlabeled"}, {"type": "text", "expr": "replace(\"\\n\", \" \")\n  final_text_list = []\n  sent_text_tokens = nltk", "word_idx": 114979, "sentence_idx": 1201, "label": "unlabeled"}, {"type": "text", "expr": "sent_tokenize(text)\n  for sentence in sent_text_tokens:\n    final_text_list", "word_idx": 115046, "sentence_idx": 1202, "label": "unlabeled"}, {"type": "text", "expr": "extend(nltk", "word_idx": 115121, "sentence_idx": 1203, "label": "unlabeled"}, {"type": "text", "expr": "word_tokenize(sentence) + [\"\\n\"])\n  return \" \"", "word_idx": 115132, "sentence_idx": 1204, "label": "unlabeled"}, {"type": "text", "expr": "join(final_text_list)", "word_idx": 115178, "sentence_idx": 1205, "label": "unlabeled"}, {"type": "text", "expr": "lower()", "word_idx": 115199, "sentence_idx": 1206, "label": "unlabeled"}, {"type": "text", "expr": "encode(\"utf-8\")", "word_idx": 115206, "sentence_idx": 1207, "label": "unlabeled"}, {"type": "text", "expr": "For Project Gutenberg and GigaWord v5 we used a very simple python script to pre-process and tokenize the data using NLTK", "word_idx": 115221, "sentence_idx": 1208, "label": "unlabeled"}, {"type": "text", "expr": " We post the Gutenberg script here for ease of reproduction", "word_idx": 115342, "sentence_idx": 1209, "label": "unlabeled"}, {"type": "text", "expr": " The GigaWord v5 script excludes the Project Gutenberg-specific selection of start / end markers to extract the text", "word_idx": 115401, "sentence_idx": 1210, "label": "unlabeled"}, {"type": "text", "expr": " The NLTK library   is used to split out sentence and word tokens, the resulting text contains lower-case text with one sentence per line", "word_idx": 115517, "sentence_idx": 1211, "label": "unlabeled"}, {"type": "text", "expr": "3 https://www", "word_idx": 115654, "sentence_idx": 1212, "label": "unlabeled"}, {"type": "text", "expr": "https://www", "word_idx": 115667, "sentence_idx": 1213, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0B  Omniglot", "word_idx": 115678, "sentence_idx": 1214, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0B", "word_idx": 115698, "sentence_idx": 1215, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a08:  Omniglot curriculum performance versus learning rate for a regular softmax architecture", "word_idx": 115708, "sentence_idx": 1216, "label": "unlabeled"}, {"type": "text", "expr": " Values of  $1e-3$  to  $8e-3$  are similarly fast to learn and are stable", "word_idx": 115806, "sentence_idx": 1217, "label": "unlabeled"}, {"type": "text", "expr": " Stability breaks down for larger values", "word_idx": 115880, "sentence_idx": 1218, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a08:", "word_idx": 115920, "sentence_idx": 1219, "label": "unlabeled"}, {"type": "math", "expr": "$$1e-3$$", "word_idx": 115929, "sentence_idx": 1220, "label": "unlabeled"}, {"type": "math", "expr": "$$8e-3$$", "word_idx": 115933, "sentence_idx": 1221, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a09:  Omniglot curriculum task", "word_idx": 115937, "sentence_idx": 1222, "label": "unlabeled"}, {"type": "text", "expr": " Starting from  $30$  classes,  $5$  new classes are added when total test error exceeds  $60$ %", "word_idx": 115972, "sentence_idx": 1223, "label": "unlabeled"}, {"type": "text", "expr": " Each line shows a  $2-\\sigma$  confidence band obtained from  $10$  independent seed runs", "word_idx": 116068, "sentence_idx": 1224, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a09:", "word_idx": 116158, "sentence_idx": 1225, "label": "unlabeled"}, {"type": "math", "expr": "$$2-\\sigma$$", "word_idx": 116167, "sentence_idx": 1226, "label": "unlabeled"}, {"type": "text", "expr": "Generated  on Tue Apr  3 11:28:32 2018 by", "word_idx": 116175, "sentence_idx": 1227, "label": "unlabeled"}, {"type": "text", "expr": "LaTeXML", "word_idx": 116216, "sentence_idx": 1228, "label": "unlabeled"}], "Unsupervised_Domain_Adaptation": [{"type": "text", "expr": "Unsupervised Domain Adaptation for3D Keypoint Prediction from a Single Depth Scan", "word_idx": 0, "sentence_idx": 0, "label": "unlabeled"}, {"type": "text", "expr": "Unsupervised Domain Adaptation for \n 3D Keypoint Prediction from a Single Depth Scan", "word_idx": 81, "sentence_idx": 1, "label": "unlabeled"}, {"type": "text", "expr": "Xingyi Zhou\n The University of Texas at Austin\n zhouxy@cs", "word_idx": 165, "sentence_idx": 2, "label": "unlabeled"}, {"type": "text", "expr": "utexas", "word_idx": 222, "sentence_idx": 3, "label": "unlabeled"}, {"type": "text", "expr": "zhouxy@cs", "word_idx": 228, "sentence_idx": 4, "label": "unlabeled"}, {"type": "text", "expr": "utexas", "word_idx": 237, "sentence_idx": 5, "label": "unlabeled"}, {"type": "text", "expr": "Arjun Karpur\n The University of Texas at Austin\n akarpur@cs", "word_idx": 243, "sentence_idx": 6, "label": "unlabeled"}, {"type": "text", "expr": "utexas", "word_idx": 302, "sentence_idx": 7, "label": "unlabeled"}, {"type": "text", "expr": "akarpur@cs", "word_idx": 308, "sentence_idx": 8, "label": "unlabeled"}, {"type": "text", "expr": "utexas", "word_idx": 318, "sentence_idx": 9, "label": "unlabeled"}, {"type": "text", "expr": "Chuang Gan\n Tsinghua University\n ganchuang1990@gmail", "word_idx": 324, "sentence_idx": 10, "label": "unlabeled"}, {"type": "text", "expr": "ganchuang1990@gmail", "word_idx": 376, "sentence_idx": 11, "label": "unlabeled"}, {"type": "text", "expr": "Linjie Luo\n Snap, Inc", "word_idx": 395, "sentence_idx": 12, "label": "unlabeled"}, {"type": "text", "expr": "\n linjie", "word_idx": 416, "sentence_idx": 13, "label": "unlabeled"}, {"type": "text", "expr": "luo@snap", "word_idx": 424, "sentence_idx": 14, "label": "unlabeled"}, {"type": "text", "expr": "linjie", "word_idx": 432, "sentence_idx": 15, "label": "unlabeled"}, {"type": "text", "expr": "luo@snap", "word_idx": 438, "sentence_idx": 16, "label": "unlabeled"}, {"type": "text", "expr": "Qixing Huang\n The University of Texas at Austin\n huangqx@cs", "word_idx": 446, "sentence_idx": 17, "label": "unlabeled"}, {"type": "text", "expr": "utexas", "word_idx": 505, "sentence_idx": 18, "label": "unlabeled"}, {"type": "text", "expr": "huangqx@cs", "word_idx": 511, "sentence_idx": 19, "label": "unlabeled"}, {"type": "text", "expr": "utexas", "word_idx": 521, "sentence_idx": 20, "label": "unlabeled"}, {"type": "text", "expr": "Abstract In this paper, we introduce a novel unsupervised domain adaptation technique for the task of 3D keypoint prediction from a single depth scan/image", "word_idx": 527, "sentence_idx": 21, "label": "unlabeled"}, {"type": "text", "expr": " Our key idea is to utilize the fact that predictions from different views of the same or similar objects should be consistent with each other", "word_idx": 682, "sentence_idx": 22, "label": "unlabeled"}, {"type": "text", "expr": " Such view consistency provides effective regularization for keypoint prediction on unlabeled instances", "word_idx": 824, "sentence_idx": 23, "label": "unlabeled"}, {"type": "text", "expr": " In addition, we introduce a geometric alignment term to regularize predictions in the target domain", "word_idx": 927, "sentence_idx": 24, "label": "unlabeled"}, {"type": "text", "expr": " The resulting loss function can be effectively optimized via alternating minimization", "word_idx": 1027, "sentence_idx": 25, "label": "unlabeled"}, {"type": "text", "expr": " We demonstrate the effectiveness of our approach on real datasets and present experimental results showing that our approach is superior to state-of-the-art general-purpose domain adaptation techniques", "word_idx": 1113, "sentence_idx": 26, "label": "unlabeled"}, {"type": "text", "expr": "Abstract", "word_idx": 1315, "sentence_idx": 27, "label": "unlabeled"}, {"type": "text", "expr": "In this paper, we introduce a novel unsupervised domain adaptation technique for the task of 3D keypoint prediction from a single depth scan/image", "word_idx": 1323, "sentence_idx": 28, "label": "unlabeled"}, {"type": "text", "expr": " Our key idea is to utilize the fact that predictions from different views of the same or similar objects should be consistent with each other", "word_idx": 1469, "sentence_idx": 29, "label": "unlabeled"}, {"type": "text", "expr": " Such view consistency provides effective regularization for keypoint prediction on unlabeled instances", "word_idx": 1611, "sentence_idx": 30, "label": "unlabeled"}, {"type": "text", "expr": " In addition, we introduce a geometric alignment term to regularize predictions in the target domain", "word_idx": 1714, "sentence_idx": 31, "label": "unlabeled"}, {"type": "text", "expr": " The resulting loss function can be effectively optimized via alternating minimization", "word_idx": 1814, "sentence_idx": 32, "label": "unlabeled"}, {"type": "text", "expr": " We demonstrate the effectiveness of our approach on real datasets and present experimental results showing that our approach is superior to state-of-the-art general-purpose domain adaptation techniques", "word_idx": 1900, "sentence_idx": 33, "label": "unlabeled"}, {"type": "text", "expr": "\\cvprfinalcopy", "word_idx": 2102, "sentence_idx": 34, "label": "unlabeled"}, {"type": "text", "expr": "1  Introduction", "word_idx": 2116, "sentence_idx": 35, "label": "unlabeled"}, {"type": "text", "expr": "label Section:Introduction", "word_idx": 2131, "sentence_idx": 36, "label": "unlabeled"}, {"type": "text", "expr": "label", "word_idx": 2157, "sentence_idx": 37, "label": "unlabeled"}, {"type": "text", "expr": "A new era has arrived with the proliferation of depth-equipped sensors in all kinds of form factors, ranging from wearables and mobile phones to on-vehicle scanners", "word_idx": 2162, "sentence_idx": 38, "label": "unlabeled"}, {"type": "text", "expr": " This ever-increasing amount of depth scans is a valuable resource that remains largely untapped, however, due to a lack of techniques capable of efficiently processing, representing, and understanding them", "word_idx": 2326, "sentence_idx": 39, "label": "unlabeled"}, {"type": "text", "expr": "A new era has arrived with the proliferation of depth-equipped sensors in all kinds of form factors, ranging from wearables and mobile phones to on-vehicle scanners", "word_idx": 2532, "sentence_idx": 40, "label": "unlabeled"}, {"type": "text", "expr": " This ever-increasing amount of depth scans is a valuable resource that remains largely untapped, however, due to a lack of techniques capable of efficiently processing, representing, and understanding them", "word_idx": 2696, "sentence_idx": 41, "label": "unlabeled"}, {"type": "text", "expr": "3D keypoints, which can be inferred from depth scans, are a compact yet semantically rich representation of 3D objects that have proven effective for many tasks, including reconstruction\u00a0 , object segmentation and recognition\u00a0 , as well as pose estimation\u00a0 ", "word_idx": 2902, "sentence_idx": 42, "label": "unlabeled"}, {"type": "text", "expr": " Despite the wide availability of depth scans of various categories of objects\u00a0 , there is a lack of 3D keypoint annotations for them, which are necessary to train reliable keypoint predictors", "word_idx": 3159, "sentence_idx": 43, "label": "unlabeled"}, {"type": "text", "expr": " This deficit is due to the fact that depth scans are inherently partial views of the underlying objects, making it difficult to annotate the object parts occluded from view", "word_idx": 3351, "sentence_idx": 44, "label": "unlabeled"}, {"type": "text", "expr": " One could automate the annotation process by leveraging the \u201cfused\u201d models created using the depth scans, but most depth-fusion methods are susceptible to scanning noise and cascading errors when depth scans are captured at scale\u00a0 ", "word_idx": 3524, "sentence_idx": 45, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:  Our proposed unsupervised domain adaptation approach improves 3D keypoint prediction results from single depth scans of the Redwood dataset\u00a0 ", "word_idx": 3756, "sentence_idx": 46, "label": "unlabeled"}, {"type": "text", "expr": "  (Top)  without domain adaptation, the pre-trained keypoint predictor from simulated examples failed to predict accurate 3D keypoints (blue)", "word_idx": 3908, "sentence_idx": 47, "label": "unlabeled"}, {"type": "text", "expr": "  (Bottom)  3D keypoint predictions (blue) after domain adaptation are significantly improved", "word_idx": 4049, "sentence_idx": 48, "label": "unlabeled"}, {"type": "text", "expr": " Note that the ground-truth keypoints are shown in red for comparison", "word_idx": 4142, "sentence_idx": 49, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:", "word_idx": 4211, "sentence_idx": 50, "label": "unlabeled"}, {"type": "text", "expr": "(Top)", "word_idx": 4220, "sentence_idx": 51, "label": "unlabeled"}, {"type": "text", "expr": "(Bottom)", "word_idx": 4225, "sentence_idx": 52, "label": "unlabeled"}, {"type": "text", "expr": "label Fig:Best:Result", "word_idx": 4233, "sentence_idx": 53, "label": "unlabeled"}, {"type": "text", "expr": "label", "word_idx": 4254, "sentence_idx": 54, "label": "unlabeled"}, {"type": "text", "expr": "In this paper, our goal is to predict 3D keypoints of an underlying object from a single raw depth scan", "word_idx": 4259, "sentence_idx": 55, "label": "unlabeled"}, {"type": "text", "expr": " To train a reliable 3D keypoint predictor, we generate a large dataset of simulated depth scans using large-scale 3D model repositories such as ShapeNet\u00a0  and ModelNet\u00a0 ", "word_idx": 4362, "sentence_idx": 56, "label": "unlabeled"}, {"type": "text", "expr": " The 3D keypoint annotations on the 3D models from these repositories can naturally carry over to the simulated depth scans for effective supervised training", "word_idx": 4532, "sentence_idx": 57, "label": "unlabeled"}, {"type": "text", "expr": " A large gap exists, however, between the simulated and real depth scan domains", "word_idx": 4689, "sentence_idx": 58, "label": "unlabeled"}, {"type": "text", "expr": " Particularly, 3D models from repositories are generally designed with interactive tools, inevitably resulting in inaccurate geometries with varying scales", "word_idx": 4768, "sentence_idx": 59, "label": "unlabeled"}, {"type": "text", "expr": " Further, the real depth scans contain noticeable measurement noise and background objects, and the class distributions of 3D models from the repositories and those from real depth scans may be different", "word_idx": 4923, "sentence_idx": 60, "label": "unlabeled"}, {"type": "text", "expr": "To close the gap between the source domain of simulated depth scans and the target domain of real depth scans, we introduce a novel approach for unsupervised domain adaptation of 3D keypoint prediction", "word_idx": 5126, "sentence_idx": 61, "label": "unlabeled"}, {"type": "text", "expr": " Our approach is motivated by the special spatial properties of the 3D keypoints and the relationship between the keypoint distributions of the source and target domains", "word_idx": 5327, "sentence_idx": 62, "label": "unlabeled"}, {"type": "text", "expr": "To close the gap between the source domain of simulated depth scans and the target domain of real depth scans, we introduce a novel approach for unsupervised domain adaptation of 3D keypoint prediction", "word_idx": 5496, "sentence_idx": 63, "label": "unlabeled"}, {"type": "text", "expr": " Our approach is motivated by the special spatial properties of the 3D keypoints and the relationship between the keypoint distributions of the source and target domains", "word_idx": 5697, "sentence_idx": 64, "label": "unlabeled"}, {"type": "text", "expr": "First, keypoint predictions from different views of the same 3D model should be consistent with each other up to a pose transformation", "word_idx": 5866, "sentence_idx": 65, "label": "unlabeled"}, {"type": "text", "expr": " This allows us to formulate a  view-consistency  regularization to propagate a good prediction, e", "word_idx": 6000, "sentence_idx": 66, "label": "unlabeled"}, {"type": "text", "expr": " from a well-posed view where the prediction is more accurately adapted, to a challenging view with less accurate adaptation", "word_idx": 6098, "sentence_idx": 67, "label": "unlabeled"}, {"type": "text", "expr": " To this end, we introduce a latent keypoint configuration to fuse the keypoint predictions from different views of the same object", "word_idx": 6222, "sentence_idx": 68, "label": "unlabeled"}, {"type": "text", "expr": " Additionally, we introduce a pose-invariant metric to compare the keypoint predictions, which allows us to use depth scans without camera pose calibration for training", "word_idx": 6353, "sentence_idx": 69, "label": "unlabeled"}, {"type": "text", "expr": "view-consistency", "word_idx": 6521, "sentence_idx": 70, "label": "unlabeled"}, {"type": "text", "expr": "Second, despite the distinctive differences between the source and target domains, their 3D keypoint distributions are highly correlated", "word_idx": 6537, "sentence_idx": 71, "label": "unlabeled"}, {"type": "text", "expr": " However, naively aligning the 3D keypoint distributions between the two domains is sub-optimal since the occurrences of the same type of objects differ", "word_idx": 6673, "sentence_idx": 72, "label": "unlabeled"}, {"type": "text", "expr": " We propose a  geometric alignment  regularization that is insensitive to varying densities of the objects in order to align the keypoint distributions of the two domains", "word_idx": 6825, "sentence_idx": 73, "label": "unlabeled"}, {"type": "text", "expr": " We make use of the target domain\u2019s latent keypoint configurations from view consistency regularization to compute the geometric alignment with the source domain", "word_idx": 6995, "sentence_idx": 74, "label": "unlabeled"}, {"type": "text", "expr": " Note that since possible keypoint configurations lie on a manifold with much lower dimension over the ambient space, the geometric alignment can provide effective regularization", "word_idx": 7156, "sentence_idx": 75, "label": "unlabeled"}, {"type": "text", "expr": "geometric alignment", "word_idx": 7334, "sentence_idx": 76, "label": "unlabeled"}, {"type": "text", "expr": "Our final formulation combines a standard supervised loss on the source domain with the two unsupervised regularization losses on view-consistency and geometric alignment", "word_idx": 7353, "sentence_idx": 77, "label": "unlabeled"}, {"type": "text", "expr": " Our formulation can be easily optimized via alternating minimization and admits a simple strategy for variable initialization", "word_idx": 7523, "sentence_idx": 78, "label": "unlabeled"}, {"type": "text", "expr": "Our final formulation combines a standard supervised loss on the source domain with the two unsupervised regularization losses on view-consistency and geometric alignment", "word_idx": 7649, "sentence_idx": 79, "label": "unlabeled"}, {"type": "text", "expr": " Our formulation can be easily optimized via alternating minimization and admits a simple strategy for variable initialization", "word_idx": 7819, "sentence_idx": 80, "label": "unlabeled"}, {"type": "text", "expr": "We evaluate the proposed approach on unsupervised domain adaptation from ModelNet\u00a0  to rendered depth scans from the synthesized ShapeNet\u00a0  3D model dataset, and to real depth scans from the Redwood Object Scans\u00a0  and 3DCNN Depth Scans\u00a0  datasets", "word_idx": 7945, "sentence_idx": 81, "label": "unlabeled"}, {"type": "text", "expr": " Experimental results show that our approach can effectively reduce the domain gap between the online 3D model repositories and the real depth scans with background noise", "word_idx": 8191, "sentence_idx": 82, "label": "unlabeled"}, {"type": "text", "expr": " Our approach is significantly better than without domain adaptation and is superior to general-purpose domain adaptation techniques such as ADDA\u00a0 ", "word_idx": 8361, "sentence_idx": 83, "label": "unlabeled"}, {"type": "text", "expr": " We provide an ablation study to justify the design choice of each component of our approach, including the unsupervised loss terms and the optimization strategy", "word_idx": 8508, "sentence_idx": 84, "label": "unlabeled"}, {"type": "text", "expr": " Code is publicly available at  https://github", "word_idx": 8669, "sentence_idx": 85, "label": "unlabeled"}, {"type": "text", "expr": "com/xingyizhou/3DKeypoints-DA ", "word_idx": 8715, "sentence_idx": 86, "label": "unlabeled"}, {"type": "text", "expr": "https://github", "word_idx": 8745, "sentence_idx": 87, "label": "unlabeled"}, {"type": "text", "expr": "com/xingyizhou/3DKeypoints-DA", "word_idx": 8759, "sentence_idx": 88, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:  Approach overview", "word_idx": 8788, "sentence_idx": 89, "label": "unlabeled"}, {"type": "text", "expr": "  We train an end-to-end 3D keypoint prediction network  $G_{\\theta}$  from labeled simulated depth scans  $\\bar{\\mathcal{I}}$  of 3D models and unlabeled and unaligned real depth scans  $\\{\\mathcal{I}_{i}\\}$  of real world objects", "word_idx": 8816, "sentence_idx": 90, "label": "unlabeled"}, {"type": "text", "expr": " We define three loss terms for training: the supervised labeled term  $f_{\\textup{labeled}}$  enforces prediction accuracy in the source domain where labels are available; the unsupervised view-consistency term  $f_{\\textup{view}}$  ensures keypoint predictions from different views are consistent in the target domain; the geometric alignment term  $f_{\\textup{align}}$  aligns the distribution of the predicted keyoints in the target domain with the source domain", "word_idx": 9047, "sentence_idx": 91, "label": "unlabeled"}, {"type": "text", "expr": " See Section\u00a0 LABEL:Section:Approach  for details", "word_idx": 9513, "sentence_idx": 92, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:", "word_idx": 9562, "sentence_idx": 93, "label": "unlabeled"}, {"type": "text", "expr": "Approach overview", "word_idx": 9571, "sentence_idx": 94, "label": "unlabeled"}, {"type": "math", "expr": "$$G_{\\theta}$$", "word_idx": 9588, "sentence_idx": 95, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{\\mathcal{I}}$$", "word_idx": 9598, "sentence_idx": 96, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{\\mathcal{I}_{i}\\}$$", "word_idx": 9615, "sentence_idx": 97, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{\\textup{labeled}}$$", "word_idx": 9634, "sentence_idx": 98, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{\\textup{view}}$$", "word_idx": 9654, "sentence_idx": 99, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{\\textup{align}}$$", "word_idx": 9671, "sentence_idx": 100, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Section:Approach", "word_idx": 9689, "sentence_idx": 101, "label": "unlabeled"}, {"type": "text", "expr": "label Figure:Approach:Overview", "word_idx": 9711, "sentence_idx": 102, "label": "unlabeled"}, {"type": "text", "expr": "label", "word_idx": 9741, "sentence_idx": 103, "label": "unlabeled"}, {"type": "text", "expr": "2  Related Works", "word_idx": 9746, "sentence_idx": 104, "label": "unlabeled"}, {"type": "text", "expr": "label Section:Related:Works", "word_idx": 9762, "sentence_idx": 105, "label": "unlabeled"}, {"type": "text", "expr": "label", "word_idx": 9789, "sentence_idx": 106, "label": "unlabeled"}, {"type": "text", "expr": "Keypoint Detection", "word_idx": 9794, "sentence_idx": 107, "label": "unlabeled"}, {"type": "text", "expr": "  Keypoint detection from a single RGB or RGB-D image is a fundamental task in computer vision", "word_idx": 9812, "sentence_idx": 108, "label": "unlabeled"}, {"type": "text", "expr": " We refer to\u00a0  for some recent advances on this topic", "word_idx": 9906, "sentence_idx": 109, "label": "unlabeled"}, {"type": "text", "expr": " While most techniques focus on developing novel neural network architectures for this task, fewer works focus on addressing the issue of domain shifts between the training data and testing data, e", "word_idx": 9959, "sentence_idx": 110, "label": "unlabeled"}, {"type": "text", "expr": ", the setting described in this paper", "word_idx": 10156, "sentence_idx": 111, "label": "unlabeled"}, {"type": "text", "expr": " In \u00a0 , the authors introduce a domain adaptation technique for 3D human pose estimation in the wild", "word_idx": 10193, "sentence_idx": 112, "label": "unlabeled"}, {"type": "text", "expr": " Additionally for human pose estimation,   proposes to align the source and target label distributions using a GAN loss", "word_idx": 10293, "sentence_idx": 113, "label": "unlabeled"}, {"type": "text", "expr": " We opt to use an alternate metric that offers more flexibility in addressing domain shifts", "word_idx": 10412, "sentence_idx": 114, "label": "unlabeled"}, {"type": "text", "expr": " Similarly to our method,   also leverages the consistency across multiple views to boost the supervision on the target domain", "word_idx": 10503, "sentence_idx": 115, "label": "unlabeled"}, {"type": "text", "expr": " However, the output of this approach is computed directly from the initial predictions from the source domain", "word_idx": 10629, "sentence_idx": 116, "label": "unlabeled"}, {"type": "text", "expr": " In contrast, our approach only uses the initial predictions to initialize final predictions", "word_idx": 10739, "sentence_idx": 117, "label": "unlabeled"}, {"type": "text", "expr": " Moreover, we utilize a latent configuration for synchronizing the predictions from multiple views, which avoids performing pair-wise analysis", "word_idx": 10831, "sentence_idx": 118, "label": "unlabeled"}, {"type": "text", "expr": "Keypoint Detection", "word_idx": 10973, "sentence_idx": 119, "label": "unlabeled"}, {"type": "text", "expr": "Multi-view supervision", "word_idx": 10991, "sentence_idx": 120, "label": "unlabeled"}, {"type": "text", "expr": "  RGB and RGB-D video sequences essentially consist of different views of the same underlying 3D environment", "word_idx": 11013, "sentence_idx": 121, "label": "unlabeled"}, {"type": "text", "expr": " In the literature, people have utilized such weak supervision for various tasks such as 3D reconstruction, novel view synthesis and 2D keypoint prediction, e", "word_idx": 11121, "sentence_idx": 122, "label": "unlabeled"}, {"type": "text", "expr": " Our work differs from most works in the sense that we do not assume that relative poses between cameras are known", "word_idx": 11279, "sentence_idx": 123, "label": "unlabeled"}, {"type": "text", "expr": " Instead, we introduce a pose invariant metric to compare keypoint configurations", "word_idx": 11393, "sentence_idx": 124, "label": "unlabeled"}, {"type": "text", "expr": "Multi-view supervision", "word_idx": 11474, "sentence_idx": 125, "label": "unlabeled"}, {"type": "text", "expr": "Supervision from Big 3D Data", "word_idx": 11496, "sentence_idx": 126, "label": "unlabeled"}, {"type": "text", "expr": "  Thanks the availability of annotated big 3D data such as ModelNet\u00a0  and ShapeNet\u00a0 , people have leveraged synthetic data generated from 3D models for various tasks, including image classification\u00a0 , object recognition\u00a0 , semantic segmentation\u00a0 , object reconstruction\u00a0 , pose estimation\u00a0  and novel-view synthesis\u00a0 ", "word_idx": 11524, "sentence_idx": 127, "label": "unlabeled"}, {"type": "text", "expr": " The fundamental challenge of these approaches is that there are domain shifts between synthetic data and real RGB or RGB-D images", "word_idx": 11841, "sentence_idx": 128, "label": "unlabeled"}, {"type": "text", "expr": " Most existing works focus on improving the simulation process to close this gap", "word_idx": 11971, "sentence_idx": 129, "label": "unlabeled"}, {"type": "text", "expr": " In contrast, we focus on developing an unsupervised loss for domain adaptation", "word_idx": 12051, "sentence_idx": 130, "label": "unlabeled"}, {"type": "text", "expr": "Supervision from Big 3D Data", "word_idx": 12130, "sentence_idx": 131, "label": "unlabeled"}, {"type": "text", "expr": "Domain adaptation", "word_idx": 12158, "sentence_idx": 132, "label": "unlabeled"}, {"type": "text", "expr": "  Domain adaptation for various visual recognition tasks is an active research area in computer vision, and our problem falls into the general category of domain adaptation", "word_idx": 12175, "sentence_idx": 133, "label": "unlabeled"}, {"type": "text", "expr": " It is beyond the scope of this paper to provide a comprehensive review of the literature, however we refer to \u00a0  for recent advances and to \u00a0  for a recent survey on this topic", "word_idx": 12347, "sentence_idx": 134, "label": "unlabeled"}, {"type": "text", "expr": " A common strategy for unsupervised domain adaptation is to align the output distributions between source and target domains, e", "word_idx": 12524, "sentence_idx": 135, "label": "unlabeled"}, {"type": "text", "expr": ", either through explicit domain-wise maps or through use of a GAN", "word_idx": 12651, "sentence_idx": 136, "label": "unlabeled"}, {"type": "text", "expr": " In contrast, our regularizations are tailored for the particular problem we consider, i", "word_idx": 12717, "sentence_idx": 137, "label": "unlabeled"}, {"type": "text", "expr": ", view-consistency and domain shifts caused by varying densities", "word_idx": 12805, "sentence_idx": 138, "label": "unlabeled"}, {"type": "text", "expr": "Domain adaptation", "word_idx": 12869, "sentence_idx": 139, "label": "unlabeled"}, {"type": "text", "expr": "3  Problem Statement", "word_idx": 12886, "sentence_idx": 140, "label": "unlabeled"}, {"type": "text", "expr": "label Section:Problem:Statement", "word_idx": 12906, "sentence_idx": 141, "label": "unlabeled"}, {"type": "text", "expr": "label", "word_idx": 12937, "sentence_idx": 142, "label": "unlabeled"}, {"type": "text", "expr": "We study the problem of predicting complete 3D keypoints of an underlying object from a single image or depth scan", "word_idx": 12942, "sentence_idx": 143, "label": "unlabeled"}, {"type": "text", "expr": "\nWe assume the input consists of a labeled dataset  $\\overline{\\mathcal{I}}$  and an unlabeled dataset  $\\mathcal{I}$ ", "word_idx": 13056, "sentence_idx": 144, "label": "unlabeled"}, {"type": "text", "expr": " Moreover, the unlabeled dataset is comprised of  $N$  subsets  $\\mathcal{I}_{i},1\\leq i\\leq N$ , where each subset collects depth scans/images of the same object from different views", "word_idx": 13174, "sentence_idx": 145, "label": "unlabeled"}, {"type": "text", "expr": " Such data naturally arises from RGB-D or RGB video sequences", "word_idx": 13357, "sentence_idx": 146, "label": "unlabeled"}, {"type": "math", "expr": "$$\\overline{\\mathcal{I}}$$", "word_idx": 13418, "sentence_idx": 147, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{I}$$", "word_idx": 13440, "sentence_idx": 148, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{I}_{i},1\\leq i\\leq N$$", "word_idx": 13451, "sentence_idx": 149, "label": "unlabeled"}, {"type": "text", "expr": "Each instance  $I\\in\\overline{\\mathcal{I}}$  in the labeled dataset possesses a ground-truth label  $Y(I)\\in\\mathbb{R}^{3\\times d}$ , which is a matrix that collects the coordinates of the ordered keypoints in its columns", "word_idx": 13480, "sentence_idx": 150, "label": "unlabeled"}, {"type": "text", "expr": " Without losing generality, we assume that the 3D local coordinate system of  $I$  is chosen so that the centroid of  $Y(I)$  is at the origin, i", "word_idx": 13701, "sentence_idx": 151, "label": "unlabeled"}, {"type": "math", "expr": "$$I\\in\\overline{\\mathcal{I}}$$", "word_idx": 13846, "sentence_idx": 152, "label": "unlabeled"}, {"type": "math", "expr": "$$Y(I)\\in\\mathbb{R}^{3\\times d}$$", "word_idx": 13872, "sentence_idx": 153, "label": "unlabeled"}, {"type": "math", "expr": "$$Y(I)$$", "word_idx": 13901, "sentence_idx": 154, "label": "unlabeled"}, {"type": "text", "expr": "$Y(I)\\boldsymbol{1}=0\\mathit{label}{Eq:Normalization}$", "word_idx": 13905, "sentence_idx": 155, "label": "unlabeled"}, {"type": "math", "expr": "$$Y(I)\\boldsymbol{1}=0.\\mathit{label}{Eq:Normalization}$$", "word_idx": 13959, "sentence_idx": 156, "label": "unlabeled"}, {"type": "text", "expr": "It is expected that the source domain of the labeled dataset and the target domain of the unlabeled dataset are different (e", "word_idx": 14012, "sentence_idx": 157, "label": "unlabeled"}, {"type": "text", "expr": ", the source domain consists of synthetic images/scans and the target domain consists of real images/scans)", "word_idx": 14136, "sentence_idx": 158, "label": "unlabeled"}, {"type": "text", "expr": " Our goal is to train a neural network  $G_{\\theta}:\\mathbb{R}^{m\\times n}\\rightarrow\\mathbb{R}^{3\\times d}$  that takes an image from the target domain as input and outputs the predicted keypoints by leveraging both the labeled dataset  $\\overline{\\mathcal{I}}$  and unlabeled subsets  $\\mathcal{I}_{i},1\\leq i\\leq N$ ", "word_idx": 14243, "sentence_idx": 159, "label": "unlabeled"}, {"type": "text", "expr": " We call this problem unsupervised domain adaptation for 3D keypoint prediction", "word_idx": 14562, "sentence_idx": 160, "label": "unlabeled"}, {"type": "math", "expr": "$$G_{\\theta}:\\mathbb{R}^{m\\times n}\\rightarrow\\mathbb{R}^{3\\times d}$$", "word_idx": 14641, "sentence_idx": 161, "label": "unlabeled"}, {"type": "math", "expr": "$$\\overline{\\mathcal{I}}$$", "word_idx": 14707, "sentence_idx": 162, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{I}_{i},1\\leq i\\leq N$$", "word_idx": 14729, "sentence_idx": 163, "label": "unlabeled"}, {"type": "text", "expr": "Note that we do not assume the underlying cameras of each unlabeled subset are calibrated, or in other words, the relative transformations between different views of the same object are not required", "word_idx": 14758, "sentence_idx": 164, "label": "unlabeled"}, {"type": "text", "expr": " Although it is possible to align the depth scans to obtain relative transformations, we found that such alignments are not always reliable in the presence of scanning discontinuities where little overlaps between consecutive scans are available", "word_idx": 14956, "sentence_idx": 165, "label": "unlabeled"}, {"type": "text", "expr": " In contrast, our formulation treats relative camera poses as latent variables, which are optimized together with the network parameters", "word_idx": 15201, "sentence_idx": 166, "label": "unlabeled"}, {"type": "text", "expr": "Note that we do not assume the underlying cameras of each unlabeled subset are calibrated, or in other words, the relative transformations between different views of the same object are not required", "word_idx": 15337, "sentence_idx": 167, "label": "unlabeled"}, {"type": "text", "expr": " Although it is possible to align the depth scans to obtain relative transformations, we found that such alignments are not always reliable in the presence of scanning discontinuities where little overlaps between consecutive scans are available", "word_idx": 15535, "sentence_idx": 168, "label": "unlabeled"}, {"type": "text", "expr": " In contrast, our formulation treats relative camera poses as latent variables, which are optimized together with the network parameters", "word_idx": 15780, "sentence_idx": 169, "label": "unlabeled"}, {"type": "text", "expr": "4  Approach", "word_idx": 15916, "sentence_idx": 170, "label": "unlabeled"}, {"type": "text", "expr": "label Section:Approach", "word_idx": 15927, "sentence_idx": 171, "label": "unlabeled"}, {"type": "text", "expr": "label", "word_idx": 15949, "sentence_idx": 172, "label": "unlabeled"}, {"type": "text", "expr": "In this section, we describe our detailed approach to unsupervised domain adaptation for 3D keypoint prediction", "word_idx": 15954, "sentence_idx": 173, "label": "unlabeled"}, {"type": "text", "expr": " We first introduce a pose-invariant distance metric to compare keypoint configurations in Section\u00a0 LABEL:Subsection:Pose:Invariant:Distance:Metric ", "word_idx": 16065, "sentence_idx": 174, "label": "unlabeled"}, {"type": "text", "expr": " This allows us to compare the predictions in different views without knowing the relative transformations between the views for uncalibrated datasets", "word_idx": 16213, "sentence_idx": 175, "label": "unlabeled"}, {"type": "text", "expr": " We then present the formulation of our approach in Section\u00a0 LABEL:Subsection:Formulation ", "word_idx": 16363, "sentence_idx": 176, "label": "unlabeled"}, {"type": "text", "expr": " Finally, we discuss our optimization strategy in Section\u00a0 LABEL:Subsection:Optimization ", "word_idx": 16453, "sentence_idx": 177, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Subsection:Pose:Invariant:Distance:Metric", "word_idx": 16542, "sentence_idx": 178, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Subsection:Formulation", "word_idx": 16589, "sentence_idx": 179, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Subsection:Optimization", "word_idx": 16617, "sentence_idx": 180, "label": "unlabeled"}, {"type": "text", "expr": "1  Pose-Invariant Distance Metric", "word_idx": 16646, "sentence_idx": 181, "label": "unlabeled"}, {"type": "text", "expr": "label Subsection:Pose:Invariant:Distance:Metric", "word_idx": 16679, "sentence_idx": 182, "label": "unlabeled"}, {"type": "text", "expr": "label", "word_idx": 16726, "sentence_idx": 183, "label": "unlabeled"}, {"type": "text", "expr": "The pose-invariant distance metric compares two keypoint configurations  $X,Y\\in\\mathbb{R}^{3\\times d}$  described in different coordinate systems", "word_idx": 16731, "sentence_idx": 184, "label": "unlabeled"}, {"type": "text", "expr": " Since the mean of each keypoint configuration is zero, we introduce a latent rotation  $R$  to account for the underlying relative transformation:", "word_idx": 16877, "sentence_idx": 185, "label": "unlabeled"}, {"type": "math", "expr": "$$X,Y\\in\\mathbb{R}^{3\\times d}$$", "word_idx": 17024, "sentence_idx": 186, "label": "unlabeled"}, {"type": "text", "expr": "$r(X,Y)=\\min\\limits_{R\\in SO(3)}\\|RX-Y\\|_{\\mathcal{F}}^{2},\\mathit{label}{Eq:%\npose-inv-mrtric}$", "word_idx": 17052, "sentence_idx": 187, "label": "unlabeled"}, {"type": "math", "expr": "$$r(X,Y)=\\min\\limits_{R\\in SO(3)}\\|RX-Y\\|_{\\mathcal{F}}^{2},\\mathit{label}{Eq:%\npose-inv-mrtric}$$", "word_idx": 17148, "sentence_idx": 188, "label": "unlabeled"}, {"type": "text", "expr": "where  $\\|\\cdot\\|_{\\mathcal{F}}$  denotes the matrix Frobenius Norm", "word_idx": 17242, "sentence_idx": 189, "label": "unlabeled"}, {"type": "text", "expr": " It is clear that  $r(X,Y)$  is independent of the coordinate systems associated with  $X$  and  $Y$ , making it particularly suitable for comparing predictions from uncalibrated views and aligning the source domain and the target domain", "word_idx": 17309, "sentence_idx": 190, "label": "unlabeled"}, {"type": "math", "expr": "$$\\|\\cdot\\|_{\\mathcal{F}}$$", "word_idx": 17546, "sentence_idx": 191, "label": "unlabeled"}, {"type": "math", "expr": "$$r(X,Y)$$", "word_idx": 17569, "sentence_idx": 192, "label": "unlabeled"}, {"type": "text", "expr": "In the following, we discuss a few key properties of  $r(X,Y)$  that will be used extensively in our approach", "word_idx": 17575, "sentence_idx": 193, "label": "unlabeled"}, {"type": "text", "expr": " First of all, both  $r(X,Y)$  and the gradient of  $r(X,Y)$  with respect to each of its argument admit closed-form expressions", "word_idx": 17684, "sentence_idx": 194, "label": "unlabeled"}, {"type": "text", "expr": " These are summarized in the following two propositions", "word_idx": 17812, "sentence_idx": 195, "label": "unlabeled"}, {"type": "math", "expr": "$$r(X,Y)$$", "word_idx": 17867, "sentence_idx": 196, "label": "unlabeled"}, {"type": "math", "expr": "$$r(X,Y)$$", "word_idx": 17873, "sentence_idx": 197, "label": "unlabeled"}, {"type": "math", "expr": "$$r(X,Y)$$", "word_idx": 17879, "sentence_idx": 198, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 1 ", "word_idx": 17885, "sentence_idx": 199, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 1", "word_idx": 17899, "sentence_idx": 200, "label": "unlabeled"}, {"type": "text", "expr": "labelProp:1", "word_idx": 17912, "sentence_idx": 201, "label": "unlabeled"}, {"type": "text", "expr": "$r(X,Y)$  admits the following analytic expression:", "word_idx": 17923, "sentence_idx": 202, "label": "unlabeled"}, {"type": "math", "expr": "$$r(X,Y)$$", "word_idx": 17974, "sentence_idx": 203, "label": "unlabeled"}, {"type": "text", "expr": "admits the following analytic expression:", "word_idx": 17980, "sentence_idx": 204, "label": "unlabeled"}, {"type": "text", "expr": "$r(X,Y)=\\|X\\|_{\\mathcal{F}}^{2}+\\|Y\\|_{\\mathcal{F}}^{2}-2\\cdot\\textup{trace}%\n\\big{(}R\\cdot(XY^{T})\\big{)}$", "word_idx": 18021, "sentence_idx": 205, "label": "unlabeled"}, {"type": "math", "expr": "$$r(X,Y)=\\|X\\|_{\\mathcal{F}}^{2}+\\|Y\\|_{\\mathcal{F}}^{2}-2\\cdot\\textup{trace}%\n\\big{(}R\\cdot(XY^{T})\\big{)}$$", "word_idx": 18128, "sentence_idx": 206, "label": "unlabeled"}, {"type": "text", "expr": "where  $R$  is derived from the singular value decomposition (or SVD) of  $YX^{T}=U\\Sigma V^{T}$ :", "word_idx": 18233, "sentence_idx": 207, "label": "unlabeled"}, {"type": "math", "expr": "$$YX^{T}=U\\Sigma V^{T}$$", "word_idx": 18331, "sentence_idx": 208, "label": "unlabeled"}, {"type": "text", "expr": "$R=U\\mathrm{diag}(1,1,s)V^{T},\\quad s=\\textup{sign}(\\mathrm{det}(XY^{T}))%\n\\mathit{label}{Eq:R:OPT}$", "word_idx": 18351, "sentence_idx": 209, "label": "unlabeled"}, {"type": "math", "expr": "$$R=U\\mathrm{diag}(1,1,s)V^{T},\\quad s=\\textup{sign}(\\mathrm{det}(XY^{T})).%\n\\mathit{label}{Eq:R:OPT}$$", "word_idx": 18451, "sentence_idx": 210, "label": "unlabeled"}, {"type": "text", "expr": "labelProp:1", "word_idx": 18550, "sentence_idx": 211, "label": "unlabeled"}, {"type": "text", "expr": "labelProp:1", "word_idx": 18561, "sentence_idx": 212, "label": "unlabeled"}, {"type": "text", "expr": "Proof:  See\u00a0 ", "word_idx": 18572, "sentence_idx": 213, "label": "unlabeled"}, {"type": "text", "expr": "Proof:", "word_idx": 18585, "sentence_idx": 214, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 2 ", "word_idx": 18591, "sentence_idx": 215, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 2", "word_idx": 18605, "sentence_idx": 216, "label": "unlabeled"}, {"type": "text", "expr": "The gradient of  $r(X,Y)$  with respect to  $X$  is given by", "word_idx": 18618, "sentence_idx": 217, "label": "unlabeled"}, {"type": "math", "expr": "$$r(X,Y)$$", "word_idx": 18678, "sentence_idx": 218, "label": "unlabeled"}, {"type": "text", "expr": "$\\frac{\\partial r}{\\partial X}(X,Y)=2(X-R^{T}Y),$", "word_idx": 18684, "sentence_idx": 219, "label": "unlabeled"}, {"type": "math", "expr": "$$\\frac{\\partial r}{\\partial X}(X,Y)=2(X-R^{T}Y),$$", "word_idx": 18733, "sentence_idx": 220, "label": "unlabeled"}, {"type": "text", "expr": "where  $R$  is given by Eq", "word_idx": 18780, "sentence_idx": 221, "label": "unlabeled"}, {"type": "text", "expr": " ( LABEL:Eq:R:OPT )", "word_idx": 18806, "sentence_idx": 222, "label": "unlabeled"}, {"type": "text", "expr": "\nlabelProp:2", "word_idx": 18825, "sentence_idx": 223, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Eq:R:OPT", "word_idx": 18837, "sentence_idx": 224, "label": "unlabeled"}, {"type": "text", "expr": "Proof:  Please refer to the supplemental material", "word_idx": 18851, "sentence_idx": 225, "label": "unlabeled"}, {"type": "text", "expr": "Proof:", "word_idx": 18900, "sentence_idx": 226, "label": "unlabeled"}, {"type": "text", "expr": "Our optimization procedure also frequently involves the following optimization problem that computes the weighted average  $X^{\\star}$  of a set of keypoint configurations  $Y_{i},1\\leq i\\leq n$  in the quotient space  $\\mathbb{R}^{3\\times d}/SO(3)$ :", "word_idx": 18906, "sentence_idx": 227, "label": "unlabeled"}, {"type": "math", "expr": "$$X^{\\star}$$", "word_idx": 19157, "sentence_idx": 228, "label": "unlabeled"}, {"type": "math", "expr": "$$Y_{i},1\\leq i\\leq n$$", "word_idx": 19166, "sentence_idx": 229, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbb{R}^{3\\times d}/SO(3)$$", "word_idx": 19185, "sentence_idx": 230, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle X^{\\star}$", "word_idx": 19213, "sentence_idx": 231, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle X^{\\star}$$", "word_idx": 19238, "sentence_idx": 232, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\underset{X\\in\\mathbb{R}^{3\\times d}}{\\textup{argmin}}\\ \\sum%\n\\limits_{i=1}^{n}c_{i}r(X,Y_{i})$", "word_idx": 19261, "sentence_idx": 233, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\underset{X\\in\\mathbb{R}^{3\\times d}}{\\textup{argmin}}\\ \\sum%\n\\limits_{i=1}^{n}c_{i}r(X,Y_{i})$$", "word_idx": 19371, "sentence_idx": 234, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\underset{X\\in\\mathbb{R}^{3\\times d}}{\\textup{argmin}}\\sum%\n\\limits_{i=1}^{n}c_{i}\\min\\limits_{R_{i}\\in SO(3)}\\|X-R_{i}^{T}Y_{i}\\|_{%\n\\mathcal{F}}^{2},\\mathit{label}{Eq:Quotient:Mean}$", "word_idx": 19479, "sentence_idx": 235, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\underset{X\\in\\mathbb{R}^{3\\times d}}{\\textup{argmin}}\\sum%\n\\limits_{i=1}^{n}c_{i}\\min\\limits_{R_{i}\\in SO(3)}\\|X-R_{i}^{T}Y_{i}\\|_{%\n\\mathcal{F}}^{2},\\mathit{label}{Eq:Quotient:Mean}$$", "word_idx": 19678, "sentence_idx": 236, "label": "unlabeled"}, {"type": "text", "expr": "where  $c_{i},1\\leq i\\leq n$  are constants", "word_idx": 19875, "sentence_idx": 237, "label": "unlabeled"}, {"type": "text", "expr": " Although Eq", "word_idx": 19918, "sentence_idx": 238, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0( LABEL:Eq:Quotient:Mean ) does not admit a closed-form solution, it can be easily optimized via alternating minimization", "word_idx": 19930, "sentence_idx": 239, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, when  $X$  is fixed, each  $R_{i}$  can be computed independently using Proposition\u00a0 LABEL:Prop:1 ", "word_idx": 20052, "sentence_idx": 240, "label": "unlabeled"}, {"type": "text", "expr": " When the  $R_{i}$  latent variables are fixed,  $X$  is simply given by the mean of  $R_{i}^{T}Y_{i}$ , i", "word_idx": 20165, "sentence_idx": 241, "label": "unlabeled"}, {"type": "text", "expr": ",  $X=\\frac{1}{\\sum c_{i}}\\sum\\limits_{i=1}^{n}c_{i}R_{i}^{T}Y_{i}$ ", "word_idx": 20271, "sentence_idx": 242, "label": "unlabeled"}, {"type": "text", "expr": " To make the solution unique, we always set  $R_{1}=I_{3}$ ", "word_idx": 20339, "sentence_idx": 243, "label": "unlabeled"}, {"type": "text", "expr": " As shown in\u00a0 , such alternating minimization scheme admits a geometric convergence rate", "word_idx": 20398, "sentence_idx": 244, "label": "unlabeled"}, {"type": "text", "expr": " Denoting  $X^{k}$  as the solution at iteration  $k$ , we stop the alternating minimization when  $\\|X^{k+1}-X^{k}\\|_{\\mathcal{F}}\\leq 10^{-3}$ ", "word_idx": 20486, "sentence_idx": 245, "label": "unlabeled"}, {"type": "text", "expr": " In our implementation, we found  $8-10$  iterations sufficient for convergence", "word_idx": 20631, "sentence_idx": 246, "label": "unlabeled"}, {"type": "math", "expr": "$$c_{i},1\\leq i\\leq n$$", "word_idx": 20710, "sentence_idx": 247, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Eq:Quotient:Mean", "word_idx": 20729, "sentence_idx": 248, "label": "unlabeled"}, {"type": "math", "expr": "$$R_{i}$$", "word_idx": 20751, "sentence_idx": 249, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Prop:1", "word_idx": 20756, "sentence_idx": 250, "label": "unlabeled"}, {"type": "math", "expr": "$$R_{i}$$", "word_idx": 20768, "sentence_idx": 251, "label": "unlabeled"}, {"type": "math", "expr": "$$R_{i}^{T}Y_{i}$$", "word_idx": 20773, "sentence_idx": 252, "label": "unlabeled"}, {"type": "math", "expr": "$$X=\\frac{1}{\\sum c_{i}}\\sum\\limits_{i=1}^{n}c_{i}R_{i}^{T}Y_{i}$$", "word_idx": 20787, "sentence_idx": 253, "label": "unlabeled"}, {"type": "math", "expr": "$$R_{1}=I_{3}$$", "word_idx": 20849, "sentence_idx": 254, "label": "unlabeled"}, {"type": "math", "expr": "$$X^{k}$$", "word_idx": 20860, "sentence_idx": 255, "label": "unlabeled"}, {"type": "math", "expr": "$$\\|X^{k+1}-X^{k}\\|_{\\mathcal{F}}\\leq 10^{-3}$$", "word_idx": 20865, "sentence_idx": 256, "label": "unlabeled"}, {"type": "math", "expr": "$$8-10$$", "word_idx": 20908, "sentence_idx": 257, "label": "unlabeled"}, {"type": "text", "expr": "2  Formulation", "word_idx": 20912, "sentence_idx": 258, "label": "unlabeled"}, {"type": "text", "expr": "label Subsection:Formulation", "word_idx": 20926, "sentence_idx": 259, "label": "unlabeled"}, {"type": "text", "expr": "label", "word_idx": 20954, "sentence_idx": 260, "label": "unlabeled"}, {"type": "text", "expr": "(a) (b) (c)", "word_idx": 20959, "sentence_idx": 261, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:  Latent Distribution and View Selection", "word_idx": 20970, "sentence_idx": 262, "label": "unlabeled"}, {"type": "text", "expr": "  This figure provides visualizations of label distributions and view selection for initializing the latent configurations from ModelNet (source domain) to Redwood (target domain) on the Chair category", "word_idx": 21019, "sentence_idx": 263, "label": "unlabeled"}, {"type": "text", "expr": " All visualizations are done by 2D projections using the first two principal components", "word_idx": 21220, "sentence_idx": 264, "label": "unlabeled"}, {"type": "text", "expr": " (a) Label distributions of the source and target domains", "word_idx": 21307, "sentence_idx": 265, "label": "unlabeled"}, {"type": "text", "expr": " We can see there is a clear shift label distribution, but each instance in one domain is surrounded by instances from the other domain", "word_idx": 21364, "sentence_idx": 266, "label": "unlabeled"}, {"type": "text", "expr": " (b) Visualizations of all predictions from different views", "word_idx": 21499, "sentence_idx": 267, "label": "unlabeled"}, {"type": "text", "expr": " (c) Visualizations of the best prediction from each object", "word_idx": 21558, "sentence_idx": 268, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:", "word_idx": 21617, "sentence_idx": 269, "label": "unlabeled"}, {"type": "text", "expr": "Latent Distribution and View Selection", "word_idx": 21626, "sentence_idx": 270, "label": "unlabeled"}, {"type": "text", "expr": "label Figure:Latent:Variable:Initialization", "word_idx": 21664, "sentence_idx": 271, "label": "unlabeled"}, {"type": "text", "expr": "label", "word_idx": 21707, "sentence_idx": 272, "label": "unlabeled"}, {"type": "text", "expr": "To train the keypoint prediction network  $G_{\\theta}(\\cdot)$ , we introduce three loss terms, namely, a labeled term  $f_{\\textup{labeled}}$ , a view-consistency term  $f_{\\textup{view}}$  and a geometric alignment term  $f_{\\textup{align}}$ ", "word_idx": 21712, "sentence_idx": 273, "label": "unlabeled"}, {"type": "math", "expr": "$$G_{\\theta}(\\cdot)$$", "word_idx": 21955, "sentence_idx": 274, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{\\textup{labeled}}$$", "word_idx": 21972, "sentence_idx": 275, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{\\textup{view}}$$", "word_idx": 21992, "sentence_idx": 276, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{\\textup{align}}$$", "word_idx": 22009, "sentence_idx": 277, "label": "unlabeled"}, {"type": "text", "expr": "The labeled term  $f_{\\textup{labeled}}$  fits predictions on the source domain labeled dataset  $\\overline{\\mathcal{I}}$  to the prescribed ground-truth labels", "word_idx": 22027, "sentence_idx": 278, "label": "unlabeled"}, {"type": "text", "expr": " We use the regression loss under the L2-norm, which works well for 3D keypoint prediction tasks (c", "word_idx": 22187, "sentence_idx": 279, "label": "unlabeled"}, {"type": "text", "expr": " \u00a0 ):", "word_idx": 22286, "sentence_idx": 280, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{\\textup{labeled}}$$", "word_idx": 22291, "sentence_idx": 281, "label": "unlabeled"}, {"type": "math", "expr": "$$\\overline{\\mathcal{I}}$$", "word_idx": 22311, "sentence_idx": 282, "label": "unlabeled"}, {"type": "text", "expr": "$f_{\\textup{labeled}}=\\frac{1}{|\\overline{\\mathcal{I}}|}\\sum\\limits_{I\\in%\n\\overline{\\mathcal{I}}}\\|G_{\\theta}(I)-Y(I)\\|_{\\mathcal{F}}^{2}\\mathit{label}%\n{Eq:Labeled:Term}$", "word_idx": 22333, "sentence_idx": 283, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{\\textup{labeled}}=\\frac{1}{|\\overline{\\mathcal{I}}|}\\sum\\limits_{I\\in%\n\\overline{\\mathcal{I}}}\\|G_{\\theta}(I)-Y(I)\\|_{\\mathcal{F}}^{2}.\\mathit{label}%\n{Eq:Labeled:Term}$$", "word_idx": 22505, "sentence_idx": 284, "label": "unlabeled"}, {"type": "text", "expr": "The view-consistency term  $f_{\\textup{view}}$  is defined on the target domain to enforce consistency between the predictions from different views of the same object", "word_idx": 22676, "sentence_idx": 285, "label": "unlabeled"}, {"type": "text", "expr": " In other words, there exist pairwise rotations that transform the predictions from one view to another", "word_idx": 22842, "sentence_idx": 286, "label": "unlabeled"}, {"type": "text", "expr": " A straightforward approach is to minimize  $r(G_{\\theta}(I_{ij}),G_{\\theta}(I_{ij^{\\prime}}))$ , where  $I_{ij}$  and  $I_{ij^{\\prime}}$  are different views of the same object", "word_idx": 22945, "sentence_idx": 287, "label": "unlabeled"}, {"type": "text", "expr": " However, we found that such approach introduces a quadratic number of terms as the number of views increases and quickly becomes intractable", "word_idx": 23122, "sentence_idx": 288, "label": "unlabeled"}, {"type": "text", "expr": " Therefore, we introduce a latent configuration  $M_{i}\\in\\mathbb{R}^{3\\times d}$  for each unlabeled subset  $\\mathcal{I}_{i}$  that characterizes the underlying ground-truth in the canonical frame", "word_idx": 23263, "sentence_idx": 289, "label": "unlabeled"}, {"type": "text", "expr": " We then define the view consistency term as:", "word_idx": 23461, "sentence_idx": 290, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{\\textup{view}}$$", "word_idx": 23506, "sentence_idx": 291, "label": "unlabeled"}, {"type": "math", "expr": "$$r(G_{\\theta}(I_{ij}),G_{\\theta}(I_{ij^{\\prime}}))$$", "word_idx": 23523, "sentence_idx": 292, "label": "unlabeled"}, {"type": "math", "expr": "$$I_{ij}$$", "word_idx": 23572, "sentence_idx": 293, "label": "unlabeled"}, {"type": "math", "expr": "$$I_{ij^{\\prime}}$$", "word_idx": 23578, "sentence_idx": 294, "label": "unlabeled"}, {"type": "math", "expr": "$$M_{i}\\in\\mathbb{R}^{3\\times d}$$", "word_idx": 23593, "sentence_idx": 295, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{I}_{i}$$", "word_idx": 23623, "sentence_idx": 296, "label": "unlabeled"}, {"type": "text", "expr": "$f_{\\textup{view}}=\\frac{1}{N}\\sum\\limits_{i=1}^{N}\\frac{1}{|\\mathcal{I}_{i}|}%\n\\sum\\limits_{I_{ij}\\in\\mathcal{I}_{i}}\\ r(G_{\\theta}(I_{ij}),M_{i})\\mathit{%\nlabel}{Eq:Unlabeled:Term}$", "word_idx": 23638, "sentence_idx": 297, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{\\textup{view}}=\\frac{1}{N}\\sum\\limits_{i=1}^{N}\\frac{1}{|\\mathcal{I}_{i}|}%\n\\sum\\limits_{I_{ij}\\in\\mathcal{I}_{i}}\\ r(G_{\\theta}(I_{ij}),M_{i}).\\mathit{%\nlabel}{Eq:Unlabeled:Term}$$", "word_idx": 23821, "sentence_idx": 298, "label": "unlabeled"}, {"type": "text", "expr": "It is clear that minimizing  $f_{\\textup{view}}$  automatically aligns the predictions across different views", "word_idx": 24003, "sentence_idx": 299, "label": "unlabeled"}, {"type": "text", "expr": " The key advantages of Eq", "word_idx": 24112, "sentence_idx": 300, "label": "unlabeled"}, {"type": "text", "expr": " ( LABEL:Eq:Unlabeled:Term ) over enforcing pairwise view-consistency are (i) the number of items is linear to the number of views, and (ii) as we will see immediately, the latent configurations  $\\{M_{i}\\}$  allow us to easily formulate the geometric alignment term  $f_{\\textup{align}}$ ", "word_idx": 24137, "sentence_idx": 301, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{\\textup{view}}$$", "word_idx": 24426, "sentence_idx": 302, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Eq:Unlabeled:Term", "word_idx": 24443, "sentence_idx": 303, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{M_{i}\\}$$", "word_idx": 24466, "sentence_idx": 304, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{\\textup{align}}$$", "word_idx": 24475, "sentence_idx": 305, "label": "unlabeled"}, {"type": "text", "expr": "The geometric alignment term  $f_{\\textup{align}}$  prioritizes that the latent configurations  $\\{M_{i},1\\leq i\\leq N\\}$ , which characterize the predictions on the target domain, shall be consistent with ground-truth labels  $\\{Y_{I}|I\\in\\overline{\\mathcal{I}}\\}$  of the source domain", "word_idx": 24493, "sentence_idx": 306, "label": "unlabeled"}, {"type": "text", "expr": " This term is conceptually similar to the idea of aligning output distributions for unsupervised domain adaptation, but our formulation is tailored to the specific problem we consider in this paper", "word_idx": 24780, "sentence_idx": 307, "label": "unlabeled"}, {"type": "text", "expr": " A straightforward formulation is to use the Earth-Mover Distance between  $\\{M_{i},1\\leq i\\leq N\\}$  and  $\\{Y(I)|I\\in\\overline{\\mathcal{I}}\\}$ , which essentially aligns the two corresponding empirical distributions", "word_idx": 24977, "sentence_idx": 308, "label": "unlabeled"}, {"type": "text", "expr": " However, we found that this strategy would force the alignment of keypoint configurations that are far apart, since the repetition counts of the same sub-types of an object may be different between the source and target domains (See Figure\u00a0 LABEL:Figure:Latent:Variable:Initialization (a))", "word_idx": 25194, "sentence_idx": 309, "label": "unlabeled"}, {"type": "text", "expr": " To address this issue, we propose to use the Chamfer distance for alignment:", "word_idx": 25484, "sentence_idx": 310, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{\\textup{align}}$$", "word_idx": 25561, "sentence_idx": 311, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{M_{i},1\\leq i\\leq N\\}$$", "word_idx": 25579, "sentence_idx": 312, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{Y_{I}|I\\in\\overline{\\mathcal{I}}\\}$$", "word_idx": 25602, "sentence_idx": 313, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{M_{i},1\\leq i\\leq N\\}$$", "word_idx": 25638, "sentence_idx": 314, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{Y(I)|I\\in\\overline{\\mathcal{I}}\\}$$", "word_idx": 25661, "sentence_idx": 315, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Figure:Latent:Variable:Initialization", "word_idx": 25696, "sentence_idx": 316, "label": "unlabeled"}, {"type": "text", "expr": "$f_{\\textup{align}}=\\frac{1}{N}\\sum\\limits_{i=1}^{N}\\min\\limits_{I\\in\\overline{%\n\\mathcal{I}}}r(M_{i},Y_{I})+\\frac{1}{|\\overline{\\mathcal{I}}|}\\sum\\limits_{I%\n\\in\\overline{\\mathcal{I}}}\\min\\limits_{1\\leq i\\leq N}\\ r(M_{i},Y_{I})\\mathit{%\nlabel}{Eq:Align:Term}$", "word_idx": 25739, "sentence_idx": 317, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{\\textup{align}}=\\frac{1}{N}\\sum\\limits_{i=1}^{N}\\min\\limits_{I\\in\\overline{%\n\\mathcal{I}}}r(M_{i},Y_{I})+\\frac{1}{|\\overline{\\mathcal{I}}|}\\sum\\limits_{I%\n\\in\\overline{\\mathcal{I}}}\\min\\limits_{1\\leq i\\leq N}\\ r(M_{i},Y_{I}).\\mathit{%\nlabel}{Eq:Align:Term}$$", "word_idx": 25999, "sentence_idx": 318, "label": "unlabeled"}, {"type": "text", "expr": "Intuitively, Eq", "word_idx": 26258, "sentence_idx": 319, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0( LABEL:Eq:Align:Term ) still aligns the source and target domains, but it is insensitive to local density variations, and provides an effective way to address domain shifts", "word_idx": 26273, "sentence_idx": 320, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Eq:Align:Term", "word_idx": 26447, "sentence_idx": 321, "label": "unlabeled"}, {"type": "text", "expr": "We combine the labeled term  $f_{\\textup{labeled}}$ , the view-consistency term  $f_{\\textup{view}}$  and the geometric alignment term  $f_{\\textup{align}}$  into the final loss function:", "word_idx": 26466, "sentence_idx": 322, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{\\textup{labeled}}$$", "word_idx": 26653, "sentence_idx": 323, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{\\textup{view}}$$", "word_idx": 26673, "sentence_idx": 324, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{\\textup{align}}$$", "word_idx": 26690, "sentence_idx": 325, "label": "unlabeled"}, {"type": "text", "expr": "$\\underset{\\theta,\\{M_{i}\\}}{\\textup{minimize}}f_{\\textup{labeled}}+\\lambda f_{%\n\\textup{view}}+\\mu f_{\\textup{align}}\\mathit{label}{Eq:Total:Loss:Function}$", "word_idx": 26708, "sentence_idx": 326, "label": "unlabeled"}, {"type": "math", "expr": "$$\\underset{\\theta,\\{M_{i}\\}}{\\textup{minimize}}f_{\\textup{labeled}}+\\lambda f_{%\n\\textup{view}}+\\mu f_{\\textup{align}}.\\mathit{label}{Eq:Total:Loss:Function}$$", "word_idx": 26865, "sentence_idx": 327, "label": "unlabeled"}, {"type": "text", "expr": "In our implementation, we set  $\\lambda=1$  and  $\\mu=01$ ", "word_idx": 27021, "sentence_idx": 328, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda=1$$", "word_idx": 27079, "sentence_idx": 329, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mu=0.1$$", "word_idx": 27088, "sentence_idx": 330, "label": "unlabeled"}, {"type": "text", "expr": "3  Optimization", "word_idx": 27095, "sentence_idx": 331, "label": "unlabeled"}, {"type": "text", "expr": "label Subsection:Optimization", "word_idx": 27110, "sentence_idx": 332, "label": "unlabeled"}, {"type": "text", "expr": "label", "word_idx": 27139, "sentence_idx": 333, "label": "unlabeled"}, {"type": "text", "expr": "The major difficulty for optimizing Eq", "word_idx": 27144, "sentence_idx": 334, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0( LABEL:Eq:Total:Loss:Function ) lies in the fact that the alignment term  $f_{\\textup{align}}$  is highly non-convex", "word_idx": 27182, "sentence_idx": 335, "label": "unlabeled"}, {"type": "text", "expr": " In our experiments, we found that obtaining good initial values of the network parameters and latent variables is critical to achieving high-quality keypoint prediction network", "word_idx": 27300, "sentence_idx": 336, "label": "unlabeled"}, {"type": "text", "expr": " In the following, we first introduce effective strategies to initialize the variables", "word_idx": 27477, "sentence_idx": 337, "label": "unlabeled"}, {"type": "text", "expr": " We then show how to refine the variables using alternating minimization", "word_idx": 27563, "sentence_idx": 338, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Eq:Total:Loss:Function", "word_idx": 27635, "sentence_idx": 339, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{\\textup{align}}$$", "word_idx": 27663, "sentence_idx": 340, "label": "unlabeled"}, {"type": "text", "expr": "It is then optimized via standard back-propagation", "word_idx": 27681, "sentence_idx": 341, "label": "unlabeled"}, {"type": "text", "expr": "Network Parameter Initialization", "word_idx": 27731, "sentence_idx": 342, "label": "unlabeled"}, {"type": "text", "expr": "  The network parameters are initialized by pre-training on the the source domain labeled dataset, i", "word_idx": 27763, "sentence_idx": 343, "label": "unlabeled"}, {"type": "text", "expr": "Network Parameter Initialization", "word_idx": 27863, "sentence_idx": 344, "label": "unlabeled"}, {"type": "text", "expr": "$\\theta^{(0)}=\\underset{\\theta}{\\min}\\ \\sum\\limits_{I\\in\\overline{\\mathcal{I}}}%\n\\|G_{\\theta}(I)-Y_{I}\\|_{\\mathcal{F}}^{2}$", "word_idx": 27895, "sentence_idx": 345, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta^{(0)}=\\underset{\\theta}{\\min}\\ \\sum\\limits_{I\\in\\overline{\\mathcal{I}}}%\n\\|G_{\\theta}(I)-Y_{I}\\|_{\\mathcal{F}}^{2}.$$", "word_idx": 28018, "sentence_idx": 346, "label": "unlabeled"}, {"type": "text", "expr": "It is then optimized via standard back-propagation", "word_idx": 28140, "sentence_idx": 347, "label": "unlabeled"}, {"type": "text", "expr": "Latent Configuration Initialization", "word_idx": 28190, "sentence_idx": 348, "label": "unlabeled"}, {"type": "text", "expr": "  We use the predictions obtained from the initial network  $G_{\\theta^{(0)}}(I_{ij}),I_{ij}\\in\\mathcal{I}_{i}$  to initialize each latent variable  $M_{i}$ ", "word_idx": 28225, "sentence_idx": 349, "label": "unlabeled"}, {"type": "text", "expr": " To this end, we define a score for each prediction and set  $M_{i}$  as the one with the highest score", "word_idx": 28382, "sentence_idx": 350, "label": "unlabeled"}, {"type": "text", "expr": " The scoring function is motivated by the fact that the latent variables are expected to align with the source domain, we thus define an un-normalized density function:", "word_idx": 28485, "sentence_idx": 351, "label": "unlabeled"}, {"type": "text", "expr": "Latent Configuration Initialization", "word_idx": 28653, "sentence_idx": 352, "label": "unlabeled"}, {"type": "math", "expr": "$$G_{\\theta^{(0)}}(I_{ij}),I_{ij}\\in\\mathcal{I}_{i}$$", "word_idx": 28688, "sentence_idx": 353, "label": "unlabeled"}, {"type": "math", "expr": "$$M_{i}$$", "word_idx": 28737, "sentence_idx": 354, "label": "unlabeled"}, {"type": "math", "expr": "$$M_{i}$$", "word_idx": 28742, "sentence_idx": 355, "label": "unlabeled"}, {"type": "text", "expr": "$p(M)=\\sum\\limits_{I\\in\\overline{\\mathcal{I}}}\\exp(-\\frac{r(M,Y(I))}{2\\sigma^{2%\n}}),\\mathit{label}{Eq:Density:Definition}$", "word_idx": 28747, "sentence_idx": 356, "label": "unlabeled"}, {"type": "math", "expr": "$$p(M)=\\sum\\limits_{I\\in\\overline{\\mathcal{I}}}\\exp(-\\frac{r(M,Y(I))}{2\\sigma^{2%\n}}),\\mathit{label}{Eq:Density:Definition}$$", "word_idx": 28870, "sentence_idx": 357, "label": "unlabeled"}, {"type": "text", "expr": "where  $\\sigma$  is chosen as mean of  $r(G_{\\theta^{(0)}}(I_{ij}),Y(I))$  between the predicted configurations and their closest labeled instances", "word_idx": 28991, "sentence_idx": 358, "label": "unlabeled"}, {"type": "text", "expr": " Given Eq", "word_idx": 29138, "sentence_idx": 359, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0( LABEL:Eq:Density:Definition ), we set", "word_idx": 29147, "sentence_idx": 360, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma$$", "word_idx": 29187, "sentence_idx": 361, "label": "unlabeled"}, {"type": "math", "expr": "$$r(G_{\\theta^{(0)}}(I_{ij}),Y(I))$$", "word_idx": 29193, "sentence_idx": 362, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Eq:Density:Definition", "word_idx": 29225, "sentence_idx": 363, "label": "unlabeled"}, {"type": "text", "expr": "$M_{i}^{(0)}=\\underset{M\\in\\{G_{\\theta^{(0)}}(I)|I\\in\\mathcal{I}_{i}\\}}{\\textup%\n{argmax}}\\ p(M)\\mathit{label}{Eq:M:init}$", "word_idx": 29252, "sentence_idx": 364, "label": "unlabeled"}, {"type": "math", "expr": "$$M_{i}^{(0)}=\\underset{M\\in\\{G_{\\theta^{(0)}}(I)|I\\in\\mathcal{I}_{i}\\}}{\\textup%\n{argmax}}\\ p(M).\\mathit{label}{Eq:M:init}$$", "word_idx": 29374, "sentence_idx": 365, "label": "unlabeled"}, {"type": "text", "expr": "As illustrated in Figure\u00a0 LABEL:Figure:Latent:Variable:Initialization (b-c), this strategy leads to initial configurations that are close to the underlying ground-truth", "word_idx": 29495, "sentence_idx": 366, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Figure:Latent:Variable:Initialization", "word_idx": 29663, "sentence_idx": 367, "label": "unlabeled"}, {"type": "text", "expr": "Alternating Minimization", "word_idx": 29706, "sentence_idx": 368, "label": "unlabeled"}, {"type": "text", "expr": " \nGiven the initial network parameter  $\\theta^{(0)}$  and the initial latent configurations  $M_{i}^{(0)},1\\leq i\\leq N$ , we then refine them by solving Eq", "word_idx": 29730, "sentence_idx": 369, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0( LABEL:Eq:Total:Loss:Function ) via alternating minimization", "word_idx": 29887, "sentence_idx": 370, "label": "unlabeled"}, {"type": "text", "expr": " With  $M_{i}^{(k)}$  and  $\\theta^{(k)}$  we denote their values at iteration k", "word_idx": 29949, "sentence_idx": 371, "label": "unlabeled"}, {"type": "text", "expr": " At each alternating minimization step, we first fix the latent variables to optimize the network parameters", "word_idx": 30029, "sentence_idx": 372, "label": "unlabeled"}, {"type": "text", "expr": " This leads to computing", "word_idx": 30137, "sentence_idx": 373, "label": "unlabeled"}, {"type": "text", "expr": "Alternating Minimization", "word_idx": 30161, "sentence_idx": 374, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta^{(0)}$$", "word_idx": 30185, "sentence_idx": 375, "label": "unlabeled"}, {"type": "math", "expr": "$$M_{i}^{(0)},1\\leq i\\leq N$$", "word_idx": 30197, "sentence_idx": 376, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Eq:Total:Loss:Function", "word_idx": 30222, "sentence_idx": 377, "label": "unlabeled"}, {"type": "math", "expr": "$$M_{i}^{(k)}$$", "word_idx": 30250, "sentence_idx": 378, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta^{(k)}$$", "word_idx": 30261, "sentence_idx": 379, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\theta^{(k+1)}=$", "word_idx": 30273, "sentence_idx": 380, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\theta^{(k+1)}=$$", "word_idx": 30303, "sentence_idx": 381, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\underset{\\theta}{\\textup{argmin}}\\ \\frac{1}{|\\overline{\\mathcal{%\nI}}|}\\sum\\limits_{I\\in\\overline{\\mathcal{I}}}\\|G_{\\theta}(I)-Y_{I}\\|_{\\mathcal%\n{F}}^{2}$", "word_idx": 30331, "sentence_idx": 382, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\underset{\\theta}{\\textup{argmin}}\\ \\frac{1}{|\\overline{\\mathcal{%\nI}}|}\\sum\\limits_{I\\in\\overline{\\mathcal{I}}}\\|G_{\\theta}(I)-Y_{I}\\|_{\\mathcal%\n{F}}^{2}$$", "word_idx": 30501, "sentence_idx": 383, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle+\\frac{\\lambda}{N}\\sum\\limits_{i=1}^{N}\\frac{1}{|\\mathcal{I}_{i}|%\n}\\sum\\limits_{I\\in\\mathcal{I}_{i}}r(G_{\\theta}(I),M_{i}^{(k)})\\mathit{label}{%\nEq:Altermin:Network:Opt}$", "word_idx": 30669, "sentence_idx": 384, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle+\\frac{\\lambda}{N}\\sum\\limits_{i=1}^{N}\\frac{1}{|\\mathcal{I}_{i}|%\n}\\sum\\limits_{I\\in\\mathcal{I}_{i}}r(G_{\\theta}(I),M_{i}^{(k)}).\\mathit{label}{%\nEq:Altermin:Network:Opt}$$", "word_idx": 30854, "sentence_idx": 385, "label": "unlabeled"}, {"type": "text", "expr": "Utilizing Proposition\u00a0 LABEL:Prop:2 , we apply stochastic gradient descent via back-propagation for solving Eq", "word_idx": 31038, "sentence_idx": 386, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0( LABEL:Eq:Altermin:Network:Opt )", "word_idx": 31148, "sentence_idx": 387, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Prop:2", "word_idx": 31182, "sentence_idx": 388, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Eq:Altermin:Network:Opt", "word_idx": 31194, "sentence_idx": 389, "label": "unlabeled"}, {"type": "text", "expr": "We then fix the network parameters  $\\theta$  and optimize the latent variables  $\\{M_{i}^{(k+1)}\\}$ ", "word_idx": 31223, "sentence_idx": 390, "label": "unlabeled"}, {"type": "text", "expr": " In this case, Eq", "word_idx": 31324, "sentence_idx": 391, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0( LABEL:Eq:Total:Loss:Function ) reduces to", "word_idx": 31341, "sentence_idx": 392, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta$$", "word_idx": 31385, "sentence_idx": 393, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{M_{i}^{(k+1)}\\}$$", "word_idx": 31391, "sentence_idx": 394, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Eq:Total:Loss:Function", "word_idx": 31408, "sentence_idx": 395, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\{M_{i}^{(k+1)}\\}=\\underset{\\{M_{i}\\}}{\\textup{argmin}}\\ \\frac{%\n\\mu}{|\\overline{\\mathcal{I}}|}\\sum\\limits_{I\\in\\overline{\\mathcal{I}}}\\min%\n\\limits_{1\\leq i\\leq N}r(M_{i},Y_{I})$", "word_idx": 31436, "sentence_idx": 396, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\{M_{i}^{(k+1)}\\}=\\underset{\\{M_{i}\\}}{\\textup{argmin}}\\ \\frac{%\n\\mu}{|\\overline{\\mathcal{I}}|}\\sum\\limits_{I\\in\\overline{\\mathcal{I}}}\\min%\n\\limits_{1\\leq i\\leq N}r(M_{i},Y_{I})$$", "word_idx": 31629, "sentence_idx": 397, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle+\\frac{1}{N}\\sum\\limits_{i=1}^{N}\\Big{(}\\frac{\\lambda}{|\\mathcal{%\nI}_{i}|}\\sum\\limits_{I\\in\\mathcal{I}_{i}}r(G_{\\theta^{(k)}}(I),M_{i})+\\mu\\min%\n\\limits_{I\\in\\overline{\\mathcal{I}}}r(M_{i},Y_{I})\\Big{)}\\mathit{label}{Eq:%\nAltermin:Latent:Variable}$", "word_idx": 31820, "sentence_idx": 398, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle+\\frac{1}{N}\\sum\\limits_{i=1}^{N}\\Big{(}\\frac{\\lambda}{|\\mathcal{%\nI}_{i}|}\\sum\\limits_{I\\in\\mathcal{I}_{i}}r(G_{\\theta^{(k)}}(I),M_{i})+\\mu\\min%\n\\limits_{I\\in\\overline{\\mathcal{I}}}r(M_{i},Y_{I})\\Big{)}.\\mathit{label}{Eq:%\nAltermin:Latent:Variable}$$", "word_idx": 32083, "sentence_idx": 399, "label": "unlabeled"}, {"type": "text", "expr": "We again apply alternating minimization to solve Eq", "word_idx": 32345, "sentence_idx": 400, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0( LABEL:Eq:Altermin:Latent:Variable )", "word_idx": 32396, "sentence_idx": 401, "label": "unlabeled"}, {"type": "text", "expr": " In particular, we fix the closest point pairs given  $\\{M_{i}^{(k)}\\}$  :", "word_idx": 32434, "sentence_idx": 402, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Eq:Altermin:Latent:Variable", "word_idx": 32508, "sentence_idx": 403, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{M_{i}^{(k)}\\}$$", "word_idx": 32541, "sentence_idx": 404, "label": "unlabeled"}, {"type": "text", "expr": "$\\hat{I}(i)=\\underset{I\\in\\overline{\\mathcal{I}}}{\\textup{argmin}}\\ r(M_{i}^{(k%\n)},Y_{I}),\\quad\\hat{i}(I)=\\underset{1\\leq i\\leq N}{\\textup{argmin}}\\ r(M_{i}^{%\n(k)},Y_{I})$", "word_idx": 32556, "sentence_idx": 405, "label": "unlabeled"}, {"type": "math", "expr": "$$\\hat{I}(i)=\\underset{I\\in\\overline{\\mathcal{I}}}{\\textup{argmin}}\\ r(M_{i}^{(k%\n)},Y_{I}),\\quad\\hat{i}(I)=\\underset{1\\leq i\\leq N}{\\textup{argmin}}\\ r(M_{i}^{%\n(k)},Y_{I}).$$", "word_idx": 32729, "sentence_idx": 406, "label": "unlabeled"}, {"type": "text", "expr": "Given these closest pairs, we can optimize the latent configurations  $M_{i}$  independently via:", "word_idx": 32901, "sentence_idx": 407, "label": "unlabeled"}, {"type": "math", "expr": "$$M_{i}$$", "word_idx": 32998, "sentence_idx": 408, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle M_{i}^{(k+1)}=\\underset{M_{i}}{\\textup{argmin}}\\frac{\\mu}{|%\n\\overline{\\mathcal{I}}|}\\sum\\limits_{I|\\hat{i}(I)=i}r(M_{i},Y_{I})$", "word_idx": 33003, "sentence_idx": 409, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle M_{i}^{(k+1)}=\\underset{M_{i}}{\\textup{argmin}}\\frac{\\mu}{|%\n\\overline{\\mathcal{I}}|}\\sum\\limits_{I|\\hat{i}(I)=i}r(M_{i},Y_{I})$$", "word_idx": 33146, "sentence_idx": 410, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle+\\frac{1}{N}\\Big{(}\\frac{\\lambda}{|\\mathcal{I}_{i}|}\\sum\\limits_{%\nI\\in\\mathcal{I}_{i}}r(G_{\\theta^{(k)}}(I),M_{i})+\\mu r(M_{i},Y_{\\hat{I}(i)})%\n\\Big{)}\\mathit{label}{Mi:OPT}$", "word_idx": 33287, "sentence_idx": 411, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle+\\frac{1}{N}\\Big{(}\\frac{\\lambda}{|\\mathcal{I}_{i}|}\\sum\\limits_{%\nI\\in\\mathcal{I}_{i}}r(G_{\\theta^{(k)}}(I),M_{i})+\\mu r(M_{i},Y_{\\hat{I}(i)})%\n\\Big{)}\\mathit{label}{Mi:OPT}.$$", "word_idx": 33476, "sentence_idx": 412, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0( LABEL:Mi:OPT ) admits a form of Eq", "word_idx": 33664, "sentence_idx": 413, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0( LABEL:Eq:Quotient:Mean ), and we apply the procedure described above to solve Eq", "word_idx": 33701, "sentence_idx": 414, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0( LABEL:Mi:OPT )", "word_idx": 33784, "sentence_idx": 415, "label": "unlabeled"}, {"type": "text", "expr": "\nIn our experiments, we typically apply the inner alternating minimizations each 5 epochs for training the network parameters  $\\theta$ ", "word_idx": 33801, "sentence_idx": 416, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Mi:OPT", "word_idx": 33937, "sentence_idx": 417, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Eq:Quotient:Mean", "word_idx": 33949, "sentence_idx": 418, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Mi:OPT", "word_idx": 33971, "sentence_idx": 419, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta$$", "word_idx": 33983, "sentence_idx": 420, "label": "unlabeled"}, {"type": "text", "expr": "5  Evaluation", "word_idx": 33989, "sentence_idx": 421, "label": "unlabeled"}, {"type": "text", "expr": "label Section:Experiments", "word_idx": 34002, "sentence_idx": 422, "label": "unlabeled"}, {"type": "text", "expr": "label", "word_idx": 34027, "sentence_idx": 423, "label": "unlabeled"}, {"type": "text", "expr": "In this section, we evaluate the proposed unsupervised domain adaptation method for 3D keypoint prediction", "word_idx": 34032, "sentence_idx": 424, "label": "unlabeled"}, {"type": "text", "expr": " We first describe the experimental setup in Section\u00a0 LABEL:Subsection:Experimental:Setup ", "word_idx": 34138, "sentence_idx": 425, "label": "unlabeled"}, {"type": "text", "expr": " We then present qualitative and quantitative results and compare our technique to baseline approaches in Section\u00a0 LABEL:Subsection:Analysis:Results ", "word_idx": 34228, "sentence_idx": 426, "label": "unlabeled"}, {"type": "text", "expr": " Finally, we present an ablation study to evaluate each component of our approach in Section\u00a0 LABEL:Subsection:Ablation:Study  and extend our method to RGB images in Section\u00a0 LABEL:Subsection:Experimental:RGB ", "word_idx": 34377, "sentence_idx": 427, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Subsection:Experimental:Setup", "word_idx": 34586, "sentence_idx": 428, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Subsection:Analysis:Results", "word_idx": 34621, "sentence_idx": 429, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Subsection:Ablation:Study", "word_idx": 34654, "sentence_idx": 430, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Subsection:Experimental:RGB", "word_idx": 34685, "sentence_idx": 431, "label": "unlabeled"}, {"type": "text", "expr": "Target #Train Models #Test Models Avg #frames", "word_idx": 34718, "sentence_idx": 432, "label": "unlabeled"}, {"type": "text", "expr": "899 100 Inf", "word_idx": 34763, "sentence_idx": 433, "label": "unlabeled"}, {"type": "text", "expr": "2500 100 Inf", "word_idx": 34774, "sentence_idx": 434, "label": "unlabeled"}, {"type": "text", "expr": "200 35 150", "word_idx": 34786, "sentence_idx": 435, "label": "unlabeled"}, {"type": "text", "expr": "9 3 80", "word_idx": 34796, "sentence_idx": 436, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:  Statistics of the datasets ", "word_idx": 34802, "sentence_idx": 437, "label": "unlabeled"}, {"type": "text", "expr": " We show the number of models and the average number of frames per model", "word_idx": 34839, "sentence_idx": 438, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:", "word_idx": 34911, "sentence_idx": 439, "label": "unlabeled"}, {"type": "text", "expr": "Statistics of the datasets", "word_idx": 34919, "sentence_idx": 440, "label": "unlabeled"}, {"type": "text", "expr": "label table:datasets", "word_idx": 34945, "sentence_idx": 441, "label": "unlabeled"}, {"type": "text", "expr": "label", "word_idx": 34965, "sentence_idx": 442, "label": "unlabeled"}, {"type": "text", "expr": "1  Experimental Setup", "word_idx": 34970, "sentence_idx": 443, "label": "unlabeled"}, {"type": "text", "expr": "label Subsection:Experimental:Setup", "word_idx": 34991, "sentence_idx": 444, "label": "unlabeled"}, {"type": "text", "expr": "label", "word_idx": 35026, "sentence_idx": 445, "label": "unlabeled"}, {"type": "text", "expr": "Dataset", "word_idx": 35031, "sentence_idx": 446, "label": "unlabeled"}, {"type": "text", "expr": "  The focus of our evaluation is on the chair object class because of its presence across many 3D model and depth scan datasets", "word_idx": 35038, "sentence_idx": 447, "label": "unlabeled"}, {"type": "text", "expr": "\nRendered depth scans of chairs from the synthesized ModelNet\u00a0  dataset serve as our source domain, and we test our domain adaptation method on three different target domains, namely another synthesized 3D model dataset ShapeNet\u00a0 , real depth scans from the Redwood Object Scans dataset\u00a0 , real depth scans from the 3DCNN dataset\u00a0 ", "word_idx": 35165, "sentence_idx": 448, "label": "unlabeled"}, {"type": "text", "expr": "\nTo provide keypoint labels for our source domain, we manually annotate the training samples in ModelNet with Meshlab\u00a0 ", "word_idx": 35496, "sentence_idx": 449, "label": "unlabeled"}, {"type": "text", "expr": "\nTo evaluate the accuracy of our system, we also annotate chair keypoints on our target domain datasets", "word_idx": 35615, "sentence_idx": 450, "label": "unlabeled"}, {"type": "text", "expr": "\nThis annotation is done by recovering the object\u2019s 3D mesh and each frame\u2019s camera pose from a depth video sequence", "word_idx": 35718, "sentence_idx": 451, "label": "unlabeled"}, {"type": "text", "expr": "\nBy knowing each frame\u2019s camera pose in relation to the reconstructed mesh, we can propagate the keypoint annotation to the whole sequence", "word_idx": 35834, "sentence_idx": 452, "label": "unlabeled"}, {"type": "text", "expr": "\nWe keep the frames which all the 2D projection of keypoints are within the image and keep the models with at least 20 different frames", "word_idx": 35972, "sentence_idx": 453, "label": "unlabeled"}, {"type": "text", "expr": "\nA summary of the number of samples of the four datasets used in our experiments is presented in Table\u00a0 LABEL:table:datasets ", "word_idx": 36107, "sentence_idx": 454, "label": "unlabeled"}, {"type": "text", "expr": "\nAs a natural extension, we also test our method on the RGB images from the same Redwood dataset\u00a0 ", "word_idx": 36232, "sentence_idx": 455, "label": "unlabeled"}, {"type": "text", "expr": "Dataset", "word_idx": 36330, "sentence_idx": 456, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:table:datasets", "word_idx": 36337, "sentence_idx": 457, "label": "unlabeled"}, {"type": "text", "expr": "Target-Metric Default-AE ADDA-AE Ours-AE Supervised-AE Default-PAE ADDA-PAE Ours-PAE Supervised-PAE", "word_idx": 36357, "sentence_idx": 458, "label": "unlabeled"}, {"type": "text", "expr": "- - - 5", "word_idx": 36456, "sentence_idx": 459, "label": "unlabeled"}, {"type": "text", "expr": "56 - - - 4", "word_idx": 36463, "sentence_idx": 460, "label": "unlabeled"}, {"type": "text", "expr": "01 15", "word_idx": 36473, "sentence_idx": 461, "label": "unlabeled"}, {"type": "text", "expr": "44 12", "word_idx": 36478, "sentence_idx": 462, "label": "unlabeled"}, {"type": "text", "expr": "67 10", "word_idx": 36483, "sentence_idx": 463, "label": "unlabeled"}, {"type": "text", "expr": "73 10", "word_idx": 36488, "sentence_idx": 464, "label": "unlabeled"}, {"type": "text", "expr": "61 11", "word_idx": 36493, "sentence_idx": 465, "label": "unlabeled"}, {"type": "text", "expr": "81 10", "word_idx": 36498, "sentence_idx": 466, "label": "unlabeled"}, {"type": "text", "expr": "59 26", "word_idx": 36503, "sentence_idx": 467, "label": "unlabeled"}, {"type": "text", "expr": "16 25", "word_idx": 36508, "sentence_idx": 468, "label": "unlabeled"}, {"type": "text", "expr": "24 11", "word_idx": 36513, "sentence_idx": 469, "label": "unlabeled"}, {"type": "text", "expr": "90 13", "word_idx": 36518, "sentence_idx": 470, "label": "unlabeled"}, {"type": "text", "expr": "44 12", "word_idx": 36523, "sentence_idx": 471, "label": "unlabeled"}, {"type": "text", "expr": "31 11", "word_idx": 36528, "sentence_idx": 472, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:  Results of our proposed methods before and after domain adaptation on different target domains", "word_idx": 36533, "sentence_idx": 473, "label": "unlabeled"}, {"type": "text", "expr": " We show Average distance Error (AE) and Pose-Invariant Average distance Error (PAE) in percentage, the lower the better", "word_idx": 36637, "sentence_idx": 474, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:", "word_idx": 36757, "sentence_idx": 475, "label": "unlabeled"}, {"type": "text", "expr": "label table:baselines", "word_idx": 36765, "sentence_idx": 476, "label": "unlabeled"}, {"type": "text", "expr": "label", "word_idx": 36786, "sentence_idx": 477, "label": "unlabeled"}, {"type": "text", "expr": "Data pre-processing", "word_idx": 36791, "sentence_idx": 478, "label": "unlabeled"}, {"type": "text", "expr": " \nWe assume the camera intrinsic and object\u2019s 3D bounding box are known both in training and testing depth images", "word_idx": 36810, "sentence_idx": 479, "label": "unlabeled"}, {"type": "text", "expr": "\nWe use the 2D projection of the 3D bounding box to crop the image", "word_idx": 36923, "sentence_idx": 480, "label": "unlabeled"}, {"type": "text", "expr": "\nAdditionally, the input depth images are centered by the mean depth and the depth values are normalized by the diagonal length of the 3D bounding box", "word_idx": 36989, "sentence_idx": 481, "label": "unlabeled"}, {"type": "text", "expr": "\nAside from the images, all keypoints are converted and evaluated in a unified coordinate system", "word_idx": 37139, "sentence_idx": 482, "label": "unlabeled"}, {"type": "text", "expr": "\nGiven a configuration, we subtract their mean and normalize by the diagonal length of the 3D bounding box", "word_idx": 37235, "sentence_idx": 483, "label": "unlabeled"}, {"type": "text", "expr": "Data pre-processing", "word_idx": 37341, "sentence_idx": 484, "label": "unlabeled"}, {"type": "text", "expr": "Evaluation protocol", "word_idx": 37360, "sentence_idx": 485, "label": "unlabeled"}, {"type": "text", "expr": " \nSimilar to\u00a0 , we measure the Average distance Error (AE) between each predicted keypoint configuration and the corresponding annotation and plot the Percentage of Correct Keypoint (PCK) with respect to a threshold for each method for detailed comparison", "word_idx": 37379, "sentence_idx": 486, "label": "unlabeled"}, {"type": "text", "expr": "\nWe also introduce a new metric, Pose-invariant Average distance Error (PAE) expressed in Equation\u00a0 LABEL:Eq:pose-inv-mrtric , for a better illustration of how our proposed method works", "word_idx": 37634, "sentence_idx": 487, "label": "unlabeled"}, {"type": "text", "expr": "\nThe AE and PAE are show in percentage since they respect to the relative ratio to the diagonal length of the 3D bounding box", "word_idx": 37819, "sentence_idx": 488, "label": "unlabeled"}, {"type": "text", "expr": "Evaluation protocol", "word_idx": 37944, "sentence_idx": 489, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Eq:pose-inv-mrtric", "word_idx": 37963, "sentence_idx": 490, "label": "unlabeled"}, {"type": "text", "expr": "Besides these baseline approaches, we also conduct an ablation study to evaluate each component of our approach", "word_idx": 37987, "sentence_idx": 491, "label": "unlabeled"}, {"type": "text", "expr": "Baseline methods", "word_idx": 38098, "sentence_idx": 492, "label": "unlabeled"}, {"type": "text", "expr": "  We consider three baseline methods for experimental evaluation", "word_idx": 38114, "sentence_idx": 493, "label": "unlabeled"}, {"type": "text", "expr": "Baseline methods", "word_idx": 38178, "sentence_idx": 494, "label": "unlabeled"}, {"type": "text", "expr": "Baseline I", "word_idx": 38194, "sentence_idx": 495, "label": "unlabeled"}, {"type": "text", "expr": "  We first test performance without any domain adaptation techniques, namely we directly apply the keypoint predictor trained on the source domain to the target domain", "word_idx": 38204, "sentence_idx": 496, "label": "unlabeled"}, {"type": "text", "expr": " This baseline serves as a performance lower bound for accessing domain adaptation techniques", "word_idx": 38371, "sentence_idx": 497, "label": "unlabeled"}, {"type": "text", "expr": "Baseline I", "word_idx": 38464, "sentence_idx": 498, "label": "unlabeled"}, {"type": "text", "expr": "Baseline II", "word_idx": 38474, "sentence_idx": 499, "label": "unlabeled"}, {"type": "text", "expr": "  We implement a state-of-the-art deep unsupervised domain adaptation technique described in\u00a0 ,\nwhich encourages domain confusion by fine-tuning the feature extractor on the target domain", "word_idx": 38485, "sentence_idx": 500, "label": "unlabeled"}, {"type": "text", "expr": "Baseline II", "word_idx": 38672, "sentence_idx": 501, "label": "unlabeled"}, {"type": "text", "expr": "Baseline III", "word_idx": 38683, "sentence_idx": 502, "label": "unlabeled"}, {"type": "text", "expr": "  We apply supervised keypoint prediction on the target domain", "word_idx": 38695, "sentence_idx": 503, "label": "unlabeled"}, {"type": "text", "expr": " To this end, we annotate 50 additional models from each domain and fine-tune Baseline I on these labeled instances", "word_idx": 38757, "sentence_idx": 504, "label": "unlabeled"}, {"type": "text", "expr": " This baseline serves as a performance upper bound for accessing domain adaptation techniques", "word_idx": 38872, "sentence_idx": 505, "label": "unlabeled"}, {"type": "text", "expr": "Baseline III", "word_idx": 38965, "sentence_idx": 506, "label": "unlabeled"}, {"type": "text", "expr": "Besides these baseline approaches, we also conduct an ablation study to evaluate each component of our approach", "word_idx": 38977, "sentence_idx": 507, "label": "unlabeled"}, {"type": "text", "expr": "Implementation details", "word_idx": 39088, "sentence_idx": 508, "label": "unlabeled"}, {"type": "text", "expr": "  We use ResNet50\u00a0  pre-trained on ImageNet as our keypoint prediction network  $G_{\\theta}$ ", "word_idx": 39110, "sentence_idx": 509, "label": "unlabeled"}, {"type": "text", "expr": "\nIn order to fit our depth scans to the ResNet50 input (and additionally, to allow for natural extension to the RGB image domain), we duplicate the depth channel three times", "word_idx": 39203, "sentence_idx": 510, "label": "unlabeled"}, {"type": "text", "expr": "\nThe network is first trained on source domain  $\\overline{\\mathcal{I}}$  for 120 epochs, and then fine-tuned on a specific target domain  $\\mathcal{I}$  for 30 epochs", "word_idx": 39376, "sentence_idx": 511, "label": "unlabeled"}, {"type": "text", "expr": "\nThe network is trained using a SGD optimizer via back-propagation, with learning rate 0", "word_idx": 39543, "sentence_idx": 512, "label": "unlabeled"}, {"type": "text", "expr": "01 (dropped to 0", "word_idx": 39631, "sentence_idx": 513, "label": "unlabeled"}, {"type": "text", "expr": "001 after 20 epochs), batch size 64, momentum 0", "word_idx": 39647, "sentence_idx": 514, "label": "unlabeled"}, {"type": "text", "expr": "9 and weight decay 1e-4, which are all the default parameters in Resnet50\u00a0 ", "word_idx": 39694, "sentence_idx": 515, "label": "unlabeled"}, {"type": "text", "expr": "\nOur implementation is done in the PyTorch", "word_idx": 39769, "sentence_idx": 516, "label": "unlabeled"}, {"type": "text", "expr": "Implementation details", "word_idx": 39811, "sentence_idx": 517, "label": "unlabeled"}, {"type": "math", "expr": "$$G_{\\theta}$$", "word_idx": 39833, "sentence_idx": 518, "label": "unlabeled"}, {"type": "math", "expr": "$$\\overline{\\mathcal{I}}$$", "word_idx": 39843, "sentence_idx": 519, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{I}$$", "word_idx": 39865, "sentence_idx": 520, "label": "unlabeled"}, {"type": "text", "expr": "2  Analysis of Results", "word_idx": 39876, "sentence_idx": 521, "label": "unlabeled"}, {"type": "text", "expr": "label Subsection:Analysis:Results", "word_idx": 39898, "sentence_idx": 522, "label": "unlabeled"}, {"type": "text", "expr": "label", "word_idx": 39931, "sentence_idx": 523, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a0 LABEL:table:baselines , Table\u00a0 LABEL:table:ablation , and Figure\u00a0 LABEL:Figure:Results  present the quantitative and qualitative results of our approach", "word_idx": 39936, "sentence_idx": 524, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:table:baselines", "word_idx": 40095, "sentence_idx": 525, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:table:ablation", "word_idx": 40116, "sentence_idx": 526, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Figure:Results", "word_idx": 40136, "sentence_idx": 527, "label": "unlabeled"}, {"type": "text", "expr": "Qualitative results", "word_idx": 40156, "sentence_idx": 528, "label": "unlabeled"}, {"type": "text", "expr": "  As illustrated in Figure\u00a0 LABEL:Figure:Results , our approach yields keypoint structures that are consistent with the underlying ground-truths", "word_idx": 40175, "sentence_idx": 529, "label": "unlabeled"}, {"type": "text", "expr": " Even under significant background noise and incomplete observations, our approach leads to faithful structures", "word_idx": 40319, "sentence_idx": 530, "label": "unlabeled"}, {"type": "text", "expr": "\nExceptions include the case for chair types that involve swivel bases", "word_idx": 40430, "sentence_idx": 531, "label": "unlabeled"}, {"type": "text", "expr": "\nIn this case, the predicted legs may be tilted", "word_idx": 40500, "sentence_idx": 532, "label": "unlabeled"}, {"type": "text", "expr": "\nThis is expected since the annotations may become unreliable in cases when the legs do not fall directly below the seat corners", "word_idx": 40547, "sentence_idx": 533, "label": "unlabeled"}, {"type": "text", "expr": "Qualitative results", "word_idx": 40675, "sentence_idx": 534, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Figure:Results", "word_idx": 40694, "sentence_idx": 535, "label": "unlabeled"}, {"type": "text", "expr": "Quantitative assessment", "word_idx": 40714, "sentence_idx": 536, "label": "unlabeled"}, {"type": "text", "expr": " \nAs shown in Table\u00a0 LABEL:table:baselines , the mean deviation of our approach in the two real depth scan datasets Redwood\u00a0  and 3DCNN\u00a0  are 12", "word_idx": 40737, "sentence_idx": 537, "label": "unlabeled"}, {"type": "text", "expr": "76% and 10", "word_idx": 40881, "sentence_idx": 538, "label": "unlabeled"}, {"type": "text", "expr": "60% of the diagonal length of object bounding box, respectively", "word_idx": 40891, "sentence_idx": 539, "label": "unlabeled"}, {"type": "text", "expr": "\nThis translates to approximately 7-10 cm, which is fairly accurate when compared to the radius of a chair\u2019s base", "word_idx": 40954, "sentence_idx": 540, "label": "unlabeled"}, {"type": "text", "expr": "Quantitative assessment", "word_idx": 41067, "sentence_idx": 541, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:table:baselines", "word_idx": 41090, "sentence_idx": 542, "label": "unlabeled"}, {"type": "text", "expr": "Analyses of performance across different datasets", "word_idx": 41111, "sentence_idx": 543, "label": "unlabeled"}, {"type": "text", "expr": " \nOur method gives consistent performance improvements on all three target depth domains", "word_idx": 41160, "sentence_idx": 544, "label": "unlabeled"}, {"type": "text", "expr": "\nFor the synthesized dataset ShapeNet\u00a0 , which has a relatively small domain shift from the supervised training set, our unsupervised terms are still able to push error rates close to the supervised upper bound", "word_idx": 41248, "sentence_idx": 545, "label": "unlabeled"}, {"type": "text", "expr": "\nThe advantages of our proposed method can be best observed in the Redwood dataset\u00a0 , where using our full error terms leads to a 44% step towards the supervised performance upper-bound", "word_idx": 41458, "sentence_idx": 546, "label": "unlabeled"}, {"type": "text", "expr": "\nAdditionally, the improvement in 3DCNN Dataset\u00a0  is still decent despite the very limited available models and poor depth image quality", "word_idx": 41643, "sentence_idx": 547, "label": "unlabeled"}, {"type": "text", "expr": "Analyses of performance across different datasets", "word_idx": 41779, "sentence_idx": 548, "label": "unlabeled"}, {"type": "text", "expr": "Analysis of performance gain", "word_idx": 41828, "sentence_idx": 549, "label": "unlabeled"}, {"type": "text", "expr": " \nOur performance gains can be attributed to our network learning more plausible keypoint configuration shapes, which is supported by the fact that the improvement of AE is always close to that of PAE", "word_idx": 41856, "sentence_idx": 550, "label": "unlabeled"}, {"type": "text", "expr": " This is expected because our unsupervised terms are viewpoint-invariant and focus on improving the keypoint configuration shape", "word_idx": 42056, "sentence_idx": 551, "label": "unlabeled"}, {"type": "text", "expr": "Analysis of performance gain", "word_idx": 42184, "sentence_idx": 552, "label": "unlabeled"}, {"type": "text", "expr": "Our approach is superior to the state-of-the-art unsupervised domain adaptation technique\u00a0  in the keypoint estimation task, which aligns the feature distributions of the source and target domains", "word_idx": 42212, "sentence_idx": 553, "label": "unlabeled"}, {"type": "text", "expr": "\nIt is complementary to our approach that constraints on the label space", "word_idx": 42408, "sentence_idx": 554, "label": "unlabeled"}, {"type": "text", "expr": "\nWe argue that there are more structure to rely on in label space for rigid objects", "word_idx": 42480, "sentence_idx": 555, "label": "unlabeled"}, {"type": "text", "expr": "\nAnother important factor is that view consistency is not Incorporated in ADDA\u00a0 ", "word_idx": 42563, "sentence_idx": 556, "label": "unlabeled"}, {"type": "text", "expr": "\nQuantitatively, ADDA does not give consistent performance boost based on our implementation", "word_idx": 42643, "sentence_idx": 557, "label": "unlabeled"}, {"type": "text", "expr": " Its AE in Redwood dataset is 15", "word_idx": 42735, "sentence_idx": 558, "label": "unlabeled"}, {"type": "text", "expr": "44% while ours is 12", "word_idx": 42767, "sentence_idx": 559, "label": "unlabeled"}, {"type": "text", "expr": "Comparison to ADDA\u00a0 ", "word_idx": 42787, "sentence_idx": 560, "label": "unlabeled"}, {"type": "text", "expr": "3  Ablation Study", "word_idx": 42807, "sentence_idx": 561, "label": "unlabeled"}, {"type": "text", "expr": "label Subsection:Ablation:Study", "word_idx": 42824, "sentence_idx": 562, "label": "unlabeled"}, {"type": "text", "expr": "label", "word_idx": 42855, "sentence_idx": 563, "label": "unlabeled"}, {"type": "text", "expr": "f\nTarget domain ShapeNet(%) Redwood depth(%)", "word_idx": 42860, "sentence_idx": 564, "label": "unlabeled"}, {"type": "text", "expr": "Ours 6", "word_idx": 42904, "sentence_idx": 565, "label": "unlabeled"}, {"type": "text", "expr": "60 12", "word_idx": 42910, "sentence_idx": 566, "label": "unlabeled"}, {"type": "text", "expr": "Re-initialize 6", "word_idx": 42915, "sentence_idx": 567, "label": "unlabeled"}, {"type": "text", "expr": "66 13", "word_idx": 42930, "sentence_idx": 568, "label": "unlabeled"}, {"type": "text", "expr": "Drop view 6", "word_idx": 42935, "sentence_idx": 569, "label": "unlabeled"}, {"type": "text", "expr": "70 13", "word_idx": 42946, "sentence_idx": 570, "label": "unlabeled"}, {"type": "text", "expr": "Drop align 6", "word_idx": 42951, "sentence_idx": 571, "label": "unlabeled"}, {"type": "text", "expr": "67 12", "word_idx": 42963, "sentence_idx": 572, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a03:  Ablation study on ShapeNet and Redwood Object Scans dataset", "word_idx": 42968, "sentence_idx": 573, "label": "unlabeled"}, {"type": "text", "expr": "  We show the Average distance Error (AE) in percentage for each approach", "word_idx": 43037, "sentence_idx": 574, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a03:", "word_idx": 43110, "sentence_idx": 575, "label": "unlabeled"}, {"type": "text", "expr": "Ablation study on ShapeNet and Redwood Object Scans dataset", "word_idx": 43118, "sentence_idx": 576, "label": "unlabeled"}, {"type": "text", "expr": "label table:ablation", "word_idx": 43177, "sentence_idx": 577, "label": "unlabeled"}, {"type": "text", "expr": "label", "word_idx": 43197, "sentence_idx": 578, "label": "unlabeled"}, {"type": "text", "expr": "We present ablation studies to justify each component of our approach", "word_idx": 43202, "sentence_idx": 579, "label": "unlabeled"}, {"type": "text", "expr": " Due to space constraints, we restrict our study to the representative target domains, Redwood depth and ShapeNet", "word_idx": 43271, "sentence_idx": 580, "label": "unlabeled"}, {"type": "text", "expr": "We present ablation studies to justify each component of our approach", "word_idx": 43384, "sentence_idx": 581, "label": "unlabeled"}, {"type": "text", "expr": " Due to space constraints, we restrict our study to the representative target domains, Redwood depth and ShapeNet", "word_idx": 43453, "sentence_idx": 582, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:  Baseline & Ablation Study", "word_idx": 43566, "sentence_idx": 583, "label": "unlabeled"}, {"type": "text", "expr": "  Comparisons between our approach with alternative approaches on Redwood depth Dataset\u00a0 ", "word_idx": 43602, "sentence_idx": 584, "label": "unlabeled"}, {"type": "text", "expr": " The Figure shows Percentage of Correct Keypoints (PCK) under a threshold", "word_idx": 43691, "sentence_idx": 585, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:", "word_idx": 43764, "sentence_idx": 586, "label": "unlabeled"}, {"type": "text", "expr": "Baseline & Ablation Study", "word_idx": 43773, "sentence_idx": 587, "label": "unlabeled"}, {"type": "text", "expr": "label Figure:Ablation:Study", "word_idx": 43798, "sentence_idx": 588, "label": "unlabeled"}, {"type": "text", "expr": "label", "word_idx": 43825, "sentence_idx": 589, "label": "unlabeled"}, {"type": "text", "expr": "Dropping the view-consistency term", "word_idx": 43830, "sentence_idx": 590, "label": "unlabeled"}, {"type": "text", "expr": " \nWe test the effects of dropping the view-consistency term", "word_idx": 43864, "sentence_idx": 591, "label": "unlabeled"}, {"type": "text", "expr": "\nIn this case, we simply align the output from all the depth scans with annotations of the source domain", "word_idx": 43923, "sentence_idx": 592, "label": "unlabeled"}, {"type": "text", "expr": "\nAs shown in Table\u00a0 LABEL:table:ablation  and Figure\u00a0 LABEL:Figure:Ablation:Study , the performance drops considerably compared to our full term, while is still better than without adaptation", "word_idx": 44027, "sentence_idx": 593, "label": "unlabeled"}, {"type": "text", "expr": "Dropping the view-consistency term", "word_idx": 44218, "sentence_idx": 594, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:table:ablation", "word_idx": 44252, "sentence_idx": 595, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Figure:Ablation:Study", "word_idx": 44272, "sentence_idx": 596, "label": "unlabeled"}, {"type": "text", "expr": "Dropping the alignment term", "word_idx": 44299, "sentence_idx": 597, "label": "unlabeled"}, {"type": "text", "expr": " \nMerely utilizing the view consistency term can significantly reduce the testing error", "word_idx": 44326, "sentence_idx": 598, "label": "unlabeled"}, {"type": "text", "expr": "\nIt can be interpreted as the network updating the latent variables in a self-guided manner, based on the consistency between different views", "word_idx": 44413, "sentence_idx": 599, "label": "unlabeled"}, {"type": "text", "expr": "\nIf the predictions on the majority of views are consistent with one another, the keypoint configuration obtained by averaging all the predictions can serve as a reliable guidance to correct the bad outliers", "word_idx": 44554, "sentence_idx": 600, "label": "unlabeled"}, {"type": "text", "expr": "Dropping the alignment term", "word_idx": 44761, "sentence_idx": 601, "label": "unlabeled"}, {"type": "text", "expr": "Latent configuration updates versus re-initialization \nInstead of updating the latent configurations  $M_{i}$  by solving Eq", "word_idx": 44788, "sentence_idx": 602, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 LABEL:Mi:OPT , we can apply Eq", "word_idx": 44912, "sentence_idx": 603, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 LABEL:Eq:M:init  to re-initialize the latent configurations, which is also consistent with our training framework", "word_idx": 44944, "sentence_idx": 604, "label": "unlabeled"}, {"type": "text", "expr": "\nThe results is worse than updating  $M_{i}$  by minimizing the view-consistency term,\nshowing an advantage of our alternating minimization schema", "word_idx": 45059, "sentence_idx": 605, "label": "unlabeled"}, {"type": "text", "expr": "Latent configuration updates versus re-initialization", "word_idx": 45205, "sentence_idx": 606, "label": "unlabeled"}, {"type": "math", "expr": "$$M_{i}$$", "word_idx": 45258, "sentence_idx": 607, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Mi:OPT", "word_idx": 45263, "sentence_idx": 608, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Eq:M:init", "word_idx": 45275, "sentence_idx": 609, "label": "unlabeled"}, {"type": "math", "expr": "$$M_{i}$$", "word_idx": 45290, "sentence_idx": 610, "label": "unlabeled"}, {"type": "text", "expr": "4  Extension to RGB images", "word_idx": 45295, "sentence_idx": 611, "label": "unlabeled"}, {"type": "text", "expr": "label Subsection:Experimental:RGB\nOur framework can seamlessly be applied in domain adaptation to RGB images", "word_idx": 45321, "sentence_idx": 612, "label": "unlabeled"}, {"type": "text", "expr": "\nWe show our preliminary results on Tab", "word_idx": 45429, "sentence_idx": 613, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 LABEL:table:baselines , and it is evident that our proposed method is able to reduce the AE from the baseline without domain adaptation", "word_idx": 45468, "sentence_idx": 614, "label": "unlabeled"}, {"type": "text", "expr": "\nAs shown in Fig", "word_idx": 45605, "sentence_idx": 615, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 LABEL:Figure:Results , our method helps regularize the output when the before-adaptation baseline predicts a seemingly random point set", "word_idx": 45621, "sentence_idx": 616, "label": "unlabeled"}, {"type": "text", "expr": "label", "word_idx": 45758, "sentence_idx": 617, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:table:baselines", "word_idx": 45763, "sentence_idx": 618, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:Figure:Results", "word_idx": 45784, "sentence_idx": 619, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a05:  Qualitative results", "word_idx": 45804, "sentence_idx": 620, "label": "unlabeled"}, {"type": "text", "expr": "  We compare 3D keypoint predictions (blue) before (left) and after (right) our unsupervised domain adaptation on different datasets", "word_idx": 45834, "sentence_idx": 621, "label": "unlabeled"}, {"type": "text", "expr": " For each model we show 2 views", "word_idx": 45966, "sentence_idx": 622, "label": "unlabeled"}, {"type": "text", "expr": " Reference ground-truth are in red", "word_idx": 45997, "sentence_idx": 623, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a05:", "word_idx": 46031, "sentence_idx": 624, "label": "unlabeled"}, {"type": "text", "expr": "Qualitative results", "word_idx": 46040, "sentence_idx": 625, "label": "unlabeled"}, {"type": "text", "expr": "label Figure:Results", "word_idx": 46059, "sentence_idx": 626, "label": "unlabeled"}, {"type": "text", "expr": "label", "word_idx": 46079, "sentence_idx": 627, "label": "unlabeled"}, {"type": "text", "expr": "6  Conclusions and Future Work", "word_idx": 46084, "sentence_idx": 628, "label": "unlabeled"}, {"type": "text", "expr": "label Section:Conclusions\nIn this paper, we introduced an unsupervised domain adaptation approach for keypoint prediction from a single depth image", "word_idx": 46114, "sentence_idx": 629, "label": "unlabeled"}, {"type": "text", "expr": " Our approach combines two task-specific regularizations, i", "word_idx": 46261, "sentence_idx": 630, "label": "unlabeled"}, {"type": "text", "expr": ", view-consistency and label distributions alignment of the source and target domains", "word_idx": 46320, "sentence_idx": 631, "label": "unlabeled"}, {"type": "text", "expr": " Experimental results show that our approach is significantly better than without domain adaptation and is superior to state-of-the-art generic domain adaptation methods", "word_idx": 46405, "sentence_idx": 632, "label": "unlabeled"}, {"type": "text", "expr": "label", "word_idx": 46574, "sentence_idx": 633, "label": "unlabeled"}, {"type": "text", "expr": "References", "word_idx": 46579, "sentence_idx": 634, "label": "unlabeled"}, {"type": "text", "expr": " Bezdek and R", "word_idx": 46589, "sentence_idx": 635, "label": "unlabeled"}, {"type": "text", "expr": " Hathaway", "word_idx": 46602, "sentence_idx": 636, "label": "unlabeled"}, {"type": "text", "expr": " Bezdek and R", "word_idx": 46611, "sentence_idx": 637, "label": "unlabeled"}, {"type": "text", "expr": " Hathaway", "word_idx": 46624, "sentence_idx": 638, "label": "unlabeled"}, {"type": "text", "expr": "Convergence of alternating optimization", "word_idx": 46633, "sentence_idx": 639, "label": "unlabeled"}, {"type": "text", "expr": "Convergence of alternating optimization", "word_idx": 46672, "sentence_idx": 640, "label": "unlabeled"}, {"type": "text", "expr": "Neural, Parallel Sci", "word_idx": 46711, "sentence_idx": 641, "label": "unlabeled"}, {"type": "text", "expr": " Comput", "word_idx": 46731, "sentence_idx": 642, "label": "unlabeled"}, {"type": "text", "expr": " , 11(4):351\u2013368, Dec", "word_idx": 46738, "sentence_idx": 643, "label": "unlabeled"}, {"type": "text", "expr": " 2003", "word_idx": 46759, "sentence_idx": 644, "label": "unlabeled"}, {"type": "text", "expr": "Neural, Parallel Sci", "word_idx": 46764, "sentence_idx": 645, "label": "unlabeled"}, {"type": "text", "expr": " Comput", "word_idx": 46784, "sentence_idx": 646, "label": "unlabeled"}, {"type": "text", "expr": ", 11(4):351\u2013368, Dec", "word_idx": 46791, "sentence_idx": 647, "label": "unlabeled"}, {"type": "text", "expr": " 2003", "word_idx": 46811, "sentence_idx": 648, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bousmalis, N", "word_idx": 46816, "sentence_idx": 649, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Silberman, D", "word_idx": 46829, "sentence_idx": 650, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dohan, D", "word_idx": 46842, "sentence_idx": 651, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Erhan, and D", "word_idx": 46851, "sentence_idx": 652, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Krishnan", "word_idx": 46864, "sentence_idx": 653, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bousmalis, N", "word_idx": 46873, "sentence_idx": 654, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Silberman, D", "word_idx": 46886, "sentence_idx": 655, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dohan, D", "word_idx": 46899, "sentence_idx": 656, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Erhan, and D", "word_idx": 46908, "sentence_idx": 657, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Krishnan", "word_idx": 46921, "sentence_idx": 658, "label": "unlabeled"}, {"type": "text", "expr": "Unsupervised pixel-level domain adaptation with generative\nadversarial networks", "word_idx": 46930, "sentence_idx": 659, "label": "unlabeled"}, {"type": "text", "expr": "Unsupervised pixel-level domain adaptation with generative\nadversarial networks", "word_idx": 47009, "sentence_idx": 660, "label": "unlabeled"}, {"type": "text", "expr": "In  The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , July 2017", "word_idx": 47088, "sentence_idx": 661, "label": "unlabeled"}, {"type": "text", "expr": "The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR)", "word_idx": 47173, "sentence_idx": 662, "label": "unlabeled"}, {"type": "text", "expr": ", July 2017", "word_idx": 47242, "sentence_idx": 663, "label": "unlabeled"}, {"type": "text", "expr": " Chang, T", "word_idx": 47253, "sentence_idx": 664, "label": "unlabeled"}, {"type": "text", "expr": " Funkhouser, L", "word_idx": 47262, "sentence_idx": 665, "label": "unlabeled"}, {"type": "text", "expr": " Guibas, P", "word_idx": 47276, "sentence_idx": 666, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hanrahan, Q", "word_idx": 47286, "sentence_idx": 667, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Huang, Z", "word_idx": 47298, "sentence_idx": 668, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Li,\nS", "word_idx": 47307, "sentence_idx": 669, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Savarese, M", "word_idx": 47313, "sentence_idx": 670, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Savva, S", "word_idx": 47325, "sentence_idx": 671, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Song, H", "word_idx": 47334, "sentence_idx": 672, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Su, J", "word_idx": 47342, "sentence_idx": 673, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xiao, L", "word_idx": 47348, "sentence_idx": 674, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yi, and F", "word_idx": 47356, "sentence_idx": 675, "label": "unlabeled"}, {"type": "text", "expr": " Chang, T", "word_idx": 47366, "sentence_idx": 676, "label": "unlabeled"}, {"type": "text", "expr": " Funkhouser, L", "word_idx": 47375, "sentence_idx": 677, "label": "unlabeled"}, {"type": "text", "expr": " Guibas, P", "word_idx": 47389, "sentence_idx": 678, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hanrahan, Q", "word_idx": 47399, "sentence_idx": 679, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Huang, Z", "word_idx": 47411, "sentence_idx": 680, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Li,\nS", "word_idx": 47420, "sentence_idx": 681, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Savarese, M", "word_idx": 47426, "sentence_idx": 682, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Savva, S", "word_idx": 47438, "sentence_idx": 683, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Song, H", "word_idx": 47447, "sentence_idx": 684, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Su, J", "word_idx": 47455, "sentence_idx": 685, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xiao, L", "word_idx": 47461, "sentence_idx": 686, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yi, and F", "word_idx": 47469, "sentence_idx": 687, "label": "unlabeled"}, {"type": "text", "expr": "Shapenet: An information-rich 3d model repository", "word_idx": 47479, "sentence_idx": 688, "label": "unlabeled"}, {"type": "text", "expr": "Shapenet: An information-rich 3d model repository", "word_idx": 47528, "sentence_idx": 689, "label": "unlabeled"}, {"type": "text", "expr": "CoRR , abs/1512", "word_idx": 47577, "sentence_idx": 690, "label": "unlabeled"}, {"type": "text", "expr": "03012, 2015", "word_idx": 47592, "sentence_idx": 691, "label": "unlabeled"}, {"type": "text", "expr": ", abs/1512", "word_idx": 47603, "sentence_idx": 692, "label": "unlabeled"}, {"type": "text", "expr": "03012, 2015", "word_idx": 47613, "sentence_idx": 693, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Choi, Q", "word_idx": 47624, "sentence_idx": 694, "label": "unlabeled"}, {"type": "text", "expr": " Zhou, S", "word_idx": 47632, "sentence_idx": 695, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Miller, and V", "word_idx": 47640, "sentence_idx": 696, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Koltun", "word_idx": 47654, "sentence_idx": 697, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Choi, Q", "word_idx": 47661, "sentence_idx": 698, "label": "unlabeled"}, {"type": "text", "expr": " Zhou, S", "word_idx": 47669, "sentence_idx": 699, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Miller, and V", "word_idx": 47677, "sentence_idx": 700, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Koltun", "word_idx": 47691, "sentence_idx": 701, "label": "unlabeled"}, {"type": "text", "expr": "A large dataset of object scans", "word_idx": 47698, "sentence_idx": 702, "label": "unlabeled"}, {"type": "text", "expr": "A large dataset of object scans", "word_idx": 47729, "sentence_idx": 703, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1602", "word_idx": 47760, "sentence_idx": 704, "label": "unlabeled"}, {"type": "text", "expr": "02481 , 2016", "word_idx": 47770, "sentence_idx": 705, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1602", "word_idx": 47782, "sentence_idx": 706, "label": "unlabeled"}, {"type": "text", "expr": "02481", "word_idx": 47792, "sentence_idx": 707, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 47797, "sentence_idx": 708, "label": "unlabeled"}, {"type": "text", "expr": " Choy, D", "word_idx": 47803, "sentence_idx": 709, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xu, J", "word_idx": 47811, "sentence_idx": 710, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gwak, K", "word_idx": 47817, "sentence_idx": 711, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chen, and S", "word_idx": 47825, "sentence_idx": 712, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Savarese", "word_idx": 47837, "sentence_idx": 713, "label": "unlabeled"}, {"type": "text", "expr": " Choy, D", "word_idx": 47846, "sentence_idx": 714, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xu, J", "word_idx": 47854, "sentence_idx": 715, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gwak, K", "word_idx": 47860, "sentence_idx": 716, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chen, and S", "word_idx": 47868, "sentence_idx": 717, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Savarese", "word_idx": 47880, "sentence_idx": 718, "label": "unlabeled"}, {"type": "text", "expr": "3d-r2n2: A unified approach for single and multi-view 3d object\nreconstruction", "word_idx": 47889, "sentence_idx": 719, "label": "unlabeled"}, {"type": "text", "expr": "3d-r2n2: A unified approach for single and multi-view 3d object\nreconstruction", "word_idx": 47967, "sentence_idx": 720, "label": "unlabeled"}, {"type": "text", "expr": "In  Proceedings of the European Conference on Computer Vision\n(ECCV) , 2016", "word_idx": 48045, "sentence_idx": 721, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the European Conference on Computer Vision\n(ECCV)", "word_idx": 48120, "sentence_idx": 722, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 48184, "sentence_idx": 723, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Cignoni, M", "word_idx": 48190, "sentence_idx": 724, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Callieri, M", "word_idx": 48201, "sentence_idx": 725, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Corsini, M", "word_idx": 48213, "sentence_idx": 726, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dellepiane, F", "word_idx": 48224, "sentence_idx": 727, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ganovelli, and\nG", "word_idx": 48238, "sentence_idx": 728, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ranzuglia", "word_idx": 48255, "sentence_idx": 729, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Cignoni, M", "word_idx": 48265, "sentence_idx": 730, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Callieri, M", "word_idx": 48276, "sentence_idx": 731, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Corsini, M", "word_idx": 48288, "sentence_idx": 732, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dellepiane, F", "word_idx": 48299, "sentence_idx": 733, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ganovelli, and\nG", "word_idx": 48313, "sentence_idx": 734, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ranzuglia", "word_idx": 48330, "sentence_idx": 735, "label": "unlabeled"}, {"type": "text", "expr": "Meshlab: an open-source mesh processing tool", "word_idx": 48340, "sentence_idx": 736, "label": "unlabeled"}, {"type": "text", "expr": "Meshlab: an open-source mesh processing tool", "word_idx": 48384, "sentence_idx": 737, "label": "unlabeled"}, {"type": "text", "expr": "In  Eurographics Italian Chapter Conference , volume 2008, pages\n129\u2013136, 2008", "word_idx": 48428, "sentence_idx": 738, "label": "unlabeled"}, {"type": "text", "expr": "Eurographics Italian Chapter Conference", "word_idx": 48506, "sentence_idx": 739, "label": "unlabeled"}, {"type": "text", "expr": ", volume 2008, pages\n129\u2013136, 2008", "word_idx": 48545, "sentence_idx": 740, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Csurka", "word_idx": 48579, "sentence_idx": 741, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Csurka", "word_idx": 48586, "sentence_idx": 742, "label": "unlabeled"}, {"type": "text", "expr": "Domain adaptation for visual applications: A comprehensive survey", "word_idx": 48593, "sentence_idx": 743, "label": "unlabeled"}, {"type": "text", "expr": "Domain adaptation for visual applications: A comprehensive survey", "word_idx": 48658, "sentence_idx": 744, "label": "unlabeled"}, {"type": "text", "expr": "CoRR , abs/1702", "word_idx": 48723, "sentence_idx": 745, "label": "unlabeled"}, {"type": "text", "expr": "05374, 2017", "word_idx": 48738, "sentence_idx": 746, "label": "unlabeled"}, {"type": "text", "expr": ", abs/1702", "word_idx": 48749, "sentence_idx": 747, "label": "unlabeled"}, {"type": "text", "expr": "05374, 2017", "word_idx": 48759, "sentence_idx": 748, "label": "unlabeled"}, {"type": "text", "expr": " Fish\u00a0Tung, A", "word_idx": 48770, "sentence_idx": 749, "label": "unlabeled"}, {"type": "text", "expr": " Harley, W", "word_idx": 48783, "sentence_idx": 750, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Seto, and K", "word_idx": 48793, "sentence_idx": 751, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fragkiadaki", "word_idx": 48805, "sentence_idx": 752, "label": "unlabeled"}, {"type": "text", "expr": " Fish\u00a0Tung, A", "word_idx": 48817, "sentence_idx": 753, "label": "unlabeled"}, {"type": "text", "expr": " Harley, W", "word_idx": 48830, "sentence_idx": 754, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Seto, and K", "word_idx": 48840, "sentence_idx": 755, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fragkiadaki", "word_idx": 48852, "sentence_idx": 756, "label": "unlabeled"}, {"type": "text", "expr": "Adversarial inverse graphics networks: Learning 2d-to-3d lifting and\nimage-to-image translation from unpaired supervision", "word_idx": 48864, "sentence_idx": 757, "label": "unlabeled"}, {"type": "text", "expr": "Adversarial inverse graphics networks: Learning 2d-to-3d lifting and\nimage-to-image translation from unpaired supervision", "word_idx": 48985, "sentence_idx": 758, "label": "unlabeled"}, {"type": "text", "expr": "In  The IEEE International Conference on Computer Vision (ICCV) ,\n Oct 2017", "word_idx": 49106, "sentence_idx": 759, "label": "unlabeled"}, {"type": "text", "expr": "The IEEE International Conference on Computer Vision (ICCV)", "word_idx": 49181, "sentence_idx": 760, "label": "unlabeled"}, {"type": "text", "expr": "Oct 2017", "word_idx": 49240, "sentence_idx": 761, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gebru, J", "word_idx": 49248, "sentence_idx": 762, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hoffman, and L", "word_idx": 49257, "sentence_idx": 763, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fei-Fei", "word_idx": 49272, "sentence_idx": 764, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gebru, J", "word_idx": 49280, "sentence_idx": 765, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hoffman, and L", "word_idx": 49289, "sentence_idx": 766, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fei-Fei", "word_idx": 49304, "sentence_idx": 767, "label": "unlabeled"}, {"type": "text", "expr": "Fine-grained recognition in the wild: A multi-task domain adaptation\napproach", "word_idx": 49312, "sentence_idx": 768, "label": "unlabeled"}, {"type": "text", "expr": "Fine-grained recognition in the wild: A multi-task domain adaptation\napproach", "word_idx": 49389, "sentence_idx": 769, "label": "unlabeled"}, {"type": "text", "expr": "In  The IEEE International Conference on Computer Vision (ICCV) ,\nOct 2017", "word_idx": 49466, "sentence_idx": 770, "label": "unlabeled"}, {"type": "text", "expr": "The IEEE International Conference on Computer Vision (ICCV)", "word_idx": 49540, "sentence_idx": 771, "label": "unlabeled"}, {"type": "text", "expr": ",\nOct 2017", "word_idx": 49599, "sentence_idx": 772, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gholami, O", "word_idx": 49609, "sentence_idx": 773, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0(Oggi)\u00a0Rudovic, and V", "word_idx": 49620, "sentence_idx": 774, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Pavlovic", "word_idx": 49642, "sentence_idx": 775, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gholami, O", "word_idx": 49651, "sentence_idx": 776, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0(Oggi)\u00a0Rudovic, and V", "word_idx": 49662, "sentence_idx": 777, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Pavlovic", "word_idx": 49684, "sentence_idx": 778, "label": "unlabeled"}, {"type": "text", "expr": "Punda: Probabilistic unsupervised domain adaptation for knowledge\ntransfer across visual categories", "word_idx": 49693, "sentence_idx": 779, "label": "unlabeled"}, {"type": "text", "expr": "Punda: Probabilistic unsupervised domain adaptation for knowledge\ntransfer across visual categories", "word_idx": 49792, "sentence_idx": 780, "label": "unlabeled"}, {"type": "text", "expr": "In  The IEEE International Conference on Computer Vision (ICCV) ,\nOct 2017", "word_idx": 49891, "sentence_idx": 781, "label": "unlabeled"}, {"type": "text", "expr": "The IEEE International Conference on Computer Vision (ICCV)", "word_idx": 49965, "sentence_idx": 782, "label": "unlabeled"}, {"type": "text", "expr": ",\nOct 2017", "word_idx": 50024, "sentence_idx": 783, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gupta, P", "word_idx": 50034, "sentence_idx": 784, "label": "unlabeled"}, {"type": "text", "expr": " Arbel\u00e1ez, R", "word_idx": 50043, "sentence_idx": 785, "label": "unlabeled"}, {"type": "text", "expr": " Girshick, and J", "word_idx": 50055, "sentence_idx": 786, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Malik", "word_idx": 50071, "sentence_idx": 787, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gupta, P", "word_idx": 50077, "sentence_idx": 788, "label": "unlabeled"}, {"type": "text", "expr": " Arbel\u00e1ez, R", "word_idx": 50086, "sentence_idx": 789, "label": "unlabeled"}, {"type": "text", "expr": " Girshick, and J", "word_idx": 50098, "sentence_idx": 790, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Malik", "word_idx": 50114, "sentence_idx": 791, "label": "unlabeled"}, {"type": "text", "expr": "Aligning 3D models to RGB-D images of cluttered scenes", "word_idx": 50120, "sentence_idx": 792, "label": "unlabeled"}, {"type": "text", "expr": "Aligning 3D models to RGB-D images of cluttered scenes", "word_idx": 50174, "sentence_idx": 793, "label": "unlabeled"}, {"type": "text", "expr": "In  Computer Vision and Pattern Recognition (CVPR) , 2015", "word_idx": 50228, "sentence_idx": 794, "label": "unlabeled"}, {"type": "text", "expr": "Computer Vision and Pattern Recognition (CVPR)", "word_idx": 50285, "sentence_idx": 795, "label": "unlabeled"}, {"type": "text", "expr": ", 2015", "word_idx": 50331, "sentence_idx": 796, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, X", "word_idx": 50337, "sentence_idx": 797, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang, S", "word_idx": 50343, "sentence_idx": 798, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ren, and J", "word_idx": 50352, "sentence_idx": 799, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, X", "word_idx": 50363, "sentence_idx": 800, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang, S", "word_idx": 50369, "sentence_idx": 801, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ren, and J", "word_idx": 50378, "sentence_idx": 802, "label": "unlabeled"}, {"type": "text", "expr": "Deep residual learning for image recognition", "word_idx": 50389, "sentence_idx": 803, "label": "unlabeled"}, {"type": "text", "expr": "Deep residual learning for image recognition", "word_idx": 50433, "sentence_idx": 804, "label": "unlabeled"}, {"type": "text", "expr": "In  Proceedings of the IEEE conference on computer vision and\npattern recognition , pages 770\u2013778, 2016", "word_idx": 50477, "sentence_idx": 805, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the IEEE conference on computer vision and\npattern recognition", "word_idx": 50580, "sentence_idx": 806, "label": "unlabeled"}, {"type": "text", "expr": ", pages 770\u2013778, 2016", "word_idx": 50657, "sentence_idx": 807, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Herath, M", "word_idx": 50678, "sentence_idx": 808, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Harandi, and F", "word_idx": 50688, "sentence_idx": 809, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Porikli", "word_idx": 50703, "sentence_idx": 810, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Herath, M", "word_idx": 50711, "sentence_idx": 811, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Harandi, and F", "word_idx": 50721, "sentence_idx": 812, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Porikli", "word_idx": 50736, "sentence_idx": 813, "label": "unlabeled"}, {"type": "text", "expr": "Learning an invariant hilbert space for domain adaptation", "word_idx": 50744, "sentence_idx": 814, "label": "unlabeled"}, {"type": "text", "expr": "Learning an invariant hilbert space for domain adaptation", "word_idx": 50801, "sentence_idx": 815, "label": "unlabeled"}, {"type": "text", "expr": "In  The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , July 2017", "word_idx": 50858, "sentence_idx": 816, "label": "unlabeled"}, {"type": "text", "expr": "The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR)", "word_idx": 50943, "sentence_idx": 817, "label": "unlabeled"}, {"type": "text", "expr": ", July 2017", "word_idx": 51012, "sentence_idx": 818, "label": "unlabeled"}, {"type": "text", "expr": " Horn", "word_idx": 51023, "sentence_idx": 819, "label": "unlabeled"}, {"type": "text", "expr": " Horn", "word_idx": 51028, "sentence_idx": 820, "label": "unlabeled"}, {"type": "text", "expr": "Closed-form solution of absolute orientation using unit quaternions", "word_idx": 51033, "sentence_idx": 821, "label": "unlabeled"}, {"type": "text", "expr": "Closed-form solution of absolute orientation using unit quaternions", "word_idx": 51100, "sentence_idx": 822, "label": "unlabeled"}, {"type": "text", "expr": "Journal of the Optical Society of America A , 4(4):629\u2013642,\n1987", "word_idx": 51167, "sentence_idx": 823, "label": "unlabeled"}, {"type": "text", "expr": "Journal of the Optical Society of America A", "word_idx": 51231, "sentence_idx": 824, "label": "unlabeled"}, {"type": "text", "expr": ", 4(4):629\u2013642,\n1987", "word_idx": 51274, "sentence_idx": 825, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kalogerakis, M", "word_idx": 51294, "sentence_idx": 826, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Averkiou, S", "word_idx": 51309, "sentence_idx": 827, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Maji, and S", "word_idx": 51321, "sentence_idx": 828, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chaudhuri", "word_idx": 51333, "sentence_idx": 829, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kalogerakis, M", "word_idx": 51343, "sentence_idx": 830, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Averkiou, S", "word_idx": 51358, "sentence_idx": 831, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Maji, and S", "word_idx": 51370, "sentence_idx": 832, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chaudhuri", "word_idx": 51382, "sentence_idx": 833, "label": "unlabeled"}, {"type": "text", "expr": "3d shape segmentation with projective convolutional networks", "word_idx": 51392, "sentence_idx": 834, "label": "unlabeled"}, {"type": "text", "expr": "3d shape segmentation with projective convolutional networks", "word_idx": 51452, "sentence_idx": 835, "label": "unlabeled"}, {"type": "text", "expr": "CoRR , abs/1612", "word_idx": 51512, "sentence_idx": 836, "label": "unlabeled"}, {"type": "text", "expr": "02808, 2016", "word_idx": 51527, "sentence_idx": 837, "label": "unlabeled"}, {"type": "text", "expr": ", abs/1612", "word_idx": 51538, "sentence_idx": 838, "label": "unlabeled"}, {"type": "text", "expr": "02808, 2016", "word_idx": 51548, "sentence_idx": 839, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Koniusz, Y", "word_idx": 51559, "sentence_idx": 840, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Tas, and F", "word_idx": 51570, "sentence_idx": 841, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Porikli", "word_idx": 51581, "sentence_idx": 842, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Koniusz, Y", "word_idx": 51589, "sentence_idx": 843, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Tas, and F", "word_idx": 51600, "sentence_idx": 844, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Porikli", "word_idx": 51611, "sentence_idx": 845, "label": "unlabeled"}, {"type": "text", "expr": "Domain adaptation by mixture of alignments of second- or higher-order\nscatter tensors", "word_idx": 51619, "sentence_idx": 846, "label": "unlabeled"}, {"type": "text", "expr": "Domain adaptation by mixture of alignments of second- or higher-order\nscatter tensors", "word_idx": 51704, "sentence_idx": 847, "label": "unlabeled"}, {"type": "text", "expr": "In  The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , July 2017", "word_idx": 51789, "sentence_idx": 848, "label": "unlabeled"}, {"type": "text", "expr": "The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR)", "word_idx": 51874, "sentence_idx": 849, "label": "unlabeled"}, {"type": "text", "expr": ", July 2017", "word_idx": 51943, "sentence_idx": 850, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Li, A", "word_idx": 51954, "sentence_idx": 851, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dai, L", "word_idx": 51960, "sentence_idx": 852, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Guibas, and M", "word_idx": 51967, "sentence_idx": 853, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Nie\u00dfner", "word_idx": 51981, "sentence_idx": 854, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Li, A", "word_idx": 51989, "sentence_idx": 855, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dai, L", "word_idx": 51995, "sentence_idx": 856, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Guibas, and M", "word_idx": 52002, "sentence_idx": 857, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Nie\u00dfner", "word_idx": 52016, "sentence_idx": 858, "label": "unlabeled"}, {"type": "text", "expr": "Database-assisted object retrieval for real-time 3d reconstruction", "word_idx": 52024, "sentence_idx": 859, "label": "unlabeled"}, {"type": "text", "expr": "Database-assisted object retrieval for real-time 3d reconstruction", "word_idx": 52090, "sentence_idx": 860, "label": "unlabeled"}, {"type": "text", "expr": "In  Computer Graphics Forum , volume\u00a034", "word_idx": 52156, "sentence_idx": 861, "label": "unlabeled"}, {"type": "text", "expr": " Wiley Online Library,\n2015", "word_idx": 52195, "sentence_idx": 862, "label": "unlabeled"}, {"type": "text", "expr": "Computer Graphics Forum", "word_idx": 52222, "sentence_idx": 863, "label": "unlabeled"}, {"type": "text", "expr": ", volume\u00a034", "word_idx": 52245, "sentence_idx": 864, "label": "unlabeled"}, {"type": "text", "expr": " Wiley Online Library,\n2015", "word_idx": 52256, "sentence_idx": 865, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Maria\u00a0Carlucci, L", "word_idx": 52283, "sentence_idx": 866, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Porzi, B", "word_idx": 52301, "sentence_idx": 867, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Caputo, E", "word_idx": 52310, "sentence_idx": 868, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ricci, and S", "word_idx": 52320, "sentence_idx": 869, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rota\u00a0Bulo", "word_idx": 52333, "sentence_idx": 870, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Maria\u00a0Carlucci, L", "word_idx": 52343, "sentence_idx": 871, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Porzi, B", "word_idx": 52361, "sentence_idx": 872, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Caputo, E", "word_idx": 52370, "sentence_idx": 873, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ricci, and S", "word_idx": 52380, "sentence_idx": 874, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rota\u00a0Bulo", "word_idx": 52393, "sentence_idx": 875, "label": "unlabeled"}, {"type": "text", "expr": "Autodial: Automatic domain alignment layers", "word_idx": 52403, "sentence_idx": 876, "label": "unlabeled"}, {"type": "text", "expr": "Autodial: Automatic domain alignment layers", "word_idx": 52446, "sentence_idx": 877, "label": "unlabeled"}, {"type": "text", "expr": "In  The IEEE International Conference on Computer Vision (ICCV) ,\nOct 2017", "word_idx": 52489, "sentence_idx": 878, "label": "unlabeled"}, {"type": "text", "expr": "The IEEE International Conference on Computer Vision (ICCV)", "word_idx": 52563, "sentence_idx": 879, "label": "unlabeled"}, {"type": "text", "expr": ",\nOct 2017", "word_idx": 52622, "sentence_idx": 880, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Newell, K", "word_idx": 52632, "sentence_idx": 881, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yang, and J", "word_idx": 52642, "sentence_idx": 882, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Deng", "word_idx": 52654, "sentence_idx": 883, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Newell, K", "word_idx": 52659, "sentence_idx": 884, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yang, and J", "word_idx": 52669, "sentence_idx": 885, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Deng", "word_idx": 52681, "sentence_idx": 886, "label": "unlabeled"}, {"type": "text", "expr": "Stacked hourglass networks for human pose estimation", "word_idx": 52686, "sentence_idx": 887, "label": "unlabeled"}, {"type": "text", "expr": "Stacked hourglass networks for human pose estimation", "word_idx": 52738, "sentence_idx": 888, "label": "unlabeled"}, {"type": "text", "expr": "In  ECCV (8) , volume 9912 of  Lecture Notes in Computer\nScience , pages 483\u2013499", "word_idx": 52790, "sentence_idx": 889, "label": "unlabeled"}, {"type": "text", "expr": " Springer, 2016", "word_idx": 52870, "sentence_idx": 890, "label": "unlabeled"}, {"type": "text", "expr": "ECCV (8)", "word_idx": 52885, "sentence_idx": 891, "label": "unlabeled"}, {"type": "text", "expr": ", volume 9912 of", "word_idx": 52893, "sentence_idx": 892, "label": "unlabeled"}, {"type": "text", "expr": "Lecture Notes in Computer\nScience", "word_idx": 52909, "sentence_idx": 893, "label": "unlabeled"}, {"type": "text", "expr": ", pages 483\u2013499", "word_idx": 52942, "sentence_idx": 894, "label": "unlabeled"}, {"type": "text", "expr": " Springer, 2016", "word_idx": 52957, "sentence_idx": 895, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Panareda\u00a0Busto and J", "word_idx": 52972, "sentence_idx": 896, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gall", "word_idx": 52993, "sentence_idx": 897, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Panareda\u00a0Busto and J", "word_idx": 52998, "sentence_idx": 898, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gall", "word_idx": 53019, "sentence_idx": 899, "label": "unlabeled"}, {"type": "text", "expr": "Open set domain adaptation", "word_idx": 53024, "sentence_idx": 900, "label": "unlabeled"}, {"type": "text", "expr": "Open set domain adaptation", "word_idx": 53050, "sentence_idx": 901, "label": "unlabeled"}, {"type": "text", "expr": "In  The IEEE International Conference on Computer Vision (ICCV) ,\nOct 2017", "word_idx": 53076, "sentence_idx": 902, "label": "unlabeled"}, {"type": "text", "expr": "The IEEE International Conference on Computer Vision (ICCV)", "word_idx": 53150, "sentence_idx": 903, "label": "unlabeled"}, {"type": "text", "expr": ",\nOct 2017", "word_idx": 53209, "sentence_idx": 904, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Peng, B", "word_idx": 53219, "sentence_idx": 905, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sun, K", "word_idx": 53227, "sentence_idx": 906, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ali, and K", "word_idx": 53234, "sentence_idx": 907, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Saenko", "word_idx": 53245, "sentence_idx": 908, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Peng, B", "word_idx": 53252, "sentence_idx": 909, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sun, K", "word_idx": 53260, "sentence_idx": 910, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ali, and K", "word_idx": 53267, "sentence_idx": 911, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Saenko", "word_idx": 53278, "sentence_idx": 912, "label": "unlabeled"}, {"type": "text", "expr": "Learning deep object detectors from 3d models", "word_idx": 53285, "sentence_idx": 913, "label": "unlabeled"}, {"type": "text", "expr": "Learning deep object detectors from 3d models", "word_idx": 53330, "sentence_idx": 914, "label": "unlabeled"}, {"type": "text", "expr": "In  ICCV , pages 1278\u20131286", "word_idx": 53375, "sentence_idx": 915, "label": "unlabeled"}, {"type": "text", "expr": " IEEE Computer Society, 2015", "word_idx": 53401, "sentence_idx": 916, "label": "unlabeled"}, {"type": "text", "expr": ", pages 1278\u20131286", "word_idx": 53429, "sentence_idx": 917, "label": "unlabeled"}, {"type": "text", "expr": " IEEE Computer Society, 2015", "word_idx": 53446, "sentence_idx": 918, "label": "unlabeled"}, {"type": "text", "expr": " Qi, H", "word_idx": 53474, "sentence_idx": 919, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Su, M", "word_idx": 53480, "sentence_idx": 920, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Nie\u00dfner, A", "word_idx": 53486, "sentence_idx": 921, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dai, M", "word_idx": 53497, "sentence_idx": 922, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yan, and L", "word_idx": 53504, "sentence_idx": 923, "label": "unlabeled"}, {"type": "text", "expr": " Guibas", "word_idx": 53515, "sentence_idx": 924, "label": "unlabeled"}, {"type": "text", "expr": " Qi, H", "word_idx": 53522, "sentence_idx": 925, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Su, M", "word_idx": 53528, "sentence_idx": 926, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Nie\u00dfner, A", "word_idx": 53534, "sentence_idx": 927, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dai, M", "word_idx": 53545, "sentence_idx": 928, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yan, and L", "word_idx": 53552, "sentence_idx": 929, "label": "unlabeled"}, {"type": "text", "expr": " Guibas", "word_idx": 53563, "sentence_idx": 930, "label": "unlabeled"}, {"type": "text", "expr": "Volumetric and multi-view cnns for object classification on 3d data", "word_idx": 53570, "sentence_idx": 931, "label": "unlabeled"}, {"type": "text", "expr": "Volumetric and multi-view cnns for object classification on 3d data", "word_idx": 53637, "sentence_idx": 932, "label": "unlabeled"}, {"type": "text", "expr": "In  Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition , pages 5648\u20135656, 2016", "word_idx": 53704, "sentence_idx": 933, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition", "word_idx": 53809, "sentence_idx": 934, "label": "unlabeled"}, {"type": "text", "expr": ", pages 5648\u20135656, 2016", "word_idx": 53886, "sentence_idx": 935, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sankaranarayanan, Y", "word_idx": 53909, "sentence_idx": 936, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Balaji, C", "word_idx": 53929, "sentence_idx": 937, "label": "unlabeled"}, {"type": "text", "expr": " Castillo, and R", "word_idx": 53939, "sentence_idx": 938, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chellappa", "word_idx": 53955, "sentence_idx": 939, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sankaranarayanan, Y", "word_idx": 53965, "sentence_idx": 940, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Balaji, C", "word_idx": 53985, "sentence_idx": 941, "label": "unlabeled"}, {"type": "text", "expr": " Castillo, and R", "word_idx": 53995, "sentence_idx": 942, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chellappa", "word_idx": 54011, "sentence_idx": 943, "label": "unlabeled"}, {"type": "text", "expr": "Generate to adapt: Aligning domains using generative adversarial\nnetworks", "word_idx": 54021, "sentence_idx": 944, "label": "unlabeled"}, {"type": "text", "expr": "Generate to adapt: Aligning domains using generative adversarial\nnetworks", "word_idx": 54094, "sentence_idx": 945, "label": "unlabeled"}, {"type": "text", "expr": "CoRR , abs/1704", "word_idx": 54167, "sentence_idx": 946, "label": "unlabeled"}, {"type": "text", "expr": "01705, 2017", "word_idx": 54182, "sentence_idx": 947, "label": "unlabeled"}, {"type": "text", "expr": ", abs/1704", "word_idx": 54193, "sentence_idx": 948, "label": "unlabeled"}, {"type": "text", "expr": "01705, 2017", "word_idx": 54203, "sentence_idx": 949, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Simon, H", "word_idx": 54214, "sentence_idx": 950, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Joo, I", "word_idx": 54223, "sentence_idx": 951, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Matthews, and Y", "word_idx": 54230, "sentence_idx": 952, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sheikh", "word_idx": 54246, "sentence_idx": 953, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Simon, H", "word_idx": 54253, "sentence_idx": 954, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Joo, I", "word_idx": 54262, "sentence_idx": 955, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Matthews, and Y", "word_idx": 54269, "sentence_idx": 956, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sheikh", "word_idx": 54285, "sentence_idx": 957, "label": "unlabeled"}, {"type": "text", "expr": "Hand keypoint detection in single images using multiview\nbootstrapping", "word_idx": 54292, "sentence_idx": 958, "label": "unlabeled"}, {"type": "text", "expr": "Hand keypoint detection in single images using multiview\nbootstrapping", "word_idx": 54362, "sentence_idx": 959, "label": "unlabeled"}, {"type": "text", "expr": "In  The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , July 2017", "word_idx": 54432, "sentence_idx": 960, "label": "unlabeled"}, {"type": "text", "expr": "The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR)", "word_idx": 54517, "sentence_idx": 961, "label": "unlabeled"}, {"type": "text", "expr": ", July 2017", "word_idx": 54586, "sentence_idx": 962, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Song and J", "word_idx": 54597, "sentence_idx": 963, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xiao", "word_idx": 54608, "sentence_idx": 964, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Song and J", "word_idx": 54613, "sentence_idx": 965, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xiao", "word_idx": 54624, "sentence_idx": 966, "label": "unlabeled"}, {"type": "text", "expr": "Sliding shapes for 3d object detection in depth images", "word_idx": 54629, "sentence_idx": 967, "label": "unlabeled"}, {"type": "text", "expr": "Sliding shapes for 3d object detection in depth images", "word_idx": 54683, "sentence_idx": 968, "label": "unlabeled"}, {"type": "text", "expr": "In  ECCV (6) , volume 8694 of  Lecture Notes in Computer\nScience , pages 634\u2013651", "word_idx": 54737, "sentence_idx": 969, "label": "unlabeled"}, {"type": "text", "expr": " Springer, 2014", "word_idx": 54817, "sentence_idx": 970, "label": "unlabeled"}, {"type": "text", "expr": "ECCV (6)", "word_idx": 54832, "sentence_idx": 971, "label": "unlabeled"}, {"type": "text", "expr": ", volume 8694 of", "word_idx": 54840, "sentence_idx": 972, "label": "unlabeled"}, {"type": "text", "expr": "Lecture Notes in Computer\nScience", "word_idx": 54856, "sentence_idx": 973, "label": "unlabeled"}, {"type": "text", "expr": ", pages 634\u2013651", "word_idx": 54889, "sentence_idx": 974, "label": "unlabeled"}, {"type": "text", "expr": " Springer, 2014", "word_idx": 54904, "sentence_idx": 975, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Song and J", "word_idx": 54919, "sentence_idx": 976, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xiao", "word_idx": 54930, "sentence_idx": 977, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Song and J", "word_idx": 54935, "sentence_idx": 978, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xiao", "word_idx": 54946, "sentence_idx": 979, "label": "unlabeled"}, {"type": "text", "expr": "Deep Sliding Shapes for amodal 3D object detection in RGB-D\nimages", "word_idx": 54951, "sentence_idx": 980, "label": "unlabeled"}, {"type": "text", "expr": "Deep Sliding Shapes for amodal 3D object detection in RGB-D\nimages", "word_idx": 55017, "sentence_idx": 981, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Song, F", "word_idx": 55083, "sentence_idx": 982, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yu, A", "word_idx": 55091, "sentence_idx": 983, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zeng, A", "word_idx": 55097, "sentence_idx": 984, "label": "unlabeled"}, {"type": "text", "expr": " Chang, M", "word_idx": 55105, "sentence_idx": 985, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Savva, and T", "word_idx": 55114, "sentence_idx": 986, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Funkhouser", "word_idx": 55127, "sentence_idx": 987, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Song, F", "word_idx": 55138, "sentence_idx": 988, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yu, A", "word_idx": 55146, "sentence_idx": 989, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zeng, A", "word_idx": 55152, "sentence_idx": 990, "label": "unlabeled"}, {"type": "text", "expr": " Chang, M", "word_idx": 55160, "sentence_idx": 991, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Savva, and T", "word_idx": 55169, "sentence_idx": 992, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Funkhouser", "word_idx": 55182, "sentence_idx": 993, "label": "unlabeled"}, {"type": "text", "expr": "Semantic scene completion from a single depth image", "word_idx": 55193, "sentence_idx": 994, "label": "unlabeled"}, {"type": "text", "expr": "Semantic scene completion from a single depth image", "word_idx": 55244, "sentence_idx": 995, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of 30th IEEE Conference on Computer Vision and\nPattern Recognition , 2017", "word_idx": 55295, "sentence_idx": 996, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of 30th IEEE Conference on Computer Vision and\nPattern Recognition", "word_idx": 55380, "sentence_idx": 997, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 55458, "sentence_idx": 998, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Su, C", "word_idx": 55464, "sentence_idx": 999, "label": "unlabeled"}, {"type": "text", "expr": " Qi, Y", "word_idx": 55470, "sentence_idx": 1000, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Li, and L", "word_idx": 55476, "sentence_idx": 1001, "label": "unlabeled"}, {"type": "text", "expr": " Guibas", "word_idx": 55486, "sentence_idx": 1002, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Su, C", "word_idx": 55493, "sentence_idx": 1003, "label": "unlabeled"}, {"type": "text", "expr": " Qi, Y", "word_idx": 55499, "sentence_idx": 1004, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Li, and L", "word_idx": 55505, "sentence_idx": 1005, "label": "unlabeled"}, {"type": "text", "expr": " Guibas", "word_idx": 55515, "sentence_idx": 1006, "label": "unlabeled"}, {"type": "text", "expr": "Render for cnn: Viewpoint estimation in images using cnns trained\nwith rendered 3d model views", "word_idx": 55522, "sentence_idx": 1007, "label": "unlabeled"}, {"type": "text", "expr": "Render for cnn: Viewpoint estimation in images using cnns trained\nwith rendered 3d model views", "word_idx": 55616, "sentence_idx": 1008, "label": "unlabeled"}, {"type": "text", "expr": "In  The IEEE International Conference on Computer Vision (ICCV) ,\nDecember 2015", "word_idx": 55710, "sentence_idx": 1009, "label": "unlabeled"}, {"type": "text", "expr": "The IEEE International Conference on Computer Vision (ICCV)", "word_idx": 55789, "sentence_idx": 1010, "label": "unlabeled"}, {"type": "text", "expr": ",\nDecember 2015", "word_idx": 55848, "sentence_idx": 1011, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Su, F", "word_idx": 55863, "sentence_idx": 1012, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wang, E", "word_idx": 55869, "sentence_idx": 1013, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yi, and L", "word_idx": 55877, "sentence_idx": 1014, "label": "unlabeled"}, {"type": "text", "expr": " Guibas", "word_idx": 55887, "sentence_idx": 1015, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Su, F", "word_idx": 55894, "sentence_idx": 1016, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wang, E", "word_idx": 55900, "sentence_idx": 1017, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yi, and L", "word_idx": 55908, "sentence_idx": 1018, "label": "unlabeled"}, {"type": "text", "expr": " Guibas", "word_idx": 55918, "sentence_idx": 1019, "label": "unlabeled"}, {"type": "text", "expr": "3d-assisted feature synthesis for novel views of an object", "word_idx": 55925, "sentence_idx": 1020, "label": "unlabeled"}, {"type": "text", "expr": "3d-assisted feature synthesis for novel views of an object", "word_idx": 55983, "sentence_idx": 1021, "label": "unlabeled"}, {"type": "text", "expr": "In  ICCV , pages 2677\u20132685", "word_idx": 56041, "sentence_idx": 1022, "label": "unlabeled"}, {"type": "text", "expr": " IEEE Computer Society, 2015", "word_idx": 56067, "sentence_idx": 1023, "label": "unlabeled"}, {"type": "text", "expr": ", pages 2677\u20132685", "word_idx": 56095, "sentence_idx": 1024, "label": "unlabeled"}, {"type": "text", "expr": " IEEE Computer Society, 2015", "word_idx": 56112, "sentence_idx": 1025, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sun, J", "word_idx": 56140, "sentence_idx": 1026, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Shang, S", "word_idx": 56147, "sentence_idx": 1027, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Liang, and Y", "word_idx": 56156, "sentence_idx": 1028, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sun, J", "word_idx": 56169, "sentence_idx": 1029, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Shang, S", "word_idx": 56176, "sentence_idx": 1030, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Liang, and Y", "word_idx": 56185, "sentence_idx": 1031, "label": "unlabeled"}, {"type": "text", "expr": "Compositional human pose regression", "word_idx": 56198, "sentence_idx": 1032, "label": "unlabeled"}, {"type": "text", "expr": "Compositional human pose regression", "word_idx": 56233, "sentence_idx": 1033, "label": "unlabeled"}, {"type": "text", "expr": "In  The IEEE International Conference on Computer Vision (ICCV) ,\nOct 2017", "word_idx": 56268, "sentence_idx": 1034, "label": "unlabeled"}, {"type": "text", "expr": "The IEEE International Conference on Computer Vision (ICCV)", "word_idx": 56342, "sentence_idx": 1035, "label": "unlabeled"}, {"type": "text", "expr": ",\nOct 2017", "word_idx": 56401, "sentence_idx": 1036, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Tatarchenko, A", "word_idx": 56411, "sentence_idx": 1037, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dosovitskiy, and T", "word_idx": 56426, "sentence_idx": 1038, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Brox", "word_idx": 56445, "sentence_idx": 1039, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Tatarchenko, A", "word_idx": 56450, "sentence_idx": 1040, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dosovitskiy, and T", "word_idx": 56465, "sentence_idx": 1041, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Brox", "word_idx": 56484, "sentence_idx": 1042, "label": "unlabeled"}, {"type": "text", "expr": "Multi-view 3d models from single images with a convolutional network", "word_idx": 56489, "sentence_idx": 1043, "label": "unlabeled"}, {"type": "text", "expr": "Multi-view 3d models from single images with a convolutional network", "word_idx": 56557, "sentence_idx": 1044, "label": "unlabeled"}, {"type": "text", "expr": "In  ECCV (7) , volume 9911 of  Lecture Notes in Computer\nScience , pages 322\u2013337", "word_idx": 56625, "sentence_idx": 1045, "label": "unlabeled"}, {"type": "text", "expr": " Springer, 2016", "word_idx": 56705, "sentence_idx": 1046, "label": "unlabeled"}, {"type": "text", "expr": "ECCV (7)", "word_idx": 56720, "sentence_idx": 1047, "label": "unlabeled"}, {"type": "text", "expr": ", volume 9911 of", "word_idx": 56728, "sentence_idx": 1048, "label": "unlabeled"}, {"type": "text", "expr": "Lecture Notes in Computer\nScience", "word_idx": 56744, "sentence_idx": 1049, "label": "unlabeled"}, {"type": "text", "expr": ", pages 322\u2013337", "word_idx": 56777, "sentence_idx": 1050, "label": "unlabeled"}, {"type": "text", "expr": " Springer, 2016", "word_idx": 56792, "sentence_idx": 1051, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Tulsiani and J", "word_idx": 56807, "sentence_idx": 1052, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Malik", "word_idx": 56822, "sentence_idx": 1053, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Tulsiani and J", "word_idx": 56828, "sentence_idx": 1054, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Malik", "word_idx": 56843, "sentence_idx": 1055, "label": "unlabeled"}, {"type": "text", "expr": "Viewpoints and keypoints", "word_idx": 56849, "sentence_idx": 1056, "label": "unlabeled"}, {"type": "text", "expr": "Viewpoints and keypoints", "word_idx": 56873, "sentence_idx": 1057, "label": "unlabeled"}, {"type": "text", "expr": "CoRR , abs/1411", "word_idx": 56897, "sentence_idx": 1058, "label": "unlabeled"}, {"type": "text", "expr": "6067, 2014", "word_idx": 56912, "sentence_idx": 1059, "label": "unlabeled"}, {"type": "text", "expr": ", abs/1411", "word_idx": 56922, "sentence_idx": 1060, "label": "unlabeled"}, {"type": "text", "expr": "6067, 2014", "word_idx": 56932, "sentence_idx": 1061, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Tulsiani, T", "word_idx": 56942, "sentence_idx": 1062, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhou, A", "word_idx": 56954, "sentence_idx": 1063, "label": "unlabeled"}, {"type": "text", "expr": " Efros, and J", "word_idx": 56962, "sentence_idx": 1064, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Malik", "word_idx": 56975, "sentence_idx": 1065, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Tulsiani, T", "word_idx": 56981, "sentence_idx": 1066, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhou, A", "word_idx": 56993, "sentence_idx": 1067, "label": "unlabeled"}, {"type": "text", "expr": " Efros, and J", "word_idx": 57001, "sentence_idx": 1068, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Malik", "word_idx": 57014, "sentence_idx": 1069, "label": "unlabeled"}, {"type": "text", "expr": "Multi-view supervision for single-view reconstruction via\ndifferentiable ray consistency", "word_idx": 57020, "sentence_idx": 1070, "label": "unlabeled"}, {"type": "text", "expr": "Multi-view supervision for single-view reconstruction via\ndifferentiable ray consistency", "word_idx": 57108, "sentence_idx": 1071, "label": "unlabeled"}, {"type": "text", "expr": "CoRR , abs/1704", "word_idx": 57196, "sentence_idx": 1072, "label": "unlabeled"}, {"type": "text", "expr": "06254, 2017", "word_idx": 57211, "sentence_idx": 1073, "label": "unlabeled"}, {"type": "text", "expr": ", abs/1704", "word_idx": 57222, "sentence_idx": 1074, "label": "unlabeled"}, {"type": "text", "expr": "06254, 2017", "word_idx": 57232, "sentence_idx": 1075, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Tzeng, J", "word_idx": 57243, "sentence_idx": 1076, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hoffman, K", "word_idx": 57252, "sentence_idx": 1077, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Saenko, and T", "word_idx": 57263, "sentence_idx": 1078, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Darrell", "word_idx": 57277, "sentence_idx": 1079, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Tzeng, J", "word_idx": 57285, "sentence_idx": 1080, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hoffman, K", "word_idx": 57294, "sentence_idx": 1081, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Saenko, and T", "word_idx": 57305, "sentence_idx": 1082, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Darrell", "word_idx": 57319, "sentence_idx": 1083, "label": "unlabeled"}, {"type": "text", "expr": "Adversarial discriminative domain adaptation", "word_idx": 57327, "sentence_idx": 1084, "label": "unlabeled"}, {"type": "text", "expr": "Adversarial discriminative domain adaptation", "word_idx": 57371, "sentence_idx": 1085, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1702", "word_idx": 57415, "sentence_idx": 1086, "label": "unlabeled"}, {"type": "text", "expr": "05464 , 2017", "word_idx": 57440, "sentence_idx": 1087, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1702", "word_idx": 57452, "sentence_idx": 1088, "label": "unlabeled"}, {"type": "text", "expr": "05464", "word_idx": 57477, "sentence_idx": 1089, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 57482, "sentence_idx": 1090, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Tzeng, J", "word_idx": 57488, "sentence_idx": 1091, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hoffman, K", "word_idx": 57497, "sentence_idx": 1092, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Saenko, and T", "word_idx": 57508, "sentence_idx": 1093, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Darrell", "word_idx": 57522, "sentence_idx": 1094, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Tzeng, J", "word_idx": 57530, "sentence_idx": 1095, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hoffman, K", "word_idx": 57539, "sentence_idx": 1096, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Saenko, and T", "word_idx": 57550, "sentence_idx": 1097, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Darrell", "word_idx": 57564, "sentence_idx": 1098, "label": "unlabeled"}, {"type": "text", "expr": "Adversarial discriminative domain adaptation", "word_idx": 57572, "sentence_idx": 1099, "label": "unlabeled"}, {"type": "text", "expr": "Adversarial discriminative domain adaptation", "word_idx": 57616, "sentence_idx": 1100, "label": "unlabeled"}, {"type": "text", "expr": "In  The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , July 2017", "word_idx": 57660, "sentence_idx": 1101, "label": "unlabeled"}, {"type": "text", "expr": "The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR)", "word_idx": 57745, "sentence_idx": 1102, "label": "unlabeled"}, {"type": "text", "expr": ", July 2017", "word_idx": 57814, "sentence_idx": 1103, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wu, T", "word_idx": 57825, "sentence_idx": 1104, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xue, J", "word_idx": 57831, "sentence_idx": 1105, "label": "unlabeled"}, {"type": "text", "expr": " Lim, Y", "word_idx": 57838, "sentence_idx": 1106, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Tian, J", "word_idx": 57845, "sentence_idx": 1107, "label": "unlabeled"}, {"type": "text", "expr": " Tenenbaum, A", "word_idx": 57853, "sentence_idx": 1108, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Torralba, and W", "word_idx": 57866, "sentence_idx": 1109, "label": "unlabeled"}, {"type": "text", "expr": "\nFreeman", "word_idx": 57882, "sentence_idx": 1110, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wu, T", "word_idx": 57890, "sentence_idx": 1111, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xue, J", "word_idx": 57896, "sentence_idx": 1112, "label": "unlabeled"}, {"type": "text", "expr": " Lim, Y", "word_idx": 57903, "sentence_idx": 1113, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Tian, J", "word_idx": 57910, "sentence_idx": 1114, "label": "unlabeled"}, {"type": "text", "expr": " Tenenbaum, A", "word_idx": 57918, "sentence_idx": 1115, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Torralba, and W", "word_idx": 57931, "sentence_idx": 1116, "label": "unlabeled"}, {"type": "text", "expr": "\nFreeman", "word_idx": 57947, "sentence_idx": 1117, "label": "unlabeled"}, {"type": "text", "expr": "Single image 3d interpreter network", "word_idx": 57955, "sentence_idx": 1118, "label": "unlabeled"}, {"type": "text", "expr": "Single image 3d interpreter network", "word_idx": 57990, "sentence_idx": 1119, "label": "unlabeled"}, {"type": "text", "expr": "CoRR , abs/1604", "word_idx": 58025, "sentence_idx": 1120, "label": "unlabeled"}, {"type": "text", "expr": "08685, 2016", "word_idx": 58040, "sentence_idx": 1121, "label": "unlabeled"}, {"type": "text", "expr": ", abs/1604", "word_idx": 58051, "sentence_idx": 1122, "label": "unlabeled"}, {"type": "text", "expr": "08685, 2016", "word_idx": 58061, "sentence_idx": 1123, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wu, S", "word_idx": 58072, "sentence_idx": 1124, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Song, A", "word_idx": 58078, "sentence_idx": 1125, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Khosla, F", "word_idx": 58086, "sentence_idx": 1126, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yu, L", "word_idx": 58096, "sentence_idx": 1127, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang, X", "word_idx": 58102, "sentence_idx": 1128, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Tang, and J", "word_idx": 58111, "sentence_idx": 1129, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xiao", "word_idx": 58123, "sentence_idx": 1130, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wu, S", "word_idx": 58128, "sentence_idx": 1131, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Song, A", "word_idx": 58134, "sentence_idx": 1132, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Khosla, F", "word_idx": 58142, "sentence_idx": 1133, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yu, L", "word_idx": 58152, "sentence_idx": 1134, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang, X", "word_idx": 58158, "sentence_idx": 1135, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Tang, and J", "word_idx": 58167, "sentence_idx": 1136, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xiao", "word_idx": 58179, "sentence_idx": 1137, "label": "unlabeled"}, {"type": "text", "expr": "3d shapenets: A deep representation for volumetric shapes", "word_idx": 58184, "sentence_idx": 1138, "label": "unlabeled"}, {"type": "text", "expr": "3d shapenets: A deep representation for volumetric shapes", "word_idx": 58241, "sentence_idx": 1139, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , pages 1912\u20131920, 2015", "word_idx": 58298, "sentence_idx": 1140, "label": "unlabeled"}, {"type": "text", "expr": ", pages 1912\u20131920, 2015", "word_idx": 58330, "sentence_idx": 1141, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yan, Y", "word_idx": 58353, "sentence_idx": 1142, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ding, P", "word_idx": 58360, "sentence_idx": 1143, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Li, Q", "word_idx": 58368, "sentence_idx": 1144, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wang, Y", "word_idx": 58374, "sentence_idx": 1145, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xu, and W", "word_idx": 58382, "sentence_idx": 1146, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yan, Y", "word_idx": 58392, "sentence_idx": 1147, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ding, P", "word_idx": 58399, "sentence_idx": 1148, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Li, Q", "word_idx": 58407, "sentence_idx": 1149, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wang, Y", "word_idx": 58413, "sentence_idx": 1150, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xu, and W", "word_idx": 58421, "sentence_idx": 1151, "label": "unlabeled"}, {"type": "text", "expr": "Mind the class weight bias: Weighted maximum mean discrepancy for\nunsupervised domain adaptation", "word_idx": 58431, "sentence_idx": 1152, "label": "unlabeled"}, {"type": "text", "expr": "Mind the class weight bias: Weighted maximum mean discrepancy for\nunsupervised domain adaptation", "word_idx": 58527, "sentence_idx": 1153, "label": "unlabeled"}, {"type": "text", "expr": "In  The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , July 2017", "word_idx": 58623, "sentence_idx": 1154, "label": "unlabeled"}, {"type": "text", "expr": "The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR)", "word_idx": 58708, "sentence_idx": 1155, "label": "unlabeled"}, {"type": "text", "expr": ", July 2017", "word_idx": 58777, "sentence_idx": 1156, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yan, J", "word_idx": 58788, "sentence_idx": 1157, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yang, E", "word_idx": 58795, "sentence_idx": 1158, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yumer, Y", "word_idx": 58803, "sentence_idx": 1159, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Guo, and H", "word_idx": 58812, "sentence_idx": 1160, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yan, J", "word_idx": 58823, "sentence_idx": 1161, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yang, E", "word_idx": 58830, "sentence_idx": 1162, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yumer, Y", "word_idx": 58838, "sentence_idx": 1163, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Guo, and H", "word_idx": 58847, "sentence_idx": 1164, "label": "unlabeled"}, {"type": "text", "expr": "Perspective transformer nets: Learning single-view 3d object\nreconstruction without 3d supervision", "word_idx": 58858, "sentence_idx": 1165, "label": "unlabeled"}, {"type": "text", "expr": "Perspective transformer nets: Learning single-view 3d object\nreconstruction without 3d supervision", "word_idx": 58956, "sentence_idx": 1166, "label": "unlabeled"}, {"type": "text", "expr": "CoRR , abs/1612", "word_idx": 59054, "sentence_idx": 1167, "label": "unlabeled"}, {"type": "text", "expr": "00814, 2016", "word_idx": 59069, "sentence_idx": 1168, "label": "unlabeled"}, {"type": "text", "expr": ", abs/1612", "word_idx": 59080, "sentence_idx": 1169, "label": "unlabeled"}, {"type": "text", "expr": "00814, 2016", "word_idx": 59090, "sentence_idx": 1170, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang, P", "word_idx": 59101, "sentence_idx": 1171, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0David, and B", "word_idx": 59110, "sentence_idx": 1172, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gong", "word_idx": 59123, "sentence_idx": 1173, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang, P", "word_idx": 59128, "sentence_idx": 1174, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0David, and B", "word_idx": 59137, "sentence_idx": 1175, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gong", "word_idx": 59150, "sentence_idx": 1176, "label": "unlabeled"}, {"type": "text", "expr": "Curriculum domain adaptation for semantic segmentation of urban\nscenes", "word_idx": 59155, "sentence_idx": 1177, "label": "unlabeled"}, {"type": "text", "expr": "Curriculum domain adaptation for semantic segmentation of urban\nscenes", "word_idx": 59225, "sentence_idx": 1178, "label": "unlabeled"}, {"type": "text", "expr": "In  The IEEE International Conference on Computer Vision (ICCV) ,\nOct 2017", "word_idx": 59295, "sentence_idx": 1179, "label": "unlabeled"}, {"type": "text", "expr": "The IEEE International Conference on Computer Vision (ICCV)", "word_idx": 59369, "sentence_idx": 1180, "label": "unlabeled"}, {"type": "text", "expr": ",\nOct 2017", "word_idx": 59428, "sentence_idx": 1181, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang, S", "word_idx": 59438, "sentence_idx": 1182, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Song, E", "word_idx": 59447, "sentence_idx": 1183, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yumer, M", "word_idx": 59455, "sentence_idx": 1184, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Savva, J", "word_idx": 59464, "sentence_idx": 1185, "label": "unlabeled"}, {"type": "text", "expr": " Lee, H", "word_idx": 59473, "sentence_idx": 1186, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Jin, and T", "word_idx": 59480, "sentence_idx": 1187, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Funkhouser", "word_idx": 59491, "sentence_idx": 1188, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang, S", "word_idx": 59502, "sentence_idx": 1189, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Song, E", "word_idx": 59511, "sentence_idx": 1190, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yumer, M", "word_idx": 59519, "sentence_idx": 1191, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Savva, J", "word_idx": 59528, "sentence_idx": 1192, "label": "unlabeled"}, {"type": "text", "expr": " Lee, H", "word_idx": 59537, "sentence_idx": 1193, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Jin, and T", "word_idx": 59544, "sentence_idx": 1194, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Funkhouser", "word_idx": 59555, "sentence_idx": 1195, "label": "unlabeled"}, {"type": "text", "expr": "Physically-based rendering for indoor scene understanding using\nconvolutional neural networks", "word_idx": 59566, "sentence_idx": 1196, "label": "unlabeled"}, {"type": "text", "expr": "Physically-based rendering for indoor scene understanding using\nconvolutional neural networks", "word_idx": 59659, "sentence_idx": 1197, "label": "unlabeled"}, {"type": "text", "expr": "The IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR) , 2017", "word_idx": 59752, "sentence_idx": 1198, "label": "unlabeled"}, {"type": "text", "expr": "The IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR)", "word_idx": 59828, "sentence_idx": 1199, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 59897, "sentence_idx": 1200, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhao, X", "word_idx": 59903, "sentence_idx": 1201, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wu, Z", "word_idx": 59911, "sentence_idx": 1202, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Cheng, H", "word_idx": 59917, "sentence_idx": 1203, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Liu, and J", "word_idx": 59926, "sentence_idx": 1204, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Feng", "word_idx": 59937, "sentence_idx": 1205, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhao, X", "word_idx": 59942, "sentence_idx": 1206, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wu, Z", "word_idx": 59950, "sentence_idx": 1207, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Cheng, H", "word_idx": 59956, "sentence_idx": 1208, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Liu, and J", "word_idx": 59965, "sentence_idx": 1209, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Feng", "word_idx": 59976, "sentence_idx": 1210, "label": "unlabeled"}, {"type": "text", "expr": "Multi-view image generation from a single-view", "word_idx": 59981, "sentence_idx": 1211, "label": "unlabeled"}, {"type": "text", "expr": "Multi-view image generation from a single-view", "word_idx": 60027, "sentence_idx": 1212, "label": "unlabeled"}, {"type": "text", "expr": "CoRR , abs/1704", "word_idx": 60073, "sentence_idx": 1213, "label": "unlabeled"}, {"type": "text", "expr": "04886, 2017", "word_idx": 60088, "sentence_idx": 1214, "label": "unlabeled"}, {"type": "text", "expr": ", abs/1704", "word_idx": 60099, "sentence_idx": 1215, "label": "unlabeled"}, {"type": "text", "expr": "04886, 2017", "word_idx": 60109, "sentence_idx": 1216, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhou, S", "word_idx": 60120, "sentence_idx": 1217, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Tulsiani, W", "word_idx": 60128, "sentence_idx": 1218, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sun, J", "word_idx": 60140, "sentence_idx": 1219, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Malik, and A", "word_idx": 60147, "sentence_idx": 1220, "label": "unlabeled"}, {"type": "text", "expr": " Efros", "word_idx": 60160, "sentence_idx": 1221, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhou, S", "word_idx": 60166, "sentence_idx": 1222, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Tulsiani, W", "word_idx": 60174, "sentence_idx": 1223, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sun, J", "word_idx": 60186, "sentence_idx": 1224, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Malik, and A", "word_idx": 60193, "sentence_idx": 1225, "label": "unlabeled"}, {"type": "text", "expr": " Efros", "word_idx": 60206, "sentence_idx": 1226, "label": "unlabeled"}, {"type": "text", "expr": "View synthesis by appearance flow", "word_idx": 60212, "sentence_idx": 1227, "label": "unlabeled"}, {"type": "text", "expr": "View synthesis by appearance flow", "word_idx": 60245, "sentence_idx": 1228, "label": "unlabeled"}, {"type": "text", "expr": "In  ECCV , 2016", "word_idx": 60278, "sentence_idx": 1229, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 60293, "sentence_idx": 1230, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhou, Q", "word_idx": 60299, "sentence_idx": 1231, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Huang, X", "word_idx": 60307, "sentence_idx": 1232, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sun, X", "word_idx": 60316, "sentence_idx": 1233, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xue, and Y", "word_idx": 60323, "sentence_idx": 1234, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhou, Q", "word_idx": 60334, "sentence_idx": 1235, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Huang, X", "word_idx": 60342, "sentence_idx": 1236, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sun, X", "word_idx": 60351, "sentence_idx": 1237, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xue, and Y", "word_idx": 60358, "sentence_idx": 1238, "label": "unlabeled"}, {"type": "text", "expr": "Towards 3d human pose estimation in the wild: A weakly-supervised\napproach", "word_idx": 60369, "sentence_idx": 1239, "label": "unlabeled"}, {"type": "text", "expr": "Towards 3d human pose estimation in the wild: A weakly-supervised\napproach", "word_idx": 60443, "sentence_idx": 1240, "label": "unlabeled"}, {"type": "text", "expr": "In  The IEEE International Conference on Computer Vision (ICCV) ,\nOct 2017", "word_idx": 60517, "sentence_idx": 1241, "label": "unlabeled"}, {"type": "text", "expr": "The IEEE International Conference on Computer Vision (ICCV)", "word_idx": 60591, "sentence_idx": 1242, "label": "unlabeled"}, {"type": "text", "expr": ",\nOct 2017", "word_idx": 60650, "sentence_idx": 1243, "label": "unlabeled"}, {"type": "text", "expr": "Generated  on Tue Apr  3 11:28:58 2018 by", "word_idx": 60660, "sentence_idx": 1244, "label": "unlabeled"}, {"type": "text", "expr": "LaTeXML", "word_idx": 60701, "sentence_idx": 1245, "label": "unlabeled"}], "Bayesian_Gradient_Descent": [{"type": "text", "expr": "Instructions for Authors", "word_idx": 0, "sentence_idx": 0, "label": "unlabeled"}, {"type": "text", "expr": "Instructions for Authors", "word_idx": 24, "sentence_idx": 1, "label": "unlabeled"}, {"type": "text", "expr": "Equal contribution", "word_idx": 48, "sentence_idx": 2, "label": "unlabeled"}, {"type": "text", "expr": "Technion - Israel Institute of Technology,\nHaifa, Israel", "word_idx": 66, "sentence_idx": 3, "label": "unlabeled"}, {"type": "text", "expr": "Chen Zeno\n\u00a0\u00a0\u00a0\u00a0Itay Golan \n\u00a0\u00a0\u00a0\u00a0Elad Hoffer\n\u00a0\u00a0\u00a0\u00a0Daniel Soudry", "word_idx": 122, "sentence_idx": 4, "label": "unlabeled"}, {"type": "text", "expr": "1 footnotemark:", "word_idx": 181, "sentence_idx": 5, "label": "unlabeled"}, {"type": "text", "expr": "footnotemark:", "word_idx": 196, "sentence_idx": 6, "label": "unlabeled"}, {"type": "text", "expr": "Equal contribution", "word_idx": 209, "sentence_idx": 7, "label": "unlabeled"}, {"type": "text", "expr": "Equal contribution", "word_idx": 227, "sentence_idx": 8, "label": "unlabeled"}, {"type": "text", "expr": "Equal contribution", "word_idx": 245, "sentence_idx": 9, "label": "unlabeled"}, {"type": "text", "expr": "Bayesian Gradient Descent: Online Variational Bayes Learning with Increased Robustness to Catastrophic Forgetting and Weight Pruning", "word_idx": 263, "sentence_idx": 10, "label": "unlabeled"}, {"type": "text", "expr": "Equal contribution", "word_idx": 395, "sentence_idx": 11, "label": "unlabeled"}, {"type": "text", "expr": "Technion - Israel Institute of Technology,\nHaifa, Israel", "word_idx": 413, "sentence_idx": 12, "label": "unlabeled"}, {"type": "text", "expr": "Chen Zeno\n\u00a0\u00a0\u00a0\u00a0Itay Golan \n\u00a0\u00a0\u00a0\u00a0Elad Hoffer\n\u00a0\u00a0\u00a0\u00a0Daniel Soudry", "word_idx": 469, "sentence_idx": 13, "label": "unlabeled"}, {"type": "text", "expr": "1 footnotemark:", "word_idx": 528, "sentence_idx": 14, "label": "unlabeled"}, {"type": "text", "expr": "footnotemark:", "word_idx": 543, "sentence_idx": 15, "label": "unlabeled"}, {"type": "text", "expr": "Equal contribution", "word_idx": 556, "sentence_idx": 16, "label": "unlabeled"}, {"type": "text", "expr": "Equal contribution", "word_idx": 574, "sentence_idx": 17, "label": "unlabeled"}, {"type": "text", "expr": "Equal contribution", "word_idx": 592, "sentence_idx": 18, "label": "unlabeled"}, {"type": "text", "expr": "Abstract", "word_idx": 610, "sentence_idx": 19, "label": "unlabeled"}, {"type": "text", "expr": "Abstract", "word_idx": 618, "sentence_idx": 20, "label": "unlabeled"}, {"type": "text", "expr": "We suggest a novel approach for the estimation of the posterior distribution of the weights of a neural network, using an online version of the variational Bayes method", "word_idx": 626, "sentence_idx": 21, "label": "unlabeled"}, {"type": "text", "expr": " Having a confidence measure of the weights allows to combat several shortcomings of neural networks, such as their parameter redundancy, and their notorious vulnerability to the change of input distribution (\u201dcatastrophic forgetting\u201d)", "word_idx": 794, "sentence_idx": 22, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, We show that this approach helps alleviate the catastrophic forgetting phenomenon \u2014 even without the knowledge of when the tasks are been switched", "word_idx": 1029, "sentence_idx": 23, "label": "unlabeled"}, {"type": "text", "expr": " Furthermore, it improves the robustness of the network to weight pruning \u2014 even without re-training", "word_idx": 1190, "sentence_idx": 24, "label": "unlabeled"}, {"type": "text", "expr": "1 Source code to replicate our results throughout the paper will be available online", "word_idx": 1290, "sentence_idx": 25, "label": "unlabeled"}, {"type": "text", "expr": "\\titlespacing", "word_idx": 1374, "sentence_idx": 26, "label": "unlabeled"}, {"type": "text", "expr": "0pt*0*0 \\titlespacing", "word_idx": 1387, "sentence_idx": 27, "label": "unlabeled"}, {"type": "text", "expr": "\\titlespacing", "word_idx": 1408, "sentence_idx": 28, "label": "unlabeled"}, {"type": "text", "expr": "0pt*0*0 \\titlespacing", "word_idx": 1421, "sentence_idx": 29, "label": "unlabeled"}, {"type": "text", "expr": "\\titlespacing", "word_idx": 1442, "sentence_idx": 30, "label": "unlabeled"}, {"type": "text", "expr": "0pt*0*0", "word_idx": 1455, "sentence_idx": 31, "label": "unlabeled"}, {"type": "text", "expr": "0pt*0*0", "word_idx": 1462, "sentence_idx": 32, "label": "unlabeled"}, {"type": "text", "expr": "2  INTRODUCTION", "word_idx": 1469, "sentence_idx": 33, "label": "unlabeled"}, {"type": "text", "expr": "For decades, many different approaches were suggested to integrate neural networks with the toolbox of Bayesian inference (as detailed in the next section)", "word_idx": 1484, "sentence_idx": 34, "label": "unlabeled"}, {"type": "text", "expr": " In such \u201dBayesian neural networks\u201d, we have, after training, not a single set of parameters (\u201dweights\u201d), but an (approximate) posterior distribution over those parameters", "word_idx": 1639, "sentence_idx": 35, "label": "unlabeled"}, {"type": "text", "expr": " Such a posterior distribution is very useful for many potential applications", "word_idx": 1810, "sentence_idx": 36, "label": "unlabeled"}, {"type": "text", "expr": " For example, it enables uncertainty estimates over the network output; selection of hyperparameters and models in a principled framework; and guided data collection (active learning)", "word_idx": 1887, "sentence_idx": 37, "label": "unlabeled"}, {"type": "text", "expr": " In this work, we will focus on other uses of the Bayesian machinery", "word_idx": 2070, "sentence_idx": 38, "label": "unlabeled"}, {"type": "text", "expr": "For decades, many different approaches were suggested to integrate neural networks with the toolbox of Bayesian inference (as detailed in the next section)", "word_idx": 2138, "sentence_idx": 39, "label": "unlabeled"}, {"type": "text", "expr": " In such \u201dBayesian neural networks\u201d, we have, after training, not a single set of parameters (\u201dweights\u201d), but an (approximate) posterior distribution over those parameters", "word_idx": 2293, "sentence_idx": 40, "label": "unlabeled"}, {"type": "text", "expr": " Such a posterior distribution is very useful for many potential applications", "word_idx": 2464, "sentence_idx": 41, "label": "unlabeled"}, {"type": "text", "expr": " For example, it enables uncertainty estimates over the network output; selection of hyperparameters and models in a principled framework; and guided data collection (active learning)", "word_idx": 2541, "sentence_idx": 42, "label": "unlabeled"}, {"type": "text", "expr": " In this work, we will focus on other uses of the Bayesian machinery", "word_idx": 2724, "sentence_idx": 43, "label": "unlabeled"}, {"type": "text", "expr": "It was already observed 30 years ago that estimating the underlying posterior distribution can be beneficial to combat \u201dcatastrophic forgetting\u201d phenomena  ", "word_idx": 2792, "sentence_idx": 44, "label": "unlabeled"}, {"type": "text", "expr": " Catastrophic forgetting is the tendency of neural networks to rapidly lose their previously learned performance when their input distribution is changed abruptly, such as when changing a task or a source", "word_idx": 2948, "sentence_idx": 45, "label": "unlabeled"}, {"type": "text", "expr": " This shortcoming was extensively investigated over the course of the past decades", "word_idx": 3152, "sentence_idx": 46, "label": "unlabeled"}, {"type": "text", "expr": " Lately, it was found that estimating confidence of the weights by the use of their posterior and using it to affect their plasticity may allow a natural transition between learned tasks while reducing the ill effect of catastrophic forgetting  ", "word_idx": 3234, "sentence_idx": 47, "label": "unlabeled"}, {"type": "text", "expr": " However, existing methods for reducing catastrophic forgetting require knowledge of when the tasks are switched", "word_idx": 3479, "sentence_idx": 48, "label": "unlabeled"}, {"type": "text", "expr": " This can be a significant disadvantage, since in many realistic application we may not posses this knowledge", "word_idx": 3591, "sentence_idx": 49, "label": "unlabeled"}, {"type": "text", "expr": "Another usage of such methods is to allow efficient weight pruning strategies, as described by  ", "word_idx": 3700, "sentence_idx": 50, "label": "unlabeled"}, {"type": "text", "expr": " Pruning neural networks, so that a significant percentage of the network\u2019s weights are discarded, has gained considerable attention over the past few years", "word_idx": 3796, "sentence_idx": 51, "label": "unlabeled"}, {"type": "text", "expr": " Using pruning, networks can be modified to be more efficient by using less memory and computational resources  , and to allow adaptation to new tasks by transfer learning using only a subset of original model  ", "word_idx": 3952, "sentence_idx": 52, "label": "unlabeled"}, {"type": "text", "expr": " Using the posterior distribution of the weights forms a natural selection criteria to pruning schemes, that can be used to replace heuristics while adapting to a specific task at hand", "word_idx": 4163, "sentence_idx": 53, "label": "unlabeled"}, {"type": "text", "expr": " However, most existing pruning schemes require re-training of the network after sparsification", "word_idx": 4347, "sentence_idx": 54, "label": "unlabeled"}, {"type": "text", "expr": " Without this re-training, the test accuracy is significantly degraded  ", "word_idx": 4442, "sentence_idx": 55, "label": "unlabeled"}, {"type": "text", "expr": " This can be a significant disadvantage, since one does not always have access to the original training set when adapting an existing model to a new hardware or task", "word_idx": 4514, "sentence_idx": 56, "label": "unlabeled"}, {"type": "text", "expr": " As a notable exception, Bayesian methods such as   do enable weight pruning without re-training \u2014 however, this approach has several issues, as we explain in the next section", "word_idx": 4679, "sentence_idx": 57, "label": "unlabeled"}, {"type": "text", "expr": "2017b", "word_idx": 4854, "sentence_idx": 58, "label": "unlabeled"}, {"type": "text", "expr": "2017b", "word_idx": 4859, "sentence_idx": 59, "label": "unlabeled"}, {"type": "text", "expr": "In this work we will present a novel approach to Bayesian inference using an online variational Bayes update of the weights", "word_idx": 4864, "sentence_idx": 60, "label": "unlabeled"}, {"type": "text", "expr": " This approach allows us to be robust to weight pruning in neural networks, and to combat catastrophic forgetting", "word_idx": 4987, "sentence_idx": 61, "label": "unlabeled"}, {"type": "text", "expr": " A notable strength of our approach, compared to previous methods, is that our method is able to adapt without an explicit indication that something has changed", "word_idx": 5100, "sentence_idx": 62, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, it is resistant to catastrophic forgetting, while being unaware of the tasks", "word_idx": 5260, "sentence_idx": 63, "label": "unlabeled"}, {"type": "text", "expr": " Also, it is resistant to weight pruning, even without re-training", "word_idx": 5351, "sentence_idx": 64, "label": "unlabeled"}, {"type": "text", "expr": "In this work we will present a novel approach to Bayesian inference using an online variational Bayes update of the weights", "word_idx": 5417, "sentence_idx": 65, "label": "unlabeled"}, {"type": "text", "expr": " This approach allows us to be robust to weight pruning in neural networks, and to combat catastrophic forgetting", "word_idx": 5540, "sentence_idx": 66, "label": "unlabeled"}, {"type": "text", "expr": " A notable strength of our approach, compared to previous methods, is that our method is able to adapt without an explicit indication that something has changed", "word_idx": 5653, "sentence_idx": 67, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, it is resistant to catastrophic forgetting, while being unaware of the tasks", "word_idx": 5813, "sentence_idx": 68, "label": "unlabeled"}, {"type": "text", "expr": " Also, it is resistant to weight pruning, even without re-training", "word_idx": 5904, "sentence_idx": 69, "label": "unlabeled"}, {"type": "text", "expr": "3  RELATED WORK", "word_idx": 5970, "sentence_idx": 70, "label": "unlabeled"}, {"type": "text", "expr": "Bayesian inference for neural network has been a subject of significant interest over many years", "word_idx": 5985, "sentence_idx": 71, "label": "unlabeled"}, {"type": "text", "expr": " As exact Bayesian inference is intractable (for any realistic network size), much research has been focused on approximation techniques", "word_idx": 6081, "sentence_idx": 72, "label": "unlabeled"}, {"type": "text", "expr": " Most modern techniques stemmed from previous seminal works which used either a Laplace approximation  , variational methods  , or Monte Carlo methods  ", "word_idx": 6217, "sentence_idx": 73, "label": "unlabeled"}, {"type": "text", "expr": " In the last years many methods for approximating the posterior distribution have been suggested, falling into one of these categories", "word_idx": 6369, "sentence_idx": 74, "label": "unlabeled"}, {"type": "text", "expr": " Those methods include assumed density filtering  , approximate power Expectation Propagation  , Stochastic Langevin Gradient Descent  , incremental moment matching   and variational Bayes  ", "word_idx": 6503, "sentence_idx": 75, "label": "unlabeled"}, {"type": "text", "expr": "In this work we will focus on variational Bayes methods", "word_idx": 6693, "sentence_idx": 76, "label": "unlabeled"}, {"type": "text", "expr": " Practical variational Bayes for modern neural networks was first introduced by  , where parametric distribution is used to approximate the posterior distribution by minimizing the variational free energy", "word_idx": 6748, "sentence_idx": 77, "label": "unlabeled"}, {"type": "text", "expr": " Calculating the variational free energy is intractable for general neural network, and thus  \nestimated its gradients using a biased Monte Carlo method, and used stochastic gradient descent (SGD)\nto performed minimization", "word_idx": 6952, "sentence_idx": 78, "label": "unlabeled"}, {"type": "text", "expr": " In a later work,   used a re-parameterization trick to introduce an unbiased estimator for the gradients", "word_idx": 7174, "sentence_idx": 79, "label": "unlabeled"}, {"type": "text", "expr": " Variational Bayes methods were also used extensively on various probabilistic models including recurrent neural networks  , auto-encoder   and fully connected networks  ", "word_idx": 7279, "sentence_idx": 80, "label": "unlabeled"}, {"type": "text", "expr": "   used variational Bayes approach of   to reduce catastrophic forgetting in MNIST", "word_idx": 7449, "sentence_idx": 81, "label": "unlabeled"}, {"type": "text", "expr": "However, the standard variational Bayes approach developed by  , \u201dBayes By Backprop\u201d (BBB), has several shortcomings", "word_idx": 7531, "sentence_idx": 82, "label": "unlabeled"}, {"type": "text", "expr": " The variational free energy minimized in BBB is a sum of a log-likelihood cost function and a complexity cost function", "word_idx": 7647, "sentence_idx": 83, "label": "unlabeled"}, {"type": "text", "expr": " The complexity cost function acts as a regularization part, enforcing a solution that satisfies the complexity of the data while preserving the posterior distribution close to the prior distribution", "word_idx": 7766, "sentence_idx": 84, "label": "unlabeled"}, {"type": "text", "expr": " Finding a good prior is usually a non-trivial task, over-restricting prior could potentially cause underfitting", "word_idx": 7965, "sentence_idx": 85, "label": "unlabeled"}, {"type": "text", "expr": " Indeed,   showed that variational Bayesian neural networks, trained using BBB, suffer from variational over-pruning", "word_idx": 8077, "sentence_idx": 86, "label": "unlabeled"}, {"type": "text", "expr": " For both the train and the test set the log-likelihood cost function initially decreases and reaches a minimum value, but then increases and eventually converge to a significantly higher value than before (even so, the variational free energy is monotonically decreasing along the course of the training process)", "word_idx": 8193, "sentence_idx": 87, "label": "unlabeled"}, {"type": "text", "expr": " In some cases, this underfitting issue might be alleviated using significant tuning of the hyper-parameters and early stopping time", "word_idx": 8506, "sentence_idx": 88, "label": "unlabeled"}, {"type": "text", "expr": " This issue significantly hurts the reproducibility of the BBB approach, as also observed by  ", "word_idx": 8638, "sentence_idx": 89, "label": "unlabeled"}, {"type": "text", "expr": " It may also be the reason that BBB has not been successfully applied yet on a larger scale then MNIST (e", "word_idx": 8732, "sentence_idx": 90, "label": "unlabeled"}, {"type": "text", "expr": ", CIFAR), to the best of our knowledge", "word_idx": 8837, "sentence_idx": 91, "label": "unlabeled"}, {"type": "text", "expr": "Variational dropout   alleviates these issues, by using an improper prior so the complexity cost function becomes constant in the weight parameters", "word_idx": 8875, "sentence_idx": 92, "label": "unlabeled"}, {"type": "text", "expr": " Later modifications of this approach   were shown to be useful for weight pruning (without re-training, similarly to BBB), but require additional fine tuning of hyper-parameters to work", "word_idx": 9022, "sentence_idx": 93, "label": "unlabeled"}, {"type": "text", "expr": " For example, the complexity cost function must be gradually ramped-up from zero, or the weights again become over-pruned and the accuracy is decreased", "word_idx": 9208, "sentence_idx": 94, "label": "unlabeled"}, {"type": "text", "expr": " Moreover, later,   noted that such variational dropout based approaches are not Bayesian", "word_idx": 9359, "sentence_idx": 95, "label": "unlabeled"}, {"type": "text", "expr": "2017a", "word_idx": 9448, "sentence_idx": 96, "label": "unlabeled"}, {"type": "text", "expr": "To avoid these issues, we suggest using a new prior for each mini-batch instead of one prior for the all data", "word_idx": 9453, "sentence_idx": 97, "label": "unlabeled"}, {"type": "text", "expr": " This adaptation allows the approximate posterior distribution to deviate from the prior distribution as more data is seen as in Bayesian inference", "word_idx": 9562, "sentence_idx": 98, "label": "unlabeled"}, {"type": "text", "expr": " Our online approach to learning is naturally implemented using online variational Bayes, as we explain in the next", "word_idx": 9709, "sentence_idx": 99, "label": "unlabeled"}, {"type": "text", "expr": "To avoid these issues, we suggest using a new prior for each mini-batch instead of one prior for the all data", "word_idx": 9824, "sentence_idx": 100, "label": "unlabeled"}, {"type": "text", "expr": " This adaptation allows the approximate posterior distribution to deviate from the prior distribution as more data is seen as in Bayesian inference", "word_idx": 9933, "sentence_idx": 101, "label": "unlabeled"}, {"type": "text", "expr": " Our online approach to learning is naturally implemented using online variational Bayes, as we explain in the next", "word_idx": 10080, "sentence_idx": 102, "label": "unlabeled"}, {"type": "text", "expr": "4  THEORY", "word_idx": 10195, "sentence_idx": 103, "label": "unlabeled"}, {"type": "text", "expr": "The process of Bayesian inference requires a full probability model\nproviding a joint probability distribution over the data and the model\nparameters", "word_idx": 10204, "sentence_idx": 104, "label": "unlabeled"}, {"type": "text", "expr": " The joint probability distribution can be written as a\nproduct of two distributions:", "word_idx": 10353, "sentence_idx": 105, "label": "unlabeled"}, {"type": "text", "expr": "The process of Bayesian inference requires a full probability model\nproviding a joint probability distribution over the data and the model\nparameters", "word_idx": 10438, "sentence_idx": 106, "label": "unlabeled"}, {"type": "text", "expr": " The joint probability distribution can be written as a\nproduct of two distributions:", "word_idx": 10587, "sentence_idx": 107, "label": "unlabeled"}, {"type": "text", "expr": "$p\\left(D,\\theta\\right)=p\\left(D|\\theta\\right)p\\left(\\theta\\right)\\,,$", "word_idx": 10672, "sentence_idx": 108, "label": "unlabeled"}, {"type": "math", "expr": "$$p\\left(D,\\theta\\right)=p\\left(D|\\theta\\right)p\\left(\\theta\\right)\\,,$$", "word_idx": 10742, "sentence_idx": 109, "label": "unlabeled"}, {"type": "text", "expr": "where  $p\\left(D|\\theta\\right)$  is the data distribution of the data\nset  $D$ , and  $p\\left(\\theta\\right)$  is the prior distribution of\nthe parameters  $\\theta$ ", "word_idx": 10810, "sentence_idx": 110, "label": "unlabeled"}, {"type": "text", "expr": " The posterior distribution can be calculated\nusing Bayes\u2019 rule:", "word_idx": 10974, "sentence_idx": 111, "label": "unlabeled"}, {"type": "math", "expr": "$$p\\left(D|\\theta\\right)$$", "word_idx": 11038, "sentence_idx": 112, "label": "unlabeled"}, {"type": "math", "expr": "$$p\\left(\\theta\\right)$$", "word_idx": 11060, "sentence_idx": 113, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta$$", "word_idx": 11080, "sentence_idx": 114, "label": "unlabeled"}, {"type": "text", "expr": "$p\\left(\\theta|D\\right)=\\frac{p\\left(D|\\theta\\right)p\\left(\\theta\\right)}{p%\n\\left(D\\right)}\\,,$", "word_idx": 11086, "sentence_idx": 115, "label": "unlabeled"}, {"type": "math", "expr": "$$p\\left(\\theta|D\\right)=\\frac{p\\left(D|\\theta\\right)p\\left(\\theta\\right)}{p%\n\\left(D\\right)}\\,,$$", "word_idx": 11182, "sentence_idx": 116, "label": "unlabeled"}, {"type": "text", "expr": "where  $p\\left(D\\right)$  is calculated using the sum rule", "word_idx": 11276, "sentence_idx": 117, "label": "unlabeled"}, {"type": "math", "expr": "$$p\\left(D\\right)$$", "word_idx": 11334, "sentence_idx": 118, "label": "unlabeled"}, {"type": "text", "expr": "Unfortunately, calculating the posterior distribution is intractable\nfor most practical probability models", "word_idx": 11349, "sentence_idx": 119, "label": "unlabeled"}, {"type": "text", "expr": " Therefore, we will use variational methods to approximate the true posterior", "word_idx": 11455, "sentence_idx": 120, "label": "unlabeled"}, {"type": "text", "expr": "In this paper we will focus on the online version of Bayesian inference, in which the data arrives sequentially, and we do a sequential update of the posterior distribution each time when new data arrives", "word_idx": 11532, "sentence_idx": 121, "label": "unlabeled"}, {"type": "text", "expr": " In each step the previous posterior distribution is used as the new prior distribution", "word_idx": 11736, "sentence_idx": 122, "label": "unlabeled"}, {"type": "text", "expr": " Therefore, according to Bayes\u2019 rule, the posterior distribution at time  $n+1$  is given by:", "word_idx": 11823, "sentence_idx": 123, "label": "unlabeled"}, {"type": "math", "expr": "$$n+1$$", "word_idx": 11916, "sentence_idx": 124, "label": "unlabeled"}, {"type": "text", "expr": "$p\\left(\\theta|D_{n+1}\\right)=\\frac{p\\left(D_{n+1}|\\theta\\right)p\\left(\\theta|D%\n_{n}\\right)}{p\\left(D_{n+1}\\right)}$", "word_idx": 11919, "sentence_idx": 125, "label": "unlabeled"}, {"type": "math", "expr": "$$p\\left(\\theta|D_{n+1}\\right)=\\frac{p\\left(D_{n+1}|\\theta\\right)p\\left(\\theta|D%\n_{n}\\right)}{p\\left(D_{n+1}\\right)}$$", "word_idx": 12036, "sentence_idx": 126, "label": "unlabeled"}, {"type": "text", "expr": "Unfortunately, calculating the posterior distribution is intractable\nfor most practical probability models", "word_idx": 12151, "sentence_idx": 127, "label": "unlabeled"}, {"type": "text", "expr": " Therefore, we will use variational methods to approximate the true posterior", "word_idx": 12257, "sentence_idx": 128, "label": "unlabeled"}, {"type": "text", "expr": "1  ONLINE VARIATIONAL BAYES", "word_idx": 12334, "sentence_idx": 129, "label": "unlabeled"}, {"type": "text", "expr": "The optimal variational parameters is the solution of the optimization problem:", "word_idx": 12361, "sentence_idx": 130, "label": "unlabeled"}, {"type": "text", "expr": "In variational Bayes, a parametric distribution  $q\\left(\\theta|\\phi\\right)$ \nis used for approximating the true posterior distribution  $p\\left(\\theta\\right|D)$ \nby minimizing the Kullback-Leibler (KL) divergence with the true posterior\ndistribution", "word_idx": 12440, "sentence_idx": 131, "label": "unlabeled"}, {"type": "math", "expr": "$$q\\left(\\theta|\\phi\\right)$$", "word_idx": 12690, "sentence_idx": 132, "label": "unlabeled"}, {"type": "math", "expr": "$$p\\left(\\theta\\right|D)$$", "word_idx": 12715, "sentence_idx": 133, "label": "unlabeled"}, {"type": "text", "expr": "$\\mathrm{\\mathrm{KL}}\\left(q\\left(\\theta|\\phi\\right)||p\\left(\\theta|D\\right)%\n\\right)=-\\mathrm{E}_{\\theta\\sim q\\left(\\theta|\\phi\\right)}\\left[\\log\\frac{p%\n\\left(\\theta|D\\right)}{q\\left(\\theta|\\phi\\right)}\\right]$", "word_idx": 12737, "sentence_idx": 134, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{\\mathrm{KL}}\\left(q\\left(\\theta|\\phi\\right)||p\\left(\\theta|D\\right)%\n\\right)=-\\mathrm{E}_{\\theta\\sim q\\left(\\theta|\\phi\\right)}\\left[\\log\\frac{p%\n\\left(\\theta|D\\right)}{q\\left(\\theta|\\phi\\right)}\\right]$$", "word_idx": 12949, "sentence_idx": 135, "label": "unlabeled"}, {"type": "text", "expr": "The optimal variational parameters is the solution of the optimization problem:", "word_idx": 13159, "sentence_idx": 136, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\phi^{*}$", "word_idx": 13238, "sentence_idx": 137, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\phi^{*}$$", "word_idx": 13261, "sentence_idx": 138, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\arg\\min_{\\phi}\\int q\\left(\\theta|\\phi\\right)\\log\\frac{q\\left(%\n\\theta|\\phi\\right)}{p\\left(\\theta|D\\right)}d\\theta$", "word_idx": 13282, "sentence_idx": 139, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\arg\\min_{\\phi}\\int q\\left(\\theta|\\phi\\right)\\log\\frac{q\\left(%\n\\theta|\\phi\\right)}{p\\left(\\theta|D\\right)}d\\theta$$", "word_idx": 13412, "sentence_idx": 140, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\arg\\min_{\\phi}\\int q\\left(\\theta|\\phi\\right)\\log\\frac{q\\left(%\n\\theta|\\phi\\right)}{p\\left(D|\\theta\\right)p\\left(\\theta\\right)}d\\theta$", "word_idx": 13540, "sentence_idx": 141, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\arg\\min_{\\phi}\\int q\\left(\\theta|\\phi\\right)\\log\\frac{q\\left(%\n\\theta|\\phi\\right)}{p\\left(D|\\theta\\right)p\\left(\\theta\\right)}d\\theta$$", "word_idx": 13690, "sentence_idx": 142, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\arg\\min_{\\phi}E_{\\theta\\sim q\\left(\\theta|\\phi\\right)}\\left[%\n\\log\\left(q\\left(\\theta|\\phi\\right)\\right)\\right$", "word_idx": 13838, "sentence_idx": 143, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\arg\\min_{\\phi}E_{\\theta\\sim q\\left(\\theta|\\phi\\right)}\\left[%\n\\log\\left(q\\left(\\theta|\\phi\\right)\\right)\\right.$$", "word_idx": 13965, "sentence_idx": 144, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\left-\\log\\left(p\\left(\\theta\\right)\\right)+\\mathrm{L\\left(%\n\\theta\\right)}\\right],$", "word_idx": 14091, "sentence_idx": 145, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\left.-\\log\\left(p\\left(\\theta\\right)\\right)+\\mathrm{L\\left(%\n\\theta\\right)}\\right],$$", "word_idx": 14189, "sentence_idx": 146, "label": "unlabeled"}, {"type": "text", "expr": "where  $\\mathrm{L}\\left(\\theta\\right)=-\\log p\\left(D|\\theta\\right)$ \nis the log-likelihood cost function ", "word_idx": 14286, "sentence_idx": 147, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{L}\\left(\\theta\\right)=-\\log p\\left(D|\\theta\\right)$$", "word_idx": 14391, "sentence_idx": 148, "label": "unlabeled"}, {"type": "text", "expr": "2 Notice that we define a cumulative log-likelihood cost function over the data", "word_idx": 14449, "sentence_idx": 149, "label": "unlabeled"}, {"type": "text", "expr": "In online variational Bayes, we aim to find the posterior in an online\nsetting, where the data arrives sequentially", "word_idx": 14528, "sentence_idx": 150, "label": "unlabeled"}, {"type": "text", "expr": " Similar to Bayesian\ninference we use the previous approximated posterior as new prior\ndistribution", "word_idx": 14643, "sentence_idx": 151, "label": "unlabeled"}, {"type": "text", "expr": " For example, at time  $n+1$  the optimal variational\nparameters is the solution of the optimization problem:", "word_idx": 14742, "sentence_idx": 152, "label": "unlabeled"}, {"type": "math", "expr": "$$n+1$$", "word_idx": 14851, "sentence_idx": 153, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\phi^{*}$", "word_idx": 14854, "sentence_idx": 154, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\phi^{*}$$", "word_idx": 14877, "sentence_idx": 155, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\arg\\min_{\\phi}\\int q_{n+1}\\left(\\theta|\\phi\\right)\\log\\frac{q_{%\nn+1}\\left(\\theta|\\phi\\right)}{p\\left(\\theta|D_{n+1}\\right)}d\\theta$", "word_idx": 14898, "sentence_idx": 156, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\arg\\min_{\\phi}\\int q_{n+1}\\left(\\theta|\\phi\\right)\\log\\frac{q_{%\nn+1}\\left(\\theta|\\phi\\right)}{p\\left(\\theta|D_{n+1}\\right)}d\\theta$$", "word_idx": 15046, "sentence_idx": 157, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\arg\\min_{\\phi}\\int q_{n+1}\\left(\\theta|\\phi\\right)\\log\\frac{q_{%\nn+1}\\left(\\theta|\\phi\\right)}{p\\left(D_{n+1}|\\theta\\right)q_{n}\\left(\\theta%\n\\right)}d\\theta$", "word_idx": 15192, "sentence_idx": 158, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\arg\\min_{\\phi}\\int q_{n+1}\\left(\\theta|\\phi\\right)\\log\\frac{q_{%\nn+1}\\left(\\theta|\\phi\\right)}{p\\left(D_{n+1}|\\theta\\right)q_{n}\\left(\\theta%\n\\right)}d\\theta$$", "word_idx": 15366, "sentence_idx": 159, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\arg\\min_{\\phi}\\mathrm{E}_{\\theta\\sim q_{n+1}\\left(\\theta|\\phi%\n\\right)}\\left[\\log\\left(q_{n+1}\\left(\\theta|\\phi\\right)\\right)\\right$", "word_idx": 15538, "sentence_idx": 160, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\arg\\min_{\\phi}\\mathrm{E}_{\\theta\\sim q_{n+1}\\left(\\theta|\\phi%\n\\right)}\\left[\\log\\left(q_{n+1}\\left(\\theta|\\phi\\right)\\right)\\right.$$", "word_idx": 15686, "sentence_idx": 161, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\left-\\log\\left(q_{n}\\left(\\theta\\right)\\right)+\\mathrm{L\\left(%\n\\theta\\right)}\\right],$", "word_idx": 15833, "sentence_idx": 162, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\left.-\\log\\left(q_{n}\\left(\\theta\\right)\\right)+\\mathrm{L\\left(%\n\\theta\\right)}\\right],$$", "word_idx": 15935, "sentence_idx": 163, "label": "unlabeled"}, {"type": "text", "expr": "where  $\\mathrm{L}\\left(\\theta\\right)=-\\log p\\left(D_{n+1}|\\theta\\right)$ \nis the log-likelihood cost function", "word_idx": 16036, "sentence_idx": 164, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{L}\\left(\\theta\\right)=-\\log p\\left(D_{n+1}|\\theta\\right)$$", "word_idx": 16146, "sentence_idx": 165, "label": "unlabeled"}, {"type": "text", "expr": "2  DIAGONAL GAUSSIAN APPROXIMATION", "word_idx": 16210, "sentence_idx": 166, "label": "unlabeled"}, {"type": "text", "expr": "And the following holds:", "word_idx": 16244, "sentence_idx": 167, "label": "unlabeled"}, {"type": "text", "expr": "A standard approach to define the parametric distribution  $q\\left(\\theta|\\phi\\right)$  so that all the components of the parameter vector  $\\theta$  would be factorized, i", "word_idx": 16268, "sentence_idx": 168, "label": "unlabeled"}, {"type": "text", "expr": " independent (\u201dmean-field approximation\u201d)", "word_idx": 16440, "sentence_idx": 169, "label": "unlabeled"}, {"type": "text", "expr": " In addition, in this paper we will focus on the case\nin which the parametric distribution  $q\\left(\\theta|\\phi\\right)$  and\nthe prior distribution are Gaussian", "word_idx": 16481, "sentence_idx": 170, "label": "unlabeled"}, {"type": "text", "expr": " Therefore:", "word_idx": 16641, "sentence_idx": 171, "label": "unlabeled"}, {"type": "math", "expr": "$$q\\left(\\theta|\\phi\\right)$$", "word_idx": 16652, "sentence_idx": 172, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta$$", "word_idx": 16677, "sentence_idx": 173, "label": "unlabeled"}, {"type": "math", "expr": "$$q\\left(\\theta|\\phi\\right)$$", "word_idx": 16683, "sentence_idx": 174, "label": "unlabeled"}, {"type": "text", "expr": "$q_{n+1}\\left(\\theta|\\phi\\right)=\\prod_{i}\\mathcal{N}\\left(\\theta_{i}|\\mu_{i},%\n\\sigma_{i}^{2}\\right)$", "word_idx": 16708, "sentence_idx": 175, "label": "unlabeled"}, {"type": "math", "expr": "$$q_{n+1}\\left(\\theta|\\phi\\right)=\\prod_{i}\\mathcal{N}\\left(\\theta_{i}|\\mu_{i},%\n\\sigma_{i}^{2}\\right)$$", "word_idx": 16810, "sentence_idx": 176, "label": "unlabeled"}, {"type": "text", "expr": "$q_{n}\\left(\\theta\\right)=\\prod_{i}\\mathcal{N}\\left(\\theta_{i}|m_{i},v_{i}^{2}\\right)$", "word_idx": 16910, "sentence_idx": 177, "label": "unlabeled"}, {"type": "math", "expr": "$$q_{n}\\left(\\theta\\right)=\\prod_{i}\\mathcal{N}\\left(\\theta_{i}|m_{i},v_{i}^{2}\\right)$$", "word_idx": 16996, "sentence_idx": 178, "label": "unlabeled"}, {"type": "text", "expr": "In order to solve the optimization problem eq", "word_idx": 17080, "sentence_idx": 179, "label": "unlabeled"}, {"type": "text", "expr": " ( 6 ), we use\nthe unbiased Monte Carlo gradients, similarly to  ", "word_idx": 17125, "sentence_idx": 180, "label": "unlabeled"}, {"type": "text", "expr": " We define deterministic\ntransformation:", "word_idx": 17190, "sentence_idx": 181, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\theta_{i}$", "word_idx": 17230, "sentence_idx": 182, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\theta_{i}$$", "word_idx": 17255, "sentence_idx": 183, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\mu_{i}+\\varepsilon_{i}\\sigma_{i}$", "word_idx": 17278, "sentence_idx": 184, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\mu_{i}+\\varepsilon_{i}\\sigma_{i}$$", "word_idx": 17327, "sentence_idx": 185, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\varepsilon_{i}$", "word_idx": 17374, "sentence_idx": 186, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\varepsilon_{i}$$", "word_idx": 17404, "sentence_idx": 187, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\sim\\mathcal{N}\\left(0,1\\right)$", "word_idx": 17432, "sentence_idx": 188, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\sim\\mathcal{N}\\left(0,1\\right)$$", "word_idx": 17478, "sentence_idx": 189, "label": "unlabeled"}, {"type": "text", "expr": "\u03d5 (9)", "word_idx": 17522, "sentence_idx": 190, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\phi$", "word_idx": 17527, "sentence_idx": 191, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\phi$$", "word_idx": 17546, "sentence_idx": 192, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\left(\\mu,\\sigma\\right)$", "word_idx": 17563, "sentence_idx": 193, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\left(\\mu,\\sigma\\right)$$", "word_idx": 17602, "sentence_idx": 194, "label": "unlabeled"}, {"type": "text", "expr": "And the following holds:", "word_idx": 17639, "sentence_idx": 195, "label": "unlabeled"}, {"type": "text", "expr": "$\\frac{\\partial}{\\partial\\phi}\\mathrm{E}_{\\theta}\\left[f\\left(\\theta,\\phi\\right%\n)\\right]=\\mathrm{E}_{\\varepsilon}\\left[\\frac{\\partial f\\left(\\theta,\\phi\\right%\n)}{\\partial\\theta}\\frac{\\partial\\theta}{\\partial\\phi}+\\frac{\\partial f\\left(%\n\\theta,\\phi\\right)}{\\partial\\phi}\\right],$", "word_idx": 17663, "sentence_idx": 196, "label": "unlabeled"}, {"type": "math", "expr": "$$\\frac{\\partial}{\\partial\\phi}\\mathrm{E}_{\\theta}\\left[f\\left(\\theta,\\phi\\right%\n)\\right]=\\mathrm{E}_{\\varepsilon}\\left[\\frac{\\partial f\\left(\\theta,\\phi\\right%\n)}{\\partial\\theta}\\frac{\\partial\\theta}{\\partial\\phi}+\\frac{\\partial f\\left(%\n\\theta,\\phi\\right)}{\\partial\\phi}\\right],$$", "word_idx": 17944, "sentence_idx": 197, "label": "unlabeled"}, {"type": "text", "expr": "where  $f\\left(\\theta,\\phi\\right)=\\log\\left(q_{n+1}\\left(\\theta|\\phi\\right)\\right)-%\n\\log\\left(q_{n}\\left(\\theta\\right)\\right)+\\mathrm{L\\left(\\theta\\right)}$ ", "word_idx": 18223, "sentence_idx": 198, "label": "unlabeled"}, {"type": "text", "expr": "\nTherefore, we can find a critical point of the objective function\nby solving the following set of equations:", "word_idx": 18381, "sentence_idx": 199, "label": "unlabeled"}, {"type": "math", "expr": "$$f\\left(\\theta,\\phi\\right)=\\log\\left(q_{n+1}\\left(\\theta|\\phi\\right)\\right)-%\n\\log\\left(q_{n}\\left(\\theta\\right)\\right)+\\mathrm{L\\left(\\theta\\right)}$$", "word_idx": 18490, "sentence_idx": 200, "label": "unlabeled"}, {"type": "text", "expr": "$\\mathrm{E}_{\\varepsilon}\\left[\\frac{\\partial f\\left(\\theta,\\phi\\right)}{%\n\\partial\\theta}\\frac{\\partial\\theta}{\\partial\\phi}+\\frac{\\partial f\\left(%\n\\theta,\\phi\\right)}{\\partial\\phi}\\right]=0$", "word_idx": 18638, "sentence_idx": 201, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{E}_{\\varepsilon}\\left[\\frac{\\partial f\\left(\\theta,\\phi\\right)}{%\n\\partial\\theta}\\frac{\\partial\\theta}{\\partial\\phi}+\\frac{\\partial f\\left(%\n\\theta,\\phi\\right)}{\\partial\\phi}\\right]=0$$", "word_idx": 18831, "sentence_idx": 202, "label": "unlabeled"}, {"type": "text", "expr": "Substituting eq", "word_idx": 19022, "sentence_idx": 203, "label": "unlabeled"}, {"type": "text", "expr": " ( 7 ), eq", "word_idx": 19037, "sentence_idx": 204, "label": "unlabeled"}, {"type": "text", "expr": " ( 8 ) and eq", "word_idx": 19047, "sentence_idx": 205, "label": "unlabeled"}, {"type": "text", "expr": " ( 9 )\ninto eq", "word_idx": 19060, "sentence_idx": 206, "label": "unlabeled"}, {"type": "text", "expr": " ( 10 ) and we get (see appendix for additional\ndetails):", "word_idx": 19074, "sentence_idx": 207, "label": "unlabeled"}, {"type": "text", "expr": "$\\mu_{i}=m_{i}-v_{i}^{2}\\mathrm{E}_{\\varepsilon}\\left[\\frac{\\partial L\\left(%\n\\theta\\right)}{\\partial\\theta_{i}}\\right]$", "word_idx": 19131, "sentence_idx": 208, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mu_{i}=m_{i}-v_{i}^{2}\\mathrm{E}_{\\varepsilon}\\left[\\frac{\\partial L\\left(%\n\\theta\\right)}{\\partial\\theta_{i}}\\right]$$", "word_idx": 19251, "sentence_idx": 209, "label": "unlabeled"}, {"type": "text", "expr": "$\\sigma_{i}=v_{i}\\sqrt{\\!1+\\!\\left(\\frac{1}{2}v_{i}E_{\\varepsilon}\\left[\\frac{%\n\\partial L\\left(\\theta\\right)}{\\partial\\theta_{i}}\\varepsilon_{i}\\right]\\right%\n)^{2}}-\\frac{1}{2}v_{i}^{2}E_{\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta%\n\\right)}{\\partial\\theta_{i}}\\varepsilon_{i}\\right]$", "word_idx": 19369, "sentence_idx": 210, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma_{i}=v_{i}\\sqrt{\\!1+\\!\\left(\\frac{1}{2}v_{i}E_{\\varepsilon}\\left[\\frac{%\n\\partial L\\left(\\theta\\right)}{\\partial\\theta_{i}}\\varepsilon_{i}\\right]\\right%\n)^{2}}-\\frac{1}{2}v_{i}^{2}E_{\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta%\n\\right)}{\\partial\\theta_{i}}\\varepsilon_{i}\\right]$$", "word_idx": 19658, "sentence_idx": 211, "label": "unlabeled"}, {"type": "text", "expr": "Notice that eq", "word_idx": 19945, "sentence_idx": 212, "label": "unlabeled"}, {"type": "text", "expr": " ( 12 ) and eq", "word_idx": 19959, "sentence_idx": 213, "label": "unlabeled"}, {"type": "text", "expr": " ( 13 ) are implicit equations,\nsince the derivative  $\\frac{\\partial L\\left(\\theta\\right)}{\\partial\\theta_{i}}$ \nis a function of  $\\mu_{i}$  and  $\\sigma_{i}$ ", "word_idx": 19973, "sentence_idx": 214, "label": "unlabeled"}, {"type": "text", "expr": " To approximate the solution we use a single explicit iteration of this equation, i", "word_idx": 20134, "sentence_idx": 215, "label": "unlabeled"}, {"type": "text", "expr": " evaluate the derivative  $\\frac{\\partial L\\left(\\theta\\right)}{\\partial\\theta_{i}}$  using the prior parameters", "word_idx": 20217, "sentence_idx": 216, "label": "unlabeled"}, {"type": "text", "expr": " In this approximation,\nwe assume that the optimal point is close at each step, which can lead to slow rate of convergence", "word_idx": 20329, "sentence_idx": 217, "label": "unlabeled"}, {"type": "text", "expr": " We use an hyperparameter  $\\eta$  to compensate over the added error", "word_idx": 20451, "sentence_idx": 218, "label": "unlabeled"}, {"type": "text", "expr": " Algorithm  Until  summarizes the implementation details", "word_idx": 20520, "sentence_idx": 219, "label": "unlabeled"}, {"type": "math", "expr": "$$\\frac{\\partial L\\left(\\theta\\right)}{\\partial\\theta_{i}}$$", "word_idx": 20576, "sentence_idx": 220, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mu_{i}$$", "word_idx": 20632, "sentence_idx": 221, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma_{i}$$", "word_idx": 20639, "sentence_idx": 222, "label": "unlabeled"}, {"type": "math", "expr": "$$\\frac{\\partial L\\left(\\theta\\right)}{\\partial\\theta_{i}}$$", "word_idx": 20649, "sentence_idx": 223, "label": "unlabeled"}, {"type": "math", "expr": "$$\\eta$$", "word_idx": 20705, "sentence_idx": 224, "label": "unlabeled"}, {"type": "text", "expr": "Until", "word_idx": 20709, "sentence_idx": 225, "label": "unlabeled"}, {"type": "text", "expr": "Until", "word_idx": 20714, "sentence_idx": 226, "label": "unlabeled"}, {"type": "text", "expr": "The expectations are estimated using Monte Carlo method: where:", "word_idx": 20719, "sentence_idx": 227, "label": "unlabeled"}, {"type": "text", "expr": "Algorithm\u00a01  Bayesian Gradient Descent (BGD)", "word_idx": 20782, "sentence_idx": 228, "label": "unlabeled"}, {"type": "text", "expr": "Algorithm\u00a01", "word_idx": 20826, "sentence_idx": 229, "label": "unlabeled"}, {"type": "text", "expr": "Initialize", "word_idx": 20837, "sentence_idx": 230, "label": "unlabeled"}, {"type": "text", "expr": "$\\mu,\\sigma,\\eta$", "word_idx": 20847, "sentence_idx": 231, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mu,\\sigma,\\eta$$", "word_idx": 20864, "sentence_idx": 232, "label": "unlabeled"}, {"type": "text", "expr": "Repeat", "word_idx": 20879, "sentence_idx": 233, "label": "unlabeled"}, {"type": "text", "expr": "$\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\mu_{i}\\leftarrow\\mu_{i}-\\eta\\sigma_{i}^{2}E_{%\n\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)}{\\partial\\theta_{i}}\\right]$", "word_idx": 20885, "sentence_idx": 234, "label": "unlabeled"}, {"type": "math", "expr": "$$\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\mu_{i}\\leftarrow\\mu_{i}-\\eta\\sigma_{i}^{2}E_{%\n\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)}{\\partial\\theta_{i}}\\right]$$", "word_idx": 21040, "sentence_idx": 235, "label": "unlabeled"}, {"type": "text", "expr": "$\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\sigma_{i}\\leftarrow\\sigma_{i}\\sqrt{1+\\left(\\frac{1}{2%\n}\\sigma_{i}E_{\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)}{\\partial%\n\\theta_{i}}\\varepsilon_{i}\\right]\\right)^{2}}-\\frac{1}{2}\\sigma_{i}^{2}E_{%\n\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)}{\\partial\\theta_{i}}%\n\\varepsilon_{i}\\right]$", "word_idx": 21193, "sentence_idx": 236, "label": "unlabeled"}, {"type": "math", "expr": "$$\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\sigma_{i}\\leftarrow\\sigma_{i}\\sqrt{1+\\left(\\frac{1}{2%\n}\\sigma_{i}E_{\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)}{\\partial%\n\\theta_{i}}\\varepsilon_{i}\\right]\\right)^{2}}-\\frac{1}{2}\\sigma_{i}^{2}E_{%\n\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)}{\\partial\\theta_{i}}%\n\\varepsilon_{i}\\right]$$", "word_idx": 21528, "sentence_idx": 237, "label": "unlabeled"}, {"type": "text", "expr": "Until", "word_idx": 21861, "sentence_idx": 238, "label": "unlabeled"}, {"type": "text", "expr": "convergence criterion is met", "word_idx": 21866, "sentence_idx": 239, "label": "unlabeled"}, {"type": "text", "expr": "convergence criterion is met", "word_idx": 21894, "sentence_idx": 240, "label": "unlabeled"}, {"type": "text", "expr": "The expectations are estimated using Monte Carlo method:", "word_idx": 21922, "sentence_idx": 241, "label": "unlabeled"}, {"type": "text", "expr": "$\\mathrm{E}_{\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)}{\\partial%\n\\theta_{i}}\\right]\\simeq\\frac{1}{K}\\sum_{k=1}^{K}\\frac{\\partial L\\left(\\theta^%\n{(k)}\\right)}{\\partial\\theta_{i}}$", "word_idx": 21978, "sentence_idx": 242, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{E}_{\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)}{\\partial%\n\\theta_{i}}\\right]\\simeq\\frac{1}{K}\\sum_{k=1}^{K}\\frac{\\partial L\\left(\\theta^%\n{(k)}\\right)}{\\partial\\theta_{i}}$$", "word_idx": 22170, "sentence_idx": 243, "label": "unlabeled"}, {"type": "text", "expr": "$\\mathrm{E}_{\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)}{\\partial%\n\\theta_{i}}\\varepsilon_{i}\\right]\\simeq\\frac{1}{K}\\sum_{k=1}^{K}\\frac{\\partial\nL%\n\\left(\\theta^{(k)}\\right)}{\\partial\\theta_{i}}\\varepsilon_{i}^{(k)}$", "word_idx": 22360, "sentence_idx": 244, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{E}_{\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)}{\\partial%\n\\theta_{i}}\\varepsilon_{i}\\right]\\simeq\\frac{1}{K}\\sum_{k=1}^{K}\\frac{\\partial\nL%\n\\left(\\theta^{(k)}\\right)}{\\partial\\theta_{i}}\\varepsilon_{i}^{(k)}$$", "word_idx": 22588, "sentence_idx": 245, "label": "unlabeled"}, {"type": "text", "expr": "where:", "word_idx": 22814, "sentence_idx": 246, "label": "unlabeled"}, {"type": "text", "expr": "$\\theta_{i}^{(k)}=\\mu_{i}+\\varepsilon_{i}^{(k)}\\sigma_{i}$", "word_idx": 22820, "sentence_idx": 247, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta_{i}^{(k)}=\\mu_{i}+\\varepsilon_{i}^{(k)}\\sigma_{i}$$", "word_idx": 22878, "sentence_idx": 248, "label": "unlabeled"}, {"type": "text", "expr": "3  THEORETICAL PROPERTIES OF BAYESIAN GRADIENT DESCENT", "word_idx": 22934, "sentence_idx": 249, "label": "unlabeled"}, {"type": "text", "expr": "Bayesian Gradient Descent (BGD) consists of a gradient descent algorithm\nfor  $\\mu$ , and a recursive update rule for  $\\sigma$ ", "word_idx": 22988, "sentence_idx": 250, "label": "unlabeled"}, {"type": "text", "expr": " Both result from an approximation to the online Bayes update in eq", "word_idx": 23116, "sentence_idx": 251, "label": "unlabeled"}, {"type": "text", "expr": " ( 3 ), and hence the name BGD", "word_idx": 23183, "sentence_idx": 252, "label": "unlabeled"}, {"type": "text", "expr": " The learning rate of  $\\mu_{i}$  is proportional to the uncertainty in the parameter  $\\theta_{i}$  according to the prior distribution", "word_idx": 23213, "sentence_idx": 253, "label": "unlabeled"}, {"type": "text", "expr": " During the learning process, as more data is seen, the learning rate of parameters with a high degree of certainty decreases, while the learning rate increase for parameters with a high degree of uncertainty", "word_idx": 23349, "sentence_idx": 254, "label": "unlabeled"}, {"type": "text", "expr": " Next, we establish this intuitive idea more precisely", "word_idx": 23557, "sentence_idx": 255, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mu$$", "word_idx": 23611, "sentence_idx": 256, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma$$", "word_idx": 23614, "sentence_idx": 257, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mu_{i}$$", "word_idx": 23620, "sentence_idx": 258, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta_{i}$$", "word_idx": 23627, "sentence_idx": 259, "label": "unlabeled"}, {"type": "text", "expr": "It is easy to verify that the update rule for  $\\sigma$  is a strictly monotonically decreasing\nfunction of  $E_{\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)}{\\partial\\theta_{i}}%\n\\varepsilon_{i}\\right]$ ", "word_idx": 23637, "sentence_idx": 260, "label": "unlabeled"}, {"type": "text", "expr": "\nTherefore:", "word_idx": 23850, "sentence_idx": 261, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma$$", "word_idx": 23861, "sentence_idx": 262, "label": "unlabeled"}, {"type": "math", "expr": "$$E_{\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)}{\\partial\\theta_{i}}%\n\\varepsilon_{i}\\right]$$", "word_idx": 23867, "sentence_idx": 263, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\mathrm{E}_{\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)%\n}{\\partial\\theta_{i}}\\varepsilon_{i}\\right]$", "word_idx": 23968, "sentence_idx": 264, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\mathrm{E}_{\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)%\n}{\\partial\\theta_{i}}\\varepsilon_{i}\\right]$$", "word_idx": 24093, "sentence_idx": 265, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle>0\\rightarrow\\sigma_{i}\\left(t+1\\right)<\\sigma_{i}\\left(t\\right)$", "word_idx": 24216, "sentence_idx": 266, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle>0\\rightarrow\\sigma_{i}\\left(t+1\\right)<\\sigma_{i}\\left(t\\right)$$", "word_idx": 24295, "sentence_idx": 267, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\mathrm{E}_{\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)%\n}{\\partial\\theta_{i}}\\varepsilon_{i}\\right]$", "word_idx": 24372, "sentence_idx": 268, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\mathrm{E}_{\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)%\n}{\\partial\\theta_{i}}\\varepsilon_{i}\\right]$$", "word_idx": 24497, "sentence_idx": 269, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle<0\\rightarrow\\sigma_{i}\\left(t+1\\right)>\\sigma_{i}\\left(t\\right)$", "word_idx": 24620, "sentence_idx": 270, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle<0\\rightarrow\\sigma_{i}\\left(t+1\\right)>\\sigma_{i}\\left(t\\right)$$", "word_idx": 24699, "sentence_idx": 271, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\mathrm{E}_{\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)%\n}{\\partial\\theta_{i}}\\varepsilon_{i}\\right]$", "word_idx": 24776, "sentence_idx": 272, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\mathrm{E}_{\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)%\n}{\\partial\\theta_{i}}\\varepsilon_{i}\\right]$$", "word_idx": 24901, "sentence_idx": 273, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=0\\rightarrow\\sigma_{i}\\left(t+1\\right)=\\sigma_{i}\\left(t\\right)$", "word_idx": 25024, "sentence_idx": 274, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=0\\rightarrow\\sigma_{i}\\left(t+1\\right)=\\sigma_{i}\\left(t\\right)$$", "word_idx": 25103, "sentence_idx": 275, "label": "unlabeled"}, {"type": "text", "expr": "Next, using a Taylor expansion, we show that for small values of  $\\sigma$ , the quantity  $E_{\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)}{\\partial\\theta_{i}}%\n\\varepsilon_{i}\\right]$ , approximates the curvature of the loss:", "word_idx": 25180, "sentence_idx": 276, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma$$", "word_idx": 25416, "sentence_idx": 277, "label": "unlabeled"}, {"type": "math", "expr": "$$E_{\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)}{\\partial\\theta_{i}}%\n\\varepsilon_{i}\\right]$$", "word_idx": 25422, "sentence_idx": 278, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\mathrm{E}_{\\epsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)}{%\n\\partial\\theta_{i}}\\epsilon_{i}\\right]=\\mathrm{E}_{\\epsilon}\\left[\\frac{%\n\\partial L\\left(\\mu+\\epsilon\\sigma\\right)}{\\partial\\theta_{i}}\\epsilon_{i}\\right]$", "word_idx": 25523, "sentence_idx": 279, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\mathrm{E}_{\\epsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)}{%\n\\partial\\theta_{i}}\\epsilon_{i}\\right]=\\mathrm{E}_{\\epsilon}\\left[\\frac{%\n\\partial L\\left(\\mu+\\epsilon\\sigma\\right)}{\\partial\\theta_{i}}\\epsilon_{i}\\right]$$", "word_idx": 25759, "sentence_idx": 280, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\mathrm{E}_{\\epsilon}\\left[\\left(\\frac{\\partial L\\left(\\mu\\right%\n)}{\\partial\\theta_{i}}+\\sum_{j}\\frac{\\partial^{2}L\\left(\\mu\\right)}{\\partial%\n\\theta_{i}\\partial\\theta_{j}}\\epsilon_{j}\\sigma_{j}+O\\left(\\left\\|\\sigma\\right%\n\\|^{2}\\right)\\right)\\epsilon_{i}\\right]$", "word_idx": 25993, "sentence_idx": 281, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\mathrm{E}_{\\epsilon}\\left[\\left(\\frac{\\partial L\\left(\\mu\\right%\n)}{\\partial\\theta_{i}}+\\sum_{j}\\frac{\\partial^{2}L\\left(\\mu\\right)}{\\partial%\n\\theta_{i}\\partial\\theta_{j}}\\epsilon_{j}\\sigma_{j}+O\\left(\\left\\|\\sigma\\right%\n\\|^{2}\\right)\\right)\\epsilon_{i}\\right]$$", "word_idx": 26272, "sentence_idx": 282, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\frac{\\partial^{2}L\\left(\\mu\\right)}{\\partial^{2}\\theta_{i}}%\n\\sigma_{i}+O\\left(\\left\\|\\sigma\\right\\|^{2}\\right),$", "word_idx": 26549, "sentence_idx": 283, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\frac{\\partial^{2}L\\left(\\mu\\right)}{\\partial^{2}\\theta_{i}}%\n\\sigma_{i}+O\\left(\\left\\|\\sigma\\right\\|^{2}\\right),$$", "word_idx": 26678, "sentence_idx": 284, "label": "unlabeled"}, {"type": "text", "expr": "where we used  $\\mathrm{E}_{\\epsilon}\\left[\\epsilon_{i}\\right]=0$  and  $\\mathrm{E}_{\\epsilon}\\left[\\epsilon_{i}\\epsilon_{j}\\right]=\\delta_{ij}$  in the last line", "word_idx": 26805, "sentence_idx": 285, "label": "unlabeled"}, {"type": "text", "expr": " Thus, in this case,  $E_{\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)}{\\partial\\theta_{i}}%\n\\varepsilon_{i}\\right]$  is a finite difference approximation to the component-wise product of the diagonal of the Hessian of the loss, and the vector  $\\sigma$ ", "word_idx": 26967, "sentence_idx": 286, "label": "unlabeled"}, {"type": "text", "expr": " Therefore, we expect that the uncertainty (learning rate) would decrease in areas with positive curvature (e", "word_idx": 27230, "sentence_idx": 287, "label": "unlabeled"}, {"type": "text", "expr": ", near local minima), or increase in areas with high negative curvature (e", "word_idx": 27339, "sentence_idx": 288, "label": "unlabeled"}, {"type": "text", "expr": ", near maxima, or saddles)", "word_idx": 27413, "sentence_idx": 289, "label": "unlabeled"}, {"type": "text", "expr": " This seems like a \u201dsensible\u201d behavior of the algorithm, since we wish to converge to local minima, and escape saddles", "word_idx": 27439, "sentence_idx": 290, "label": "unlabeled"}, {"type": "text", "expr": " This in contrast to many common optimization methods, which are either insensitive to the sign of the curvature, or use it the wrong way  ", "word_idx": 27557, "sentence_idx": 291, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{E}_{\\epsilon}\\left[\\epsilon_{i}\\right]=0$$", "word_idx": 27696, "sentence_idx": 292, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{E}_{\\epsilon}\\left[\\epsilon_{i}\\epsilon_{j}\\right]=\\delta_{ij}$$", "word_idx": 27744, "sentence_idx": 293, "label": "unlabeled"}, {"type": "math", "expr": "$$E_{\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)}{\\partial\\theta_{i}}%\n\\varepsilon_{i}\\right]$$", "word_idx": 27814, "sentence_idx": 294, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma$$", "word_idx": 27915, "sentence_idx": 295, "label": "unlabeled"}, {"type": "text", "expr": "In the case of strongly convex loss, we can make a more rigorous statement, which we prove in the appendix:", "word_idx": 27921, "sentence_idx": 296, "label": "unlabeled"}, {"type": "text", "expr": "In the case of strongly convex loss, we can make a more rigorous statement, which we prove in the appendix:", "word_idx": 28028, "sentence_idx": 297, "label": "unlabeled"}, {"type": "text", "expr": "Theorem 1 ", "word_idx": 28135, "sentence_idx": 298, "label": "unlabeled"}, {"type": "text", "expr": "Theorem 1", "word_idx": 28145, "sentence_idx": 299, "label": "unlabeled"}, {"type": "text", "expr": "We examine BGD with a diagonal Gaussian distribution for  $\\theta$ ", "word_idx": 28154, "sentence_idx": 300, "label": "unlabeled"}, {"type": "text", "expr": " If  $L\\left(\\theta\\right)$  is a strongly convex function with parameter  $m>0$  and a continuously\ndifferentiable function over  $\\mathbb{R}^{n}$ , then\n $\\mathbf{\\mathrm{E}}_{\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)}{%\n\\partial\\theta_{i}}\\varepsilon_{i}\\right]>0$ ", "word_idx": 28221, "sentence_idx": 301, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta$$", "word_idx": 28501, "sentence_idx": 302, "label": "unlabeled"}, {"type": "math", "expr": "$$L\\left(\\theta\\right)$$", "word_idx": 28507, "sentence_idx": 303, "label": "unlabeled"}, {"type": "math", "expr": "$$m>0$$", "word_idx": 28527, "sentence_idx": 304, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbb{R}^{n}$$", "word_idx": 28530, "sentence_idx": 305, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{\\mathrm{E}}_{\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)}{%\n\\partial\\theta_{i}}\\varepsilon_{i}\\right]>0$$", "word_idx": 28544, "sentence_idx": 306, "label": "unlabeled"}, {"type": "text", "expr": "As a consequence eq", "word_idx": 28665, "sentence_idx": 307, "label": "unlabeled"}, {"type": "text", "expr": " ( 14 ) and Theorem  1 , we have", "word_idx": 28684, "sentence_idx": 308, "label": "unlabeled"}, {"type": "text", "expr": "Corollary 1 ", "word_idx": 28716, "sentence_idx": 309, "label": "unlabeled"}, {"type": "text", "expr": "Corollary 1", "word_idx": 28728, "sentence_idx": 310, "label": "unlabeled"}, {"type": "text", "expr": "If  $L\\left(\\theta\\right)$  is strongly convex, then the sequence  $\\left\\{\\sigma_{i}\\left(t\\right)\\right\\}_{t=1}^{T}$ \nis monotonically decreasing", "word_idx": 28739, "sentence_idx": 311, "label": "unlabeled"}, {"type": "math", "expr": "$$L\\left(\\theta\\right)$$", "word_idx": 28886, "sentence_idx": 312, "label": "unlabeled"}, {"type": "math", "expr": "$$\\left\\{\\sigma_{i}\\left(t\\right)\\right\\}_{t=1}^{T}$$", "word_idx": 28906, "sentence_idx": 313, "label": "unlabeled"}, {"type": "text", "expr": "Similarly, one can show that if  $L\\left(\\theta\\right)$  is strongly concave, then the sequence  $\\left\\{\\sigma_{i}\\left(t\\right)\\right\\}_{t=1}^{T}$  is monotonically increasing", "word_idx": 28955, "sentence_idx": 314, "label": "unlabeled"}, {"type": "text", "expr": " Furthermore, one can generalize these results and show that if a restriction of  $L\\left(\\theta\\right)$  to an axis  $\\theta=\\theta_{i}$  is strongly convex then  $\\left\\{\\sigma_{i}\\left(t\\right)\\right\\}_{t=1}^{T}$ \nis monotonic decreasing, while if a restriction of  $L\\left(\\theta\\right)$ \nto an axis  $\\theta=\\theta_{i}$  is strongly concave then  $\\left\\{\\sigma_{i}\\left(t\\right)\\right\\}_{t=1}^{T}$  is monotonic increasing", "word_idx": 29132, "sentence_idx": 315, "label": "unlabeled"}, {"type": "math", "expr": "$$L\\left(\\theta\\right)$$", "word_idx": 29560, "sentence_idx": 316, "label": "unlabeled"}, {"type": "math", "expr": "$$\\left\\{\\sigma_{i}\\left(t\\right)\\right\\}_{t=1}^{T}$$", "word_idx": 29580, "sentence_idx": 317, "label": "unlabeled"}, {"type": "math", "expr": "$$L\\left(\\theta\\right)$$", "word_idx": 29629, "sentence_idx": 318, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta=\\theta_{i}$$", "word_idx": 29649, "sentence_idx": 319, "label": "unlabeled"}, {"type": "math", "expr": "$$\\left\\{\\sigma_{i}\\left(t\\right)\\right\\}_{t=1}^{T}$$", "word_idx": 29666, "sentence_idx": 320, "label": "unlabeled"}, {"type": "math", "expr": "$$L\\left(\\theta\\right)$$", "word_idx": 29715, "sentence_idx": 321, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta=\\theta_{i}$$", "word_idx": 29735, "sentence_idx": 322, "label": "unlabeled"}, {"type": "math", "expr": "$$\\left\\{\\sigma_{i}\\left(t\\right)\\right\\}_{t=1}^{T}$$", "word_idx": 29752, "sentence_idx": 323, "label": "unlabeled"}, {"type": "text", "expr": "4  PRACTICAL PROPERTIES OF BAYESIAN GRADIENT DESCENT", "word_idx": 29801, "sentence_idx": 324, "label": "unlabeled"}, {"type": "text", "expr": "Learning the distribution of a weight\u2019s value rather than learning the value itself gives additional information which can be harnessed for a variety of usages", "word_idx": 29853, "sentence_idx": 325, "label": "unlabeled"}, {"type": "text", "expr": "Learning the distribution of a weight\u2019s value rather than learning the value itself gives additional information which can be harnessed for a variety of usages", "word_idx": 30012, "sentence_idx": 326, "label": "unlabeled"}, {"type": "text", "expr": "Weight pruning", "word_idx": 30171, "sentence_idx": 327, "label": "unlabeled"}, {"type": "text", "expr": "Standard deviation and mean learned by BGD can be used for weights pruning by defining the signal to noise ratio as  $\\mathrm{SNR}=\\left|\\mu\\right|/\\sigma$  and using it as a measure of a weight\u2019s necessity", "word_idx": 30185, "sentence_idx": 328, "label": "unlabeled"}, {"type": "text", "expr": "\nSmaller SNR implies large STD, small mean, or both", "word_idx": 30391, "sentence_idx": 329, "label": "unlabeled"}, {"type": "text", "expr": " For large STD the sampled weight value expected to spread over a large range of values", "word_idx": 30442, "sentence_idx": 330, "label": "unlabeled"}, {"type": "text", "expr": "\nFor small mean the sampled value is likely to be close to 0", "word_idx": 30529, "sentence_idx": 331, "label": "unlabeled"}, {"type": "text", "expr": "\nIn both cases, the weight might have no effect the network\u2019s output, or even a negative effect in terms of loss", "word_idx": 30589, "sentence_idx": 332, "label": "unlabeled"}, {"type": "text", "expr": "\nHence, the SNR seems to be a relevant measure for weights pruning, preferring to prune weights with lower SNR", "word_idx": 30701, "sentence_idx": 333, "label": "unlabeled"}, {"type": "text", "expr": "\nIn subsection  5", "word_idx": 30811, "sentence_idx": 334, "label": "unlabeled"}, {"type": "text", "expr": "4  we show that the SNR is indeed a suitable measure for this task", "word_idx": 30828, "sentence_idx": 335, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{SNR}=\\left|\\mu\\right|/\\sigma$$", "word_idx": 30894, "sentence_idx": 336, "label": "unlabeled"}, {"type": "text", "expr": "Alleviating catastrophic forgetting", "word_idx": 30930, "sentence_idx": 337, "label": "unlabeled"}, {"type": "text", "expr": "Another advantage of BGD is it enables the neural network to sequentially learn a set of multiple tasks, while alleviating catastrophic forgetting, which commonly plagues training using stochastic gradient descent (SGD), or its variants", "word_idx": 30965, "sentence_idx": 338, "label": "unlabeled"}, {"type": "text", "expr": " Bayesian Gradient Descent introduce the linkage between learning rate and the uncertainty (STD), where larger certainty (smaller STD) leads to smaller learning rate", "word_idx": 31201, "sentence_idx": 339, "label": "unlabeled"}, {"type": "text", "expr": "\nWe believe weights with high certainty has the largest effect on the network\u2019s output as their value vary the least", "word_idx": 31366, "sentence_idx": 340, "label": "unlabeled"}, {"type": "text", "expr": "\nThis property leads to that, even when switching between different tasks, the weights which are the most relevant to the previous tasks, change less", "word_idx": 31482, "sentence_idx": 341, "label": "unlabeled"}, {"type": "text", "expr": "\nThis is an internal property of the algorithm, and it not need to be aware of tasks being switched", "word_idx": 31631, "sentence_idx": 342, "label": "unlabeled"}, {"type": "text", "expr": "\nOn subsections  5", "word_idx": 31730, "sentence_idx": 343, "label": "unlabeled"}, {"type": "text", "expr": "3  and  5", "word_idx": 31748, "sentence_idx": 344, "label": "unlabeled"}, {"type": "text", "expr": "5  we examine the algorithm on such tasks", "word_idx": 31757, "sentence_idx": 345, "label": "unlabeled"}, {"type": "text", "expr": "Multiple inference methods", "word_idx": 31798, "sentence_idx": 346, "label": "unlabeled"}, {"type": "text", "expr": "Lastly, multiple methods for inference are available when using BGD", "word_idx": 31824, "sentence_idx": 347, "label": "unlabeled"}, {"type": "text", "expr": "\nOne method is using weights sampled from the approximate posterior distribution for every mini-batch", "word_idx": 31891, "sentence_idx": 348, "label": "unlabeled"}, {"type": "text", "expr": "\nAnother method is to create several networks by sampling, and use them as a committee, predicting the output according to the majority vote", "word_idx": 31992, "sentence_idx": 349, "label": "unlabeled"}, {"type": "text", "expr": " Finally, one can set the weights to be the learned mean which is equivalent to Maximum A Posteriori (MAP) estimation of the network weights", "word_idx": 32132, "sentence_idx": 350, "label": "unlabeled"}, {"type": "text", "expr": "\nOn subsection  5", "word_idx": 32272, "sentence_idx": 351, "label": "unlabeled"}, {"type": "text", "expr": "4  we compare those methods", "word_idx": 32289, "sentence_idx": 352, "label": "unlabeled"}, {"type": "text", "expr": "5  COMPLEXITY", "word_idx": 32316, "sentence_idx": 353, "label": "unlabeled"}, {"type": "text", "expr": "BGD requires  $\\times 2$  more memory as it stores both mean and STD per weight", "word_idx": 32329, "sentence_idx": 354, "label": "unlabeled"}, {"type": "text", "expr": "\nAs for time complexity, Monte Carlo samples may increase the time complexity, but since the samples are completely independent, it can be done in parallel, increasing the memory complexity to store the randomized weights for each sample", "word_idx": 32408, "sentence_idx": 355, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times 2$$", "word_idx": 32645, "sentence_idx": 356, "label": "unlabeled"}, {"type": "text", "expr": "5  EXPERIMENTS", "word_idx": 32653, "sentence_idx": 357, "label": "unlabeled"}, {"type": "text", "expr": "The following experiments show the algorithm\u2019s performance on multiple benchmarks, compared with different algorithms", "word_idx": 32667, "sentence_idx": 358, "label": "unlabeled"}, {"type": "text", "expr": "\nOn all experiments we initialize the mean of the weights  $\\mu$  by sampling from a Gaussian\ndistribution with zero mean and variance of  $2/(n_{\\mathrm{input}}+n_{\\mathrm{output}})$ \nwhere  $n_{\\mathrm{input}}$  and  $n_{\\mathrm{output}}$  are, respectively, the numbers of input and output\nconnections to each neuron in the layer  ", "word_idx": 32784, "sentence_idx": 359, "label": "unlabeled"}, {"type": "text", "expr": "\nWe use  $10$  Monte Carlo samples to estimate the expected gradient during training, and average the accuracy of 10 sampled networks during testing, unless stated otherwise", "word_idx": 33118, "sentence_idx": 360, "label": "unlabeled"}, {"type": "text", "expr": "\nTo prune weights, we set to zero both the mean and STD of weights under some SNR threshold", "word_idx": 33291, "sentence_idx": 361, "label": "unlabeled"}, {"type": "text", "expr": " The threshold is determined according to the percent of weights we wish to prune", "word_idx": 33382, "sentence_idx": 362, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mu$$", "word_idx": 33463, "sentence_idx": 363, "label": "unlabeled"}, {"type": "math", "expr": "$$2/(n_{\\mathrm{input}}+n_{\\mathrm{output}})$$", "word_idx": 33466, "sentence_idx": 364, "label": "unlabeled"}, {"type": "math", "expr": "$$n_{\\mathrm{input}}$$", "word_idx": 33508, "sentence_idx": 365, "label": "unlabeled"}, {"type": "math", "expr": "$$n_{\\mathrm{output}}$$", "word_idx": 33526, "sentence_idx": 366, "label": "unlabeled"}, {"type": "text", "expr": "1  DATASETS", "word_idx": 33545, "sentence_idx": 367, "label": "unlabeled"}, {"type": "text", "expr": "MNIST  is a database of handwritten digits, which has a training set of 60,000\nexamples, and a test set of 10,000 examples \u2014 each a 28 by 28 image", "word_idx": 33556, "sentence_idx": 368, "label": "unlabeled"}, {"type": "text", "expr": "\nThe image is labeled with a number in the range 0 to 9", "word_idx": 33702, "sentence_idx": 369, "label": "unlabeled"}, {"type": "text", "expr": "MNIST", "word_idx": 33757, "sentence_idx": 370, "label": "unlabeled"}, {"type": "text", "expr": "Permuted MNIST \nis a set of tasks constructed by random\npermutation of MNIST pixels", "word_idx": 33762, "sentence_idx": 371, "label": "unlabeled"}, {"type": "text", "expr": " Each task has different permutation of\npixels from the previous one", "word_idx": 33845, "sentence_idx": 372, "label": "unlabeled"}, {"type": "text", "expr": "Permuted MNIST", "word_idx": 33913, "sentence_idx": 373, "label": "unlabeled"}, {"type": "text", "expr": "CIFAR10 \nis a dataset which consists of 60000 32x32 colour images in 10 classes, with 6000 images per class", "word_idx": 33927, "sentence_idx": 374, "label": "unlabeled"}, {"type": "text", "expr": " There are 50000 training images and 10000 test images", "word_idx": 34034, "sentence_idx": 375, "label": "unlabeled"}, {"type": "text", "expr": "CIFAR10", "word_idx": 34088, "sentence_idx": 376, "label": "unlabeled"}, {"type": "text", "expr": "CIFAR100 \nis a dataset which consists of 60000 32x32 colour images in 100 classes, with 600 images per class  ", "word_idx": 34095, "sentence_idx": 377, "label": "unlabeled"}, {"type": "text", "expr": " There are 50000 training images and 10000 test images", "word_idx": 34205, "sentence_idx": 378, "label": "unlabeled"}, {"type": "text", "expr": "\nThe 100 classes in the CIFAR-100 are grouped into 20 superclasses", "word_idx": 34259, "sentence_idx": 379, "label": "unlabeled"}, {"type": "text", "expr": " Each image comes with a \u201cfine\u201d label (the class to which it belongs) and a \u201ccoarse\u201d label (the superclass to which it belongs)", "word_idx": 34325, "sentence_idx": 380, "label": "unlabeled"}, {"type": "text", "expr": "CIFAR100", "word_idx": 34452, "sentence_idx": 381, "label": "unlabeled"}, {"type": "text", "expr": "2  CLASSIFICATION ON MNIST", "word_idx": 34460, "sentence_idx": 382, "label": "unlabeled"}, {"type": "text", "expr": "We use a fully\nconnected neural network with 2 hidden layers of various widths, ReLU\u2019s\nas activation functions and softmax output layer with 10 units", "word_idx": 34486, "sentence_idx": 383, "label": "unlabeled"}, {"type": "text", "expr": " We\npreprocessed the data to have pixel values in the range of  $\\left[0,1\\right]$ ", "word_idx": 34635, "sentence_idx": 384, "label": "unlabeled"}, {"type": "text", "expr": "\nWe use mini-batch of size 128 and  $\\eta=1$ ", "word_idx": 34718, "sentence_idx": 385, "label": "unlabeled"}, {"type": "text", "expr": " In addition we used a validation set to find\nthe the initialization value for  $\\sigma$ ", "word_idx": 34763, "sentence_idx": 386, "label": "unlabeled"}, {"type": "text", "expr": " In order to compare the results of Bayesian Gradient Descent (BGD) with the results of Bayes By Backprop\n(BBB) algorithm by  \n, we used a training set of 50,000 examples and validation set of 10,000 examples and present results of BBB with the same prior as BGD use", "word_idx": 34852, "sentence_idx": 387, "label": "unlabeled"}, {"type": "text", "expr": " We train the network for 600 epochs", "word_idx": 35118, "sentence_idx": 388, "label": "unlabeled"}, {"type": "math", "expr": "$$\\left[0,1\\right]$$", "word_idx": 35154, "sentence_idx": 389, "label": "unlabeled"}, {"type": "math", "expr": "$$\\eta=1$$", "word_idx": 35170, "sentence_idx": 390, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma$$", "word_idx": 35176, "sentence_idx": 391, "label": "unlabeled"}, {"type": "text", "expr": "3 Results of SGD and BBB in Table  1  and  2  are as reported on  ", "word_idx": 35182, "sentence_idx": 392, "label": "unlabeled"}, {"type": "text", "expr": "Table  1  summarizes the results of Bayesian Gradient Descent for various\nwidth networks compared to other algorithms", "word_idx": 35248, "sentence_idx": 393, "label": "unlabeled"}, {"type": "text", "expr": " Overall, Bayesian Gradient Descent performs better than Bayes By Backprop", "word_idx": 35365, "sentence_idx": 394, "label": "unlabeled"}, {"type": "text", "expr": "{savenotes}", "word_idx": 35439, "sentence_idx": 395, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:  Classification error on MNIST", "word_idx": 35450, "sentence_idx": 396, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:", "word_idx": 35489, "sentence_idx": 397, "label": "unlabeled"}, {"type": "text", "expr": "Method Layer Width Test Error", "word_idx": 35497, "sentence_idx": 398, "label": "unlabeled"}, {"type": "text", "expr": "Method", "word_idx": 35526, "sentence_idx": 399, "label": "unlabeled"}, {"type": "text", "expr": "Layer Width", "word_idx": 35532, "sentence_idx": 400, "label": "unlabeled"}, {"type": "text", "expr": "Test Error", "word_idx": 35543, "sentence_idx": 401, "label": "unlabeled"}, {"type": "text", "expr": "BBB, Gaussian 400 1", "word_idx": 35553, "sentence_idx": 402, "label": "unlabeled"}, {"type": "text", "expr": "BBB, Gaussian", "word_idx": 35572, "sentence_idx": 403, "label": "unlabeled"}, {"type": "text", "expr": "(Blundell, 2015) 800 1", "word_idx": 35585, "sentence_idx": 404, "label": "unlabeled"}, {"type": "text", "expr": "(Blundell, 2015)", "word_idx": 35607, "sentence_idx": 405, "label": "unlabeled"}, {"type": "text", "expr": "1200 2", "word_idx": 35623, "sentence_idx": 406, "label": "unlabeled"}, {"type": "text", "expr": "BGD 400 1", "word_idx": 35629, "sentence_idx": 407, "label": "unlabeled"}, {"type": "text", "expr": "800 1", "word_idx": 35638, "sentence_idx": 408, "label": "unlabeled"}, {"type": "text", "expr": "1200 1", "word_idx": 35643, "sentence_idx": 409, "label": "unlabeled"}, {"type": "text", "expr": "SGD 400 1", "word_idx": 35649, "sentence_idx": 410, "label": "unlabeled"}, {"type": "text", "expr": "800 1", "word_idx": 35658, "sentence_idx": 411, "label": "unlabeled"}, {"type": "text", "expr": "1200 1", "word_idx": 35663, "sentence_idx": 412, "label": "unlabeled"}, {"type": "text", "expr": "Table  2  summarizes the results of weight pruning", "word_idx": 35669, "sentence_idx": 413, "label": "unlabeled"}, {"type": "text", "expr": " We train fully\nconnected network with with 2 hidden layers of 1200 width, using Bayesian\nGradient Descent", "word_idx": 35719, "sentence_idx": 414, "label": "unlabeled"}, {"type": "text", "expr": " We remove weights with low signal to noise ratio\n-  $\\mathrm{SNR}=\\left|\\mu\\right|/\\sigma$  and replace the value with zero", "word_idx": 35825, "sentence_idx": 415, "label": "unlabeled"}, {"type": "text", "expr": "\nResults compared to SGD  and BBB", "word_idx": 35949, "sentence_idx": 416, "label": "unlabeled"}, {"type": "text", "expr": " Note that in BBB the results for pruning were reported for a mixture of Gaussians (both prior and variational approximation), which is a more powerful then a single Gaussian \u2014 this improved the baseline accuracy results, but had similar relative success in weight pruning as our method", "word_idx": 35982, "sentence_idx": 417, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{SNR}=\\left|\\mu\\right|/\\sigma$$", "word_idx": 36268, "sentence_idx": 418, "label": "unlabeled"}, {"type": "text", "expr": "4 We remove weights a with low value and replace the value with zero", "word_idx": 36304, "sentence_idx": 419, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:  Weight pruning on MNIST", "word_idx": 36372, "sentence_idx": 420, "label": "unlabeled"}, {"type": "text", "expr": " Showing the ratio  Error(Pruning)/Error(No Pruning) , smaller values are better", "word_idx": 36405, "sentence_idx": 421, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:", "word_idx": 36485, "sentence_idx": 422, "label": "unlabeled"}, {"type": "text", "expr": "Error(Pruning)/Error(No Pruning)", "word_idx": 36493, "sentence_idx": 423, "label": "unlabeled"}, {"type": "text", "expr": "Error ratio", "word_idx": 36525, "sentence_idx": 424, "label": "unlabeled"}, {"type": "text", "expr": "Error ratio", "word_idx": 36536, "sentence_idx": 425, "label": "unlabeled"}, {"type": "text", "expr": "Error ratio", "word_idx": 36547, "sentence_idx": 426, "label": "unlabeled"}, {"type": "text", "expr": "% Removed # Weights BGD BBB SGD", "word_idx": 36558, "sentence_idx": 427, "label": "unlabeled"}, {"type": "text", "expr": "% Removed", "word_idx": 36589, "sentence_idx": 428, "label": "unlabeled"}, {"type": "text", "expr": "# Weights", "word_idx": 36598, "sentence_idx": 429, "label": "unlabeled"}, {"type": "text", "expr": "4M 1 1 1", "word_idx": 36607, "sentence_idx": 430, "label": "unlabeled"}, {"type": "text", "expr": "95% 120K 1", "word_idx": 36615, "sentence_idx": 431, "label": "unlabeled"}, {"type": "text", "expr": "3  CONTINUAL LEARNING ON PERMUTED MNIST", "word_idx": 36625, "sentence_idx": 432, "label": "unlabeled"}, {"type": "text", "expr": "We use a fully connected neural network\nwith 2 hidden layers of 2000 width, ReLU\u2019s as activation functions\nand softmax output layer with 10 units", "word_idx": 36664, "sentence_idx": 433, "label": "unlabeled"}, {"type": "text", "expr": " We trained the network using\nBayesian Gradient Descent", "word_idx": 36809, "sentence_idx": 434, "label": "unlabeled"}, {"type": "text", "expr": " The preprocessing is the same as with MNIST classification, but we used a training set of 60,000 examples", "word_idx": 36864, "sentence_idx": 435, "label": "unlabeled"}, {"type": "text", "expr": "We use a fully connected neural network\nwith 2 hidden layers of 2000 width, ReLU\u2019s as activation functions\nand softmax output layer with 10 units", "word_idx": 36970, "sentence_idx": 436, "label": "unlabeled"}, {"type": "text", "expr": " We trained the network using\nBayesian Gradient Descent", "word_idx": 37115, "sentence_idx": 437, "label": "unlabeled"}, {"type": "text", "expr": " The preprocessing is the same as with MNIST classification, but we used a training set of 60,000 examples", "word_idx": 37170, "sentence_idx": 438, "label": "unlabeled"}, {"type": "text", "expr": "Figure  1  shows results on Permuted MNIST benchmark", "word_idx": 37276, "sentence_idx": 439, "label": "unlabeled"}, {"type": "text", "expr": " As can be seen, the network learns to solve the tasks with high accuracy", "word_idx": 37328, "sentence_idx": 440, "label": "unlabeled"}, {"type": "text", "expr": " It is important to notice that Bayesian Gradient Descent is an online algorithm", "word_idx": 37401, "sentence_idx": 441, "label": "unlabeled"}, {"type": "text", "expr": " To the best of our knowledge, we demonstrate first results on catastrophic forgetting using a task-agnostic algorithm", "word_idx": 37481, "sentence_idx": 442, "label": "unlabeled"}, {"type": "text", "expr": " In other words, during the learning process, the training algorithm is unaware of tasks being changed", "word_idx": 37599, "sentence_idx": 443, "label": "unlabeled"}, {"type": "text", "expr": " Existing algorithms for preventing catastrophic forgetting such as EWC   and SI  , adjust the parameters of the cost function after each task", "word_idx": 37701, "sentence_idx": 444, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:  Average test accuracy on permuted MNIST vs the number of tasks", "word_idx": 37843, "sentence_idx": 445, "label": "unlabeled"}, {"type": "text", "expr": " BGD (red), SI (green, as reported on  ) and SGD (blue)", "word_idx": 37916, "sentence_idx": 446, "label": "unlabeled"}, {"type": "text", "expr": " Notice that BGD is task agnostic (i", "word_idx": 37971, "sentence_idx": 447, "label": "unlabeled"}, {"type": "text", "expr": " unaware of tasks changing), while still significantly alleviates catastrophic forgetting", "word_idx": 38007, "sentence_idx": 448, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:", "word_idx": 38096, "sentence_idx": 449, "label": "unlabeled"}, {"type": "text", "expr": "4  CLASSIFICATION ON CIFAR10", "word_idx": 38105, "sentence_idx": 450, "label": "unlabeled"}, {"type": "text", "expr": "To evaluate BGD\u2019s performance on a more complex dataset and larger network we trained VGG11   for 400 epochs on CIFAR10 with SGD as reference", "word_idx": 38133, "sentence_idx": 451, "label": "unlabeled"}, {"type": "text", "expr": "Results reported in Figure  3  show that, depending on the inference method, BGD achieves on par or better results compared to SGD", "word_idx": 38274, "sentence_idx": 452, "label": "unlabeled"}, {"type": "text", "expr": " We use this setup to examine more properties of BGD discussed in  4", "word_idx": 38404, "sentence_idx": 453, "label": "unlabeled"}, {"type": "text", "expr": "Inference methods", "word_idx": 38472, "sentence_idx": 454, "label": "unlabeled"}, {"type": "text", "expr": "We compare different inference methods of BGD", "word_idx": 38489, "sentence_idx": 455, "label": "unlabeled"}, {"type": "text", "expr": " Figure  3  shows that the three inference methods performs similarly, with a slight advantage to committee voting", "word_idx": 38534, "sentence_idx": 456, "label": "unlabeled"}, {"type": "text", "expr": "Weights pruning", "word_idx": 38648, "sentence_idx": 457, "label": "unlabeled"}, {"type": "text", "expr": "To demonstrate BGD usage for weights pruning, we evaluated a pruned version of the trained network, pruning 90% of the weights and using the 10 Monte Carlo samples inference method", "word_idx": 38663, "sentence_idx": 458, "label": "unlabeled"}, {"type": "text", "expr": "\nFor BGD, SNR was used as a measure to determine which weights to prune, where those with the smallest SNR were pruned", "word_idx": 38843, "sentence_idx": 459, "label": "unlabeled"}, {"type": "text", "expr": "\nOn SGD we used the weights\u2019 absolute value as a measure, pruning the smallest values", "word_idx": 38961, "sentence_idx": 460, "label": "unlabeled"}, {"type": "text", "expr": "\nBGD pruned networks succeed to retain most of the accuracy achieved by the non-pruned network, while SGD pruned networks accuracy drops", "word_idx": 39046, "sentence_idx": 461, "label": "unlabeled"}, {"type": "text", "expr": "\nResults reported in Table  3 ", "word_idx": 39182, "sentence_idx": 462, "label": "unlabeled"}, {"type": "text", "expr": " BBB results were not tested on CIFAR10 in  ", "word_idx": 39212, "sentence_idx": 463, "label": "unlabeled"}, {"type": "text", "expr": " The results indicate that using a Bayesian algorithm we are able to easily prune weights", "word_idx": 39256, "sentence_idx": 464, "label": "unlabeled"}, {"type": "text", "expr": " An additional boost to the pruned networks performance can be achieved by training the pruned network as suggested in  , but this requires access to the training set after pruning, as discussed in the introduction", "word_idx": 39345, "sentence_idx": 465, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a03:  Weight pruning on CIFAR10", "word_idx": 39559, "sentence_idx": 466, "label": "unlabeled"}, {"type": "text", "expr": " Showing the ratio  Error(Pruning)/Error(No Pruning) , smaller values are better", "word_idx": 39594, "sentence_idx": 467, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a03:", "word_idx": 39674, "sentence_idx": 468, "label": "unlabeled"}, {"type": "text", "expr": "Error(Pruning)/Error(No Pruning)", "word_idx": 39682, "sentence_idx": 469, "label": "unlabeled"}, {"type": "text", "expr": "Error ratio", "word_idx": 39714, "sentence_idx": 470, "label": "unlabeled"}, {"type": "text", "expr": "Error ratio", "word_idx": 39725, "sentence_idx": 471, "label": "unlabeled"}, {"type": "text", "expr": "Error ratio", "word_idx": 39736, "sentence_idx": 472, "label": "unlabeled"}, {"type": "text", "expr": "% Removed # Weights BGD SGD", "word_idx": 39747, "sentence_idx": 473, "label": "unlabeled"}, {"type": "text", "expr": "% Removed", "word_idx": 39774, "sentence_idx": 474, "label": "unlabeled"}, {"type": "text", "expr": "# Weights", "word_idx": 39783, "sentence_idx": 475, "label": "unlabeled"}, {"type": "text", "expr": "2M 1 1", "word_idx": 39792, "sentence_idx": 476, "label": "unlabeled"}, {"type": "text", "expr": "90% 0", "word_idx": 39798, "sentence_idx": 477, "label": "unlabeled"}, {"type": "text", "expr": "Figure  2  shows the histogram of learned STD values at the end of BGD\u2019s training", "word_idx": 39803, "sentence_idx": 478, "label": "unlabeled"}, {"type": "text", "expr": "\nThe STD represents the uncertainty in the weight\u2019s value and as can be seen, many of the values are close to the initial value of 0", "word_idx": 39884, "sentence_idx": 479, "label": "unlabeled"}, {"type": "text", "expr": "015, while a small fraction of the weights have much lower or higher STD values", "word_idx": 40016, "sentence_idx": 480, "label": "unlabeled"}, {"type": "text", "expr": "\nThis is might be an explanation for BGD\u2019s robustness to weight pruning and catastrophic forgetting, where only a small part of the weights are important for each task", "word_idx": 40095, "sentence_idx": 481, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:  Histogram of STD values at the end of CIFAR10 training", "word_idx": 40262, "sentence_idx": 482, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:", "word_idx": 40327, "sentence_idx": 483, "label": "unlabeled"}, {"type": "text", "expr": "Implementation details", "word_idx": 40336, "sentence_idx": 484, "label": "unlabeled"}, {"type": "text", "expr": "SGD was trained with learning rate starting at 0", "word_idx": 40358, "sentence_idx": 485, "label": "unlabeled"}, {"type": "text", "expr": "01 and divided by 10 every 100 epochs, momentum was set to 0", "word_idx": 40406, "sentence_idx": 486, "label": "unlabeled"}, {"type": "text", "expr": "\nOn both BGD and SGD we used data augmentation of random cropping and flipping", "word_idx": 40466, "sentence_idx": 487, "label": "unlabeled"}, {"type": "text", "expr": " Batch size was 128", "word_idx": 40544, "sentence_idx": 488, "label": "unlabeled"}, {"type": "text", "expr": "\nOn BGD the initial STD was set to 0", "word_idx": 40563, "sentence_idx": 489, "label": "unlabeled"}, {"type": "text", "expr": "015 and  $\\eta=6$ ", "word_idx": 40599, "sentence_idx": 490, "label": "unlabeled"}, {"type": "math", "expr": "$$\\eta=6$$", "word_idx": 40617, "sentence_idx": 491, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:  Test accuracy on CIFAR10 with VGG11", "word_idx": 40623, "sentence_idx": 492, "label": "unlabeled"}, {"type": "text", "expr": " Different inference methods of BGD are compared with SGD as reference", "word_idx": 40669, "sentence_idx": 493, "label": "unlabeled"}, {"type": "text", "expr": " BGD is on par or better than SGD", "word_idx": 40739, "sentence_idx": 494, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:", "word_idx": 40772, "sentence_idx": 495, "label": "unlabeled"}, {"type": "text", "expr": "5  CONTINUAL LEARNING ON CIFAR10/CIFAR100", "word_idx": 40781, "sentence_idx": 496, "label": "unlabeled"}, {"type": "text", "expr": "To further show BGD usage for catastrophic forgetting we followed   experiment, training sequentially on 6 tasks from CIFAR10 and CIFAR100", "word_idx": 40822, "sentence_idx": 497, "label": "unlabeled"}, {"type": "text", "expr": "\nThe first task is the full CIFAR10 dataset and the next 5 tasks are of subsets CIFAR100 with 10 classes", "word_idx": 40960, "sentence_idx": 498, "label": "unlabeled"}, {"type": "text", "expr": "\nEach task was trained for 150 epochs", "word_idx": 41064, "sentence_idx": 499, "label": "unlabeled"}, {"type": "text", "expr": "\nAs reference we present results of SGD and SI  ", "word_idx": 41101, "sentence_idx": 500, "label": "unlabeled"}, {"type": "text", "expr": "Results reported in Figure  4 ", "word_idx": 41149, "sentence_idx": 501, "label": "unlabeled"}, {"type": "text", "expr": " We think that BGD attains a good balance between retaining reasonable accuracy on previous tasks and achieving high accuracy on newer ones", "word_idx": 41179, "sentence_idx": 502, "label": "unlabeled"}, {"type": "text", "expr": "We used the same network as the original experiment, which consists of 4 convolutional layers followed by a fully connected layer with dropout and a separated last fully connected layer per task", "word_idx": 41318, "sentence_idx": 503, "label": "unlabeled"}, {"type": "text", "expr": "\nBatch size is 256", "word_idx": 41512, "sentence_idx": 504, "label": "unlabeled"}, {"type": "text", "expr": " On BGD we used committee voting inference, initial STD was 0", "word_idx": 41530, "sentence_idx": 505, "label": "unlabeled"}, {"type": "text", "expr": "019 and  $\\eta=2$ ", "word_idx": 41591, "sentence_idx": 506, "label": "unlabeled"}, {"type": "text", "expr": " For SGD we used constant learning rate of 0", "word_idx": 41609, "sentence_idx": 507, "label": "unlabeled"}, {"type": "text", "expr": "01 and for SI we used  $c=001$ ", "word_idx": 41653, "sentence_idx": 508, "label": "unlabeled"}, {"type": "text", "expr": " Hyper parameters for all algorithms were chosen after using grid search and taking the optimal value", "word_idx": 41684, "sentence_idx": 509, "label": "unlabeled"}, {"type": "math", "expr": "$$\\eta=2$$", "word_idx": 41785, "sentence_idx": 510, "label": "unlabeled"}, {"type": "math", "expr": "$$c=0.01$$", "word_idx": 41791, "sentence_idx": 511, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:  Continual learning on CIFAR10 and subsets of CIFAR100", "word_idx": 41797, "sentence_idx": 512, "label": "unlabeled"}, {"type": "text", "expr": " Reporting test accuracy per task on last epoch", "word_idx": 41861, "sentence_idx": 513, "label": "unlabeled"}, {"type": "text", "expr": " Task 1 is the first seen task and task 6 is the last", "word_idx": 41908, "sentence_idx": 514, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:", "word_idx": 41961, "sentence_idx": 515, "label": "unlabeled"}, {"type": "text", "expr": "6  DISCUSSION AND FUTURE WORK", "word_idx": 41970, "sentence_idx": 516, "label": "unlabeled"}, {"type": "text", "expr": "In this work we have suggested a novel Bayesian approach to gradient descent using online variational Bayes", "word_idx": 41999, "sentence_idx": 517, "label": "unlabeled"}, {"type": "text", "expr": " Our method, Bayesian Gradient Descent (BGD), was shown to have better classification accuracy than previous Bayesian methods on MNIST, and on par with SGD on CIFAR10", "word_idx": 42106, "sentence_idx": 518, "label": "unlabeled"}, {"type": "text", "expr": " Moreover, it is significantly more robust to weight pruning, without requiring re-training of the network", "word_idx": 42272, "sentence_idx": 519, "label": "unlabeled"}, {"type": "text", "expr": " We have also demonstrated that this approach can be used efficiently for continual learning, mitigating the notorious catastrophic forgetting phenomena that plagues neural networks", "word_idx": 42378, "sentence_idx": 520, "label": "unlabeled"}, {"type": "text", "expr": " Most noteworthy, our approach, for the first time, allows models to better adapt to new tasks without explicitly instructed to do so", "word_idx": 42559, "sentence_idx": 521, "label": "unlabeled"}, {"type": "text", "expr": "In this work we have suggested a novel Bayesian approach to gradient descent using online variational Bayes", "word_idx": 42692, "sentence_idx": 522, "label": "unlabeled"}, {"type": "text", "expr": " Our method, Bayesian Gradient Descent (BGD), was shown to have better classification accuracy than previous Bayesian methods on MNIST, and on par with SGD on CIFAR10", "word_idx": 42799, "sentence_idx": 523, "label": "unlabeled"}, {"type": "text", "expr": " Moreover, it is significantly more robust to weight pruning, without requiring re-training of the network", "word_idx": 42965, "sentence_idx": 524, "label": "unlabeled"}, {"type": "text", "expr": " We have also demonstrated that this approach can be used efficiently for continual learning, mitigating the notorious catastrophic forgetting phenomena that plagues neural networks", "word_idx": 43071, "sentence_idx": 525, "label": "unlabeled"}, {"type": "text", "expr": " Most noteworthy, our approach, for the first time, allows models to better adapt to new tasks without explicitly instructed to do so", "word_idx": 43252, "sentence_idx": 526, "label": "unlabeled"}, {"type": "text", "expr": "BGD is an online version of the variational Bayes approach of  ", "word_idx": 43385, "sentence_idx": 527, "label": "unlabeled"}, {"type": "text", "expr": " BGD does not seem to have the underfitting and over-pruning issues previously observed in the variational Bayes approach of  ", "word_idx": 43448, "sentence_idx": 528, "label": "unlabeled"}, {"type": "text", "expr": " This allowed us to scale this approach beyond MNIST, to CIFAR", "word_idx": 43574, "sentence_idx": 529, "label": "unlabeled"}, {"type": "text", "expr": " The BGD approach is also closely related to the assumed density filtering approach of  ", "word_idx": 43636, "sentence_idx": 530, "label": "unlabeled"}, {"type": "text", "expr": " However, these methods rely on certain analytic approximations which are not easily applicable to different neural architectures (e", "word_idx": 43724, "sentence_idx": 531, "label": "unlabeled"}, {"type": "text", "expr": " convnets)", "word_idx": 43856, "sentence_idx": 532, "label": "unlabeled"}, {"type": "text", "expr": " In contrast, it straightforward to implement BGD for any neural architecture", "word_idx": 43866, "sentence_idx": 533, "label": "unlabeled"}, {"type": "text", "expr": " This approach is also somewhat similar to the Stochastic Gradient Langevin Dynamics (SGLD) approach  , in the sense that we use multiple copies of the network during training", "word_idx": 43943, "sentence_idx": 534, "label": "unlabeled"}, {"type": "text", "expr": " However, in contrast to the SGLD approach, we are not required to store all copies of the networks for inference (  had to use distillation to overcome this), but only two parameters for each weight ( $\\mu$  and  $\\sigma$ )", "word_idx": 44118, "sentence_idx": 535, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mu$$", "word_idx": 44342, "sentence_idx": 536, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma$$", "word_idx": 44345, "sentence_idx": 537, "label": "unlabeled"}, {"type": "text", "expr": "There are many possible extensions and uses of BGD, which were not explored in this work", "word_idx": 44351, "sentence_idx": 538, "label": "unlabeled"}, {"type": "text", "expr": " First, in this work our variational approximation used a diagonal Gaussian distribution", "word_idx": 44439, "sentence_idx": 539, "label": "unlabeled"}, {"type": "text", "expr": " This assumption may be relaxed in the future to non-diagonal or non-Gaussian (e", "word_idx": 44527, "sentence_idx": 540, "label": "unlabeled"}, {"type": "text", "expr": ", mixtures) distributions, to allow better flexibility during learning", "word_idx": 44607, "sentence_idx": 541, "label": "unlabeled"}, {"type": "text", "expr": " Second, in this work we focused on weight pruning and catastrophical forgetting", "word_idx": 44677, "sentence_idx": 542, "label": "unlabeled"}, {"type": "text", "expr": " However, Bayesian neural networks might also be beneficial for many other uses, as explained in the introduction", "word_idx": 44757, "sentence_idx": 543, "label": "unlabeled"}, {"type": "text", "expr": "There are many possible extensions and uses of BGD, which were not explored in this work", "word_idx": 44870, "sentence_idx": 544, "label": "unlabeled"}, {"type": "text", "expr": " First, in this work our variational approximation used a diagonal Gaussian distribution", "word_idx": 44958, "sentence_idx": 545, "label": "unlabeled"}, {"type": "text", "expr": " This assumption may be relaxed in the future to non-diagonal or non-Gaussian (e", "word_idx": 45046, "sentence_idx": 546, "label": "unlabeled"}, {"type": "text", "expr": ", mixtures) distributions, to allow better flexibility during learning", "word_idx": 45126, "sentence_idx": 547, "label": "unlabeled"}, {"type": "text", "expr": " Second, in this work we focused on weight pruning and catastrophical forgetting", "word_idx": 45196, "sentence_idx": 548, "label": "unlabeled"}, {"type": "text", "expr": " However, Bayesian neural networks might also be beneficial for many other uses, as explained in the introduction", "word_idx": 45276, "sentence_idx": 549, "label": "unlabeled"}, {"type": "text", "expr": "7  APPENDIX", "word_idx": 45389, "sentence_idx": 550, "label": "unlabeled"}, {"type": "text", "expr": "1  Derivation of eqs", "word_idx": 45400, "sentence_idx": 551, "label": "unlabeled"}, {"type": "text", "expr": "  12  and  13", "word_idx": 45420, "sentence_idx": 552, "label": "unlabeled"}, {"type": "text", "expr": "Substituting the derivatives, we obtain:", "word_idx": 45433, "sentence_idx": 553, "label": "unlabeled"}, {"type": "text", "expr": "In this section we provide additional details on the derivation of eq", "word_idx": 45473, "sentence_idx": 554, "label": "unlabeled"}, {"type": "text", "expr": " ( 12 ) and eq", "word_idx": 45542, "sentence_idx": 555, "label": "unlabeled"}, {"type": "text", "expr": " ( 13 )", "word_idx": 45556, "sentence_idx": 556, "label": "unlabeled"}, {"type": "text", "expr": " The objective function is  $f\\left(\\theta,\\phi\\right)=\\log\\left(q_{n+1}\\left(\\theta|\\phi\\right)\\right)-%\n\\log\\left(q_{n}\\left(\\theta\\right)\\right)+\\mathrm{L\\left(\\theta\\right)}$ ,\nwhere:", "word_idx": 45563, "sentence_idx": 557, "label": "unlabeled"}, {"type": "math", "expr": "$$f\\left(\\theta,\\phi\\right)=\\log\\left(q_{n+1}\\left(\\theta|\\phi\\right)\\right)-%\n\\log\\left(q_{n}\\left(\\theta\\right)\\right)+\\mathrm{L\\left(\\theta\\right)}$$", "word_idx": 45750, "sentence_idx": 558, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\log\\left(q_{n+1}\\left(\\theta|\\phi\\right)\\right)$", "word_idx": 45898, "sentence_idx": 559, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\log\\left(q_{n+1}\\left(\\theta|\\phi\\right)\\right)$$", "word_idx": 45961, "sentence_idx": 560, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=-\\frac{N}{2}\\log\\left(2\\pi\\right)-\\sum_{k}\\log\\left(\\sigma_{k}\\right)$", "word_idx": 46022, "sentence_idx": 561, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=-\\frac{N}{2}\\log\\left(2\\pi\\right)-\\sum_{k}\\log\\left(\\sigma_{k}\\right)$$", "word_idx": 46107, "sentence_idx": 562, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\,\\,\\,-\\sum_{k}\\frac{1}{2\\sigma_{k}^{2}}\\left(\\theta_{k}-\\mu_{k}%\n\\right)^{2}$", "word_idx": 46190, "sentence_idx": 563, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\,\\,\\,-\\sum_{k}\\frac{1}{2\\sigma_{k}^{2}}\\left(\\theta_{k}-\\mu_{k}%\n\\right)^{2}$$", "word_idx": 46282, "sentence_idx": 564, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\log\\left(q_{n}\\left(\\theta\\right)\\right)$", "word_idx": 46372, "sentence_idx": 565, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\log\\left(q_{n}\\left(\\theta\\right)\\right)$$", "word_idx": 46428, "sentence_idx": 566, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=-\\frac{N}{2}\\log\\left(2\\pi\\right)-\\sum_{k}\\log\\left(v_{k}\\right)$", "word_idx": 46482, "sentence_idx": 567, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=-\\frac{N}{2}\\log\\left(2\\pi\\right)-\\sum_{k}\\log\\left(v_{k}\\right)$$", "word_idx": 46562, "sentence_idx": 568, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\,\\,\\,-\\sum_{k}\\frac{1}{2v_{k}^{2}}\\left(\\theta_{k}-m_{k}\\right)^%\n{2}\\,$", "word_idx": 46640, "sentence_idx": 569, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\,\\,\\,-\\sum_{k}\\frac{1}{2v_{k}^{2}}\\left(\\theta_{k}-m_{k}\\right)^%\n{2}\\,.$$", "word_idx": 46727, "sentence_idx": 570, "label": "unlabeled"}, {"type": "text", "expr": "We derive eq", "word_idx": 46813, "sentence_idx": 571, "label": "unlabeled"}, {"type": "text", "expr": " ( 12 ) by using the first-order necessary conditions for the optimal  $\\mu_{i}$ :", "word_idx": 46825, "sentence_idx": 572, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mu_{i}$$", "word_idx": 46907, "sentence_idx": 573, "label": "unlabeled"}, {"type": "text", "expr": "$E_{\\varepsilon}\\left[\\frac{\\partial f\\left(\\theta,\\phi\\right)}{\\partial\\theta_%\n{i}}\\frac{\\partial\\theta_{i}}{\\partial\\mu_{i}}+\\frac{\\partial f\\left(\\theta,w%\n\\right)}{\\partial\\mu_{i}}\\right]=0$", "word_idx": 46914, "sentence_idx": 574, "label": "unlabeled"}, {"type": "math", "expr": "$$E_{\\varepsilon}\\left[\\frac{\\partial f\\left(\\theta,\\phi\\right)}{\\partial\\theta_%\n{i}}\\frac{\\partial\\theta_{i}}{\\partial\\mu_{i}}+\\frac{\\partial f\\left(\\theta,w%\n\\right)}{\\partial\\mu_{i}}\\right]=0$$", "word_idx": 47109, "sentence_idx": 575, "label": "unlabeled"}, {"type": "text", "expr": "Substituting the derivatives, we obtain:", "word_idx": 47302, "sentence_idx": 576, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle E_{\\varepsilon}\\left[-\\frac{1}{\\sigma_{i}^{2}}\\left(\\theta_{i}-%\n\\mu_{i}\\right)+\\frac{1}{v_{i}^{2}}\\left(\\theta_{i}-m_{i}\\right)\\right$", "word_idx": 47342, "sentence_idx": 577, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle E_{\\varepsilon}\\left[-\\frac{1}{\\sigma_{i}^{2}}\\left(\\theta_{i}-%\n\\mu_{i}\\right)+\\frac{1}{v_{i}^{2}}\\left(\\theta_{i}-m_{i}\\right)\\right.$$", "word_idx": 47492, "sentence_idx": 578, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\left+\\frac{\\partial L\\left(\\theta\\right)}{\\partial\\theta_{i}}+%\n\\frac{1}{\\sigma_{i}^{2}}\\left(\\theta_{i}-\\mu_{i}\\right)\\right]$", "word_idx": 47641, "sentence_idx": 579, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\left.+\\frac{\\partial L\\left(\\theta\\right)}{\\partial\\theta_{i}}+%\n\\frac{1}{\\sigma_{i}^{2}}\\left(\\theta_{i}-\\mu_{i}\\right)\\right]$$", "word_idx": 47783, "sentence_idx": 580, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=$", "word_idx": 47924, "sentence_idx": 581, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=$$", "word_idx": 47940, "sentence_idx": 582, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\frac{1}{v_{i}^{2}}\\left(\\mu_{i}-m_{i}\\right)+E_{\\varepsilon}%\n\\left[\\frac{\\partial L\\left(\\theta\\right)}{\\partial\\theta_{i}}\\right]$", "word_idx": 47954, "sentence_idx": 583, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\frac{1}{v_{i}^{2}}\\left(\\mu_{i}-m_{i}\\right)+E_{\\varepsilon}%\n\\left[\\frac{\\partial L\\left(\\theta\\right)}{\\partial\\theta_{i}}\\right]$$", "word_idx": 48101, "sentence_idx": 584, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=0$", "word_idx": 48246, "sentence_idx": 585, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=0.$$", "word_idx": 48263, "sentence_idx": 586, "label": "unlabeled"}, {"type": "text", "expr": "And so we obtained eq", "word_idx": 48279, "sentence_idx": 587, "label": "unlabeled"}, {"type": "text", "expr": " ( 12 )", "word_idx": 48300, "sentence_idx": 588, "label": "unlabeled"}, {"type": "text", "expr": "Substituting the derivatives we obtain:", "word_idx": 48307, "sentence_idx": 589, "label": "unlabeled"}, {"type": "text", "expr": "Next, we derive eq", "word_idx": 48346, "sentence_idx": 590, "label": "unlabeled"}, {"type": "text", "expr": " ( 13 ), using the first-order necessary conditions for optimal  $\\sigma_{i}$ :", "word_idx": 48364, "sentence_idx": 591, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma_{i}$$", "word_idx": 48443, "sentence_idx": 592, "label": "unlabeled"}, {"type": "text", "expr": "$E_{\\varepsilon}\\left[\\frac{\\partial f\\left(\\theta,\\phi\\right)}{\\partial\\theta_%\n{i}}\\frac{\\partial\\theta_{i}}{\\partial\\sigma_{i}}+\\frac{\\partial f\\left(\\theta%\n,w\\right)}{\\partial\\sigma_{i}}\\right]=0$", "word_idx": 48453, "sentence_idx": 593, "label": "unlabeled"}, {"type": "math", "expr": "$$E_{\\varepsilon}\\left[\\frac{\\partial f\\left(\\theta,\\phi\\right)}{\\partial\\theta_%\n{i}}\\frac{\\partial\\theta_{i}}{\\partial\\sigma_{i}}+\\frac{\\partial f\\left(\\theta%\n,w\\right)}{\\partial\\sigma_{i}}\\right]=0$$", "word_idx": 48654, "sentence_idx": 594, "label": "unlabeled"}, {"type": "text", "expr": "Substituting the derivatives we obtain:", "word_idx": 48853, "sentence_idx": 595, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle E_{\\varepsilon}\\left[\\left(-\\frac{1}{\\sigma_{i}^{2}}\\left(\\theta%\n_{i}-\\mu_{i}\\right)+\\frac{1}{v_{i}^{2}}\\left(\\theta_{i}-m_{i}\\right)+\\frac{%\n\\partial L\\left(\\theta\\right)}{\\partial\\theta_{i}}\\right)\\varepsilon_{i}\\right$", "word_idx": 48892, "sentence_idx": 596, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle E_{\\varepsilon}\\left[\\left(-\\frac{1}{\\sigma_{i}^{2}}\\left(\\theta%\n_{i}-\\mu_{i}\\right)+\\frac{1}{v_{i}^{2}}\\left(\\theta_{i}-m_{i}\\right)+\\frac{%\n\\partial L\\left(\\theta\\right)}{\\partial\\theta_{i}}\\right)\\varepsilon_{i}\\right.$$", "word_idx": 49129, "sentence_idx": 597, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\left-\\frac{1}{\\sigma_{i}}+\\frac{1}{\\sigma_{i}^{3}}\\left(\\theta_%\n{i}-\\mu_{i}\\right)^{2}\\right]$", "word_idx": 49365, "sentence_idx": 598, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\left.-\\frac{1}{\\sigma_{i}}+\\frac{1}{\\sigma_{i}^{3}}\\left(\\theta_%\n{i}-\\mu_{i}\\right)^{2}\\right]$$", "word_idx": 49475, "sentence_idx": 599, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=$", "word_idx": 49584, "sentence_idx": 600, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=$$", "word_idx": 49600, "sentence_idx": 601, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle-\\frac{1}{\\sigma_{i}}+\\frac{\\sigma_{i}}{v_{i}^{2}}+E_{\\varepsilon%\n}\\left[\\frac{\\partial L\\left(\\theta\\right)}{\\partial\\theta_{i}}\\varepsilon_{i}\\right]$", "word_idx": 49614, "sentence_idx": 602, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle-\\frac{1}{\\sigma_{i}}+\\frac{\\sigma_{i}}{v_{i}^{2}}+E_{\\varepsilon%\n}\\left[\\frac{\\partial L\\left(\\theta\\right)}{\\partial\\theta_{i}}\\varepsilon_{i}\\right]$$", "word_idx": 49781, "sentence_idx": 603, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=0$", "word_idx": 49946, "sentence_idx": 604, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=0$$", "word_idx": 49963, "sentence_idx": 605, "label": "unlabeled"}, {"type": "text", "expr": "We get a quadratic equation for  $\\sigma_{i}$", "word_idx": 49978, "sentence_idx": 606, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma_{i}$$", "word_idx": 50023, "sentence_idx": 607, "label": "unlabeled"}, {"type": "text", "expr": "$\\sigma_{i}^{2}+\\sigma_{i}v_{i}^{2}E_{q\\left(\\varepsilon\\right)}\\left[\\frac{%\n\\partial L\\left(\\theta\\right)}{\\partial\\theta_{i}}\\varepsilon_{i}\\right]-v_{i}%\n^{2}=0$", "word_idx": 50033, "sentence_idx": 608, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma_{i}^{2}+\\sigma_{i}v_{i}^{2}E_{q\\left(\\varepsilon\\right)}\\left[\\frac{%\n\\partial L\\left(\\theta\\right)}{\\partial\\theta_{i}}\\varepsilon_{i}\\right]-v_{i}%\n^{2}=0.$$", "word_idx": 50198, "sentence_idx": 609, "label": "unlabeled"}, {"type": "text", "expr": "Since  $\\sigma_{i}>0$ , the solution is eq", "word_idx": 50362, "sentence_idx": 610, "label": "unlabeled"}, {"type": "text", "expr": " ( 13 )", "word_idx": 50404, "sentence_idx": 611, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma_{i}>0$$", "word_idx": 50411, "sentence_idx": 612, "label": "unlabeled"}, {"type": "text", "expr": "2  Proof of Theorem  1", "word_idx": 50423, "sentence_idx": 613, "label": "unlabeled"}, {"type": "text", "expr": "The conditional expectation is:", "word_idx": 50445, "sentence_idx": 614, "label": "unlabeled"}, {"type": "text", "expr": "We define  $\\theta_{j}=\\mu_{j}+\\varepsilon_{j}\\sigma_{j}$  where  $\\varepsilon_{j}\\sim\\mathcal{N}\\left(0,1\\right)$ ", "word_idx": 50476, "sentence_idx": 615, "label": "unlabeled"}, {"type": "text", "expr": "\nAccording to the smoothing theorem, the following holds:", "word_idx": 50591, "sentence_idx": 616, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta_{j}=\\mu_{j}+\\varepsilon_{j}\\sigma_{j}$$", "word_idx": 50648, "sentence_idx": 617, "label": "unlabeled"}, {"type": "math", "expr": "$$\\varepsilon_{j}\\sim\\mathcal{N}\\left(0,1\\right)$$", "word_idx": 50692, "sentence_idx": 618, "label": "unlabeled"}, {"type": "text", "expr": "$\\mathbf{\\mathrm{E}}_{\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)}{%\n\\partial\\theta_{i}}\\varepsilon_{i}\\right]=\\mathrm{E}_{\\varepsilon_{j\\neq i}}%\n\\left[\\mathbf{\\mathrm{E}}_{\\varepsilon_{i}}\\left[\\left\\frac{\\partial L\\left(%\n\\theta\\right)}{\\partial\\theta_{i}}\\varepsilon_{i}\\right|\\varepsilon_{j\\neq i}%\n\\right]\\right]$", "word_idx": 50738, "sentence_idx": 619, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{\\mathrm{E}}_{\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)}{%\n\\partial\\theta_{i}}\\varepsilon_{i}\\right]=\\mathrm{E}_{\\varepsilon_{j\\neq i}}%\n\\left[\\mathbf{\\mathrm{E}}_{\\varepsilon_{i}}\\left[\\left.\\frac{\\partial L\\left(%\n\\theta\\right)}{\\partial\\theta_{i}}\\varepsilon_{i}\\right|\\varepsilon_{j\\neq i}%\n\\right]\\right]$$", "word_idx": 51067, "sentence_idx": 620, "label": "unlabeled"}, {"type": "text", "expr": "The conditional expectation is:", "word_idx": 51395, "sentence_idx": 621, "label": "unlabeled"}, {"type": "text", "expr": "$\\mathbf{\\mathrm{E}}_{\\varepsilon_{i}}\\left[\\left\\frac{\\partial L\\left(\\theta%\n\\right)}{\\partial\\theta_{i}}\\varepsilon_{i}\\right|\\varepsilon_{j\\neq i}\\right]%\n=\\intop_{-\\infty}^{\\infty}\\frac{\\partial L\\left(\\theta\\right)}{\\partial\\theta_%\n{i}}\\varepsilon_{i}f_{\\varepsilon_{i}}\\left(\\varepsilon_{i}\\right)d\\varepsilon%\n_{i}\\,,$", "word_idx": 51426, "sentence_idx": 622, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{\\mathrm{E}}_{\\varepsilon_{i}}\\left[\\left.\\frac{\\partial L\\left(\\theta%\n\\right)}{\\partial\\theta_{i}}\\varepsilon_{i}\\right|\\varepsilon_{j\\neq i}\\right]%\n=\\intop_{-\\infty}^{\\infty}\\frac{\\partial L\\left(\\theta\\right)}{\\partial\\theta_%\n{i}}\\varepsilon_{i}f_{\\varepsilon_{i}}\\left(\\varepsilon_{i}\\right)d\\varepsilon%\n_{i}\\,,$$", "word_idx": 51753, "sentence_idx": 623, "label": "unlabeled"}, {"type": "text", "expr": "where  $f$  is the probability density function of a standard normal distribution", "word_idx": 52079, "sentence_idx": 624, "label": "unlabeled"}, {"type": "text", "expr": " Since  $f$  is an even function:", "word_idx": 52160, "sentence_idx": 625, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\mathbf{\\mathrm{E}}_{\\varepsilon_{i}}\\left[\\left\\frac{\\partial L%\n\\left(\\theta\\right)}{\\partial\\theta_{i}}\\varepsilon_{i}\\right|\\varepsilon_{j%\n\\neq i}\\right]=$", "word_idx": 52193, "sentence_idx": 626, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\mathbf{\\mathrm{E}}_{\\varepsilon_{i}}\\left[\\left.\\frac{\\partial L%\n\\left(\\theta\\right)}{\\partial\\theta_{i}}\\varepsilon_{i}\\right|\\varepsilon_{j%\n\\neq i}\\right]=$$", "word_idx": 52367, "sentence_idx": 627, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\intop_{0}^{\\infty}\\frac{\\partial L\\left(\\mu_{i}+\\varepsilon_{i}%\n\\sigma_{i},\\theta_{-i}\\right)}{\\partial\\theta_{i}}\\varepsilon_{i}f_{%\n\\varepsilon_{i}}\\left(\\varepsilon_{i}\\right)d\\varepsilon_{i}$", "word_idx": 52540, "sentence_idx": 628, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\intop_{0}^{\\infty}\\frac{\\partial L\\left(\\mu_{i}+\\varepsilon_{i}%\n\\sigma_{i},\\theta_{-i}\\right)}{\\partial\\theta_{i}}\\varepsilon_{i}f_{%\n\\varepsilon_{i}}\\left(\\varepsilon_{i}\\right)d\\varepsilon_{i}$$", "word_idx": 52751, "sentence_idx": 629, "label": "unlabeled"}, {"type": "text", "expr": "- (24)", "word_idx": 52960, "sentence_idx": 630, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle-$", "word_idx": 52966, "sentence_idx": 631, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle-$$", "word_idx": 52982, "sentence_idx": 632, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\intop_{0}^{\\infty}\\frac{\\partial L\\left(\\mu_{i}-\\varepsilon_{i}%\n\\sigma_{i},\\theta_{-i}\\right)}{\\partial\\theta_{i}}\\varepsilon_{i}f_{%\n\\varepsilon_{i}}\\left(\\varepsilon_{i}\\right)d\\varepsilon_{i}$", "word_idx": 52996, "sentence_idx": 633, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\intop_{0}^{\\infty}\\frac{\\partial L\\left(\\mu_{i}-\\varepsilon_{i}%\n\\sigma_{i},\\theta_{-i}\\right)}{\\partial\\theta_{i}}\\varepsilon_{i}f_{%\n\\varepsilon_{i}}\\left(\\varepsilon_{i}\\right)d\\varepsilon_{i}.$$", "word_idx": 53207, "sentence_idx": 634, "label": "unlabeled"}, {"type": "text", "expr": "Now, since  $L\\left(\\theta\\right)$  is strongly convex function\nwith parameter  $m>0$  and continuously differentiable function over\n $\\mathbb{R}^{n}$ , the following holds  $\\forall\\theta_{1},\\theta_{2}\\in\\mathbb{R}^{n}$ :", "word_idx": 53417, "sentence_idx": 635, "label": "unlabeled"}, {"type": "math", "expr": "$$L\\left(\\theta\\right)$$", "word_idx": 53640, "sentence_idx": 636, "label": "unlabeled"}, {"type": "math", "expr": "$$m>0$$", "word_idx": 53660, "sentence_idx": 637, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbb{R}^{n}$$", "word_idx": 53663, "sentence_idx": 638, "label": "unlabeled"}, {"type": "math", "expr": "$$\\forall\\theta_{1},\\theta_{2}\\in\\mathbb{R}^{n}$$", "word_idx": 53677, "sentence_idx": 639, "label": "unlabeled"}, {"type": "text", "expr": "$\\left(\\nabla L\\left(\\theta_{1}\\right)-\\nabla L\\left(\\theta_{2}\\right)\\right)^{%\nT}\\left(\\theta_{1}-\\theta_{2}\\right)\\geq m\\left\\|\\theta_{1}-\\theta_{2}\\right\\|$", "word_idx": 53722, "sentence_idx": 640, "label": "unlabeled"}, {"type": "math", "expr": "$$\\left(\\nabla L\\left(\\theta_{1}\\right)-\\nabla L\\left(\\theta_{2}\\right)\\right)^{%\nT}\\left(\\theta_{1}-\\theta_{2}\\right)\\geq m\\left\\|\\theta_{1}-\\theta_{2}\\right\\|$$", "word_idx": 53882, "sentence_idx": 641, "label": "unlabeled"}, {"type": "text", "expr": "For  $\\theta_{1}=\\left(\\mu_{i}+\\varepsilon_{i}\\sigma_{i},\\theta_{-i}\\right)$ \nand  $\\theta_{2}=\\left(\\mu_{i}-\\varepsilon_{i}\\sigma_{i},\\theta_{-i}\\right)$ ,\nthe following holds:", "word_idx": 54040, "sentence_idx": 642, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta_{1}=\\left(\\mu_{i}+\\varepsilon_{i}\\sigma_{i},\\theta_{-i}\\right)$$", "word_idx": 54217, "sentence_idx": 643, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta_{2}=\\left(\\mu_{i}-\\varepsilon_{i}\\sigma_{i},\\theta_{-i}\\right)$$", "word_idx": 54286, "sentence_idx": 644, "label": "unlabeled"}, {"type": "text", "expr": "$\\left(\\frac{\\partial L\\left(\\theta_{1}\\right)}{\\partial\\theta_{i}}-\\frac{%\n\\partial L\\left(\\theta_{2}\\right)}{\\partial\\theta_{i}}\\right)\\varepsilon_{i}%\n\\geq m\\left|\\varepsilon_{i}\\right|$", "word_idx": 54355, "sentence_idx": 645, "label": "unlabeled"}, {"type": "math", "expr": "$$\\left(\\frac{\\partial L\\left(\\theta_{1}\\right)}{\\partial\\theta_{i}}-\\frac{%\n\\partial L\\left(\\theta_{2}\\right)}{\\partial\\theta_{i}}\\right)\\varepsilon_{i}%\n\\geq m\\left|\\varepsilon_{i}\\right|$$", "word_idx": 54544, "sentence_idx": 646, "label": "unlabeled"}, {"type": "text", "expr": "Therefore, substituting this inequality into eq", "word_idx": 54731, "sentence_idx": 647, "label": "unlabeled"}, {"type": "text", "expr": "  24 , we obtain:", "word_idx": 54778, "sentence_idx": 648, "label": "unlabeled"}, {"type": "text", "expr": "$\\mathbf{\\mathrm{E}}_{\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)}{%\n\\partial\\theta_{i}}\\varepsilon_{i}\\right]\\geq\\frac{m}{\\sqrt{2\\pi}}>0$", "word_idx": 54795, "sentence_idx": 649, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{\\mathrm{E}}_{\\varepsilon}\\left[\\frac{\\partial L\\left(\\theta\\right)}{%\n\\partial\\theta_{i}}\\varepsilon_{i}\\right]\\geq\\frac{m}{\\sqrt{2\\pi}}>0$$", "word_idx": 54943, "sentence_idx": 650, "label": "unlabeled"}, {"type": "text", "expr": "$\\blacksquare$", "word_idx": 55089, "sentence_idx": 651, "label": "unlabeled"}, {"type": "math", "expr": "$$\\blacksquare$$", "word_idx": 55103, "sentence_idx": 652, "label": "unlabeled"}, {"type": "text", "expr": "Acknowledgments", "word_idx": 55115, "sentence_idx": 653, "label": "unlabeled"}, {"type": "text", "expr": "The authors are grateful to R", "word_idx": 55130, "sentence_idx": 654, "label": "unlabeled"}, {"type": "text", "expr": " Amit, M", "word_idx": 55159, "sentence_idx": 655, "label": "unlabeled"}, {"type": "text", "expr": " Shpigel Nacson and N", "word_idx": 55167, "sentence_idx": 656, "label": "unlabeled"}, {"type": "text", "expr": " Merlis for helpful comments on the manuscript", "word_idx": 55188, "sentence_idx": 657, "label": "unlabeled"}, {"type": "text", "expr": " The research of DS was supported by the Taub foundation", "word_idx": 55234, "sentence_idx": 658, "label": "unlabeled"}, {"type": "text", "expr": "The authors are grateful to R", "word_idx": 55290, "sentence_idx": 659, "label": "unlabeled"}, {"type": "text", "expr": " Amit, M", "word_idx": 55319, "sentence_idx": 660, "label": "unlabeled"}, {"type": "text", "expr": " Shpigel Nacson and N", "word_idx": 55327, "sentence_idx": 661, "label": "unlabeled"}, {"type": "text", "expr": " Merlis for helpful comments on the manuscript", "word_idx": 55348, "sentence_idx": 662, "label": "unlabeled"}, {"type": "text", "expr": " The research of DS was supported by the Taub foundation", "word_idx": 55394, "sentence_idx": 663, "label": "unlabeled"}, {"type": "text", "expr": "References", "word_idx": 55450, "sentence_idx": 664, "label": "unlabeled"}, {"type": "text", "expr": "Achterhold et\u00a0al", "word_idx": 55460, "sentence_idx": 665, "label": "unlabeled"}, {"type": "text", "expr": " (2018) \nJan Achterhold, Jan\u00a0Mathias Koehler, Anke Schmeink, and Tim Genewein", "word_idx": 55476, "sentence_idx": 666, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Variational Network Quantization", "word_idx": 55553, "sentence_idx": 667, "label": "unlabeled"}, {"type": "text", "expr": "Achterhold et\u00a0al", "word_idx": 55588, "sentence_idx": 668, "label": "unlabeled"}, {"type": "text", "expr": " (2018)", "word_idx": 55604, "sentence_idx": 669, "label": "unlabeled"}, {"type": "text", "expr": "Jan Achterhold, Jan\u00a0Mathias Koehler, Anke Schmeink, and Tim Genewein", "word_idx": 55611, "sentence_idx": 670, "label": "unlabeled"}, {"type": "text", "expr": "Variational Network Quantization", "word_idx": 55679, "sentence_idx": 671, "label": "unlabeled"}, {"type": "text", "expr": "In  ICLR , number\u00a01, pages 1\u201318, 2018", "word_idx": 55711, "sentence_idx": 672, "label": "unlabeled"}, {"type": "text", "expr": "Balan et\u00a0al", "word_idx": 55748, "sentence_idx": 673, "label": "unlabeled"}, {"type": "text", "expr": " (2015) \nAnoop\u00a0Korattikara Balan, Vivek Rathod, Kevin\u00a0P Murphy, and Max Welling", "word_idx": 55759, "sentence_idx": 674, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Bayesian dark knowledge", "word_idx": 55838, "sentence_idx": 675, "label": "unlabeled"}, {"type": "text", "expr": "Balan et\u00a0al", "word_idx": 55864, "sentence_idx": 676, "label": "unlabeled"}, {"type": "text", "expr": " (2015)", "word_idx": 55875, "sentence_idx": 677, "label": "unlabeled"}, {"type": "text", "expr": "Anoop\u00a0Korattikara Balan, Vivek Rathod, Kevin\u00a0P Murphy, and Max Welling", "word_idx": 55882, "sentence_idx": 678, "label": "unlabeled"}, {"type": "text", "expr": "Bayesian dark knowledge", "word_idx": 55952, "sentence_idx": 679, "label": "unlabeled"}, {"type": "text", "expr": "In  Advances in Neural Information Processing Systems , pages\n3438\u20133446, 2015", "word_idx": 55975, "sentence_idx": 680, "label": "unlabeled"}, {"type": "text", "expr": "Advances in Neural Information Processing Systems", "word_idx": 56052, "sentence_idx": 681, "label": "unlabeled"}, {"type": "text", "expr": "Blundell et\u00a0al", "word_idx": 56101, "sentence_idx": 682, "label": "unlabeled"}, {"type": "text", "expr": " (2015) \nCharles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra", "word_idx": 56115, "sentence_idx": 683, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Weight uncertainty in neural network", "word_idx": 56196, "sentence_idx": 684, "label": "unlabeled"}, {"type": "text", "expr": "Blundell et\u00a0al", "word_idx": 56235, "sentence_idx": 685, "label": "unlabeled"}, {"type": "text", "expr": " (2015)", "word_idx": 56249, "sentence_idx": 686, "label": "unlabeled"}, {"type": "text", "expr": "Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra", "word_idx": 56256, "sentence_idx": 687, "label": "unlabeled"}, {"type": "text", "expr": "Weight uncertainty in neural network", "word_idx": 56328, "sentence_idx": 688, "label": "unlabeled"}, {"type": "text", "expr": "In  International Conference on Machine Learning , pages\n1613\u20131622, 2015", "word_idx": 56364, "sentence_idx": 689, "label": "unlabeled"}, {"type": "text", "expr": "International Conference on Machine Learning", "word_idx": 56436, "sentence_idx": 690, "label": "unlabeled"}, {"type": "text", "expr": "Dauphin et\u00a0al", "word_idx": 56480, "sentence_idx": 691, "label": "unlabeled"}, {"type": "text", "expr": " (2014) \nYann Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli,\nand Yoshua Bengio", "word_idx": 56493, "sentence_idx": 692, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Identifying and attacking the saddle point problem in\nhigh-dimensional non-convex optimization", "word_idx": 56596, "sentence_idx": 693, "label": "unlabeled"}, {"type": "text", "expr": "\n\n ISSN 10495258", "word_idx": 56693, "sentence_idx": 694, "label": "unlabeled"}, {"type": "text", "expr": "Dauphin et\u00a0al", "word_idx": 56709, "sentence_idx": 695, "label": "unlabeled"}, {"type": "text", "expr": " (2014)", "word_idx": 56722, "sentence_idx": 696, "label": "unlabeled"}, {"type": "text", "expr": "Yann Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli,\nand Yoshua Bengio", "word_idx": 56729, "sentence_idx": 697, "label": "unlabeled"}, {"type": "text", "expr": "Identifying and attacking the saddle point problem in\nhigh-dimensional non-convex optimization", "word_idx": 56823, "sentence_idx": 698, "label": "unlabeled"}, {"type": "text", "expr": "NIPS , pages 1\u20139, 2014", "word_idx": 56917, "sentence_idx": 699, "label": "unlabeled"}, {"type": "text", "expr": "ISSN 10495258", "word_idx": 56939, "sentence_idx": 700, "label": "unlabeled"}, {"type": "text", "expr": "Graves (2011) \nAlex Graves", "word_idx": 56952, "sentence_idx": 701, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Practical variational inference for neural networks", "word_idx": 56978, "sentence_idx": 702, "label": "unlabeled"}, {"type": "text", "expr": "Graves (2011)", "word_idx": 57032, "sentence_idx": 703, "label": "unlabeled"}, {"type": "text", "expr": "Alex Graves", "word_idx": 57045, "sentence_idx": 704, "label": "unlabeled"}, {"type": "text", "expr": "Practical variational inference for neural networks", "word_idx": 57056, "sentence_idx": 705, "label": "unlabeled"}, {"type": "text", "expr": "In  Advances in Neural Information Processing Systems , pages\n2348\u20132356, 2011", "word_idx": 57107, "sentence_idx": 706, "label": "unlabeled"}, {"type": "text", "expr": "Advances in Neural Information Processing Systems", "word_idx": 57184, "sentence_idx": 707, "label": "unlabeled"}, {"type": "text", "expr": "Han et\u00a0al", "word_idx": 57233, "sentence_idx": 708, "label": "unlabeled"}, {"type": "text", "expr": " (2015) \nSong Han, Jeff Pool, John Tran, and William Dally", "word_idx": 57242, "sentence_idx": 709, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Learning both weights and connections for efficient neural network", "word_idx": 57300, "sentence_idx": 710, "label": "unlabeled"}, {"type": "text", "expr": "Han et\u00a0al", "word_idx": 57369, "sentence_idx": 711, "label": "unlabeled"}, {"type": "text", "expr": " (2015)", "word_idx": 57378, "sentence_idx": 712, "label": "unlabeled"}, {"type": "text", "expr": "Song Han, Jeff Pool, John Tran, and William Dally", "word_idx": 57385, "sentence_idx": 713, "label": "unlabeled"}, {"type": "text", "expr": "Learning both weights and connections for efficient neural network", "word_idx": 57434, "sentence_idx": 714, "label": "unlabeled"}, {"type": "text", "expr": "In  Advances in neural information processing systems , pages\n1135\u20131143, 2015", "word_idx": 57500, "sentence_idx": 715, "label": "unlabeled"}, {"type": "text", "expr": "Advances in neural information processing systems", "word_idx": 57577, "sentence_idx": 716, "label": "unlabeled"}, {"type": "text", "expr": "He et\u00a0al", "word_idx": 57626, "sentence_idx": 717, "label": "unlabeled"}, {"type": "text", "expr": " (2015) \nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun", "word_idx": 57634, "sentence_idx": 718, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Delving deep into rectifiers: Surpassing human-level performance on\nimagenet classification", "word_idx": 57696, "sentence_idx": 719, "label": "unlabeled"}, {"type": "text", "expr": "He et\u00a0al", "word_idx": 57790, "sentence_idx": 720, "label": "unlabeled"}, {"type": "text", "expr": " (2015)", "word_idx": 57798, "sentence_idx": 721, "label": "unlabeled"}, {"type": "text", "expr": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun", "word_idx": 57805, "sentence_idx": 722, "label": "unlabeled"}, {"type": "text", "expr": "Delving deep into rectifiers: Surpassing human-level performance on\nimagenet classification", "word_idx": 57858, "sentence_idx": 723, "label": "unlabeled"}, {"type": "text", "expr": "In  Proceedings of the IEEE international conference on computer\nvision , pages 1026\u20131034, 2015", "word_idx": 57949, "sentence_idx": 724, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the IEEE international conference on computer\nvision", "word_idx": 58044, "sentence_idx": 725, "label": "unlabeled"}, {"type": "text", "expr": "Hern\u00e1ndez-Lobato and Adams (2015) \nJos\u00e9\u00a0Miguel Hern\u00e1ndez-Lobato and Ryan Adams", "word_idx": 58111, "sentence_idx": 726, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Probabilistic backpropagation for scalable learning of bayesian\nneural networks", "word_idx": 58189, "sentence_idx": 727, "label": "unlabeled"}, {"type": "text", "expr": "Hern\u00e1ndez-Lobato and Adams (2015)", "word_idx": 58271, "sentence_idx": 728, "label": "unlabeled"}, {"type": "text", "expr": "Jos\u00e9\u00a0Miguel Hern\u00e1ndez-Lobato and Ryan Adams", "word_idx": 58304, "sentence_idx": 729, "label": "unlabeled"}, {"type": "text", "expr": "Probabilistic backpropagation for scalable learning of bayesian\nneural networks", "word_idx": 58347, "sentence_idx": 730, "label": "unlabeled"}, {"type": "text", "expr": "In  International Conference on Machine Learning , pages\n1861\u20131869, 2015", "word_idx": 58426, "sentence_idx": 731, "label": "unlabeled"}, {"type": "text", "expr": "International Conference on Machine Learning", "word_idx": 58498, "sentence_idx": 732, "label": "unlabeled"}, {"type": "text", "expr": "Hern\u00e1ndez-Lobato et\u00a0al", "word_idx": 58542, "sentence_idx": 733, "label": "unlabeled"}, {"type": "text", "expr": " (2016) \nJos\u00e9\u00a0Miguel Hern\u00e1ndez-Lobato, Yingzhen Li, Mark Rowland, Daniel\nHern\u00e1ndez-Lobato, Thang Bui, and Richard\u00a0Eric Turner", "word_idx": 58564, "sentence_idx": 734, "label": "unlabeled"}, {"type": "text", "expr": "\n\n 2016", "word_idx": 58689, "sentence_idx": 735, "label": "unlabeled"}, {"type": "text", "expr": "Hern\u00e1ndez-Lobato et\u00a0al", "word_idx": 58696, "sentence_idx": 736, "label": "unlabeled"}, {"type": "text", "expr": " (2016)", "word_idx": 58718, "sentence_idx": 737, "label": "unlabeled"}, {"type": "text", "expr": "Jos\u00e9\u00a0Miguel Hern\u00e1ndez-Lobato, Yingzhen Li, Mark Rowland, Daniel\nHern\u00e1ndez-Lobato, Thang Bui, and Richard\u00a0Eric Turner", "word_idx": 58725, "sentence_idx": 738, "label": "unlabeled"}, {"type": "text", "expr": "Black-box  $\\alpha$ -divergence minimization", "word_idx": 58841, "sentence_idx": 739, "label": "unlabeled"}, {"type": "math", "expr": "$$\\alpha$$", "word_idx": 58885, "sentence_idx": 740, "label": "unlabeled"}, {"type": "text", "expr": "Hinton and Camp (1993) \nG\u00a0E Hinton and D\u00a0Van Camp", "word_idx": 58891, "sentence_idx": 741, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Keeping the neural networks simple by minimizing the description\nlength of the weights", "word_idx": 58940, "sentence_idx": 742, "label": "unlabeled"}, {"type": "text", "expr": "Hinton and Camp (1993)", "word_idx": 59029, "sentence_idx": 743, "label": "unlabeled"}, {"type": "text", "expr": "G\u00a0E Hinton and D\u00a0Van Camp", "word_idx": 59051, "sentence_idx": 744, "label": "unlabeled"}, {"type": "text", "expr": "Keeping the neural networks simple by minimizing the description\nlength of the weights", "word_idx": 59076, "sentence_idx": 745, "label": "unlabeled"}, {"type": "text", "expr": "In  COLT \u201993 , 1993", "word_idx": 59162, "sentence_idx": 746, "label": "unlabeled"}, {"type": "text", "expr": "COLT \u201993", "word_idx": 59181, "sentence_idx": 747, "label": "unlabeled"}, {"type": "text", "expr": "URL  http://dl", "word_idx": 59189, "sentence_idx": 748, "label": "unlabeled"}, {"type": "text", "expr": "org/citation", "word_idx": 59203, "sentence_idx": 749, "label": "unlabeled"}, {"type": "text", "expr": "cfm?id=168306 ", "word_idx": 59215, "sentence_idx": 750, "label": "unlabeled"}, {"type": "text", "expr": "http://dl", "word_idx": 59229, "sentence_idx": 751, "label": "unlabeled"}, {"type": "text", "expr": "org/citation", "word_idx": 59238, "sentence_idx": 752, "label": "unlabeled"}, {"type": "text", "expr": "cfm?id=168306", "word_idx": 59250, "sentence_idx": 753, "label": "unlabeled"}, {"type": "text", "expr": "Hron et\u00a0al", "word_idx": 59263, "sentence_idx": 754, "label": "unlabeled"}, {"type": "text", "expr": " (2017) \nJiri Hron, Alexander G", "word_idx": 59273, "sentence_idx": 755, "label": "unlabeled"}, {"type": "text", "expr": " de\u00a0G", "word_idx": 59304, "sentence_idx": 756, "label": "unlabeled"}, {"type": "text", "expr": " Matthews, and Zoubin Ghahramani", "word_idx": 59309, "sentence_idx": 757, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Variational Gaussian Dropout is not Bayesian", "word_idx": 59341, "sentence_idx": 758, "label": "unlabeled"}, {"type": "text", "expr": "\n\n 2017", "word_idx": 59388, "sentence_idx": 759, "label": "unlabeled"}, {"type": "text", "expr": "Hron et\u00a0al", "word_idx": 59395, "sentence_idx": 760, "label": "unlabeled"}, {"type": "text", "expr": " (2017)", "word_idx": 59405, "sentence_idx": 761, "label": "unlabeled"}, {"type": "text", "expr": "Jiri Hron, Alexander G", "word_idx": 59412, "sentence_idx": 762, "label": "unlabeled"}, {"type": "text", "expr": " de\u00a0G", "word_idx": 59434, "sentence_idx": 763, "label": "unlabeled"}, {"type": "text", "expr": " Matthews, and Zoubin Ghahramani", "word_idx": 59439, "sentence_idx": 764, "label": "unlabeled"}, {"type": "text", "expr": "Variational Gaussian Dropout is not Bayesian", "word_idx": 59471, "sentence_idx": 765, "label": "unlabeled"}, {"type": "text", "expr": "URL  http://arxiv", "word_idx": 59515, "sentence_idx": 766, "label": "unlabeled"}, {"type": "text", "expr": "org/abs/1711", "word_idx": 59532, "sentence_idx": 767, "label": "unlabeled"}, {"type": "text", "expr": "02989 ", "word_idx": 59544, "sentence_idx": 768, "label": "unlabeled"}, {"type": "text", "expr": "http://arxiv", "word_idx": 59550, "sentence_idx": 769, "label": "unlabeled"}, {"type": "text", "expr": "org/abs/1711", "word_idx": 59562, "sentence_idx": 770, "label": "unlabeled"}, {"type": "text", "expr": "02989", "word_idx": 59574, "sentence_idx": 771, "label": "unlabeled"}, {"type": "text", "expr": "Kingma and Welling (2013) \nDiederik\u00a0P Kingma and Max Welling", "word_idx": 59579, "sentence_idx": 772, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Auto-encoding variational bayes", "word_idx": 59639, "sentence_idx": 773, "label": "unlabeled"}, {"type": "text", "expr": "Kingma and Welling (2013)", "word_idx": 59673, "sentence_idx": 774, "label": "unlabeled"}, {"type": "text", "expr": "Diederik\u00a0P Kingma and Max Welling", "word_idx": 59698, "sentence_idx": 775, "label": "unlabeled"}, {"type": "text", "expr": "Auto-encoding variational bayes", "word_idx": 59731, "sentence_idx": 776, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1312", "word_idx": 59762, "sentence_idx": 777, "label": "unlabeled"}, {"type": "text", "expr": "6114 , 2013", "word_idx": 59787, "sentence_idx": 778, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1312", "word_idx": 59798, "sentence_idx": 779, "label": "unlabeled"}, {"type": "text", "expr": "Kingma et\u00a0al", "word_idx": 59823, "sentence_idx": 780, "label": "unlabeled"}, {"type": "text", "expr": " (2015) \nDiederik\u00a0P", "word_idx": 59835, "sentence_idx": 781, "label": "unlabeled"}, {"type": "text", "expr": " Kingma, Tim Salimans, and Max Welling", "word_idx": 59854, "sentence_idx": 782, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Variational Dropout and the Local Reparameterization Trick", "word_idx": 59892, "sentence_idx": 783, "label": "unlabeled"}, {"type": "text", "expr": "\n\n ISBN 1506", "word_idx": 59953, "sentence_idx": 784, "label": "unlabeled"}, {"type": "text", "expr": "02557", "word_idx": 59965, "sentence_idx": 785, "label": "unlabeled"}, {"type": "text", "expr": "Kingma et\u00a0al", "word_idx": 59970, "sentence_idx": 786, "label": "unlabeled"}, {"type": "text", "expr": " (2015)", "word_idx": 59982, "sentence_idx": 787, "label": "unlabeled"}, {"type": "text", "expr": "Diederik\u00a0P", "word_idx": 59989, "sentence_idx": 788, "label": "unlabeled"}, {"type": "text", "expr": " Kingma, Tim Salimans, and Max Welling", "word_idx": 59999, "sentence_idx": 789, "label": "unlabeled"}, {"type": "text", "expr": "Variational Dropout and the Local Reparameterization Trick", "word_idx": 60037, "sentence_idx": 790, "label": "unlabeled"}, {"type": "text", "expr": "In  NIPS , pages 1\u201314, 2015", "word_idx": 60095, "sentence_idx": 791, "label": "unlabeled"}, {"type": "text", "expr": "ISBN 1506", "word_idx": 60122, "sentence_idx": 792, "label": "unlabeled"}, {"type": "text", "expr": "02557", "word_idx": 60131, "sentence_idx": 793, "label": "unlabeled"}, {"type": "text", "expr": "doi:  10", "word_idx": 60136, "sentence_idx": 794, "label": "unlabeled"}, {"type": "text", "expr": "1016/S0733-8619(03)00096-3 ", "word_idx": 60144, "sentence_idx": 795, "label": "unlabeled"}, {"type": "text", "expr": "1016/S0733-8619(03)00096-3", "word_idx": 60171, "sentence_idx": 796, "label": "unlabeled"}, {"type": "text", "expr": "Kirkpatrick et\u00a0al", "word_idx": 60197, "sentence_idx": 797, "label": "unlabeled"}, {"type": "text", "expr": " (2017) \nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume\nDesjardins, Andrei\u00a0A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka\nGrabska-Barwinska, et\u00a0al", "word_idx": 60214, "sentence_idx": 798, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Overcoming catastrophic forgetting in neural networks", "word_idx": 60399, "sentence_idx": 799, "label": "unlabeled"}, {"type": "text", "expr": "Kirkpatrick et\u00a0al", "word_idx": 60455, "sentence_idx": 800, "label": "unlabeled"}, {"type": "text", "expr": " (2017)", "word_idx": 60472, "sentence_idx": 801, "label": "unlabeled"}, {"type": "text", "expr": "James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume\nDesjardins, Andrei\u00a0A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka\nGrabska-Barwinska, et\u00a0al", "word_idx": 60479, "sentence_idx": 802, "label": "unlabeled"}, {"type": "text", "expr": "Overcoming catastrophic forgetting in neural networks", "word_idx": 60655, "sentence_idx": 803, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the National Academy of Sciences , 114(13):3521\u20133526, 2017", "word_idx": 60708, "sentence_idx": 804, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the National Academy of Sciences", "word_idx": 60781, "sentence_idx": 805, "label": "unlabeled"}, {"type": "text", "expr": "Krizhevsky and Hinton (2009) \nAlex Krizhevsky and Geoffrey Hinton", "word_idx": 60828, "sentence_idx": 806, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Learning multiple layers of features from tiny images", "word_idx": 60893, "sentence_idx": 807, "label": "unlabeled"}, {"type": "text", "expr": "\n\n 2009", "word_idx": 60949, "sentence_idx": 808, "label": "unlabeled"}, {"type": "text", "expr": "Krizhevsky and Hinton (2009)", "word_idx": 60956, "sentence_idx": 809, "label": "unlabeled"}, {"type": "text", "expr": "Alex Krizhevsky and Geoffrey Hinton", "word_idx": 60984, "sentence_idx": 810, "label": "unlabeled"}, {"type": "text", "expr": "Learning multiple layers of features from tiny images", "word_idx": 61019, "sentence_idx": 811, "label": "unlabeled"}, {"type": "text", "expr": "Lee et\u00a0al", "word_idx": 61072, "sentence_idx": 812, "label": "unlabeled"}, {"type": "text", "expr": " (2017) \nSang-Woo Lee, Jin-Hwa Kim, JungWoo Ha, and Byoung-Tak Zhang", "word_idx": 61081, "sentence_idx": 813, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Overcoming catastrophic forgetting by incremental moment matching", "word_idx": 61149, "sentence_idx": 814, "label": "unlabeled"}, {"type": "text", "expr": "\n\n 2017", "word_idx": 61217, "sentence_idx": 815, "label": "unlabeled"}, {"type": "text", "expr": "Lee et\u00a0al", "word_idx": 61224, "sentence_idx": 816, "label": "unlabeled"}, {"type": "text", "expr": " (2017)", "word_idx": 61233, "sentence_idx": 817, "label": "unlabeled"}, {"type": "text", "expr": "Sang-Woo Lee, Jin-Hwa Kim, JungWoo Ha, and Byoung-Tak Zhang", "word_idx": 61240, "sentence_idx": 818, "label": "unlabeled"}, {"type": "text", "expr": "Overcoming catastrophic forgetting by incremental moment matching", "word_idx": 61299, "sentence_idx": 819, "label": "unlabeled"}, {"type": "text", "expr": "Li et\u00a0al", "word_idx": 61364, "sentence_idx": 820, "label": "unlabeled"}, {"type": "text", "expr": " (2016) \nHao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans\u00a0Peter Graf", "word_idx": 61372, "sentence_idx": 821, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Pruning filters for efficient convnets", "word_idx": 61450, "sentence_idx": 822, "label": "unlabeled"}, {"type": "text", "expr": "Li et\u00a0al", "word_idx": 61491, "sentence_idx": 823, "label": "unlabeled"}, {"type": "text", "expr": " (2016)", "word_idx": 61499, "sentence_idx": 824, "label": "unlabeled"}, {"type": "text", "expr": "Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans\u00a0Peter Graf", "word_idx": 61506, "sentence_idx": 825, "label": "unlabeled"}, {"type": "text", "expr": "Pruning filters for efficient convnets", "word_idx": 61575, "sentence_idx": 826, "label": "unlabeled"}, {"type": "text", "expr": "ICLR2017 , 2016", "word_idx": 61613, "sentence_idx": 827, "label": "unlabeled"}, {"type": "text", "expr": "ICLR2017", "word_idx": 61628, "sentence_idx": 828, "label": "unlabeled"}, {"type": "text", "expr": "MacKay (1992) \nD\u00a0J\u00a0C MacKay", "word_idx": 61636, "sentence_idx": 829, "label": "unlabeled"}, {"type": "text", "expr": "\n\n A practical Bayesian framework for backpropagation networks", "word_idx": 61663, "sentence_idx": 830, "label": "unlabeled"}, {"type": "text", "expr": "MacKay (1992)", "word_idx": 61725, "sentence_idx": 831, "label": "unlabeled"}, {"type": "text", "expr": "D\u00a0J\u00a0C MacKay", "word_idx": 61738, "sentence_idx": 832, "label": "unlabeled"}, {"type": "text", "expr": "A practical Bayesian framework for backpropagation networks", "word_idx": 61750, "sentence_idx": 833, "label": "unlabeled"}, {"type": "text", "expr": "Neural computation , 472(1):448\u2013472, 1992", "word_idx": 61809, "sentence_idx": 834, "label": "unlabeled"}, {"type": "text", "expr": "Neural computation", "word_idx": 61850, "sentence_idx": 835, "label": "unlabeled"}, {"type": "text", "expr": "McCloskey and Cohen (1989) \nMichael McCloskey and Neal\u00a0J Cohen", "word_idx": 61868, "sentence_idx": 836, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Catastrophic interference in connectionist networks: The sequential\nlearning problem", "word_idx": 61930, "sentence_idx": 837, "label": "unlabeled"}, {"type": "text", "expr": "McCloskey and Cohen (1989)", "word_idx": 62017, "sentence_idx": 838, "label": "unlabeled"}, {"type": "text", "expr": "Michael McCloskey and Neal\u00a0J Cohen", "word_idx": 62043, "sentence_idx": 839, "label": "unlabeled"}, {"type": "text", "expr": "Catastrophic interference in connectionist networks: The sequential\nlearning problem", "word_idx": 62077, "sentence_idx": 840, "label": "unlabeled"}, {"type": "text", "expr": "In  Psychology of learning and motivation , volume\u00a024, pages\n109\u2013165", "word_idx": 62161, "sentence_idx": 841, "label": "unlabeled"}, {"type": "text", "expr": " Elsevier, 1989", "word_idx": 62229, "sentence_idx": 842, "label": "unlabeled"}, {"type": "text", "expr": "Psychology of learning and motivation", "word_idx": 62244, "sentence_idx": 843, "label": "unlabeled"}, {"type": "text", "expr": "Molchanov et\u00a0al", "word_idx": 62281, "sentence_idx": 844, "label": "unlabeled"}, {"type": "text", "expr": " (2017a) \nDmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov", "word_idx": 62296, "sentence_idx": 845, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Variational Dropout Sparsifies Deep Neural Networks", "word_idx": 62358, "sentence_idx": 846, "label": "unlabeled"}, {"type": "text", "expr": "Molchanov et\u00a0al", "word_idx": 62412, "sentence_idx": 847, "label": "unlabeled"}, {"type": "text", "expr": " (2017a)", "word_idx": 62427, "sentence_idx": 848, "label": "unlabeled"}, {"type": "text", "expr": "Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov", "word_idx": 62435, "sentence_idx": 849, "label": "unlabeled"}, {"type": "text", "expr": "Variational Dropout Sparsifies Deep Neural Networks", "word_idx": 62487, "sentence_idx": 850, "label": "unlabeled"}, {"type": "text", "expr": "In  ICML , 2017a", "word_idx": 62538, "sentence_idx": 851, "label": "unlabeled"}, {"type": "text", "expr": "URL  http://arxiv", "word_idx": 62554, "sentence_idx": 852, "label": "unlabeled"}, {"type": "text", "expr": "org/abs/1701", "word_idx": 62571, "sentence_idx": 853, "label": "unlabeled"}, {"type": "text", "expr": "05369 ", "word_idx": 62583, "sentence_idx": 854, "label": "unlabeled"}, {"type": "text", "expr": "http://arxiv", "word_idx": 62589, "sentence_idx": 855, "label": "unlabeled"}, {"type": "text", "expr": "org/abs/1701", "word_idx": 62601, "sentence_idx": 856, "label": "unlabeled"}, {"type": "text", "expr": "05369", "word_idx": 62613, "sentence_idx": 857, "label": "unlabeled"}, {"type": "text", "expr": "Molchanov et\u00a0al", "word_idx": 62618, "sentence_idx": 858, "label": "unlabeled"}, {"type": "text", "expr": " (2017b) \nPavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz", "word_idx": 62633, "sentence_idx": 859, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Pruning convolutional neural networks for resource efficient\ninference", "word_idx": 62712, "sentence_idx": 860, "label": "unlabeled"}, {"type": "text", "expr": "Molchanov et\u00a0al", "word_idx": 62785, "sentence_idx": 861, "label": "unlabeled"}, {"type": "text", "expr": " (2017b)", "word_idx": 62800, "sentence_idx": 862, "label": "unlabeled"}, {"type": "text", "expr": "Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz", "word_idx": 62808, "sentence_idx": 863, "label": "unlabeled"}, {"type": "text", "expr": "Pruning convolutional neural networks for resource efficient\ninference", "word_idx": 62877, "sentence_idx": 864, "label": "unlabeled"}, {"type": "text", "expr": "ICLR2017 , 2017b", "word_idx": 62947, "sentence_idx": 865, "label": "unlabeled"}, {"type": "text", "expr": "ICLR2017", "word_idx": 62963, "sentence_idx": 866, "label": "unlabeled"}, {"type": "text", "expr": "Neal (1994) \nR\u00a0M Neal", "word_idx": 62971, "sentence_idx": 867, "label": "unlabeled"}, {"type": "text", "expr": "\n\n PhD thesis, Dept", "word_idx": 62992, "sentence_idx": 868, "label": "unlabeled"}, {"type": "text", "expr": " of Computer Science, University of Toronto, 1994", "word_idx": 63011, "sentence_idx": 869, "label": "unlabeled"}, {"type": "text", "expr": "Neal (1994)", "word_idx": 63060, "sentence_idx": 870, "label": "unlabeled"}, {"type": "text", "expr": "R\u00a0M Neal", "word_idx": 63071, "sentence_idx": 871, "label": "unlabeled"}, {"type": "text", "expr": "Bayesian Learning for Neural Networks ", "word_idx": 63079, "sentence_idx": 872, "label": "unlabeled"}, {"type": "text", "expr": "Bayesian Learning for Neural Networks", "word_idx": 63117, "sentence_idx": 873, "label": "unlabeled"}, {"type": "text", "expr": "PhD thesis, Dept", "word_idx": 63154, "sentence_idx": 874, "label": "unlabeled"}, {"type": "text", "expr": " of Computer Science, University of Toronto, 1994", "word_idx": 63170, "sentence_idx": 875, "label": "unlabeled"}, {"type": "text", "expr": "Nguyen et\u00a0al", "word_idx": 63219, "sentence_idx": 876, "label": "unlabeled"}, {"type": "text", "expr": " (2017) \nCuong\u00a0V Nguyen, Yingzhen Li, Thang\u00a0D Bui, and Richard\u00a0E Turner", "word_idx": 63231, "sentence_idx": 877, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Variational continual learning", "word_idx": 63302, "sentence_idx": 878, "label": "unlabeled"}, {"type": "text", "expr": "Nguyen et\u00a0al", "word_idx": 63335, "sentence_idx": 879, "label": "unlabeled"}, {"type": "text", "expr": " (2017)", "word_idx": 63347, "sentence_idx": 880, "label": "unlabeled"}, {"type": "text", "expr": "Cuong\u00a0V Nguyen, Yingzhen Li, Thang\u00a0D Bui, and Richard\u00a0E Turner", "word_idx": 63354, "sentence_idx": 881, "label": "unlabeled"}, {"type": "text", "expr": "Variational continual learning", "word_idx": 63416, "sentence_idx": 882, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1710", "word_idx": 63446, "sentence_idx": 883, "label": "unlabeled"}, {"type": "text", "expr": "10628 , 2017", "word_idx": 63471, "sentence_idx": 884, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1710", "word_idx": 63483, "sentence_idx": 885, "label": "unlabeled"}, {"type": "text", "expr": "10628", "word_idx": 63508, "sentence_idx": 886, "label": "unlabeled"}, {"type": "text", "expr": "Simonyan and Zisserman (2014) \nKaren Simonyan and Andrew Zisserman", "word_idx": 63513, "sentence_idx": 887, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Very deep convolutional networks for large-scale image recognition", "word_idx": 63579, "sentence_idx": 888, "label": "unlabeled"}, {"type": "text", "expr": "Simonyan and Zisserman (2014)", "word_idx": 63648, "sentence_idx": 889, "label": "unlabeled"}, {"type": "text", "expr": "Karen Simonyan and Andrew Zisserman", "word_idx": 63677, "sentence_idx": 890, "label": "unlabeled"}, {"type": "text", "expr": "Very deep convolutional networks for large-scale image recognition", "word_idx": 63712, "sentence_idx": 891, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1409", "word_idx": 63778, "sentence_idx": 892, "label": "unlabeled"}, {"type": "text", "expr": "1556 , 2014", "word_idx": 63803, "sentence_idx": 893, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1409", "word_idx": 63814, "sentence_idx": 894, "label": "unlabeled"}, {"type": "text", "expr": "Soudry et\u00a0al", "word_idx": 63839, "sentence_idx": 895, "label": "unlabeled"}, {"type": "text", "expr": " (2014) \nDaniel Soudry, Itay Hubara, and Ron Meir", "word_idx": 63851, "sentence_idx": 896, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Expectation backpropagation: Parameter-free training of multilayer\nneural networks with continuous or discrete weights", "word_idx": 63900, "sentence_idx": 897, "label": "unlabeled"}, {"type": "text", "expr": "Soudry et\u00a0al", "word_idx": 64021, "sentence_idx": 898, "label": "unlabeled"}, {"type": "text", "expr": " (2014)", "word_idx": 64033, "sentence_idx": 899, "label": "unlabeled"}, {"type": "text", "expr": "Daniel Soudry, Itay Hubara, and Ron Meir", "word_idx": 64040, "sentence_idx": 900, "label": "unlabeled"}, {"type": "text", "expr": "Expectation backpropagation: Parameter-free training of multilayer\nneural networks with continuous or discrete weights", "word_idx": 64080, "sentence_idx": 901, "label": "unlabeled"}, {"type": "text", "expr": "In  Advances in Neural Information Processing Systems , pages\n963\u2013971, 2014", "word_idx": 64198, "sentence_idx": 902, "label": "unlabeled"}, {"type": "text", "expr": "Advances in Neural Information Processing Systems", "word_idx": 64273, "sentence_idx": 903, "label": "unlabeled"}, {"type": "text", "expr": "Trippe and Turner (2018) \nBrian Trippe and Richard Turner", "word_idx": 64322, "sentence_idx": 904, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Overpruning in variational bayesian neural networks", "word_idx": 64379, "sentence_idx": 905, "label": "unlabeled"}, {"type": "text", "expr": "Trippe and Turner (2018)", "word_idx": 64433, "sentence_idx": 906, "label": "unlabeled"}, {"type": "text", "expr": "Brian Trippe and Richard Turner", "word_idx": 64457, "sentence_idx": 907, "label": "unlabeled"}, {"type": "text", "expr": "Overpruning in variational bayesian neural networks", "word_idx": 64488, "sentence_idx": 908, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1801", "word_idx": 64539, "sentence_idx": 909, "label": "unlabeled"}, {"type": "text", "expr": "06230 , 2018", "word_idx": 64564, "sentence_idx": 910, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1801", "word_idx": 64576, "sentence_idx": 911, "label": "unlabeled"}, {"type": "text", "expr": "06230", "word_idx": 64601, "sentence_idx": 912, "label": "unlabeled"}, {"type": "text", "expr": "Welling and Teh (2011) \nMax Welling and Yee\u00a0W Teh", "word_idx": 64606, "sentence_idx": 913, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Bayesian learning via stochastic gradient langevin dynamics", "word_idx": 64655, "sentence_idx": 914, "label": "unlabeled"}, {"type": "text", "expr": "Welling and Teh (2011)", "word_idx": 64717, "sentence_idx": 915, "label": "unlabeled"}, {"type": "text", "expr": "Max Welling and Yee\u00a0W Teh", "word_idx": 64739, "sentence_idx": 916, "label": "unlabeled"}, {"type": "text", "expr": "Bayesian learning via stochastic gradient langevin dynamics", "word_idx": 64764, "sentence_idx": 917, "label": "unlabeled"}, {"type": "text", "expr": "In  Proceedings of the 28th International Conference on Machine\nLearning (ICML-11) , pages 681\u2013688, 2011", "word_idx": 64823, "sentence_idx": 918, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the 28th International Conference on Machine\nLearning (ICML-11)", "word_idx": 64927, "sentence_idx": 919, "label": "unlabeled"}, {"type": "text", "expr": "Zenke et\u00a0al", "word_idx": 65005, "sentence_idx": 920, "label": "unlabeled"}, {"type": "text", "expr": " (2017) \nFriedemann Zenke, Ben Poole, and Surya Ganguli", "word_idx": 65016, "sentence_idx": 921, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Continual learning through synaptic intelligence", "word_idx": 65071, "sentence_idx": 922, "label": "unlabeled"}, {"type": "text", "expr": "Zenke et\u00a0al", "word_idx": 65122, "sentence_idx": 923, "label": "unlabeled"}, {"type": "text", "expr": " (2017)", "word_idx": 65133, "sentence_idx": 924, "label": "unlabeled"}, {"type": "text", "expr": "Friedemann Zenke, Ben Poole, and Surya Ganguli", "word_idx": 65140, "sentence_idx": 925, "label": "unlabeled"}, {"type": "text", "expr": "Continual learning through synaptic intelligence", "word_idx": 65186, "sentence_idx": 926, "label": "unlabeled"}, {"type": "text", "expr": "In  International Conference on Machine Learning , pages\n3987\u20133995, 2017", "word_idx": 65234, "sentence_idx": 927, "label": "unlabeled"}, {"type": "text", "expr": "International Conference on Machine Learning", "word_idx": 65306, "sentence_idx": 928, "label": "unlabeled"}, {"type": "text", "expr": "Generated  on Tue Apr  3 11:29:44 2018 by", "word_idx": 65350, "sentence_idx": 929, "label": "unlabeled"}, {"type": "text", "expr": "LaTeXML", "word_idx": 65391, "sentence_idx": 930, "label": "unlabeled"}], "Social_GAN": [{"type": "text", "expr": "Social GAN: Socially Acceptable Trajectorieswith Generative Adversarial Networks", "word_idx": 0, "sentence_idx": 0, "label": "unlabeled"}, {"type": "text", "expr": "Social GAN: Socially Acceptable Trajectories \n with Generative Adversarial Networks", "word_idx": 80, "sentence_idx": 1, "label": "unlabeled"}, {"type": "text", "expr": "Agrim Gupta 1  \u00a0\u00a0\u00a0\nJustin Johnson 1  \u00a0\u00a0\u00a0\nLi Fei-Fei 1  \u00a0\u00a0\u00a0\nSilvio Savarese 1  \u00a0\u00a0\u00a0\nAlexandre Alahi 1,2 Stanford University 1  \u00a0\u00a0\u00a0\u00a0\u00a0\n\u00c9cole Polytechnique F\u00e9d\u00e9rate de Lausanne 2", "word_idx": 163, "sentence_idx": 2, "label": "unlabeled"}, {"type": "text", "expr": "Abstract Understanding human motion behavior is critical for autonomous moving platforms (like self-driving cars and social robots) if they are to navigate human-centric environments", "word_idx": 336, "sentence_idx": 3, "label": "unlabeled"}, {"type": "text", "expr": " This is challenging because human motion is inherently multimodal: given a history of human motion paths, there are many socially plausible ways that people could move in the future", "word_idx": 518, "sentence_idx": 4, "label": "unlabeled"}, {"type": "text", "expr": " We tackle this problem by combining tools from sequence prediction and generative adversarial networks: a recurrent sequence-to-sequence model observes motion histories and predicts future behavior, using a novel pooling mechanism to aggregate information across people", "word_idx": 700, "sentence_idx": 5, "label": "unlabeled"}, {"type": "text", "expr": " We predict socially plausible futures by training adversarially against a recurrent discriminator, and encourage diverse predictions with a novel variety loss", "word_idx": 970, "sentence_idx": 6, "label": "unlabeled"}, {"type": "text", "expr": " Through experiments on several datasets we demonstrate that our approach outperforms prior work in terms of accuracy, variety, collision avoidance, and computational complexity", "word_idx": 1129, "sentence_idx": 7, "label": "unlabeled"}, {"type": "text", "expr": "Abstract", "word_idx": 1306, "sentence_idx": 8, "label": "unlabeled"}, {"type": "text", "expr": "Understanding human motion behavior is critical for autonomous moving platforms (like self-driving cars and social robots) if they are to navigate human-centric environments", "word_idx": 1314, "sentence_idx": 9, "label": "unlabeled"}, {"type": "text", "expr": " This is challenging because human motion is inherently multimodal: given a history of human motion paths, there are many socially plausible ways that people could move in the future", "word_idx": 1487, "sentence_idx": 10, "label": "unlabeled"}, {"type": "text", "expr": " We tackle this problem by combining tools from sequence prediction and generative adversarial networks: a recurrent sequence-to-sequence model observes motion histories and predicts future behavior, using a novel pooling mechanism to aggregate information across people", "word_idx": 1669, "sentence_idx": 11, "label": "unlabeled"}, {"type": "text", "expr": " We predict socially plausible futures by training adversarially against a recurrent discriminator, and encourage diverse predictions with a novel variety loss", "word_idx": 1939, "sentence_idx": 12, "label": "unlabeled"}, {"type": "text", "expr": " Through experiments on several datasets we demonstrate that our approach outperforms prior work in terms of accuracy, variety, collision avoidance, and computational complexity", "word_idx": 2098, "sentence_idx": 13, "label": "unlabeled"}, {"type": "text", "expr": "\\cvprfinalcopy", "word_idx": 2275, "sentence_idx": 14, "label": "unlabeled"}, {"type": "text", "expr": "1  Introduction", "word_idx": 2289, "sentence_idx": 15, "label": "unlabeled"}, {"type": "text", "expr": "Predicting the motion behavior of pedestrians is essential for autonomous moving platforms like self-driving cars or social robots that will share the same ecosystem as humans", "word_idx": 2304, "sentence_idx": 16, "label": "unlabeled"}, {"type": "text", "expr": "\nHumans can effectively negotiate complex social interactions, and these machines ought to be able to do the same", "word_idx": 2479, "sentence_idx": 17, "label": "unlabeled"}, {"type": "text", "expr": " One concrete and important task to this end is the following: given observed motion trajectories of pedestrians (coordinates for the past  e", "word_idx": 2592, "sentence_idx": 18, "label": "unlabeled"}, {"type": "text", "expr": "2 seconds), predict  all  possible future trajectories (Figure\u00a0 1 )", "word_idx": 2733, "sentence_idx": 19, "label": "unlabeled"}, {"type": "text", "expr": "Forecasting the behavior of humans is challenging due to the inherent properties of human motion in crowded scenes:", "word_idx": 2800, "sentence_idx": 20, "label": "unlabeled"}, {"type": "text", "expr": "Forecasting the behavior of humans is challenging due to the inherent properties of human motion in crowded scenes:", "word_idx": 2915, "sentence_idx": 21, "label": "unlabeled"}, {"type": "text", "expr": "leftmirgin=*", "word_idx": 3030, "sentence_idx": 22, "label": "unlabeled"}, {"type": "text", "expr": "leftmirgin=*", "word_idx": 3042, "sentence_idx": 23, "label": "unlabeled"}, {"type": "text", "expr": "Interpersonal ", "word_idx": 3054, "sentence_idx": 24, "label": "unlabeled"}, {"type": "text", "expr": " Each person\u2019s motion depends on the people around them", "word_idx": 3068, "sentence_idx": 25, "label": "unlabeled"}, {"type": "text", "expr": " Humans have the innate ability to read the behavior of others when navigating crowds", "word_idx": 3123, "sentence_idx": 26, "label": "unlabeled"}, {"type": "text", "expr": " Jointly modeling these dependencies is a challenge", "word_idx": 3208, "sentence_idx": 27, "label": "unlabeled"}, {"type": "text", "expr": "Interpersonal", "word_idx": 3259, "sentence_idx": 28, "label": "unlabeled"}, {"type": "text", "expr": "leftmiirgiin=*", "word_idx": 3272, "sentence_idx": 29, "label": "unlabeled"}, {"type": "text", "expr": "leftmiirgiin=*", "word_idx": 3286, "sentence_idx": 30, "label": "unlabeled"}, {"type": "text", "expr": "Socially Acceptable ", "word_idx": 3300, "sentence_idx": 31, "label": "unlabeled"}, {"type": "text", "expr": " Some trajectories are physically possible but socially unacceptable", "word_idx": 3320, "sentence_idx": 32, "label": "unlabeled"}, {"type": "text", "expr": " Pedestrians are governed by social norms like yielding right-of-way or respecting personal space", "word_idx": 3388, "sentence_idx": 33, "label": "unlabeled"}, {"type": "text", "expr": " Formalizing them is not trivial", "word_idx": 3485, "sentence_idx": 34, "label": "unlabeled"}, {"type": "text", "expr": "Socially Acceptable", "word_idx": 3517, "sentence_idx": 35, "label": "unlabeled"}, {"type": "text", "expr": "leftmiiirgiiin=*", "word_idx": 3536, "sentence_idx": 36, "label": "unlabeled"}, {"type": "text", "expr": "leftmiiirgiiin=*", "word_idx": 3552, "sentence_idx": 37, "label": "unlabeled"}, {"type": "text", "expr": "Multimodal ", "word_idx": 3568, "sentence_idx": 38, "label": "unlabeled"}, {"type": "text", "expr": " Given a partial history, there is no single correct future prediction", "word_idx": 3579, "sentence_idx": 39, "label": "unlabeled"}, {"type": "text", "expr": " Multiple trajectories are plausible and socially-acceptable", "word_idx": 3649, "sentence_idx": 40, "label": "unlabeled"}, {"type": "text", "expr": "Multimodal", "word_idx": 3709, "sentence_idx": 41, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:  \nIllustration of a scenario where two pedestrians want to avoid each other", "word_idx": 3719, "sentence_idx": 42, "label": "unlabeled"}, {"type": "text", "expr": " There are many possible ways that they can avoid a potential collision", "word_idx": 3804, "sentence_idx": 43, "label": "unlabeled"}, {"type": "text", "expr": " We present a method that given the same observed past, predicts multiple socially acceptable outputs in crowded scenes", "word_idx": 3875, "sentence_idx": 44, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:", "word_idx": 3994, "sentence_idx": 45, "label": "unlabeled"}, {"type": "text", "expr": "Pioneering work in trajectory prediction has tackled some of the above challenges", "word_idx": 4003, "sentence_idx": 46, "label": "unlabeled"}, {"type": "text", "expr": " The interpersonal aspect has been exhaustively addressed by traditional methods based on hand-crafted features  ", "word_idx": 4084, "sentence_idx": 47, "label": "unlabeled"}, {"type": "text", "expr": " Social acceptability has been recently revisited with data-driven techniques based on Recurrent Neural Networks (RNNs)  ", "word_idx": 4197, "sentence_idx": 48, "label": "unlabeled"}, {"type": "text", "expr": " Finally, the multimodal aspect of the problem has been studied in the context of route choices given a static scene ( e", "word_idx": 4318, "sentence_idx": 49, "label": "unlabeled"}, {"type": "text", "expr": " , which streets to take at an intersection  )", "word_idx": 4438, "sentence_idx": 50, "label": "unlabeled"}, {"type": "text", "expr": " Robicquet  et al \\onedot  have shown that pedestrians have multiple navigation styles in crowded scenes given a mild or aggressive style of navigation", "word_idx": 4484, "sentence_idx": 51, "label": "unlabeled"}, {"type": "text", "expr": " Therefore, the forecasting task entails outputting different possible outcomes", "word_idx": 4635, "sentence_idx": 52, "label": "unlabeled"}, {"type": "text", "expr": "et al", "word_idx": 4714, "sentence_idx": 53, "label": "unlabeled"}, {"type": "text", "expr": "\\onedot", "word_idx": 4719, "sentence_idx": 54, "label": "unlabeled"}, {"type": "text", "expr": "While existing methods have made great progress in addressing specific challenges, they suffer from two limitations", "word_idx": 4726, "sentence_idx": 55, "label": "unlabeled"}, {"type": "text", "expr": " First, they model a local neighborhood around each person when making the prediction", "word_idx": 4841, "sentence_idx": 56, "label": "unlabeled"}, {"type": "text", "expr": " Hence, they do not have the capacity to model interactions between all people in a scene in a computationally efficient fashion", "word_idx": 4926, "sentence_idx": 57, "label": "unlabeled"}, {"type": "text", "expr": " Second, they tend to learn the \u201caverage behavior\u201d because of the commonly used loss function that minimizes the euclidean distance between the ground truth and forecasted outputs", "word_idx": 5054, "sentence_idx": 58, "label": "unlabeled"}, {"type": "text", "expr": " In contrast, we aim in learning multiple \u201cgood behaviors\u201d,  i", "word_idx": 5233, "sentence_idx": 59, "label": "unlabeled"}, {"type": "text", "expr": " , multiple socially acceptable trajectories", "word_idx": 5295, "sentence_idx": 60, "label": "unlabeled"}, {"type": "text", "expr": "To address the limitations of previous works, we propose to leverage the recent progress in generative models", "word_idx": 5339, "sentence_idx": 61, "label": "unlabeled"}, {"type": "text", "expr": " Generative Adversarial Networks (GANs) have been recently developed to overcome the difficulties in approximating intractable probabilistic computation and behavioral inference  ", "word_idx": 5448, "sentence_idx": 62, "label": "unlabeled"}, {"type": "text", "expr": " While they have been used to produce photo-realistic signals such as images  , we propose to use them to generate multiple socially-acceptable trajectories given an observed past", "word_idx": 5627, "sentence_idx": 63, "label": "unlabeled"}, {"type": "text", "expr": " One network (the generator) generates candidates and the other (the discriminator) evaluates them", "word_idx": 5806, "sentence_idx": 64, "label": "unlabeled"}, {"type": "text", "expr": " The adversarial loss enables our forecasting model to go beyond the limitation of L2 loss and potentially learn the distribution of \u201cgood behaviors\u201d that can fool the discriminator", "word_idx": 5904, "sentence_idx": 65, "label": "unlabeled"}, {"type": "text", "expr": " In our work, these behaviors are referred to as  socially-accepted  motion trajectories in crowded scenes", "word_idx": 6085, "sentence_idx": 66, "label": "unlabeled"}, {"type": "text", "expr": "socially-accepted", "word_idx": 6191, "sentence_idx": 67, "label": "unlabeled"}, {"type": "text", "expr": "Our proposed GAN is a RNN Encoder-Decoder generator and a RNN based encoder discriminator with the following two novelties: (i) we introduce a variety loss which encourages the generative network of our GAN to spread its distribution and cover the space of possible paths while being consistent with the observed inputs", "word_idx": 6208, "sentence_idx": 68, "label": "unlabeled"}, {"type": "text", "expr": " (ii) We propose a new pooling mechanism that learns a \u201cglobal\u201d pooling vector which encodes the subtle cues for all people involved in a scene", "word_idx": 6527, "sentence_idx": 69, "label": "unlabeled"}, {"type": "text", "expr": " We refer to our model as \u201cSocial GAN\u201d", "word_idx": 6670, "sentence_idx": 70, "label": "unlabeled"}, {"type": "text", "expr": "\nThrough experiments on several publicly available real-world crowd datasets, we show state-of-the-art accuracy, speed and demonstrate that our model has the capacity to generate a variety of socially-acceptable trajectories", "word_idx": 6708, "sentence_idx": 71, "label": "unlabeled"}, {"type": "text", "expr": "Our proposed GAN is a RNN Encoder-Decoder generator and a RNN based encoder discriminator with the following two novelties: (i) we introduce a variety loss which encourages the generative network of our GAN to spread its distribution and cover the space of possible paths while being consistent with the observed inputs", "word_idx": 6932, "sentence_idx": 72, "label": "unlabeled"}, {"type": "text", "expr": " (ii) We propose a new pooling mechanism that learns a \u201cglobal\u201d pooling vector which encodes the subtle cues for all people involved in a scene", "word_idx": 7251, "sentence_idx": 73, "label": "unlabeled"}, {"type": "text", "expr": " We refer to our model as \u201cSocial GAN\u201d", "word_idx": 7394, "sentence_idx": 74, "label": "unlabeled"}, {"type": "text", "expr": "\nThrough experiments on several publicly available real-world crowd datasets, we show state-of-the-art accuracy, speed and demonstrate that our model has the capacity to generate a variety of socially-acceptable trajectories", "word_idx": 7432, "sentence_idx": 75, "label": "unlabeled"}, {"type": "text", "expr": "2  Related Work", "word_idx": 7656, "sentence_idx": 76, "label": "unlabeled"}, {"type": "text", "expr": "Research in forecasting human behavior can be grouped as learning to predict human-space interactions or human-human interactions", "word_idx": 7671, "sentence_idx": 77, "label": "unlabeled"}, {"type": "text", "expr": " The former learns scene-specific motion patterns  ", "word_idx": 7800, "sentence_idx": 78, "label": "unlabeled"}, {"type": "text", "expr": " The latter models the dynamic content of the scenes,  i", "word_idx": 7851, "sentence_idx": 79, "label": "unlabeled"}, {"type": "text", "expr": "e \\onedot how pedestrians interact with each other", "word_idx": 7907, "sentence_idx": 80, "label": "unlabeled"}, {"type": "text", "expr": " The focus of our work is the latter: learning to predict human-human interactions", "word_idx": 7957, "sentence_idx": 81, "label": "unlabeled"}, {"type": "text", "expr": " We discuss existing work on this topic as well as relevant work in RNN for sequence prediction and Generative models", "word_idx": 8039, "sentence_idx": 82, "label": "unlabeled"}, {"type": "text", "expr": "\\onedot", "word_idx": 8156, "sentence_idx": 83, "label": "unlabeled"}, {"type": "text", "expr": "Human-Human Interaction", "word_idx": 8163, "sentence_idx": 84, "label": "unlabeled"}, {"type": "text", "expr": "  Human behavior has been studied from a crowd perspective in  macroscopic models  or from a individual perspective in  microscopic models  (the focus of our work)", "word_idx": 8186, "sentence_idx": 85, "label": "unlabeled"}, {"type": "text", "expr": " One example of microscopic model is the Social Forces by Helbing and Molnar   which models pedestrian behavior with attractive forces guiding them towards their goal and repulsive forces encouraging collision avoidance", "word_idx": 8349, "sentence_idx": 86, "label": "unlabeled"}, {"type": "text", "expr": " Over the past decades, this method has been often revisited  ", "word_idx": 8568, "sentence_idx": 87, "label": "unlabeled"}, {"type": "text", "expr": " Tools popular in economics have also been used such as the Discrete Choice framework by Antonini  et", "word_idx": 8630, "sentence_idx": 88, "label": "unlabeled"}, {"type": "text", "expr": " Treuille  et", "word_idx": 8731, "sentence_idx": 89, "label": "unlabeled"}, {"type": "text", "expr": "    use continuum dynamics, and Wang  et", "word_idx": 8744, "sentence_idx": 90, "label": "unlabeled"}, {"type": "text", "expr": "   , Tay  et", "word_idx": 8784, "sentence_idx": 91, "label": "unlabeled"}, {"type": "text", "expr": "    use Gaussian processes", "word_idx": 8796, "sentence_idx": 92, "label": "unlabeled"}, {"type": "text", "expr": " Such functions have also been used to study stationary groups  ", "word_idx": 8822, "sentence_idx": 93, "label": "unlabeled"}, {"type": "text", "expr": " However, all these methods use hand crafted energy potentials based on relative distances and specific rules", "word_idx": 8886, "sentence_idx": 94, "label": "unlabeled"}, {"type": "text", "expr": " In contrast, over the past two years, data-driven methods based on RNNs have been used to outperform the above traditional ones", "word_idx": 8995, "sentence_idx": 95, "label": "unlabeled"}, {"type": "text", "expr": "Human-Human Interaction", "word_idx": 9123, "sentence_idx": 96, "label": "unlabeled"}, {"type": "text", "expr": "macroscopic models", "word_idx": 9146, "sentence_idx": 97, "label": "unlabeled"}, {"type": "text", "expr": "microscopic models", "word_idx": 9164, "sentence_idx": 98, "label": "unlabeled"}, {"type": "text", "expr": "RNNs for Sequence Prediction", "word_idx": 9182, "sentence_idx": 99, "label": "unlabeled"}, {"type": "text", "expr": "  Recurrent Neural Networks are a rich class of dynamic models which extend feedforward networks for sequence generation in diverse domains like speech recognition  , machine translation   and image captioning  ", "word_idx": 9210, "sentence_idx": 100, "label": "unlabeled"}, {"type": "text", "expr": " However, they lack high-level and spatio-temporal structure  ", "word_idx": 9421, "sentence_idx": 101, "label": "unlabeled"}, {"type": "text", "expr": " Several attempts have been made to use multiple networks to capture complex interactions  ", "word_idx": 9483, "sentence_idx": 102, "label": "unlabeled"}, {"type": "text", "expr": " Alahi  et al \\onedot  use a social pooling layer that models nearby pedestrians", "word_idx": 9574, "sentence_idx": 103, "label": "unlabeled"}, {"type": "text", "expr": " In the rest of this paper, we show that using a Multi-Layer Perceptron (MLP) followed by max pooling is computationally more efficient and works as well or better than the social pooling method from  ", "word_idx": 9654, "sentence_idx": 104, "label": "unlabeled"}, {"type": "text", "expr": " Lee  et al", "word_idx": 9855, "sentence_idx": 105, "label": "unlabeled"}, {"type": "text", "expr": "    introduce a RNN Encoder-Decoder framework which uses variational auto-encoder (VAE) for trajectory prediction", "word_idx": 9866, "sentence_idx": 106, "label": "unlabeled"}, {"type": "text", "expr": " However, they did not model human-human interactions in crowded scenes", "word_idx": 9979, "sentence_idx": 107, "label": "unlabeled"}, {"type": "text", "expr": "RNNs for Sequence Prediction", "word_idx": 10050, "sentence_idx": 108, "label": "unlabeled"}, {"type": "text", "expr": "et al", "word_idx": 10078, "sentence_idx": 109, "label": "unlabeled"}, {"type": "text", "expr": "\\onedot", "word_idx": 10083, "sentence_idx": 110, "label": "unlabeled"}, {"type": "text", "expr": "et al", "word_idx": 10090, "sentence_idx": 111, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:  \nSystem overview", "word_idx": 10095, "sentence_idx": 112, "label": "unlabeled"}, {"type": "text", "expr": " Our model consists of three key components: Generator (G), Pooling Module, and Discriminator (D)", "word_idx": 10122, "sentence_idx": 113, "label": "unlabeled"}, {"type": "text", "expr": " G takes as input past trajectories  $X_{i}$  and encodes the history of the person  $i$  as  $H_{i}^{t}$ ", "word_idx": 10219, "sentence_idx": 114, "label": "unlabeled"}, {"type": "text", "expr": " The pooling module takes as input all  $H_{i}^{t_{obs}}$  and outputs a pooled vector  $P_{i}$  for each person", "word_idx": 10325, "sentence_idx": 115, "label": "unlabeled"}, {"type": "text", "expr": " The decoder generates the future trajectory conditioned on  $H_{i}^{t_{obs}}$  and  $P_{i}$ ", "word_idx": 10437, "sentence_idx": 116, "label": "unlabeled"}, {"type": "text", "expr": " D takes as input  $T_{real}$  or  $T_{fake}$  and classifies them as socially acceptable or not (see Figure  3  for PM)", "word_idx": 10530, "sentence_idx": 117, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:", "word_idx": 10650, "sentence_idx": 118, "label": "unlabeled"}, {"type": "math", "expr": "$$X_{i}$$", "word_idx": 10659, "sentence_idx": 119, "label": "unlabeled"}, {"type": "math", "expr": "$$H_{i}^{t}$$", "word_idx": 10664, "sentence_idx": 120, "label": "unlabeled"}, {"type": "math", "expr": "$$H_{i}^{t_{obs}}$$", "word_idx": 10673, "sentence_idx": 121, "label": "unlabeled"}, {"type": "math", "expr": "$$P_{i}$$", "word_idx": 10688, "sentence_idx": 122, "label": "unlabeled"}, {"type": "math", "expr": "$$H_{i}^{t_{obs}}$$", "word_idx": 10693, "sentence_idx": 123, "label": "unlabeled"}, {"type": "math", "expr": "$$P_{i}$$", "word_idx": 10708, "sentence_idx": 124, "label": "unlabeled"}, {"type": "math", "expr": "$$T_{real}$$", "word_idx": 10713, "sentence_idx": 125, "label": "unlabeled"}, {"type": "math", "expr": "$$T_{fake}$$", "word_idx": 10721, "sentence_idx": 126, "label": "unlabeled"}, {"type": "text", "expr": "Generative Modeling", "word_idx": 10729, "sentence_idx": 127, "label": "unlabeled"}, {"type": "text", "expr": "  Generative models like variational autoencoders   are trained by maximizing the lower bound of training data likelihood", "word_idx": 10748, "sentence_idx": 128, "label": "unlabeled"}, {"type": "text", "expr": " Goodfellow  et al \\onedot  propose an alternative approach, Generative Adversarial Networks (GANs), where the training procedure is a minimax game between a generative model and a discriminative model; this overcomes the difficulty of approximating intractable probabilistic computations", "word_idx": 10869, "sentence_idx": 129, "label": "unlabeled"}, {"type": "text", "expr": " Generative models have shown promising results in tasks like super-resolution  , image to image translation  , and image synthesis   which have multiple possible outputs for a given input", "word_idx": 11157, "sentence_idx": 130, "label": "unlabeled"}, {"type": "text", "expr": " However, their application in sequence generation problems like natural language processing has lagged since sampling from these generated outputs to feed to the discriminator is a non-differentiable operation", "word_idx": 11345, "sentence_idx": 131, "label": "unlabeled"}, {"type": "text", "expr": "Generative Modeling", "word_idx": 11555, "sentence_idx": 132, "label": "unlabeled"}, {"type": "text", "expr": "et al", "word_idx": 11574, "sentence_idx": 133, "label": "unlabeled"}, {"type": "text", "expr": "\\onedot", "word_idx": 11579, "sentence_idx": 134, "label": "unlabeled"}, {"type": "text", "expr": "3  Method", "word_idx": 11586, "sentence_idx": 135, "label": "unlabeled"}, {"type": "text", "expr": "Humans possess an intuitive ability to navigate crowds taking into account the people around them", "word_idx": 11595, "sentence_idx": 136, "label": "unlabeled"}, {"type": "text", "expr": " We plan our paths keeping in mind our goal and also simultaneously taking into account the motion of surrounding people like their direction of motion, velocity, etc", "word_idx": 11692, "sentence_idx": 137, "label": "unlabeled"}, {"type": "text", "expr": " However, often in such situations multiple possible options exist", "word_idx": 11858, "sentence_idx": 138, "label": "unlabeled"}, {"type": "text", "expr": " We need models which not only can understand these complex human interactions but can also capture the variety of options", "word_idx": 11924, "sentence_idx": 139, "label": "unlabeled"}, {"type": "text", "expr": " Current approaches have focused on predicting the average future trajectory which minimizes the  $L2$  distance from the ground truth future trajectory whereas we want to predict multiple \u201cgood\u201d trajectories", "word_idx": 12046, "sentence_idx": 140, "label": "unlabeled"}, {"type": "text", "expr": " In this section, we first present our GAN based encoder-decoder architecture to address this issue, we then describe our novel pooling layer which models human-human interactions and finally we introduce our variety loss which encourages the network to produce multiple diverse future trajectories for the same observed sequence", "word_idx": 12254, "sentence_idx": 141, "label": "unlabeled"}, {"type": "text", "expr": "1  Problem Definition", "word_idx": 12583, "sentence_idx": 142, "label": "unlabeled"}, {"type": "text", "expr": "Our goal is to  jointly  reason and predict the future trajectories of  all  the agents involved in a scene", "word_idx": 12604, "sentence_idx": 143, "label": "unlabeled"}, {"type": "text", "expr": " We assume that we receive as input all the trajectories for people in a scene as  $\\mathbf{X}={X_{1},X_{2},,X_{n}}$  and predict the future trajectories  $\\mathbf{\\hat{Y}}={\\hat{Y_{1}},\\hat{Y_{2}},,\\hat{Y_{n}}}$  of all the people  simultaneously ", "word_idx": 12711, "sentence_idx": 144, "label": "unlabeled"}, {"type": "text", "expr": " The input trajectory of a person  $i$  is defined as  $X_{i}={(x_{i}^{t},y_{i}^{t})}$  from time steps  $t=1,,t_{obs}$  and the future trajectory (ground truth) can be defined similarly as  ${Y_{i}}={({x}_{i}^{t},{y}_{i}^{t})}$  from time steps  $t=t_{obs}+1,,t_{pred}$ ", "word_idx": 12959, "sentence_idx": 145, "label": "unlabeled"}, {"type": "text", "expr": " We denote predictions as  $\\hat{Y_{i}}$ ", "word_idx": 13230, "sentence_idx": 146, "label": "unlabeled"}, {"type": "text", "expr": "jointly", "word_idx": 13271, "sentence_idx": 147, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{X}={X_{1},X_{2},...,X_{n}}$$", "word_idx": 13278, "sentence_idx": 148, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{\\hat{Y}}={\\hat{Y_{1}},\\hat{Y_{2}},...,\\hat{Y_{n}}}$$", "word_idx": 13312, "sentence_idx": 149, "label": "unlabeled"}, {"type": "text", "expr": "simultaneously", "word_idx": 13370, "sentence_idx": 150, "label": "unlabeled"}, {"type": "math", "expr": "$$X_{i}={(x_{i}^{t},y_{i}^{t})}$$", "word_idx": 13384, "sentence_idx": 151, "label": "unlabeled"}, {"type": "math", "expr": "$$t=1,...,t_{obs}$$", "word_idx": 13413, "sentence_idx": 152, "label": "unlabeled"}, {"type": "math", "expr": "$${Y_{i}}={({x}_{i}^{t},{y}_{i}^{t})}$$", "word_idx": 13428, "sentence_idx": 153, "label": "unlabeled"}, {"type": "math", "expr": "$$t=t_{obs}+1,...,t_{pred}$$", "word_idx": 13463, "sentence_idx": 154, "label": "unlabeled"}, {"type": "math", "expr": "$$\\hat{Y_{i}}$$", "word_idx": 13487, "sentence_idx": 155, "label": "unlabeled"}, {"type": "text", "expr": "2  Generative Adversarial Networks", "word_idx": 13498, "sentence_idx": 156, "label": "unlabeled"}, {"type": "text", "expr": "A Generative Adversarial Network (GAN) consists of two neural networks trained in opposition to each other  ", "word_idx": 13532, "sentence_idx": 157, "label": "unlabeled"}, {"type": "text", "expr": " The two adversarially trained models are: a generative model  $G$  that captures the data distribution, and a discriminative model  $D$  that estimates the probability that a sample came from the training data rather than  $G$ ", "word_idx": 13640, "sentence_idx": 158, "label": "unlabeled"}, {"type": "text", "expr": " The generator  $G$  takes a latent variable  $z$  as input, and outputs sample  $G(z)$ ", "word_idx": 13868, "sentence_idx": 159, "label": "unlabeled"}, {"type": "text", "expr": " The discriminator  $D$  takes a sample  $x$  as input and outputs  $D(x)$  which represents the probability that it is real", "word_idx": 13956, "sentence_idx": 160, "label": "unlabeled"}, {"type": "text", "expr": " The training procedure is similar to a two-player min-max game with the following objective function:", "word_idx": 14080, "sentence_idx": 161, "label": "unlabeled"}, {"type": "math", "expr": "$$G(z)$$", "word_idx": 14182, "sentence_idx": 162, "label": "unlabeled"}, {"type": "math", "expr": "$$D(x)$$", "word_idx": 14186, "sentence_idx": 163, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\min_{G}\\max_{D}V(G,D)=$", "word_idx": 14190, "sentence_idx": 164, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\min_{G}\\max_{D}V(G,D)=$$", "word_idx": 14228, "sentence_idx": 165, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\mathbb{E}_{x\\sim p_{data}(x)}[\\log D(x)]+\\mathbb{E}_{z\\sim p_{(z%\n)}}[\\log(1-D(G(z)))]$", "word_idx": 14264, "sentence_idx": 166, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\mathbb{E}_{x\\sim p_{data}(x)}[\\log D(x)]+\\mathbb{E}_{z\\sim p_{(z%\n)}}[\\log(1-D(G(z)))].$$", "word_idx": 14366, "sentence_idx": 167, "label": "unlabeled"}, {"type": "text", "expr": "GANs can used for conditional models by providing both the generator and discriminator with additional input  $c$ ,\nyielding  $G(z,c)$  and  $D(x,c)$   ", "word_idx": 14467, "sentence_idx": 168, "label": "unlabeled"}, {"type": "math", "expr": "$$G(z,c)$$", "word_idx": 14619, "sentence_idx": 169, "label": "unlabeled"}, {"type": "math", "expr": "$$D(x,c)$$", "word_idx": 14625, "sentence_idx": 170, "label": "unlabeled"}, {"type": "text", "expr": "3  Socially-Aware GAN", "word_idx": 14631, "sentence_idx": 171, "label": "unlabeled"}, {"type": "text", "expr": "As discussed in Section  1  trajectory prediction is a multi-modal problem", "word_idx": 14652, "sentence_idx": 172, "label": "unlabeled"}, {"type": "text", "expr": " Generative models can be used with time-series data to simulate possible futures", "word_idx": 14726, "sentence_idx": 173, "label": "unlabeled"}, {"type": "text", "expr": " We leverage this insight in designing SGAN which addresses the multi-modality of the problem using GANs (see Figure  2 )", "word_idx": 14807, "sentence_idx": 174, "label": "unlabeled"}, {"type": "text", "expr": " Our model consists of three key components: Generator (G), Pooling Module (PM) and Discriminator (D)", "word_idx": 14928, "sentence_idx": 175, "label": "unlabeled"}, {"type": "text", "expr": " G is based on encoder-decoder framework where we link the hidden states of encoder and decoder via PM", "word_idx": 15029, "sentence_idx": 176, "label": "unlabeled"}, {"type": "text", "expr": " G takes as input  $X_{i}$  and outputs predicted trajectory  $\\hat{Y}_{i}$ ", "word_idx": 15131, "sentence_idx": 177, "label": "unlabeled"}, {"type": "text", "expr": " D inputs the entire sequence comprising both input trajectory  $X_{i}$  and future prediction  $\\hat{Y}_{i}$  (or  $Y_{i}$ ) and classifies them as \u201creal/fake\u201d", "word_idx": 15207, "sentence_idx": 178, "label": "unlabeled"}, {"type": "math", "expr": "$$X_{i}$$", "word_idx": 15367, "sentence_idx": 179, "label": "unlabeled"}, {"type": "math", "expr": "$$\\hat{Y}_{i}$$", "word_idx": 15372, "sentence_idx": 180, "label": "unlabeled"}, {"type": "math", "expr": "$$X_{i}$$", "word_idx": 15383, "sentence_idx": 181, "label": "unlabeled"}, {"type": "math", "expr": "$$\\hat{Y}_{i}$$", "word_idx": 15388, "sentence_idx": 182, "label": "unlabeled"}, {"type": "math", "expr": "$$Y_{i}$$", "word_idx": 15399, "sentence_idx": 183, "label": "unlabeled"}, {"type": "text", "expr": "Generator", "word_idx": 15404, "sentence_idx": 184, "label": "unlabeled"}, {"type": "text", "expr": "  We first embed the location of each person using a single layer MLP to get a fixed length vector  $e_{i}^{t}$ ", "word_idx": 15413, "sentence_idx": 185, "label": "unlabeled"}, {"type": "text", "expr": " These embeddings are used as input to the LSTM cell of the encoder at time  $t$  introducing the following recurrence:", "word_idx": 15525, "sentence_idx": 186, "label": "unlabeled"}, {"type": "text", "expr": "Generator", "word_idx": 15644, "sentence_idx": 187, "label": "unlabeled"}, {"type": "math", "expr": "$$e_{i}^{t}$$", "word_idx": 15653, "sentence_idx": 188, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle e_{i}^{t}$", "word_idx": 15662, "sentence_idx": 189, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle e_{i}^{t}$$", "word_idx": 15687, "sentence_idx": 190, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\phi(x_{i}^{t},y_{i}^{t};W_{ee})$", "word_idx": 15710, "sentence_idx": 191, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\phi(x_{i}^{t},y_{i}^{t};W_{ee})$$", "word_idx": 15758, "sentence_idx": 192, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle h_{ei}^{t}$", "word_idx": 15804, "sentence_idx": 193, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle h_{ei}^{t}$$", "word_idx": 15830, "sentence_idx": 194, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=LSTM(h_{ei}^{t-1},e_{i}^{t};W_{encoder})$", "word_idx": 15854, "sentence_idx": 195, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=LSTM(h_{ei}^{t-1},e_{i}^{t};W_{encoder})$$", "word_idx": 15910, "sentence_idx": 196, "label": "unlabeled"}, {"type": "text", "expr": "where  $\\phi(\\cdot)$  is an embedding function with ReLU non-linearity,  $W_{ee}$  is the embedding weight", "word_idx": 15964, "sentence_idx": 197, "label": "unlabeled"}, {"type": "text", "expr": " The LSTM weights ( $W_{encoder}$ ) are shared between all people in a scene", "word_idx": 16070, "sentence_idx": 198, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi(\\cdot)$$", "word_idx": 16146, "sentence_idx": 199, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{ee}$$", "word_idx": 16157, "sentence_idx": 200, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{encoder}$$", "word_idx": 16163, "sentence_idx": 201, "label": "unlabeled"}, {"type": "text", "expr": "After initializing the decoder states as described above we can obtain predictions as follows:", "word_idx": 16174, "sentence_idx": 202, "label": "unlabeled"}, {"type": "text", "expr": "Na\u00efve use of one LSTM per person fails to capture interaction between people", "word_idx": 16268, "sentence_idx": 203, "label": "unlabeled"}, {"type": "text", "expr": " Encoder learns the state of a person and stores their history of motion", "word_idx": 16344, "sentence_idx": 204, "label": "unlabeled"}, {"type": "text", "expr": " However, as shown by Alahi  et al \\onedot  we need a compact representation which combines information from different encoders to effectively reason about social interactions", "word_idx": 16416, "sentence_idx": 205, "label": "unlabeled"}, {"type": "text", "expr": " In our method, we model human-human interaction via a Pooling Module (PM)", "word_idx": 16591, "sentence_idx": 206, "label": "unlabeled"}, {"type": "text", "expr": " After  $t_{obs}$  we pool hidden states of all the people present in the scene to get a pooled tensor  $P_{i}$  for each person", "word_idx": 16665, "sentence_idx": 207, "label": "unlabeled"}, {"type": "text", "expr": " Traditionally, GANs take as input noise and generate samples", "word_idx": 16793, "sentence_idx": 208, "label": "unlabeled"}, {"type": "text", "expr": " Our goal is to produce future scenarios which are consistent with the past", "word_idx": 16854, "sentence_idx": 209, "label": "unlabeled"}, {"type": "text", "expr": " To achieve this we condition the generation of output trajectories by initializing the hidden state of the decoder as:", "word_idx": 16929, "sentence_idx": 210, "label": "unlabeled"}, {"type": "text", "expr": "et al", "word_idx": 17048, "sentence_idx": 211, "label": "unlabeled"}, {"type": "text", "expr": "\\onedot", "word_idx": 17053, "sentence_idx": 212, "label": "unlabeled"}, {"type": "math", "expr": "$$t_{obs}$$", "word_idx": 17060, "sentence_idx": 213, "label": "unlabeled"}, {"type": "math", "expr": "$$P_{i}$$", "word_idx": 17067, "sentence_idx": 214, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle c_{i}^{t}$", "word_idx": 17072, "sentence_idx": 215, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle c_{i}^{t}$$", "word_idx": 17097, "sentence_idx": 216, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\gamma(P_{i},h_{ei}^{t};W_{c})$", "word_idx": 17120, "sentence_idx": 217, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\gamma(P_{i},h_{ei}^{t};W_{c})$$", "word_idx": 17166, "sentence_idx": 218, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle h_{di}^{t}$", "word_idx": 17210, "sentence_idx": 219, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle h_{di}^{t}$$", "word_idx": 17236, "sentence_idx": 220, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=[c_{i}^{t},z]$", "word_idx": 17260, "sentence_idx": 221, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=[c_{i}^{t},z]$$", "word_idx": 17289, "sentence_idx": 222, "label": "unlabeled"}, {"type": "text", "expr": "where  $\\gamma(\\cdot)$  is a multi-layer perceptron (MLP) with ReLU non-linearity and  $W_{c}$  is the embedding weight", "word_idx": 17316, "sentence_idx": 223, "label": "unlabeled"}, {"type": "text", "expr": " We deviate from prior work in two important ways regarding trajectory prediction:", "word_idx": 17435, "sentence_idx": 224, "label": "unlabeled"}, {"type": "math", "expr": "$$\\gamma(\\cdot)$$", "word_idx": 17517, "sentence_idx": 225, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{c}$$", "word_idx": 17530, "sentence_idx": 226, "label": "unlabeled"}, {"type": "text", "expr": "noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt", "word_idx": 17535, "sentence_idx": 227, "label": "unlabeled"}, {"type": "text", "expr": "noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt", "word_idx": 17580, "sentence_idx": 228, "label": "unlabeled"}, {"type": "text", "expr": "Prior work   uses the hidden state to predict parameters of a bivariate Gaussian distribution", "word_idx": 17625, "sentence_idx": 229, "label": "unlabeled"}, {"type": "text", "expr": " However, this introduces difficulty in the training process as backpropagation through sampling process in non-differentiable", "word_idx": 17718, "sentence_idx": 230, "label": "unlabeled"}, {"type": "text", "expr": " We avoid this by directly predicting the coordinates  $(\\hat{x}_{i}^{t},\\hat{y}_{i}^{t})$ ", "word_idx": 17844, "sentence_idx": 231, "label": "unlabeled"}, {"type": "math", "expr": "$$(\\hat{x}_{i}^{t},\\hat{y}_{i}^{t})$$", "word_idx": 17935, "sentence_idx": 232, "label": "unlabeled"}, {"type": "text", "expr": "noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt", "word_idx": 17968, "sentence_idx": 233, "label": "unlabeled"}, {"type": "text", "expr": "noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt", "word_idx": 18013, "sentence_idx": 234, "label": "unlabeled"}, {"type": "text", "expr": "\u201cSocial\u201d context is generally provided as input to the LSTM cell   ", "word_idx": 18058, "sentence_idx": 235, "label": "unlabeled"}, {"type": "text", "expr": " Instead we provide the pooled context only once as input to the decoder", "word_idx": 18125, "sentence_idx": 236, "label": "unlabeled"}, {"type": "text", "expr": " This also provides us with the ability to choose to pool at specific time steps and results in  16x  speed increase as compared to S-LSTM   (see Table  2 )", "word_idx": 18197, "sentence_idx": 237, "label": "unlabeled"}, {"type": "text", "expr": "After initializing the decoder states as described above we can obtain predictions as follows:", "word_idx": 18353, "sentence_idx": 238, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle e_{i}^{t}$", "word_idx": 18447, "sentence_idx": 239, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle e_{i}^{t}$$", "word_idx": 18472, "sentence_idx": 240, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\phi(x_{i}^{t-1},y_{i}^{t-1};W_{ed})$", "word_idx": 18495, "sentence_idx": 241, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\phi(x_{i}^{t-1},y_{i}^{t-1};W_{ed})$$", "word_idx": 18547, "sentence_idx": 242, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle P_{i}$", "word_idx": 18597, "sentence_idx": 243, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle P_{i}$$", "word_idx": 18618, "sentence_idx": 244, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=PM(h_{d1}^{t-1},,h_{dn}^{t})$", "word_idx": 18637, "sentence_idx": 245, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=PM(h_{d1}^{t-1},...,h_{dn}^{t})$$", "word_idx": 18681, "sentence_idx": 246, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle h_{di}^{t}$", "word_idx": 18726, "sentence_idx": 247, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle h_{di}^{t}$$", "word_idx": 18752, "sentence_idx": 248, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=LSTM(\\gamma(P_{i},h_{di}^{t-1}),e_{i}^{t};W_{decoder})$", "word_idx": 18776, "sentence_idx": 249, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=LSTM(\\gamma(P_{i},h_{di}^{t-1}),e_{i}^{t};W_{decoder})$$", "word_idx": 18846, "sentence_idx": 250, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle(\\hat{x}_{i}^{t},\\hat{y}_{i}^{t})$", "word_idx": 18914, "sentence_idx": 251, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle(\\hat{x}_{i}^{t},\\hat{y}_{i}^{t})$$", "word_idx": 18962, "sentence_idx": 252, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\gamma(h_{di}^{t})$", "word_idx": 19008, "sentence_idx": 253, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\gamma(h_{di}^{t})$$", "word_idx": 19042, "sentence_idx": 254, "label": "unlabeled"}, {"type": "text", "expr": "where  $\\phi(\\cdot)$  is an embedding function with ReLU non-linearity with  $W_{ed}$  as the embedding weights", "word_idx": 19074, "sentence_idx": 255, "label": "unlabeled"}, {"type": "text", "expr": " The LSTM weights are denoted by  $W_{decoder}$  and  $\\gamma$  is an MLP", "word_idx": 19185, "sentence_idx": 256, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi(\\cdot)$$", "word_idx": 19258, "sentence_idx": 257, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{ed}$$", "word_idx": 19269, "sentence_idx": 258, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{decoder}$$", "word_idx": 19275, "sentence_idx": 259, "label": "unlabeled"}, {"type": "math", "expr": "$$\\gamma$$", "word_idx": 19286, "sentence_idx": 260, "label": "unlabeled"}, {"type": "text", "expr": "Discriminator ", "word_idx": 19292, "sentence_idx": 261, "label": "unlabeled"}, {"type": "text", "expr": " The discriminator consists of a separate encoder", "word_idx": 19306, "sentence_idx": 262, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, it takes as input  $T_{real}=[X_{i},Y_{i}]$  or  $T_{fake}=[X_{i},\\hat{Y}_{i}]$  and classifies them as real/fake", "word_idx": 19355, "sentence_idx": 263, "label": "unlabeled"}, {"type": "text", "expr": " We apply a MLP on the encoder\u2019s last hidden state to obtain a classification score", "word_idx": 19483, "sentence_idx": 264, "label": "unlabeled"}, {"type": "text", "expr": " The discriminator will ideally learn subtle social interaction rules and classify trajectories which are not socially acceptable as \u201cfake\u201d", "word_idx": 19566, "sentence_idx": 265, "label": "unlabeled"}, {"type": "text", "expr": "Discriminator", "word_idx": 19705, "sentence_idx": 266, "label": "unlabeled"}, {"type": "math", "expr": "$$T_{real}=[X_{i},Y_{i}]$$", "word_idx": 19718, "sentence_idx": 267, "label": "unlabeled"}, {"type": "math", "expr": "$$T_{fake}=[X_{i},\\hat{Y}_{i}]$$", "word_idx": 19740, "sentence_idx": 268, "label": "unlabeled"}, {"type": "text", "expr": "Losses ", "word_idx": 19768, "sentence_idx": 269, "label": "unlabeled"}, {"type": "text", "expr": " In addition to adversarial loss, we also apply  $L2$  loss on the predicted trajectory which measures how far the generated samples are from the actual ground truth", "word_idx": 19775, "sentence_idx": 270, "label": "unlabeled"}, {"type": "text", "expr": "Losses", "word_idx": 19940, "sentence_idx": 271, "label": "unlabeled"}, {"type": "text", "expr": "4  Pooling Module", "word_idx": 19946, "sentence_idx": 272, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:  \nComparison between our pooling mechanism (red dotted arrows) and Social Pooling\u00a0  (red dashed grid) for the red person", "word_idx": 19963, "sentence_idx": 273, "label": "unlabeled"}, {"type": "text", "expr": " Our method computes relative positions between the red and all other people; these positions are concatenated with each person\u2019s hidden state, processed independently by an MLP, then pooled elementwise to compute red person\u2019s pooling vector  $P_{1}$ ", "word_idx": 20093, "sentence_idx": 274, "label": "unlabeled"}, {"type": "text", "expr": " Social pooling only considers people inside the grid, and cannot model interactions between all pairs of people", "word_idx": 20344, "sentence_idx": 275, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:", "word_idx": 20456, "sentence_idx": 276, "label": "unlabeled"}, {"type": "math", "expr": "$$P_{1}$$", "word_idx": 20465, "sentence_idx": 277, "label": "unlabeled"}, {"type": "text", "expr": "In order to jointly reason across multiple people we need a mechanism to share information across LSTMs", "word_idx": 20470, "sentence_idx": 278, "label": "unlabeled"}, {"type": "text", "expr": " However, there are several challenges which a method should address:", "word_idx": 20573, "sentence_idx": 279, "label": "unlabeled"}, {"type": "text", "expr": "In order to jointly reason across multiple people we need a mechanism to share information across LSTMs", "word_idx": 20642, "sentence_idx": 280, "label": "unlabeled"}, {"type": "text", "expr": " However, there are several challenges which a method should address:", "word_idx": 20745, "sentence_idx": 281, "label": "unlabeled"}, {"type": "text", "expr": "noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt", "word_idx": 20814, "sentence_idx": 282, "label": "unlabeled"}, {"type": "text", "expr": "noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt", "word_idx": 20859, "sentence_idx": 283, "label": "unlabeled"}, {"type": "text", "expr": "Variable and (potentially) large number of people in a scene", "word_idx": 20904, "sentence_idx": 284, "label": "unlabeled"}, {"type": "text", "expr": " We need a compact representation which combines information from all the people", "word_idx": 20964, "sentence_idx": 285, "label": "unlabeled"}, {"type": "text", "expr": "Variable and (potentially) large number of people in a scene", "word_idx": 21044, "sentence_idx": 286, "label": "unlabeled"}, {"type": "text", "expr": " We need a compact representation which combines information from all the people", "word_idx": 21104, "sentence_idx": 287, "label": "unlabeled"}, {"type": "text", "expr": "noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt", "word_idx": 21184, "sentence_idx": 288, "label": "unlabeled"}, {"type": "text", "expr": "noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt", "word_idx": 21229, "sentence_idx": 289, "label": "unlabeled"}, {"type": "text", "expr": "Scattered Human-Human Interaction", "word_idx": 21274, "sentence_idx": 290, "label": "unlabeled"}, {"type": "text", "expr": " Local information is not always sufficient", "word_idx": 21307, "sentence_idx": 291, "label": "unlabeled"}, {"type": "text", "expr": " Far-away pedestrians might impact each others", "word_idx": 21350, "sentence_idx": 292, "label": "unlabeled"}, {"type": "text", "expr": " Hence, the network needs to model global configuration", "word_idx": 21396, "sentence_idx": 293, "label": "unlabeled"}, {"type": "text", "expr": "Scattered Human-Human Interaction", "word_idx": 21451, "sentence_idx": 294, "label": "unlabeled"}, {"type": "text", "expr": " Local information is not always sufficient", "word_idx": 21484, "sentence_idx": 295, "label": "unlabeled"}, {"type": "text", "expr": " Far-away pedestrians might impact each others", "word_idx": 21527, "sentence_idx": 296, "label": "unlabeled"}, {"type": "text", "expr": " Hence, the network needs to model global configuration", "word_idx": 21573, "sentence_idx": 297, "label": "unlabeled"}, {"type": "text", "expr": "Social Pooling   addresses the first issue by proposing a grid based pooling scheme", "word_idx": 21628, "sentence_idx": 298, "label": "unlabeled"}, {"type": "text", "expr": " However, this hand-crafted solution is slow and fails to capture global context", "word_idx": 21711, "sentence_idx": 299, "label": "unlabeled"}, {"type": "text", "expr": " Qi  et al \\onedot  show that above properties can be achieved by applying a learned symmetric function on transformed elements of the input set of points", "word_idx": 21791, "sentence_idx": 300, "label": "unlabeled"}, {"type": "text", "expr": " As shown in Figure  2  this can be achieved by passing the input coordinates through a MLP followed by a symmetric function (we use Max-Pooling)", "word_idx": 21945, "sentence_idx": 301, "label": "unlabeled"}, {"type": "text", "expr": " The pooled vector  $P_{i}$  needs to summarize all the information a person needs to make a decision", "word_idx": 22090, "sentence_idx": 302, "label": "unlabeled"}, {"type": "text", "expr": " Since, we use relative coordinates for translation invariance we augment the input to the pooling module with relative position of each person with respect to person  $i$ ", "word_idx": 22191, "sentence_idx": 303, "label": "unlabeled"}, {"type": "text", "expr": "et al", "word_idx": 22363, "sentence_idx": 304, "label": "unlabeled"}, {"type": "text", "expr": "\\onedot", "word_idx": 22368, "sentence_idx": 305, "label": "unlabeled"}, {"type": "math", "expr": "$$P_{i}$$", "word_idx": 22375, "sentence_idx": 306, "label": "unlabeled"}, {"type": "text", "expr": "Metric Dataset Linear LSTM S-LSTM SGAN (Ours)", "word_idx": 22380, "sentence_idx": 307, "label": "unlabeled"}, {"type": "text", "expr": "Metric", "word_idx": 22425, "sentence_idx": 308, "label": "unlabeled"}, {"type": "text", "expr": "Metric", "word_idx": 22431, "sentence_idx": 309, "label": "unlabeled"}, {"type": "text", "expr": "Dataset", "word_idx": 22437, "sentence_idx": 310, "label": "unlabeled"}, {"type": "text", "expr": "Dataset", "word_idx": 22444, "sentence_idx": 311, "label": "unlabeled"}, {"type": "text", "expr": "Linear", "word_idx": 22451, "sentence_idx": 312, "label": "unlabeled"}, {"type": "text", "expr": "Linear", "word_idx": 22457, "sentence_idx": 313, "label": "unlabeled"}, {"type": "text", "expr": "S-LSTM", "word_idx": 22463, "sentence_idx": 314, "label": "unlabeled"}, {"type": "text", "expr": "S-LSTM", "word_idx": 22469, "sentence_idx": 315, "label": "unlabeled"}, {"type": "text", "expr": "SGAN (Ours)", "word_idx": 22475, "sentence_idx": 316, "label": "unlabeled"}, {"type": "text", "expr": "SGAN (Ours)", "word_idx": 22486, "sentence_idx": 317, "label": "unlabeled"}, {"type": "text", "expr": "1V-1 1V-20 20V-20 20VP-20", "word_idx": 22497, "sentence_idx": 318, "label": "unlabeled"}, {"type": "text", "expr": "1V-20", "word_idx": 22522, "sentence_idx": 319, "label": "unlabeled"}, {"type": "text", "expr": "20V-20", "word_idx": 22527, "sentence_idx": 320, "label": "unlabeled"}, {"type": "text", "expr": "20VP-20", "word_idx": 22533, "sentence_idx": 321, "label": "unlabeled"}, {"type": "text", "expr": "ADE ETH 0", "word_idx": 22540, "sentence_idx": 322, "label": "unlabeled"}, {"type": "text", "expr": "84 / 1", "word_idx": 22549, "sentence_idx": 323, "label": "unlabeled"}, {"type": "text", "expr": "70 / 1", "word_idx": 22555, "sentence_idx": 324, "label": "unlabeled"}, {"type": "text", "expr": "73 / 1", "word_idx": 22561, "sentence_idx": 325, "label": "unlabeled"}, {"type": "text", "expr": "79 / 1", "word_idx": 22567, "sentence_idx": 326, "label": "unlabeled"}, {"type": "text", "expr": "75 / 1", "word_idx": 22573, "sentence_idx": 327, "label": "unlabeled"}, {"type": "text", "expr": "84 / 1", "word_idx": 22579, "sentence_idx": 328, "label": "unlabeled"}, {"type": "text", "expr": "70 / 1", "word_idx": 22585, "sentence_idx": 329, "label": "unlabeled"}, {"type": "text", "expr": "73 / 1", "word_idx": 22591, "sentence_idx": 330, "label": "unlabeled"}, {"type": "text", "expr": "79 / 1", "word_idx": 22597, "sentence_idx": 331, "label": "unlabeled"}, {"type": "text", "expr": "75 / 1", "word_idx": 22603, "sentence_idx": 332, "label": "unlabeled"}, {"type": "text", "expr": "61 /  0", "word_idx": 22609, "sentence_idx": 333, "label": "unlabeled"}, {"type": "text", "expr": "60  / 0", "word_idx": 22616, "sentence_idx": 334, "label": "unlabeled"}, {"type": "text", "expr": "HOTEL 0", "word_idx": 22623, "sentence_idx": 335, "label": "unlabeled"}, {"type": "text", "expr": "55 / 0", "word_idx": 22630, "sentence_idx": 336, "label": "unlabeled"}, {"type": "text", "expr": "49 / 0", "word_idx": 22636, "sentence_idx": 337, "label": "unlabeled"}, {"type": "text", "expr": "71 / 1", "word_idx": 22642, "sentence_idx": 338, "label": "unlabeled"}, {"type": "text", "expr": "63 / 0", "word_idx": 22648, "sentence_idx": 339, "label": "unlabeled"}, {"type": "text", "expr": "48 / 0", "word_idx": 22654, "sentence_idx": 340, "label": "unlabeled"}, {"type": "text", "expr": "52 / 0", "word_idx": 22660, "sentence_idx": 341, "label": "unlabeled"}, {"type": "text", "expr": "HOTEL", "word_idx": 22666, "sentence_idx": 342, "label": "unlabeled"}, {"type": "text", "expr": "HOTEL", "word_idx": 22671, "sentence_idx": 343, "label": "unlabeled"}, {"type": "text", "expr": "35  /  0", "word_idx": 22676, "sentence_idx": 344, "label": "unlabeled"}, {"type": "text", "expr": "55 / 0", "word_idx": 22684, "sentence_idx": 345, "label": "unlabeled"}, {"type": "text", "expr": "49 / 0", "word_idx": 22690, "sentence_idx": 346, "label": "unlabeled"}, {"type": "text", "expr": "71 / 1", "word_idx": 22696, "sentence_idx": 347, "label": "unlabeled"}, {"type": "text", "expr": "63 / 0", "word_idx": 22702, "sentence_idx": 348, "label": "unlabeled"}, {"type": "text", "expr": "48 / 0", "word_idx": 22708, "sentence_idx": 349, "label": "unlabeled"}, {"type": "text", "expr": "52 / 0", "word_idx": 22714, "sentence_idx": 350, "label": "unlabeled"}, {"type": "text", "expr": "UNIV 0", "word_idx": 22720, "sentence_idx": 351, "label": "unlabeled"}, {"type": "text", "expr": "56 / 0", "word_idx": 22726, "sentence_idx": 352, "label": "unlabeled"}, {"type": "text", "expr": "36 / 0", "word_idx": 22732, "sentence_idx": 353, "label": "unlabeled"}, {"type": "text", "expr": "41 / 0", "word_idx": 22738, "sentence_idx": 354, "label": "unlabeled"}, {"type": "text", "expr": "37 / 0", "word_idx": 22744, "sentence_idx": 355, "label": "unlabeled"}, {"type": "text", "expr": "36 / 0", "word_idx": 22750, "sentence_idx": 356, "label": "unlabeled"}, {"type": "text", "expr": "44 / 0", "word_idx": 22756, "sentence_idx": 357, "label": "unlabeled"}, {"type": "text", "expr": "56 / 0", "word_idx": 22762, "sentence_idx": 358, "label": "unlabeled"}, {"type": "text", "expr": "36 / 0", "word_idx": 22768, "sentence_idx": 359, "label": "unlabeled"}, {"type": "text", "expr": "41 / 0", "word_idx": 22774, "sentence_idx": 360, "label": "unlabeled"}, {"type": "text", "expr": "37 / 0", "word_idx": 22780, "sentence_idx": 361, "label": "unlabeled"}, {"type": "text", "expr": "36 / 0", "word_idx": 22786, "sentence_idx": 362, "label": "unlabeled"}, {"type": "text", "expr": "36  /  0", "word_idx": 22792, "sentence_idx": 363, "label": "unlabeled"}, {"type": "text", "expr": "44 / 0", "word_idx": 22800, "sentence_idx": 364, "label": "unlabeled"}, {"type": "text", "expr": "ZARA1 0", "word_idx": 22806, "sentence_idx": 365, "label": "unlabeled"}, {"type": "text", "expr": "41 / 0", "word_idx": 22813, "sentence_idx": 366, "label": "unlabeled"}, {"type": "text", "expr": "25 / 0", "word_idx": 22819, "sentence_idx": 367, "label": "unlabeled"}, {"type": "text", "expr": "27 / 0", "word_idx": 22825, "sentence_idx": 368, "label": "unlabeled"}, {"type": "text", "expr": "25 / 0", "word_idx": 22831, "sentence_idx": 369, "label": "unlabeled"}, {"type": "text", "expr": "23 / 0", "word_idx": 22837, "sentence_idx": 370, "label": "unlabeled"}, {"type": "text", "expr": "22 / 0", "word_idx": 22843, "sentence_idx": 371, "label": "unlabeled"}, {"type": "text", "expr": "ZARA1", "word_idx": 22849, "sentence_idx": 372, "label": "unlabeled"}, {"type": "text", "expr": "ZARA1", "word_idx": 22854, "sentence_idx": 373, "label": "unlabeled"}, {"type": "text", "expr": "41 / 0", "word_idx": 22859, "sentence_idx": 374, "label": "unlabeled"}, {"type": "text", "expr": "25 / 0", "word_idx": 22865, "sentence_idx": 375, "label": "unlabeled"}, {"type": "text", "expr": "27 / 0", "word_idx": 22871, "sentence_idx": 376, "label": "unlabeled"}, {"type": "text", "expr": "25 / 0", "word_idx": 22877, "sentence_idx": 377, "label": "unlabeled"}, {"type": "text", "expr": "23 / 0", "word_idx": 22883, "sentence_idx": 378, "label": "unlabeled"}, {"type": "text", "expr": "21  /  0", "word_idx": 22889, "sentence_idx": 379, "label": "unlabeled"}, {"type": "text", "expr": "22 / 0", "word_idx": 22897, "sentence_idx": 380, "label": "unlabeled"}, {"type": "text", "expr": "ZARA2 0", "word_idx": 22903, "sentence_idx": 381, "label": "unlabeled"}, {"type": "text", "expr": "53 / 0", "word_idx": 22910, "sentence_idx": 382, "label": "unlabeled"}, {"type": "text", "expr": "31 / 0", "word_idx": 22916, "sentence_idx": 383, "label": "unlabeled"}, {"type": "text", "expr": "33 / 0", "word_idx": 22922, "sentence_idx": 384, "label": "unlabeled"}, {"type": "text", "expr": "32 / 0", "word_idx": 22928, "sentence_idx": 385, "label": "unlabeled"}, {"type": "text", "expr": "29 / 0", "word_idx": 22934, "sentence_idx": 386, "label": "unlabeled"}, {"type": "text", "expr": "ZARA2", "word_idx": 22940, "sentence_idx": 387, "label": "unlabeled"}, {"type": "text", "expr": "ZARA2", "word_idx": 22945, "sentence_idx": 388, "label": "unlabeled"}, {"type": "text", "expr": "53 / 0", "word_idx": 22950, "sentence_idx": 389, "label": "unlabeled"}, {"type": "text", "expr": "31 / 0", "word_idx": 22956, "sentence_idx": 390, "label": "unlabeled"}, {"type": "text", "expr": "33 / 0", "word_idx": 22962, "sentence_idx": 391, "label": "unlabeled"}, {"type": "text", "expr": "32 / 0", "word_idx": 22968, "sentence_idx": 392, "label": "unlabeled"}, {"type": "text", "expr": "29 / 0", "word_idx": 22974, "sentence_idx": 393, "label": "unlabeled"}, {"type": "text", "expr": "27  / 0", "word_idx": 22980, "sentence_idx": 394, "label": "unlabeled"}, {"type": "text", "expr": "29 /  0", "word_idx": 22987, "sentence_idx": 395, "label": "unlabeled"}, {"type": "text", "expr": "AVG 0", "word_idx": 22994, "sentence_idx": 396, "label": "unlabeled"}, {"type": "text", "expr": "54 / 0", "word_idx": 22999, "sentence_idx": 397, "label": "unlabeled"}, {"type": "text", "expr": "43 / 0", "word_idx": 23005, "sentence_idx": 398, "label": "unlabeled"}, {"type": "text", "expr": "45 / 0", "word_idx": 23011, "sentence_idx": 399, "label": "unlabeled"}, {"type": "text", "expr": "49 / 0", "word_idx": 23017, "sentence_idx": 400, "label": "unlabeled"}, {"type": "text", "expr": "45 / 0", "word_idx": 23023, "sentence_idx": 401, "label": "unlabeled"}, {"type": "text", "expr": "41 / 0", "word_idx": 23029, "sentence_idx": 402, "label": "unlabeled"}, {"type": "text", "expr": "54 / 0", "word_idx": 23035, "sentence_idx": 403, "label": "unlabeled"}, {"type": "text", "expr": "43 / 0", "word_idx": 23041, "sentence_idx": 404, "label": "unlabeled"}, {"type": "text", "expr": "45 / 0", "word_idx": 23047, "sentence_idx": 405, "label": "unlabeled"}, {"type": "text", "expr": "49 / 0", "word_idx": 23053, "sentence_idx": 406, "label": "unlabeled"}, {"type": "text", "expr": "45 / 0", "word_idx": 23059, "sentence_idx": 407, "label": "unlabeled"}, {"type": "text", "expr": "39  /  0", "word_idx": 23065, "sentence_idx": 408, "label": "unlabeled"}, {"type": "text", "expr": "41 / 0", "word_idx": 23073, "sentence_idx": 409, "label": "unlabeled"}, {"type": "text", "expr": "FDE ETH 1", "word_idx": 23079, "sentence_idx": 410, "label": "unlabeled"}, {"type": "text", "expr": "60 / 2", "word_idx": 23088, "sentence_idx": 411, "label": "unlabeled"}, {"type": "text", "expr": "45 / 2", "word_idx": 23094, "sentence_idx": 412, "label": "unlabeled"}, {"type": "text", "expr": "48 / 2", "word_idx": 23100, "sentence_idx": 413, "label": "unlabeled"}, {"type": "text", "expr": "61 / 2", "word_idx": 23106, "sentence_idx": 414, "label": "unlabeled"}, {"type": "text", "expr": "52 / 2", "word_idx": 23112, "sentence_idx": 415, "label": "unlabeled"}, {"type": "text", "expr": "60 / 2", "word_idx": 23118, "sentence_idx": 416, "label": "unlabeled"}, {"type": "text", "expr": "45 / 2", "word_idx": 23124, "sentence_idx": 417, "label": "unlabeled"}, {"type": "text", "expr": "48 / 2", "word_idx": 23130, "sentence_idx": 418, "label": "unlabeled"}, {"type": "text", "expr": "61 / 2", "word_idx": 23136, "sentence_idx": 419, "label": "unlabeled"}, {"type": "text", "expr": "52 / 2", "word_idx": 23142, "sentence_idx": 420, "label": "unlabeled"}, {"type": "text", "expr": "22 /  1", "word_idx": 23148, "sentence_idx": 421, "label": "unlabeled"}, {"type": "text", "expr": "19  / 1", "word_idx": 23155, "sentence_idx": 422, "label": "unlabeled"}, {"type": "text", "expr": "HOTEL 1", "word_idx": 23162, "sentence_idx": 423, "label": "unlabeled"}, {"type": "text", "expr": "17 / 1", "word_idx": 23169, "sentence_idx": 424, "label": "unlabeled"}, {"type": "text", "expr": "01 / 1", "word_idx": 23175, "sentence_idx": 425, "label": "unlabeled"}, {"type": "text", "expr": "44 / 2", "word_idx": 23181, "sentence_idx": 426, "label": "unlabeled"}, {"type": "text", "expr": "32 / 1", "word_idx": 23187, "sentence_idx": 427, "label": "unlabeled"}, {"type": "text", "expr": "95 / 1", "word_idx": 23193, "sentence_idx": 428, "label": "unlabeled"}, {"type": "text", "expr": "02 / 1", "word_idx": 23199, "sentence_idx": 429, "label": "unlabeled"}, {"type": "text", "expr": "HOTEL", "word_idx": 23205, "sentence_idx": 430, "label": "unlabeled"}, {"type": "text", "expr": "HOTEL", "word_idx": 23210, "sentence_idx": 431, "label": "unlabeled"}, {"type": "text", "expr": "60  /  0", "word_idx": 23215, "sentence_idx": 432, "label": "unlabeled"}, {"type": "text", "expr": "17 / 1", "word_idx": 23223, "sentence_idx": 433, "label": "unlabeled"}, {"type": "text", "expr": "01 / 1", "word_idx": 23229, "sentence_idx": 434, "label": "unlabeled"}, {"type": "text", "expr": "44 / 2", "word_idx": 23235, "sentence_idx": 435, "label": "unlabeled"}, {"type": "text", "expr": "32 / 1", "word_idx": 23241, "sentence_idx": 436, "label": "unlabeled"}, {"type": "text", "expr": "95 / 1", "word_idx": 23247, "sentence_idx": 437, "label": "unlabeled"}, {"type": "text", "expr": "02 / 1", "word_idx": 23253, "sentence_idx": 438, "label": "unlabeled"}, {"type": "text", "expr": "UNIV 1", "word_idx": 23259, "sentence_idx": 439, "label": "unlabeled"}, {"type": "text", "expr": "01 / 1", "word_idx": 23265, "sentence_idx": 440, "label": "unlabeled"}, {"type": "text", "expr": "77 / 1", "word_idx": 23271, "sentence_idx": 441, "label": "unlabeled"}, {"type": "text", "expr": "84 / 1", "word_idx": 23277, "sentence_idx": 442, "label": "unlabeled"}, {"type": "text", "expr": "75 / 1", "word_idx": 23283, "sentence_idx": 443, "label": "unlabeled"}, {"type": "text", "expr": "75 / 1", "word_idx": 23289, "sentence_idx": 444, "label": "unlabeled"}, {"type": "text", "expr": "84 / 1", "word_idx": 23295, "sentence_idx": 445, "label": "unlabeled"}, {"type": "text", "expr": "01 / 1", "word_idx": 23301, "sentence_idx": 446, "label": "unlabeled"}, {"type": "text", "expr": "77 / 1", "word_idx": 23307, "sentence_idx": 447, "label": "unlabeled"}, {"type": "text", "expr": "84 / 1", "word_idx": 23313, "sentence_idx": 448, "label": "unlabeled"}, {"type": "text", "expr": "75 / 1", "word_idx": 23319, "sentence_idx": 449, "label": "unlabeled"}, {"type": "text", "expr": "73  /  1", "word_idx": 23325, "sentence_idx": 450, "label": "unlabeled"}, {"type": "text", "expr": "75 / 1", "word_idx": 23333, "sentence_idx": 451, "label": "unlabeled"}, {"type": "text", "expr": "84 / 1", "word_idx": 23339, "sentence_idx": 452, "label": "unlabeled"}, {"type": "text", "expr": "ZARA1 0", "word_idx": 23345, "sentence_idx": 453, "label": "unlabeled"}, {"type": "text", "expr": "74 / 1", "word_idx": 23352, "sentence_idx": 454, "label": "unlabeled"}, {"type": "text", "expr": "53 / 0", "word_idx": 23358, "sentence_idx": 455, "label": "unlabeled"}, {"type": "text", "expr": "56 / 1", "word_idx": 23364, "sentence_idx": 456, "label": "unlabeled"}, {"type": "text", "expr": "53 / 0", "word_idx": 23370, "sentence_idx": 457, "label": "unlabeled"}, {"type": "text", "expr": "48 / 0", "word_idx": 23376, "sentence_idx": 458, "label": "unlabeled"}, {"type": "text", "expr": "ZARA1", "word_idx": 23382, "sentence_idx": 459, "label": "unlabeled"}, {"type": "text", "expr": "ZARA1", "word_idx": 23387, "sentence_idx": 460, "label": "unlabeled"}, {"type": "text", "expr": "74 / 1", "word_idx": 23392, "sentence_idx": 461, "label": "unlabeled"}, {"type": "text", "expr": "53 / 0", "word_idx": 23398, "sentence_idx": 462, "label": "unlabeled"}, {"type": "text", "expr": "56 / 1", "word_idx": 23404, "sentence_idx": 463, "label": "unlabeled"}, {"type": "text", "expr": "53 / 0", "word_idx": 23410, "sentence_idx": 464, "label": "unlabeled"}, {"type": "text", "expr": "48 / 0", "word_idx": 23416, "sentence_idx": 465, "label": "unlabeled"}, {"type": "text", "expr": "42  / 0", "word_idx": 23422, "sentence_idx": 466, "label": "unlabeled"}, {"type": "text", "expr": "43 /  0", "word_idx": 23429, "sentence_idx": 467, "label": "unlabeled"}, {"type": "text", "expr": "ZARA2 0", "word_idx": 23436, "sentence_idx": 468, "label": "unlabeled"}, {"type": "text", "expr": "95 / 1", "word_idx": 23443, "sentence_idx": 469, "label": "unlabeled"}, {"type": "text", "expr": "65 / 1", "word_idx": 23449, "sentence_idx": 470, "label": "unlabeled"}, {"type": "text", "expr": "70 / 1", "word_idx": 23455, "sentence_idx": 471, "label": "unlabeled"}, {"type": "text", "expr": "66 / 1", "word_idx": 23461, "sentence_idx": 472, "label": "unlabeled"}, {"type": "text", "expr": "61 / 1", "word_idx": 23467, "sentence_idx": 473, "label": "unlabeled"}, {"type": "text", "expr": "58 / 0", "word_idx": 23473, "sentence_idx": 474, "label": "unlabeled"}, {"type": "text", "expr": "ZARA2", "word_idx": 23479, "sentence_idx": 475, "label": "unlabeled"}, {"type": "text", "expr": "ZARA2", "word_idx": 23484, "sentence_idx": 476, "label": "unlabeled"}, {"type": "text", "expr": "95 / 1", "word_idx": 23489, "sentence_idx": 477, "label": "unlabeled"}, {"type": "text", "expr": "65 / 1", "word_idx": 23495, "sentence_idx": 478, "label": "unlabeled"}, {"type": "text", "expr": "70 / 1", "word_idx": 23501, "sentence_idx": 479, "label": "unlabeled"}, {"type": "text", "expr": "66 / 1", "word_idx": 23507, "sentence_idx": 480, "label": "unlabeled"}, {"type": "text", "expr": "61 / 1", "word_idx": 23513, "sentence_idx": 481, "label": "unlabeled"}, {"type": "text", "expr": "54  /  0", "word_idx": 23519, "sentence_idx": 482, "label": "unlabeled"}, {"type": "text", "expr": "58 / 0", "word_idx": 23527, "sentence_idx": 483, "label": "unlabeled"}, {"type": "text", "expr": "AVG 0", "word_idx": 23533, "sentence_idx": 484, "label": "unlabeled"}, {"type": "text", "expr": "98 / 1", "word_idx": 23538, "sentence_idx": 485, "label": "unlabeled"}, {"type": "text", "expr": "91 / 1", "word_idx": 23544, "sentence_idx": 486, "label": "unlabeled"}, {"type": "text", "expr": "91 / 1", "word_idx": 23550, "sentence_idx": 487, "label": "unlabeled"}, {"type": "text", "expr": "00 / 1", "word_idx": 23556, "sentence_idx": 488, "label": "unlabeled"}, {"type": "text", "expr": "93 / 1", "word_idx": 23562, "sentence_idx": 489, "label": "unlabeled"}, {"type": "text", "expr": "81 / 1", "word_idx": 23568, "sentence_idx": 490, "label": "unlabeled"}, {"type": "text", "expr": "98 / 1", "word_idx": 23574, "sentence_idx": 491, "label": "unlabeled"}, {"type": "text", "expr": "91 / 1", "word_idx": 23580, "sentence_idx": 492, "label": "unlabeled"}, {"type": "text", "expr": "91 / 1", "word_idx": 23586, "sentence_idx": 493, "label": "unlabeled"}, {"type": "text", "expr": "00 / 1", "word_idx": 23592, "sentence_idx": 494, "label": "unlabeled"}, {"type": "text", "expr": "93 / 1", "word_idx": 23598, "sentence_idx": 495, "label": "unlabeled"}, {"type": "text", "expr": "78  /  1", "word_idx": 23604, "sentence_idx": 496, "label": "unlabeled"}, {"type": "text", "expr": "81 / 1", "word_idx": 23612, "sentence_idx": 497, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:  Quantitative results of all methods across datasets", "word_idx": 23618, "sentence_idx": 498, "label": "unlabeled"}, {"type": "text", "expr": " We report two error metrics Average Displacement Error (ADE) and Final Displacement Error (FDE) for  $t_{pred}=8$  and  $t_{pred}=12$  (8 / 12) in meters", "word_idx": 23679, "sentence_idx": 499, "label": "unlabeled"}, {"type": "text", "expr": " Our method consistently outperforms state-of-the-art S-LSTM method and is especially good for long term predictions (lower is better)", "word_idx": 23833, "sentence_idx": 500, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:", "word_idx": 23967, "sentence_idx": 501, "label": "unlabeled"}, {"type": "math", "expr": "$$t_{pred}=8$$", "word_idx": 23975, "sentence_idx": 502, "label": "unlabeled"}, {"type": "math", "expr": "$$t_{pred}=12$$", "word_idx": 23985, "sentence_idx": 503, "label": "unlabeled"}, {"type": "text", "expr": "5  Encouraging Diverse Sample Generation", "word_idx": 23996, "sentence_idx": 504, "label": "unlabeled"}, {"type": "text", "expr": "Trajectory prediction is challenging as given limited past history a model has to reason about multiple possible outcomes", "word_idx": 24036, "sentence_idx": 505, "label": "unlabeled"}, {"type": "text", "expr": " The method described so far produces good predictions, but these predictions try to produce the \u201caverage\u201d prediction in cases where there can be multiple outputs", "word_idx": 24157, "sentence_idx": 506, "label": "unlabeled"}, {"type": "text", "expr": " Further, we found that outputs were not very sensitive to changes in noise and produced very similar predictions", "word_idx": 24319, "sentence_idx": 507, "label": "unlabeled"}, {"type": "text", "expr": "Trajectory prediction is challenging as given limited past history a model has to reason about multiple possible outcomes", "word_idx": 24432, "sentence_idx": 508, "label": "unlabeled"}, {"type": "text", "expr": " The method described so far produces good predictions, but these predictions try to produce the \u201caverage\u201d prediction in cases where there can be multiple outputs", "word_idx": 24553, "sentence_idx": 509, "label": "unlabeled"}, {"type": "text", "expr": " Further, we found that outputs were not very sensitive to changes in noise and produced very similar predictions", "word_idx": 24715, "sentence_idx": 510, "label": "unlabeled"}, {"type": "text", "expr": "We propose a variety loss function that encourages the network to produce diverse samples", "word_idx": 24828, "sentence_idx": 511, "label": "unlabeled"}, {"type": "text", "expr": " For each scene we generate  $k$  possible output predictions by randomly sampling  $z$  from  $\\mathcal{N}(0,1)$  and choosing the \u201cbest\u201d prediction in  $L2$  sense as our prediction", "word_idx": 24917, "sentence_idx": 512, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{N}(0,1)$$", "word_idx": 25100, "sentence_idx": 513, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\mathcal{L}_{variety}=\\min_{k}\\|Y_{i}-\\hat{Y}_{i}^{(k)}\\|_{2},$", "word_idx": 25116, "sentence_idx": 514, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\mathcal{L}_{variety}=\\min_{k}\\|Y_{i}-\\hat{Y}_{i}^{(k)}\\|_{2},$$", "word_idx": 25193, "sentence_idx": 515, "label": "unlabeled"}, {"type": "text", "expr": "where  $k$  is a hyperparameter", "word_idx": 25268, "sentence_idx": 516, "label": "unlabeled"}, {"type": "text", "expr": "By considering only the best trajectory, this loss encourages the network to hedge its bets and cover the space of outputs that conform to the past trajectory", "word_idx": 25299, "sentence_idx": 517, "label": "unlabeled"}, {"type": "text", "expr": " The loss is structurally akin to Minimum over N (MoN) loss   but to the best of our knowledge this has not been used in the context of GANs to encourage diversity of generated samples", "word_idx": 25457, "sentence_idx": 518, "label": "unlabeled"}, {"type": "text", "expr": "6  Implementation Details", "word_idx": 25641, "sentence_idx": 519, "label": "unlabeled"}, {"type": "text", "expr": "We use LSTM as the RNN in our model for both decoder and encoder", "word_idx": 25666, "sentence_idx": 520, "label": "unlabeled"}, {"type": "text", "expr": " The dimensions of the hidden state for encoder is 16 and decoder is 32", "word_idx": 25730, "sentence_idx": 521, "label": "unlabeled"}, {"type": "text", "expr": " We embed the input coordinates as  $16$  dimensional vectors", "word_idx": 25801, "sentence_idx": 522, "label": "unlabeled"}, {"type": "text", "expr": " We iteratively train the Generator and Discriminator with a batch size of 64 for 200 epochs using Adam   with an initial learning rate of 0", "word_idx": 25862, "sentence_idx": 523, "label": "unlabeled"}, {"type": "text", "expr": "4  Experiments", "word_idx": 26002, "sentence_idx": 524, "label": "unlabeled"}, {"type": "text", "expr": "In this section, we evaluate our method on two publicly available datasets: ETH   and UCY  ", "word_idx": 26016, "sentence_idx": 525, "label": "unlabeled"}, {"type": "text", "expr": " These datasets consist of real world human trajectories with rich human-human interaction scenarios", "word_idx": 26107, "sentence_idx": 526, "label": "unlabeled"}, {"type": "text", "expr": " We convert all the data to real world coordinates and interpolate to obtain values at every  $04$  seconds", "word_idx": 26207, "sentence_idx": 527, "label": "unlabeled"}, {"type": "text", "expr": " In total there are  $5$  sets of data (ETH - 2, UCY-3) with  $4$  different scenes which consists of  $1536$  pedestrians in crowded settings with challenging scenarios like group behavior, people crossing each other, collision avoidance and groups forming and dispersing", "word_idx": 26314, "sentence_idx": 528, "label": "unlabeled"}, {"type": "math", "expr": "$$0.4$$", "word_idx": 26586, "sentence_idx": 529, "label": "unlabeled"}, {"type": "math", "expr": "$$1536$$", "word_idx": 26589, "sentence_idx": 530, "label": "unlabeled"}, {"type": "text", "expr": "Evaluation Metrics ", "word_idx": 26593, "sentence_idx": 531, "label": "unlabeled"}, {"type": "text", "expr": " Similar to prior work   we use two error metrics:", "word_idx": 26612, "sentence_idx": 532, "label": "unlabeled"}, {"type": "text", "expr": "Evaluation Metrics", "word_idx": 26662, "sentence_idx": 533, "label": "unlabeled"}, {"type": "text", "expr": "leftmirgin=*", "word_idx": 26680, "sentence_idx": 534, "label": "unlabeled"}, {"type": "text", "expr": "leftmirgin=*", "word_idx": 26692, "sentence_idx": 535, "label": "unlabeled"}, {"type": "text", "expr": "Average Displacement Error (ADE) : Average  $L2$  distance between ground truth and our prediction over all predicted time steps", "word_idx": 26704, "sentence_idx": 536, "label": "unlabeled"}, {"type": "text", "expr": "Average Displacement Error (ADE)", "word_idx": 26832, "sentence_idx": 537, "label": "unlabeled"}, {"type": "text", "expr": "leftmiirgiin=*", "word_idx": 26864, "sentence_idx": 538, "label": "unlabeled"}, {"type": "text", "expr": "leftmiirgiin=*", "word_idx": 26878, "sentence_idx": 539, "label": "unlabeled"}, {"type": "text", "expr": "Final Displacement Error (FDE) : The distance between the predicted final destination and the true final destination at end of the prediction period  $T_{pred}$ ", "word_idx": 26892, "sentence_idx": 540, "label": "unlabeled"}, {"type": "text", "expr": "Final Displacement Error (FDE)", "word_idx": 27053, "sentence_idx": 541, "label": "unlabeled"}, {"type": "math", "expr": "$$T_{pred}$$", "word_idx": 27083, "sentence_idx": 542, "label": "unlabeled"}, {"type": "text", "expr": "Baselines : We compare against the following baselines:", "word_idx": 27091, "sentence_idx": 543, "label": "unlabeled"}, {"type": "text", "expr": "Baselines", "word_idx": 27146, "sentence_idx": 544, "label": "unlabeled"}, {"type": "text", "expr": "leftmirgin=*", "word_idx": 27155, "sentence_idx": 545, "label": "unlabeled"}, {"type": "text", "expr": "leftmirgin=*", "word_idx": 27167, "sentence_idx": 546, "label": "unlabeled"}, {"type": "text", "expr": "Linear : A linear regressor that estimates linear parameters by minimizing the least square error", "word_idx": 27179, "sentence_idx": 547, "label": "unlabeled"}, {"type": "text", "expr": "Linear", "word_idx": 27276, "sentence_idx": 548, "label": "unlabeled"}, {"type": "text", "expr": "leftmiirgiin=*", "word_idx": 27282, "sentence_idx": 549, "label": "unlabeled"}, {"type": "text", "expr": "leftmiirgiin=*", "word_idx": 27296, "sentence_idx": 550, "label": "unlabeled"}, {"type": "text", "expr": "LSTM : A simple LSTM with no pooling mechanism", "word_idx": 27310, "sentence_idx": 551, "label": "unlabeled"}, {"type": "text", "expr": "leftmiiirgiiin=*", "word_idx": 27356, "sentence_idx": 552, "label": "unlabeled"}, {"type": "text", "expr": "leftmiiirgiiin=*", "word_idx": 27372, "sentence_idx": 553, "label": "unlabeled"}, {"type": "text", "expr": "S-LSTM : The method proposed by Alahi  et al \\onedot ", "word_idx": 27388, "sentence_idx": 554, "label": "unlabeled"}, {"type": "text", "expr": " Each person is modeled via an LSTM with the hidden states being pooled at each time step using the social pooling layer", "word_idx": 27441, "sentence_idx": 555, "label": "unlabeled"}, {"type": "text", "expr": "S-LSTM", "word_idx": 27561, "sentence_idx": 556, "label": "unlabeled"}, {"type": "text", "expr": "et al", "word_idx": 27567, "sentence_idx": 557, "label": "unlabeled"}, {"type": "text", "expr": "\\onedot", "word_idx": 27572, "sentence_idx": 558, "label": "unlabeled"}, {"type": "text", "expr": "We also do an ablation study of our model with different control settings", "word_idx": 27579, "sentence_idx": 559, "label": "unlabeled"}, {"type": "text", "expr": " We refer our full method in the section as  SGAN-kVP-N  where  $kV$  signifies if the model was trained using variety loss ( $k=1$  essentially means no variety loss) and  $P$  signifies usage of our proposed pooling module", "word_idx": 27652, "sentence_idx": 560, "label": "unlabeled"}, {"type": "text", "expr": " At test time we sample multiple times from the model and chose the best prediction in  $L2$  sense for quantitative evaluation", "word_idx": 27876, "sentence_idx": 561, "label": "unlabeled"}, {"type": "text", "expr": "  $N$  refers to the number of time we sample from our model during test time", "word_idx": 28003, "sentence_idx": 562, "label": "unlabeled"}, {"type": "text", "expr": "SGAN-kVP-N", "word_idx": 28080, "sentence_idx": 563, "label": "unlabeled"}, {"type": "math", "expr": "$$k=1$$", "word_idx": 28090, "sentence_idx": 564, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:  Effect of variety loss", "word_idx": 28093, "sentence_idx": 565, "label": "unlabeled"}, {"type": "text", "expr": " For SGAN-1V-N we train a single model, drawing one sample for each sequence during training and  $N$  samples during testing", "word_idx": 28126, "sentence_idx": 566, "label": "unlabeled"}, {"type": "text", "expr": " For SGAN-NV-N we train several models with our variety loss, using  $N$  samples during both training and testing", "word_idx": 28251, "sentence_idx": 567, "label": "unlabeled"}, {"type": "text", "expr": " Training with the variety loss significantly improves accuracy", "word_idx": 28365, "sentence_idx": 568, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:", "word_idx": 28428, "sentence_idx": 569, "label": "unlabeled"}, {"type": "text", "expr": "Evaluation Methodology ", "word_idx": 28437, "sentence_idx": 570, "label": "unlabeled"}, {"type": "text", "expr": " We follow similar evaluation methodology as  ", "word_idx": 28460, "sentence_idx": 571, "label": "unlabeled"}, {"type": "text", "expr": " We use leave-one-out approach, train on  $4$  sets and test on the remaining set", "word_idx": 28506, "sentence_idx": 572, "label": "unlabeled"}, {"type": "text", "expr": " We observe the trajectory for  $8$  times steps ( $32$  seconds) and show prediction results for  $8$  (3", "word_idx": 28587, "sentence_idx": 573, "label": "unlabeled"}, {"type": "text", "expr": "2 seconds) and  $12$  (4", "word_idx": 28693, "sentence_idx": 574, "label": "unlabeled"}, {"type": "text", "expr": "8 seconds) time steps", "word_idx": 28717, "sentence_idx": 575, "label": "unlabeled"}, {"type": "text", "expr": "Evaluation Methodology", "word_idx": 28738, "sentence_idx": 576, "label": "unlabeled"}, {"type": "math", "expr": "$$3.2$$", "word_idx": 28760, "sentence_idx": 577, "label": "unlabeled"}, {"type": "text", "expr": "1  Quantitative Evaluation", "word_idx": 28763, "sentence_idx": 578, "label": "unlabeled"}, {"type": "text", "expr": "We compare our method on two metrics ADE and FDE against different baselines in Table  1 ", "word_idx": 28789, "sentence_idx": 579, "label": "unlabeled"}, {"type": "text", "expr": " As expected Linear model is only capable of modeling straight paths and does especially bad in case of longer predictions ( $t_{pred}=12$ )", "word_idx": 28878, "sentence_idx": 580, "label": "unlabeled"}, {"type": "text", "expr": " Both LSTM and S-LSTM perform much better than the linear baseline as they can model more complex trajectories", "word_idx": 29018, "sentence_idx": 581, "label": "unlabeled"}, {"type": "text", "expr": " However, in our experiments S-LSTM does not outperform LSTM", "word_idx": 29128, "sentence_idx": 582, "label": "unlabeled"}, {"type": "text", "expr": " We tried our best to reproduce the results of the paper", "word_idx": 29188, "sentence_idx": 583, "label": "unlabeled"}, {"type": "text", "expr": "   trained the model on synthetic dataset and then fine-tuned on real datasets", "word_idx": 29244, "sentence_idx": 584, "label": "unlabeled"}, {"type": "text", "expr": " We don\u2019t use synthetic data to train any of our models which could potentially lead to worse performance", "word_idx": 29322, "sentence_idx": 585, "label": "unlabeled"}, {"type": "math", "expr": "$$t_{pred}=12$$", "word_idx": 29427, "sentence_idx": 586, "label": "unlabeled"}, {"type": "text", "expr": "SGAN-1V-1 performs worse than LSTM as each predicted sample can be any of the multiple possible future trajectories", "word_idx": 29438, "sentence_idx": 587, "label": "unlabeled"}, {"type": "text", "expr": " The conditional output generated by the model represents one of many plausible future predictions which might be different from ground truth prediction", "word_idx": 29553, "sentence_idx": 588, "label": "unlabeled"}, {"type": "text", "expr": " When we consider multiple samples our model outperforms the baseline methods confirming the multi-modal nature of the problem", "word_idx": 29705, "sentence_idx": 589, "label": "unlabeled"}, {"type": "text", "expr": " GANs face mode collapse problem, where the generator resorts to generating a handful of samples which are assigned high probability by the discriminator", "word_idx": 29831, "sentence_idx": 590, "label": "unlabeled"}, {"type": "text", "expr": " We found that samples generated by SGAN-1V-1 didn\u2019t capture all possible scenarios", "word_idx": 29984, "sentence_idx": 591, "label": "unlabeled"}, {"type": "text", "expr": " However, SGAN-20V-20 significantly outperforms all other models as the variety loss encourages the network to produce diverse samples", "word_idx": 30067, "sentence_idx": 592, "label": "unlabeled"}, {"type": "text", "expr": " Although our full model with proposed pooling layer performs slightly worse we show in the next section that pooling layer helps the model predict more \u201csocially\u201d plausible paths", "word_idx": 30201, "sentence_idx": 593, "label": "unlabeled"}, {"type": "text", "expr": "SGAN-1V-1 performs worse than LSTM as each predicted sample can be any of the multiple possible future trajectories", "word_idx": 30380, "sentence_idx": 594, "label": "unlabeled"}, {"type": "text", "expr": " The conditional output generated by the model represents one of many plausible future predictions which might be different from ground truth prediction", "word_idx": 30495, "sentence_idx": 595, "label": "unlabeled"}, {"type": "text", "expr": " When we consider multiple samples our model outperforms the baseline methods confirming the multi-modal nature of the problem", "word_idx": 30647, "sentence_idx": 596, "label": "unlabeled"}, {"type": "text", "expr": " GANs face mode collapse problem, where the generator resorts to generating a handful of samples which are assigned high probability by the discriminator", "word_idx": 30773, "sentence_idx": 597, "label": "unlabeled"}, {"type": "text", "expr": " We found that samples generated by SGAN-1V-1 didn\u2019t capture all possible scenarios", "word_idx": 30926, "sentence_idx": 598, "label": "unlabeled"}, {"type": "text", "expr": " However, SGAN-20V-20 significantly outperforms all other models as the variety loss encourages the network to produce diverse samples", "word_idx": 31009, "sentence_idx": 599, "label": "unlabeled"}, {"type": "text", "expr": " Although our full model with proposed pooling layer performs slightly worse we show in the next section that pooling layer helps the model predict more \u201csocially\u201d plausible paths", "word_idx": 31143, "sentence_idx": 600, "label": "unlabeled"}, {"type": "text", "expr": "Speed", "word_idx": 31322, "sentence_idx": 601, "label": "unlabeled"}, {"type": "text", "expr": "  Speed is crucial for a method to be used in a real-world setting like autonomous vehicles where you need accurate predictions about pedestrian behavior", "word_idx": 31327, "sentence_idx": 602, "label": "unlabeled"}, {"type": "text", "expr": " We compare our method with two baselines LSTM and S-LSTM", "word_idx": 31480, "sentence_idx": 603, "label": "unlabeled"}, {"type": "text", "expr": " A simple LSTM performs the fastest but can\u2019t avoid collisions or make accurate multi-modal predictions", "word_idx": 31537, "sentence_idx": 604, "label": "unlabeled"}, {"type": "text", "expr": " Our method is  16x  faster than S-LSTM (see Table  2 )", "word_idx": 31640, "sentence_idx": 605, "label": "unlabeled"}, {"type": "text", "expr": " Speed improvement is because we don\u2019t do pooling at each time step", "word_idx": 31695, "sentence_idx": 606, "label": "unlabeled"}, {"type": "text", "expr": " Also, unlike S-LSTM which requires computing a occupancy grid for each pedestrian our pooling mechanism is a simple MLP followed by max pooling", "word_idx": 31762, "sentence_idx": 607, "label": "unlabeled"}, {"type": "text", "expr": " In real-world applications our model can quickly generate  $20$  samples in the same time it takes S-LSTM to make  $1$  prediction", "word_idx": 31906, "sentence_idx": 608, "label": "unlabeled"}, {"type": "text", "expr": "Speed", "word_idx": 32037, "sentence_idx": 609, "label": "unlabeled"}, {"type": "text", "expr": "Evaluating Effect of Diversity", "word_idx": 32042, "sentence_idx": 610, "label": "unlabeled"}, {"type": "text", "expr": "  One might wonder what will happen if we simply draw more samples from our model without the variety loss? We compare the performance of SGAN-1V-N with SGAN-NV-N", "word_idx": 32072, "sentence_idx": 611, "label": "unlabeled"}, {"type": "text", "expr": " As a reminder SGAN-NV-N refers to a model trained with variety loss with  $k=N$  and drawing  $N$  samples during testing", "word_idx": 32234, "sentence_idx": 612, "label": "unlabeled"}, {"type": "text", "expr": " As shown in Figure  4  across all datasets simply drawing more samples from the model trained without variety loss does not lead to better accuracy", "word_idx": 32356, "sentence_idx": 613, "label": "unlabeled"}, {"type": "text", "expr": " Instead, we see a significant performance increase as we increase  $k$  with models on average performing  $33\\%$  better with  $k=100$  ", "word_idx": 32504, "sentence_idx": 614, "label": "unlabeled"}, {"type": "text", "expr": "Evaluating Effect of Diversity", "word_idx": 32642, "sentence_idx": 615, "label": "unlabeled"}, {"type": "math", "expr": "$$k=N$$", "word_idx": 32672, "sentence_idx": 616, "label": "unlabeled"}, {"type": "math", "expr": "$$33\\%$$", "word_idx": 32675, "sentence_idx": 617, "label": "unlabeled"}, {"type": "math", "expr": "$$k=100$$", "word_idx": 32679, "sentence_idx": 618, "label": "unlabeled"}, {"type": "text", "expr": "LSTM S-LSTM SGAN SGAN-P", "word_idx": 32684, "sentence_idx": 619, "label": "unlabeled"}, {"type": "text", "expr": "S-LSTM", "word_idx": 32707, "sentence_idx": 620, "label": "unlabeled"}, {"type": "text", "expr": "SGAN-P", "word_idx": 32713, "sentence_idx": 621, "label": "unlabeled"}, {"type": "text", "expr": "Speed-Up 82x 1x 49x 16x", "word_idx": 32719, "sentence_idx": 622, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:  Speed (in seconds) comparison with S-LSTM", "word_idx": 32742, "sentence_idx": 623, "label": "unlabeled"}, {"type": "text", "expr": " We get 16x speedup as compared to S-LSTM allowing us to draw 16 samples in the same time S-LSTM makes a single prediction", "word_idx": 32793, "sentence_idx": 624, "label": "unlabeled"}, {"type": "text", "expr": " Unlike S-LSTM we don\u2019t perform pooling at each time step resulting in significant speed bump without suffering on accuracy", "word_idx": 32915, "sentence_idx": 625, "label": "unlabeled"}, {"type": "text", "expr": " All methods are benchmarked on Tesla P100 GPU", "word_idx": 33038, "sentence_idx": 626, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:", "word_idx": 33084, "sentence_idx": 627, "label": "unlabeled"}, {"type": "text", "expr": "2  Qualitative Evaluation", "word_idx": 33092, "sentence_idx": 628, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a05:  Comparison between our model without pooling (SGAN, top) and with pooling (SGAN-P, bottom) in four collision avoidance scenarios: two people meeting (1), one person meeting a group (2), one person behind another (3), and two people meeting at an angle (4)", "word_idx": 33117, "sentence_idx": 629, "label": "unlabeled"}, {"type": "text", "expr": " For each example we draw 300 samples from the model and visualize their density and mean", "word_idx": 33383, "sentence_idx": 630, "label": "unlabeled"}, {"type": "text", "expr": " Due to pooling, SGAN-P predicts socially acceptable trajectories which avoid collisions", "word_idx": 33472, "sentence_idx": 631, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a05:", "word_idx": 33560, "sentence_idx": 632, "label": "unlabeled"}, {"type": "text", "expr": "In multi-agent (people) scenarios, it is imperative to model how actions of one person can influence the actions of other people", "word_idx": 33569, "sentence_idx": 633, "label": "unlabeled"}, {"type": "text", "expr": " Traditional approaches for activity forecasting and human trajectory prediction have focused on hand crafted energy potentials modeling attractive and repulsive forces to model these complex interactions", "word_idx": 33697, "sentence_idx": 634, "label": "unlabeled"}, {"type": "text", "expr": " We use a purely data driven approach which models human-human interaction via a novel pooling mechanism", "word_idx": 33901, "sentence_idx": 635, "label": "unlabeled"}, {"type": "text", "expr": " Humans walking in the presence of other people plan their path taking into account their personal space, perceived potential for collision, final destination and their own past motion", "word_idx": 34005, "sentence_idx": 636, "label": "unlabeled"}, {"type": "text", "expr": " In this section, we first evaluate the effect of the pooling layer and then analyze the predictions made by our network in three common social interaction scenarios", "word_idx": 34189, "sentence_idx": 637, "label": "unlabeled"}, {"type": "text", "expr": " Even though our model makes  joint  predictions for  all  people in a scene we show predictions for a subset for simplicity", "word_idx": 34354, "sentence_idx": 638, "label": "unlabeled"}, {"type": "text", "expr": " We refer to each person in the scene by the first letter of the color in the figure (e", "word_idx": 34478, "sentence_idx": 639, "label": "unlabeled"}, {"type": "text", "expr": ", Person B (Black), Person R (Red) and so on)", "word_idx": 34565, "sentence_idx": 640, "label": "unlabeled"}, {"type": "text", "expr": " Also for simplicity we refer SGAN-20VP-20 as SGAN-P and SGAN-20V-20 as SGAN", "word_idx": 34610, "sentence_idx": 641, "label": "unlabeled"}, {"type": "text", "expr": "joint", "word_idx": 34686, "sentence_idx": 642, "label": "unlabeled"}, {"type": "text", "expr": "1  Pooling Vs No-Pooling", "word_idx": 34691, "sentence_idx": 643, "label": "unlabeled"}, {"type": "text", "expr": "On quantitative metrics both methods perform similarly with SGAN slightly outperforming SGAN-P (see Table  1 ", "word_idx": 34715, "sentence_idx": 644, "label": "unlabeled"}, {"type": "text", "expr": " However, qualitatively we find that pooling enforces a global coherency and conformity to social norms", "word_idx": 34824, "sentence_idx": 645, "label": "unlabeled"}, {"type": "text", "expr": " We compare how SGAN and SGAN-P perform in four common social interaction scenarios (see Figure  5 )", "word_idx": 34927, "sentence_idx": 646, "label": "unlabeled"}, {"type": "text", "expr": " We would like to highlight that even though these scenarios were created synthetically, we used models trained on real world data", "word_idx": 35027, "sentence_idx": 647, "label": "unlabeled"}, {"type": "text", "expr": " Moreover, these scenarios were created to evaluate the models and nothing in our design makes these scenarios particularly easy or hard", "word_idx": 35157, "sentence_idx": 648, "label": "unlabeled"}, {"type": "text", "expr": " For each setup we draw  $300$  samples and plot an approximate distribution of trajectories along with average trajectory prediction", "word_idx": 35293, "sentence_idx": 649, "label": "unlabeled"}, {"type": "math", "expr": "$$300$$", "word_idx": 35426, "sentence_idx": 650, "label": "unlabeled"}, {"type": "text", "expr": "Scenario 1 and 2 depict the collision avoidance capacity of our model by changing direction", "word_idx": 35429, "sentence_idx": 651, "label": "unlabeled"}, {"type": "text", "expr": " In the case of two people heading in the same direction pooling enables the model to predict a socially accepted way of yielding the right of way towards the right", "word_idx": 35520, "sentence_idx": 652, "label": "unlabeled"}, {"type": "text", "expr": " However, SGAN prediction leads to a collision", "word_idx": 35684, "sentence_idx": 653, "label": "unlabeled"}, {"type": "text", "expr": " Similarly, unlike SGAN, SGAN-P is able to model group behavior and predict avoidance while preserving the notion of couple walking together (Scenario 2)", "word_idx": 35730, "sentence_idx": 654, "label": "unlabeled"}, {"type": "text", "expr": "Scenario 1 and 2 depict the collision avoidance capacity of our model by changing direction", "word_idx": 35883, "sentence_idx": 655, "label": "unlabeled"}, {"type": "text", "expr": " In the case of two people heading in the same direction pooling enables the model to predict a socially accepted way of yielding the right of way towards the right", "word_idx": 35974, "sentence_idx": 656, "label": "unlabeled"}, {"type": "text", "expr": " However, SGAN prediction leads to a collision", "word_idx": 36138, "sentence_idx": 657, "label": "unlabeled"}, {"type": "text", "expr": " Similarly, unlike SGAN, SGAN-P is able to model group behavior and predict avoidance while preserving the notion of couple walking together (Scenario 2)", "word_idx": 36184, "sentence_idx": 658, "label": "unlabeled"}, {"type": "text", "expr": "Humans also tend to vary pace to avoid collisions", "word_idx": 36337, "sentence_idx": 659, "label": "unlabeled"}, {"type": "text", "expr": " Scenario 3 is depicts a person G walking behind person B albeit faster", "word_idx": 36386, "sentence_idx": 660, "label": "unlabeled"}, {"type": "text", "expr": " If they both continue to maintain their pace and direction they would collide", "word_idx": 36457, "sentence_idx": 661, "label": "unlabeled"}, {"type": "text", "expr": " Our model predicts person G overtaking from the right", "word_idx": 36535, "sentence_idx": 662, "label": "unlabeled"}, {"type": "text", "expr": " SGAN fails to predict a socially acceptable path", "word_idx": 36589, "sentence_idx": 663, "label": "unlabeled"}, {"type": "text", "expr": " In Scenario 4, we notice that the model predicts person B slowing down and yielding for person G", "word_idx": 36638, "sentence_idx": 664, "label": "unlabeled"}, {"type": "text", "expr": "Humans also tend to vary pace to avoid collisions", "word_idx": 36735, "sentence_idx": 665, "label": "unlabeled"}, {"type": "text", "expr": " Scenario 3 is depicts a person G walking behind person B albeit faster", "word_idx": 36784, "sentence_idx": 666, "label": "unlabeled"}, {"type": "text", "expr": " If they both continue to maintain their pace and direction they would collide", "word_idx": 36855, "sentence_idx": 667, "label": "unlabeled"}, {"type": "text", "expr": " Our model predicts person G overtaking from the right", "word_idx": 36933, "sentence_idx": 668, "label": "unlabeled"}, {"type": "text", "expr": " SGAN fails to predict a socially acceptable path", "word_idx": 36987, "sentence_idx": 669, "label": "unlabeled"}, {"type": "text", "expr": " In Scenario 4, we notice that the model predicts person B slowing down and yielding for person G", "word_idx": 37036, "sentence_idx": 670, "label": "unlabeled"}, {"type": "text", "expr": "2  Pooling in Action", "word_idx": 37133, "sentence_idx": 671, "label": "unlabeled"}, {"type": "text", "expr": "People Merging", "word_idx": 37153, "sentence_idx": 672, "label": "unlabeled"}, {"type": "text", "expr": "People", "word_idx": 37167, "sentence_idx": 673, "label": "unlabeled"}, {"type": "text", "expr": "People", "word_idx": 37173, "sentence_idx": 674, "label": "unlabeled"}, {"type": "text", "expr": "Merging", "word_idx": 37179, "sentence_idx": 675, "label": "unlabeled"}, {"type": "text", "expr": "Merging", "word_idx": 37186, "sentence_idx": 676, "label": "unlabeled"}, {"type": "text", "expr": "Group Avoiding", "word_idx": 37193, "sentence_idx": 677, "label": "unlabeled"}, {"type": "text", "expr": "Group", "word_idx": 37207, "sentence_idx": 678, "label": "unlabeled"}, {"type": "text", "expr": "Group", "word_idx": 37212, "sentence_idx": 679, "label": "unlabeled"}, {"type": "text", "expr": "Avoiding", "word_idx": 37217, "sentence_idx": 680, "label": "unlabeled"}, {"type": "text", "expr": "Avoiding", "word_idx": 37225, "sentence_idx": 681, "label": "unlabeled"}, {"type": "text", "expr": "Person Following", "word_idx": 37233, "sentence_idx": 682, "label": "unlabeled"}, {"type": "text", "expr": "Person", "word_idx": 37249, "sentence_idx": 683, "label": "unlabeled"}, {"type": "text", "expr": "Person", "word_idx": 37255, "sentence_idx": 684, "label": "unlabeled"}, {"type": "text", "expr": "Following", "word_idx": 37261, "sentence_idx": 685, "label": "unlabeled"}, {"type": "text", "expr": "Following", "word_idx": 37270, "sentence_idx": 686, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a06:  \nExamples of diverse predictions from our model", "word_idx": 37279, "sentence_idx": 687, "label": "unlabeled"}, {"type": "text", "expr": " Each row shows a different set of observed trajectories; columns\nshow four different samples from our model for each scenario which demonstrate different types of socially acceptable behavior", "word_idx": 37337, "sentence_idx": 688, "label": "unlabeled"}, {"type": "text", "expr": " BEST is the sample closest to the ground-truth; in SLOW and FAST samples, people change speed\nto avoid collision; in DIR samples people change direction to avoid each other", "word_idx": 37529, "sentence_idx": 689, "label": "unlabeled"}, {"type": "text", "expr": " Our model learns these different avoidance strategies in a data-driven manner, and jointly predicts globally consistent and socially acceptable trajectories for all people in the scene", "word_idx": 37702, "sentence_idx": 690, "label": "unlabeled"}, {"type": "text", "expr": " We also show some failure cases in supplementary material", "word_idx": 37887, "sentence_idx": 691, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a06:", "word_idx": 37945, "sentence_idx": 692, "label": "unlabeled"}, {"type": "text", "expr": "We consider three real-scenarios where people have to alter their course to avoid collision (see Figure  6 )", "word_idx": 37954, "sentence_idx": 693, "label": "unlabeled"}, {"type": "text", "expr": "People Merging", "word_idx": 38062, "sentence_idx": 694, "label": "unlabeled"}, {"type": "text", "expr": "  (Row 1) In hallways or in roads it is common for people coming from different directions to merge and walk towards a common destination", "word_idx": 38076, "sentence_idx": 695, "label": "unlabeled"}, {"type": "text", "expr": " People use various ways to avoid colliding while continuing towards their destination", "word_idx": 38213, "sentence_idx": 696, "label": "unlabeled"}, {"type": "text", "expr": " For instance a person might slow down, alter their course slightly or use a combination of both depending on the context and behavior of other surrounding people", "word_idx": 38299, "sentence_idx": 697, "label": "unlabeled"}, {"type": "text", "expr": " Our model is able predict variation in both speed and direction of a person to effectively navigate a situation", "word_idx": 38461, "sentence_idx": 698, "label": "unlabeled"}, {"type": "text", "expr": " For instance model predicts that either person B slows down (col 2) or both person B and R change direction to avoid collision", "word_idx": 38573, "sentence_idx": 699, "label": "unlabeled"}, {"type": "text", "expr": " The last prediction (col 4) is particularly interesting as the model predicts a sudden turn for person R but also predicts that person B significantly slows down in response; thus making a globally consistent prediction", "word_idx": 38700, "sentence_idx": 700, "label": "unlabeled"}, {"type": "text", "expr": "People Merging", "word_idx": 38920, "sentence_idx": 701, "label": "unlabeled"}, {"type": "text", "expr": "Group Avoiding ", "word_idx": 38934, "sentence_idx": 702, "label": "unlabeled"}, {"type": "text", "expr": " (Row 2) People avoiding each other when moving in opposite direction is another common scenario", "word_idx": 38949, "sentence_idx": 703, "label": "unlabeled"}, {"type": "text", "expr": " This can manifest in various forms like a person avoiding a couple, a couple avoiding a couple etc", "word_idx": 39045, "sentence_idx": 704, "label": "unlabeled"}, {"type": "text", "expr": " To make correct predictions in such cases a person needs to plan ahead and look beyond it\u2019s immediate neighborhood", "word_idx": 39144, "sentence_idx": 705, "label": "unlabeled"}, {"type": "text", "expr": " Our model is able to recognize that the people are moving in groups and model group behavior", "word_idx": 39259, "sentence_idx": 706, "label": "unlabeled"}, {"type": "text", "expr": " The model predicts change of direction for either groups as a way of avoiding collision (col 3, 4)", "word_idx": 39352, "sentence_idx": 707, "label": "unlabeled"}, {"type": "text", "expr": " In contrast to Figure  5  even though the convention might be to give way to the right in this particular situation that would lead to a collision", "word_idx": 39451, "sentence_idx": 708, "label": "unlabeled"}, {"type": "text", "expr": " Hence, our models makes prediction where couples give way towards the left", "word_idx": 39598, "sentence_idx": 709, "label": "unlabeled"}, {"type": "text", "expr": "Group Avoiding", "word_idx": 39673, "sentence_idx": 710, "label": "unlabeled"}, {"type": "text", "expr": "Person Following ", "word_idx": 39687, "sentence_idx": 711, "label": "unlabeled"}, {"type": "text", "expr": " (Row 3) Another common scenario is when a person is walking behind someone", "word_idx": 39704, "sentence_idx": 712, "label": "unlabeled"}, {"type": "text", "expr": " One might want to either maintain pace or maybe overtake the person in front", "word_idx": 39779, "sentence_idx": 713, "label": "unlabeled"}, {"type": "text", "expr": " We would like to draw attention to a subtle difference between this situation and its real-life counterpart", "word_idx": 39856, "sentence_idx": 714, "label": "unlabeled"}, {"type": "text", "expr": " In reality a person\u2019s decision making ability is restricted by their field of view", "word_idx": 39964, "sentence_idx": 715, "label": "unlabeled"}, {"type": "text", "expr": " In contrast, our model has access to ground truth positions of all the people involved in the scene at the time of pooling", "word_idx": 40047, "sentence_idx": 716, "label": "unlabeled"}, {"type": "text", "expr": " This manifests in some interesting cases (see col 3)", "word_idx": 40170, "sentence_idx": 717, "label": "unlabeled"}, {"type": "text", "expr": " The model understands that person R is behind person B and is moving faster", "word_idx": 40223, "sentence_idx": 718, "label": "unlabeled"}, {"type": "text", "expr": " Consequently, it predicts that person B gives way by changing their direction and person R maintains their direction and speed", "word_idx": 40299, "sentence_idx": 719, "label": "unlabeled"}, {"type": "text", "expr": " The model is also able to predict overtaking (matching the ground truth)", "word_idx": 40426, "sentence_idx": 720, "label": "unlabeled"}, {"type": "text", "expr": "Person Following", "word_idx": 40499, "sentence_idx": 721, "label": "unlabeled"}, {"type": "text", "expr": "3  Structure in Latent Space", "word_idx": 40515, "sentence_idx": 722, "label": "unlabeled"}, {"type": "text", "expr": "In this experiment we attempt to understand the landscape of the latent space  $z$ ", "word_idx": 40543, "sentence_idx": 723, "label": "unlabeled"}, {"type": "text", "expr": " Walking on the manifold that is learnt can give us insights about how the model is able to generate diverse samples", "word_idx": 40626, "sentence_idx": 724, "label": "unlabeled"}, {"type": "text", "expr": " Ideally, one can expect that the network imposes some structure in the latent space", "word_idx": 40742, "sentence_idx": 725, "label": "unlabeled"}, {"type": "text", "expr": " We found that certain directions in the latent space were associated with direction and speed (Figure  7 )", "word_idx": 40826, "sentence_idx": 726, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a07:  Latent Space Exploration", "word_idx": 40933, "sentence_idx": 727, "label": "unlabeled"}, {"type": "text", "expr": " Certain directions in the latent manifold are associated with direction (left) and speed (right)", "word_idx": 40968, "sentence_idx": 728, "label": "unlabeled"}, {"type": "text", "expr": " Observing the same past but varying the input  $z$  along different directions causes the model to predict trajectories going either right/left or fast/slow on average", "word_idx": 41065, "sentence_idx": 729, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a07:", "word_idx": 41233, "sentence_idx": 730, "label": "unlabeled"}, {"type": "text", "expr": "5  Conclusion", "word_idx": 41242, "sentence_idx": 731, "label": "unlabeled"}, {"type": "text", "expr": "In this work we tackle the problem of modeling human-human interaction and jointly predicting trajectories for all people in a scene", "word_idx": 41255, "sentence_idx": 732, "label": "unlabeled"}, {"type": "text", "expr": " We propose a novel GAN based encoder-decoder framework for trajectory prediction capturing the multi-modality of the future prediction problem", "word_idx": 41387, "sentence_idx": 733, "label": "unlabeled"}, {"type": "text", "expr": " We also propose a novel pooling mechanism enabling the network to learn social norms in a purely data-driven approach", "word_idx": 41530, "sentence_idx": 734, "label": "unlabeled"}, {"type": "text", "expr": " To encourage diversity among predicted samples we propose a simple variety loss which coupled with the pooling layer encourages the network to produce globally coherent, socially compliant diverse samples", "word_idx": 41648, "sentence_idx": 735, "label": "unlabeled"}, {"type": "text", "expr": " We show the efficacy of our method on several complicated real-life scenarios where social norms must be followed", "word_idx": 41853, "sentence_idx": 736, "label": "unlabeled"}, {"type": "text", "expr": "In this work we tackle the problem of modeling human-human interaction and jointly predicting trajectories for all people in a scene", "word_idx": 41967, "sentence_idx": 737, "label": "unlabeled"}, {"type": "text", "expr": " We propose a novel GAN based encoder-decoder framework for trajectory prediction capturing the multi-modality of the future prediction problem", "word_idx": 42099, "sentence_idx": 738, "label": "unlabeled"}, {"type": "text", "expr": " We also propose a novel pooling mechanism enabling the network to learn social norms in a purely data-driven approach", "word_idx": 42242, "sentence_idx": 739, "label": "unlabeled"}, {"type": "text", "expr": " To encourage diversity among predicted samples we propose a simple variety loss which coupled with the pooling layer encourages the network to produce globally coherent, socially compliant diverse samples", "word_idx": 42360, "sentence_idx": 740, "label": "unlabeled"}, {"type": "text", "expr": " We show the efficacy of our method on several complicated real-life scenarios where social norms must be followed", "word_idx": 42565, "sentence_idx": 741, "label": "unlabeled"}, {"type": "text", "expr": "6  Acknowledgment", "word_idx": 42679, "sentence_idx": 742, "label": "unlabeled"}, {"type": "text", "expr": "We thank Jayanth Koushik and De-An Huang for their helpful comments and suggestions", "word_idx": 42696, "sentence_idx": 743, "label": "unlabeled"}, {"type": "text", "expr": "References", "word_idx": 42779, "sentence_idx": 744, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Alahi, K", "word_idx": 42789, "sentence_idx": 745, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Goel, V", "word_idx": 42798, "sentence_idx": 746, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ramanathan, A", "word_idx": 42806, "sentence_idx": 747, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Robicquet, L", "word_idx": 42820, "sentence_idx": 748, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fei-Fei, and S", "word_idx": 42833, "sentence_idx": 749, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Savarese", "word_idx": 42848, "sentence_idx": 750, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Alahi, K", "word_idx": 42857, "sentence_idx": 751, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Goel, V", "word_idx": 42866, "sentence_idx": 752, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ramanathan, A", "word_idx": 42874, "sentence_idx": 753, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Robicquet, L", "word_idx": 42888, "sentence_idx": 754, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fei-Fei, and S", "word_idx": 42901, "sentence_idx": 755, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Savarese", "word_idx": 42916, "sentence_idx": 756, "label": "unlabeled"}, {"type": "text", "expr": "Social lstm: Human trajectory prediction in crowded spaces", "word_idx": 42925, "sentence_idx": 757, "label": "unlabeled"}, {"type": "text", "expr": "Social lstm: Human trajectory prediction in crowded spaces", "word_idx": 42983, "sentence_idx": 758, "label": "unlabeled"}, {"type": "text", "expr": "In  Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition , pages 961\u2013971, 2016", "word_idx": 43041, "sentence_idx": 759, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition", "word_idx": 43144, "sentence_idx": 760, "label": "unlabeled"}, {"type": "text", "expr": ", pages 961\u2013971, 2016", "word_idx": 43221, "sentence_idx": 761, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Antonini, M", "word_idx": 43242, "sentence_idx": 762, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bierlaire, and M", "word_idx": 43254, "sentence_idx": 763, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Weber", "word_idx": 43271, "sentence_idx": 764, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Antonini, M", "word_idx": 43277, "sentence_idx": 765, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bierlaire, and M", "word_idx": 43289, "sentence_idx": 766, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Weber", "word_idx": 43306, "sentence_idx": 767, "label": "unlabeled"}, {"type": "text", "expr": "Discrete choice models of pedestrian walking behavior", "word_idx": 43312, "sentence_idx": 768, "label": "unlabeled"}, {"type": "text", "expr": "Discrete choice models of pedestrian walking behavior", "word_idx": 43365, "sentence_idx": 769, "label": "unlabeled"}, {"type": "text", "expr": "Transportation Research Part B: Methodological , 40(8):667\u2013687,\n2006", "word_idx": 43418, "sentence_idx": 770, "label": "unlabeled"}, {"type": "text", "expr": "Transportation Research Part B: Methodological", "word_idx": 43486, "sentence_idx": 771, "label": "unlabeled"}, {"type": "text", "expr": ", 40(8):667\u2013687,\n2006", "word_idx": 43532, "sentence_idx": 772, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ballan, F", "word_idx": 43553, "sentence_idx": 773, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Castaldo, A", "word_idx": 43563, "sentence_idx": 774, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Alahi, F", "word_idx": 43575, "sentence_idx": 775, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Palmieri, and S", "word_idx": 43584, "sentence_idx": 776, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Savarese", "word_idx": 43600, "sentence_idx": 777, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ballan, F", "word_idx": 43609, "sentence_idx": 778, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Castaldo, A", "word_idx": 43619, "sentence_idx": 779, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Alahi, F", "word_idx": 43631, "sentence_idx": 780, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Palmieri, and S", "word_idx": 43640, "sentence_idx": 781, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Savarese", "word_idx": 43656, "sentence_idx": 782, "label": "unlabeled"}, {"type": "text", "expr": "Knowledge transfer for scene-specific motion prediction", "word_idx": 43665, "sentence_idx": 783, "label": "unlabeled"}, {"type": "text", "expr": "Knowledge transfer for scene-specific motion prediction", "word_idx": 43720, "sentence_idx": 784, "label": "unlabeled"}, {"type": "text", "expr": "In  European Conference on Computer Vision , pages 697\u2013713", "word_idx": 43775, "sentence_idx": 785, "label": "unlabeled"}, {"type": "text", "expr": "\nSpringer, 2016", "word_idx": 43833, "sentence_idx": 786, "label": "unlabeled"}, {"type": "text", "expr": "European Conference on Computer Vision", "word_idx": 43848, "sentence_idx": 787, "label": "unlabeled"}, {"type": "text", "expr": ", pages 697\u2013713", "word_idx": 43886, "sentence_idx": 788, "label": "unlabeled"}, {"type": "text", "expr": "\nSpringer, 2016", "word_idx": 43901, "sentence_idx": 789, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bartoli, G", "word_idx": 43916, "sentence_idx": 790, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lisanti, L", "word_idx": 43927, "sentence_idx": 791, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ballan, and A", "word_idx": 43938, "sentence_idx": 792, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Del\u00a0Bimbo", "word_idx": 43952, "sentence_idx": 793, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bartoli, G", "word_idx": 43962, "sentence_idx": 794, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lisanti, L", "word_idx": 43973, "sentence_idx": 795, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ballan, and A", "word_idx": 43984, "sentence_idx": 796, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Del\u00a0Bimbo", "word_idx": 43998, "sentence_idx": 797, "label": "unlabeled"}, {"type": "text", "expr": "Context-aware trajectory prediction", "word_idx": 44008, "sentence_idx": 798, "label": "unlabeled"}, {"type": "text", "expr": "Context-aware trajectory prediction", "word_idx": 44043, "sentence_idx": 799, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1705", "word_idx": 44078, "sentence_idx": 800, "label": "unlabeled"}, {"type": "text", "expr": "02503 , 2017", "word_idx": 44103, "sentence_idx": 801, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1705", "word_idx": 44115, "sentence_idx": 802, "label": "unlabeled"}, {"type": "text", "expr": "02503", "word_idx": 44140, "sentence_idx": 803, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 44145, "sentence_idx": 804, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Choi and S", "word_idx": 44151, "sentence_idx": 805, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Savarese", "word_idx": 44162, "sentence_idx": 806, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Choi and S", "word_idx": 44171, "sentence_idx": 807, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Savarese", "word_idx": 44182, "sentence_idx": 808, "label": "unlabeled"}, {"type": "text", "expr": "A unified framework for multi-target tracking and collective activity\nrecognition", "word_idx": 44191, "sentence_idx": 809, "label": "unlabeled"}, {"type": "text", "expr": "A unified framework for multi-target tracking and collective activity\nrecognition", "word_idx": 44272, "sentence_idx": 810, "label": "unlabeled"}, {"type": "text", "expr": "In  Computer Vision\u2013ECCV 2012 , pages 215\u2013230", "word_idx": 44353, "sentence_idx": 811, "label": "unlabeled"}, {"type": "text", "expr": " Springer, 2012", "word_idx": 44398, "sentence_idx": 812, "label": "unlabeled"}, {"type": "text", "expr": "Computer Vision\u2013ECCV 2012", "word_idx": 44413, "sentence_idx": 813, "label": "unlabeled"}, {"type": "text", "expr": ", pages 215\u2013230", "word_idx": 44438, "sentence_idx": 814, "label": "unlabeled"}, {"type": "text", "expr": " Springer, 2012", "word_idx": 44453, "sentence_idx": 815, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Choi and S", "word_idx": 44468, "sentence_idx": 816, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Savarese", "word_idx": 44479, "sentence_idx": 817, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Choi and S", "word_idx": 44488, "sentence_idx": 818, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Savarese", "word_idx": 44499, "sentence_idx": 819, "label": "unlabeled"}, {"type": "text", "expr": "Understanding collective activitiesof people from videos", "word_idx": 44508, "sentence_idx": 820, "label": "unlabeled"}, {"type": "text", "expr": "Understanding collective activitiesof people from videos", "word_idx": 44564, "sentence_idx": 821, "label": "unlabeled"}, {"type": "text", "expr": "Pattern Analysis and Machine Intelligence, IEEE Transactions\non , 36(6):1242\u20131257, 2014", "word_idx": 44620, "sentence_idx": 822, "label": "unlabeled"}, {"type": "text", "expr": "Pattern Analysis and Machine Intelligence, IEEE Transactions\non", "word_idx": 44707, "sentence_idx": 823, "label": "unlabeled"}, {"type": "text", "expr": ", 36(6):1242\u20131257, 2014", "word_idx": 44770, "sentence_idx": 824, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chorowski, D", "word_idx": 44793, "sentence_idx": 825, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bahdanau, K", "word_idx": 44806, "sentence_idx": 826, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Cho, and Y", "word_idx": 44818, "sentence_idx": 827, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bengio", "word_idx": 44829, "sentence_idx": 828, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chorowski, D", "word_idx": 44836, "sentence_idx": 829, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bahdanau, K", "word_idx": 44849, "sentence_idx": 830, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Cho, and Y", "word_idx": 44861, "sentence_idx": 831, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bengio", "word_idx": 44872, "sentence_idx": 832, "label": "unlabeled"}, {"type": "text", "expr": "End-to-end continuous speech recognition using attention-based\nrecurrent nn: First results", "word_idx": 44879, "sentence_idx": 833, "label": "unlabeled"}, {"type": "text", "expr": "End-to-end continuous speech recognition using attention-based\nrecurrent nn: First results", "word_idx": 44969, "sentence_idx": 834, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1412", "word_idx": 45059, "sentence_idx": 835, "label": "unlabeled"}, {"type": "text", "expr": "1602 , 2014", "word_idx": 45084, "sentence_idx": 836, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1412", "word_idx": 45095, "sentence_idx": 837, "label": "unlabeled"}, {"type": "text", "expr": ", 2014", "word_idx": 45120, "sentence_idx": 838, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chung, K", "word_idx": 45126, "sentence_idx": 839, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kastner, L", "word_idx": 45135, "sentence_idx": 840, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dinh, K", "word_idx": 45146, "sentence_idx": 841, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Goel, A", "word_idx": 45154, "sentence_idx": 842, "label": "unlabeled"}, {"type": "text", "expr": " Courville, and Y", "word_idx": 45162, "sentence_idx": 843, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bengio", "word_idx": 45179, "sentence_idx": 844, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chung, K", "word_idx": 45186, "sentence_idx": 845, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kastner, L", "word_idx": 45195, "sentence_idx": 846, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dinh, K", "word_idx": 45206, "sentence_idx": 847, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Goel, A", "word_idx": 45214, "sentence_idx": 848, "label": "unlabeled"}, {"type": "text", "expr": " Courville, and Y", "word_idx": 45222, "sentence_idx": 849, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bengio", "word_idx": 45239, "sentence_idx": 850, "label": "unlabeled"}, {"type": "text", "expr": "A recurrent latent variable model for sequential data", "word_idx": 45246, "sentence_idx": 851, "label": "unlabeled"}, {"type": "text", "expr": "A recurrent latent variable model for sequential data", "word_idx": 45299, "sentence_idx": 852, "label": "unlabeled"}, {"type": "text", "expr": "In  Advances in neural information processing systems , pages\n2980\u20132988, 2015", "word_idx": 45352, "sentence_idx": 853, "label": "unlabeled"}, {"type": "text", "expr": "Advances in neural information processing systems", "word_idx": 45429, "sentence_idx": 854, "label": "unlabeled"}, {"type": "text", "expr": ", pages\n2980\u20132988, 2015", "word_idx": 45478, "sentence_idx": 855, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Coscia, F", "word_idx": 45501, "sentence_idx": 856, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Castaldo, F", "word_idx": 45511, "sentence_idx": 857, "label": "unlabeled"}, {"type": "text", "expr": " Palmieri, L", "word_idx": 45523, "sentence_idx": 858, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ballan, A", "word_idx": 45535, "sentence_idx": 859, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Alahi, and S", "word_idx": 45545, "sentence_idx": 860, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Savarese", "word_idx": 45558, "sentence_idx": 861, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Coscia, F", "word_idx": 45567, "sentence_idx": 862, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Castaldo, F", "word_idx": 45577, "sentence_idx": 863, "label": "unlabeled"}, {"type": "text", "expr": " Palmieri, L", "word_idx": 45589, "sentence_idx": 864, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ballan, A", "word_idx": 45601, "sentence_idx": 865, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Alahi, and S", "word_idx": 45611, "sentence_idx": 866, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Savarese", "word_idx": 45624, "sentence_idx": 867, "label": "unlabeled"}, {"type": "text", "expr": "Point-based path prediction from polar histograms", "word_idx": 45633, "sentence_idx": 868, "label": "unlabeled"}, {"type": "text", "expr": "Point-based path prediction from polar histograms", "word_idx": 45682, "sentence_idx": 869, "label": "unlabeled"}, {"type": "text", "expr": "In  Information Fusion (FUSION), 2016 19th International\nConference on , pages 1961\u20131967", "word_idx": 45731, "sentence_idx": 870, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 2016", "word_idx": 45819, "sentence_idx": 871, "label": "unlabeled"}, {"type": "text", "expr": "Information Fusion (FUSION), 2016 19th International\nConference on", "word_idx": 45830, "sentence_idx": 872, "label": "unlabeled"}, {"type": "text", "expr": ", pages 1961\u20131967", "word_idx": 45896, "sentence_idx": 873, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 2016", "word_idx": 45913, "sentence_idx": 874, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Du, W", "word_idx": 45924, "sentence_idx": 875, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wang, and L", "word_idx": 45930, "sentence_idx": 876, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wang", "word_idx": 45942, "sentence_idx": 877, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Du, W", "word_idx": 45947, "sentence_idx": 878, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wang, and L", "word_idx": 45953, "sentence_idx": 879, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wang", "word_idx": 45965, "sentence_idx": 880, "label": "unlabeled"}, {"type": "text", "expr": "Hierarchical recurrent neural network for skeleton based action\nrecognition", "word_idx": 45970, "sentence_idx": 881, "label": "unlabeled"}, {"type": "text", "expr": "Hierarchical recurrent neural network for skeleton based action\nrecognition", "word_idx": 46045, "sentence_idx": 882, "label": "unlabeled"}, {"type": "text", "expr": "In  Proceedings of the IEEE conference on computer vision and\npattern recognition , pages 1110\u20131118, 2015", "word_idx": 46120, "sentence_idx": 883, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the IEEE conference on computer vision and\npattern recognition", "word_idx": 46225, "sentence_idx": 884, "label": "unlabeled"}, {"type": "text", "expr": ", pages 1110\u20131118, 2015", "word_idx": 46302, "sentence_idx": 885, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fan, H", "word_idx": 46325, "sentence_idx": 886, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Su, and L", "word_idx": 46332, "sentence_idx": 887, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Guibas", "word_idx": 46342, "sentence_idx": 888, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fan, H", "word_idx": 46349, "sentence_idx": 889, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Su, and L", "word_idx": 46356, "sentence_idx": 890, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Guibas", "word_idx": 46366, "sentence_idx": 891, "label": "unlabeled"}, {"type": "text", "expr": "A point set generation network for 3d object reconstruction from a\nsingle image", "word_idx": 46373, "sentence_idx": 892, "label": "unlabeled"}, {"type": "text", "expr": "A point set generation network for 3d object reconstruction from a\nsingle image", "word_idx": 46452, "sentence_idx": 893, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1612", "word_idx": 46531, "sentence_idx": 894, "label": "unlabeled"}, {"type": "text", "expr": "00603 , 2016", "word_idx": 46556, "sentence_idx": 895, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1612", "word_idx": 46568, "sentence_idx": 896, "label": "unlabeled"}, {"type": "text", "expr": "00603", "word_idx": 46593, "sentence_idx": 897, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 46598, "sentence_idx": 898, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fernando, S", "word_idx": 46604, "sentence_idx": 899, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Denman, S", "word_idx": 46616, "sentence_idx": 900, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sridharan, and C", "word_idx": 46626, "sentence_idx": 901, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fookes", "word_idx": 46643, "sentence_idx": 902, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fernando, S", "word_idx": 46650, "sentence_idx": 903, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Denman, S", "word_idx": 46662, "sentence_idx": 904, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sridharan, and C", "word_idx": 46672, "sentence_idx": 905, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fookes", "word_idx": 46689, "sentence_idx": 906, "label": "unlabeled"}, {"type": "text", "expr": "Soft+ hardwired attention: An lstm framework for human trajectory\nprediction and abnormal event detection", "word_idx": 46696, "sentence_idx": 907, "label": "unlabeled"}, {"type": "text", "expr": "Soft+ hardwired attention: An lstm framework for human trajectory\nprediction and abnormal event detection", "word_idx": 46801, "sentence_idx": 908, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1702", "word_idx": 46906, "sentence_idx": 909, "label": "unlabeled"}, {"type": "text", "expr": "05552 , 2017", "word_idx": 46931, "sentence_idx": 910, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1702", "word_idx": 46943, "sentence_idx": 911, "label": "unlabeled"}, {"type": "text", "expr": "05552", "word_idx": 46968, "sentence_idx": 912, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 46973, "sentence_idx": 913, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gauthier", "word_idx": 46979, "sentence_idx": 914, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gauthier", "word_idx": 46988, "sentence_idx": 915, "label": "unlabeled"}, {"type": "text", "expr": "Conditional generative adversarial nets for convolutional face\ngeneration", "word_idx": 46997, "sentence_idx": 916, "label": "unlabeled"}, {"type": "text", "expr": "Conditional generative adversarial nets for convolutional face\ngeneration", "word_idx": 47070, "sentence_idx": 917, "label": "unlabeled"}, {"type": "text", "expr": "Class Project for Stanford CS231N: Convolutional Neural Networks\nfor Visual Recognition, Winter semester , 2014(5):2, 2014", "word_idx": 47143, "sentence_idx": 918, "label": "unlabeled"}, {"type": "text", "expr": "Class Project for Stanford CS231N: Convolutional Neural Networks\nfor Visual Recognition, Winter semester", "word_idx": 47265, "sentence_idx": 919, "label": "unlabeled"}, {"type": "text", "expr": ", 2014(5):2, 2014", "word_idx": 47369, "sentence_idx": 920, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Goodfellow, J", "word_idx": 47386, "sentence_idx": 921, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Pouget-Abadie, M", "word_idx": 47400, "sentence_idx": 922, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mirza, B", "word_idx": 47417, "sentence_idx": 923, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xu, D", "word_idx": 47426, "sentence_idx": 924, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Warde-Farley, S", "word_idx": 47432, "sentence_idx": 925, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ozair,\nA", "word_idx": 47448, "sentence_idx": 926, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Courville, and Y", "word_idx": 47457, "sentence_idx": 927, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bengio", "word_idx": 47474, "sentence_idx": 928, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Goodfellow, J", "word_idx": 47481, "sentence_idx": 929, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Pouget-Abadie, M", "word_idx": 47495, "sentence_idx": 930, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mirza, B", "word_idx": 47512, "sentence_idx": 931, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xu, D", "word_idx": 47521, "sentence_idx": 932, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Warde-Farley, S", "word_idx": 47527, "sentence_idx": 933, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ozair,\nA", "word_idx": 47543, "sentence_idx": 934, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Courville, and Y", "word_idx": 47552, "sentence_idx": 935, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bengio", "word_idx": 47569, "sentence_idx": 936, "label": "unlabeled"}, {"type": "text", "expr": "Generative adversarial nets", "word_idx": 47576, "sentence_idx": 937, "label": "unlabeled"}, {"type": "text", "expr": "Generative adversarial nets", "word_idx": 47603, "sentence_idx": 938, "label": "unlabeled"}, {"type": "text", "expr": "In  Advances in neural information processing systems , pages\n2672\u20132680, 2014", "word_idx": 47630, "sentence_idx": 939, "label": "unlabeled"}, {"type": "text", "expr": "Advances in neural information processing systems", "word_idx": 47707, "sentence_idx": 940, "label": "unlabeled"}, {"type": "text", "expr": ", pages\n2672\u20132680, 2014", "word_idx": 47756, "sentence_idx": 941, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Graves and N", "word_idx": 47779, "sentence_idx": 942, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Jaitly", "word_idx": 47792, "sentence_idx": 943, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Graves and N", "word_idx": 47799, "sentence_idx": 944, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Jaitly", "word_idx": 47812, "sentence_idx": 945, "label": "unlabeled"}, {"type": "text", "expr": "Towards end-to-end speech recognition with recurrent neural networks", "word_idx": 47819, "sentence_idx": 946, "label": "unlabeled"}, {"type": "text", "expr": "Towards end-to-end speech recognition with recurrent neural networks", "word_idx": 47887, "sentence_idx": 947, "label": "unlabeled"}, {"type": "text", "expr": "In  Proceedings of the 31st International Conference on Machine\nLearning (ICML-14) , pages 1764\u20131772, 2014", "word_idx": 47955, "sentence_idx": 948, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the 31st International Conference on Machine\nLearning (ICML-14)", "word_idx": 48061, "sentence_idx": 949, "label": "unlabeled"}, {"type": "text", "expr": ", pages 1764\u20131772, 2014", "word_idx": 48139, "sentence_idx": 950, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gregor, I", "word_idx": 48162, "sentence_idx": 951, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Danihelka, A", "word_idx": 48172, "sentence_idx": 952, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Graves, D", "word_idx": 48185, "sentence_idx": 953, "label": "unlabeled"}, {"type": "text", "expr": " Rezende, and D", "word_idx": 48195, "sentence_idx": 954, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wierstra", "word_idx": 48210, "sentence_idx": 955, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gregor, I", "word_idx": 48219, "sentence_idx": 956, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Danihelka, A", "word_idx": 48229, "sentence_idx": 957, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Graves, D", "word_idx": 48242, "sentence_idx": 958, "label": "unlabeled"}, {"type": "text", "expr": " Rezende, and D", "word_idx": 48252, "sentence_idx": 959, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wierstra", "word_idx": 48267, "sentence_idx": 960, "label": "unlabeled"}, {"type": "text", "expr": "Draw: A recurrent neural network for image generation", "word_idx": 48276, "sentence_idx": 961, "label": "unlabeled"}, {"type": "text", "expr": "Draw: A recurrent neural network for image generation", "word_idx": 48329, "sentence_idx": 962, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1502", "word_idx": 48382, "sentence_idx": 963, "label": "unlabeled"}, {"type": "text", "expr": "04623 , 2015", "word_idx": 48407, "sentence_idx": 964, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1502", "word_idx": 48419, "sentence_idx": 965, "label": "unlabeled"}, {"type": "text", "expr": "04623", "word_idx": 48444, "sentence_idx": 966, "label": "unlabeled"}, {"type": "text", "expr": ", 2015", "word_idx": 48449, "sentence_idx": 967, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Helbing and P", "word_idx": 48455, "sentence_idx": 968, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Molnar", "word_idx": 48469, "sentence_idx": 969, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Helbing and P", "word_idx": 48476, "sentence_idx": 970, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Molnar", "word_idx": 48490, "sentence_idx": 971, "label": "unlabeled"}, {"type": "text", "expr": "Social force model for pedestrian dynamics", "word_idx": 48497, "sentence_idx": 972, "label": "unlabeled"}, {"type": "text", "expr": "Social force model for pedestrian dynamics", "word_idx": 48539, "sentence_idx": 973, "label": "unlabeled"}, {"type": "text", "expr": "Physical review E , 51(5):4282, 1995", "word_idx": 48581, "sentence_idx": 974, "label": "unlabeled"}, {"type": "text", "expr": "Physical review E", "word_idx": 48617, "sentence_idx": 975, "label": "unlabeled"}, {"type": "text", "expr": ", 51(5):4282, 1995", "word_idx": 48634, "sentence_idx": 976, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hu, D", "word_idx": 48652, "sentence_idx": 977, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xie, Z", "word_idx": 48658, "sentence_idx": 978, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fu, W", "word_idx": 48665, "sentence_idx": 979, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zeng, and S", "word_idx": 48671, "sentence_idx": 980, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Maybank", "word_idx": 48683, "sentence_idx": 981, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hu, D", "word_idx": 48691, "sentence_idx": 982, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xie, Z", "word_idx": 48697, "sentence_idx": 983, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fu, W", "word_idx": 48704, "sentence_idx": 984, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zeng, and S", "word_idx": 48710, "sentence_idx": 985, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Maybank", "word_idx": 48722, "sentence_idx": 986, "label": "unlabeled"}, {"type": "text", "expr": "Semantic-based surveillance video retrieval", "word_idx": 48730, "sentence_idx": 987, "label": "unlabeled"}, {"type": "text", "expr": "Semantic-based surveillance video retrieval", "word_idx": 48773, "sentence_idx": 988, "label": "unlabeled"}, {"type": "text", "expr": "Image Processing, IEEE Transactions on , 16(4):1168\u20131181, 2007", "word_idx": 48816, "sentence_idx": 989, "label": "unlabeled"}, {"type": "text", "expr": "Image Processing, IEEE Transactions on", "word_idx": 48878, "sentence_idx": 990, "label": "unlabeled"}, {"type": "text", "expr": ", 16(4):1168\u20131181, 2007", "word_idx": 48916, "sentence_idx": 991, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Isola, J", "word_idx": 48939, "sentence_idx": 992, "label": "unlabeled"}, {"type": "text", "expr": " Zhu, T", "word_idx": 48948, "sentence_idx": 993, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhou, and A", "word_idx": 48955, "sentence_idx": 994, "label": "unlabeled"}, {"type": "text", "expr": " Efros", "word_idx": 48967, "sentence_idx": 995, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Isola, J", "word_idx": 48973, "sentence_idx": 996, "label": "unlabeled"}, {"type": "text", "expr": " Zhu, T", "word_idx": 48982, "sentence_idx": 997, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhou, and A", "word_idx": 48989, "sentence_idx": 998, "label": "unlabeled"}, {"type": "text", "expr": " Efros", "word_idx": 49001, "sentence_idx": 999, "label": "unlabeled"}, {"type": "text", "expr": "Image-to-image translation with conditional adversarial networks", "word_idx": 49007, "sentence_idx": 1000, "label": "unlabeled"}, {"type": "text", "expr": "Image-to-image translation with conditional adversarial networks", "word_idx": 49071, "sentence_idx": 1001, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1611", "word_idx": 49135, "sentence_idx": 1002, "label": "unlabeled"}, {"type": "text", "expr": "07004 , 2016", "word_idx": 49160, "sentence_idx": 1003, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1611", "word_idx": 49172, "sentence_idx": 1004, "label": "unlabeled"}, {"type": "text", "expr": "07004", "word_idx": 49197, "sentence_idx": 1005, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 49202, "sentence_idx": 1006, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Karpathy, A", "word_idx": 49208, "sentence_idx": 1007, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Joulin, and F", "word_idx": 49220, "sentence_idx": 1008, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Karpathy, A", "word_idx": 49234, "sentence_idx": 1009, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Joulin, and F", "word_idx": 49246, "sentence_idx": 1010, "label": "unlabeled"}, {"type": "text", "expr": "Deep fragment embeddings for bidirectional image sentence mapping", "word_idx": 49260, "sentence_idx": 1011, "label": "unlabeled"}, {"type": "text", "expr": "Deep fragment embeddings for bidirectional image sentence mapping", "word_idx": 49325, "sentence_idx": 1012, "label": "unlabeled"}, {"type": "text", "expr": "In  Advances in neural information processing systems , pages\n1889\u20131897, 2014", "word_idx": 49390, "sentence_idx": 1013, "label": "unlabeled"}, {"type": "text", "expr": "Advances in neural information processing systems", "word_idx": 49467, "sentence_idx": 1014, "label": "unlabeled"}, {"type": "text", "expr": ", pages\n1889\u20131897, 2014", "word_idx": 49516, "sentence_idx": 1015, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kim, D", "word_idx": 49539, "sentence_idx": 1016, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lee, and I", "word_idx": 49546, "sentence_idx": 1017, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Essa", "word_idx": 49557, "sentence_idx": 1018, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kim, D", "word_idx": 49562, "sentence_idx": 1019, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lee, and I", "word_idx": 49569, "sentence_idx": 1020, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Essa", "word_idx": 49580, "sentence_idx": 1021, "label": "unlabeled"}, {"type": "text", "expr": "Gaussian process regression flow for analysis of motion trajectories", "word_idx": 49585, "sentence_idx": 1022, "label": "unlabeled"}, {"type": "text", "expr": "Gaussian process regression flow for analysis of motion trajectories", "word_idx": 49653, "sentence_idx": 1023, "label": "unlabeled"}, {"type": "text", "expr": "In  Computer Vision (ICCV), 2011 IEEE International Conference\non , pages 1164\u20131171", "word_idx": 49721, "sentence_idx": 1024, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 2011", "word_idx": 49804, "sentence_idx": 1025, "label": "unlabeled"}, {"type": "text", "expr": "Computer Vision (ICCV), 2011 IEEE International Conference\non", "word_idx": 49815, "sentence_idx": 1026, "label": "unlabeled"}, {"type": "text", "expr": ", pages 1164\u20131171", "word_idx": 49876, "sentence_idx": 1027, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 2011", "word_idx": 49893, "sentence_idx": 1028, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kingma and J", "word_idx": 49904, "sentence_idx": 1029, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kingma and J", "word_idx": 49917, "sentence_idx": 1030, "label": "unlabeled"}, {"type": "text", "expr": "Adam: A method for stochastic optimization", "word_idx": 49930, "sentence_idx": 1031, "label": "unlabeled"}, {"type": "text", "expr": "Adam: A method for stochastic optimization", "word_idx": 49972, "sentence_idx": 1032, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1412", "word_idx": 50014, "sentence_idx": 1033, "label": "unlabeled"}, {"type": "text", "expr": "6980 , 2014", "word_idx": 50039, "sentence_idx": 1034, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1412", "word_idx": 50050, "sentence_idx": 1035, "label": "unlabeled"}, {"type": "text", "expr": ", 2014", "word_idx": 50075, "sentence_idx": 1036, "label": "unlabeled"}, {"type": "text", "expr": " Kingma and M", "word_idx": 50081, "sentence_idx": 1037, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Welling", "word_idx": 50094, "sentence_idx": 1038, "label": "unlabeled"}, {"type": "text", "expr": " Kingma and M", "word_idx": 50102, "sentence_idx": 1039, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Welling", "word_idx": 50115, "sentence_idx": 1040, "label": "unlabeled"}, {"type": "text", "expr": "Auto-encoding variational bayes", "word_idx": 50123, "sentence_idx": 1041, "label": "unlabeled"}, {"type": "text", "expr": "Auto-encoding variational bayes", "word_idx": 50154, "sentence_idx": 1042, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1312", "word_idx": 50185, "sentence_idx": 1043, "label": "unlabeled"}, {"type": "text", "expr": "6114 , 2013", "word_idx": 50210, "sentence_idx": 1044, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1312", "word_idx": 50221, "sentence_idx": 1045, "label": "unlabeled"}, {"type": "text", "expr": ", 2013", "word_idx": 50246, "sentence_idx": 1046, "label": "unlabeled"}, {"type": "text", "expr": " Kitani, B", "word_idx": 50252, "sentence_idx": 1047, "label": "unlabeled"}, {"type": "text", "expr": " Ziebart, J", "word_idx": 50262, "sentence_idx": 1048, "label": "unlabeled"}, {"type": "text", "expr": " Bagnell, and M", "word_idx": 50273, "sentence_idx": 1049, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hebert", "word_idx": 50288, "sentence_idx": 1050, "label": "unlabeled"}, {"type": "text", "expr": " Kitani, B", "word_idx": 50295, "sentence_idx": 1051, "label": "unlabeled"}, {"type": "text", "expr": " Ziebart, J", "word_idx": 50305, "sentence_idx": 1052, "label": "unlabeled"}, {"type": "text", "expr": " Bagnell, and M", "word_idx": 50316, "sentence_idx": 1053, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hebert", "word_idx": 50331, "sentence_idx": 1054, "label": "unlabeled"}, {"type": "text", "expr": "Activity forecasting", "word_idx": 50338, "sentence_idx": 1055, "label": "unlabeled"}, {"type": "text", "expr": "Activity forecasting", "word_idx": 50358, "sentence_idx": 1056, "label": "unlabeled"}, {"type": "text", "expr": "In  Computer Vision\u2013ECCV 2012 , pages 201\u2013214", "word_idx": 50378, "sentence_idx": 1057, "label": "unlabeled"}, {"type": "text", "expr": " Springer, 2012", "word_idx": 50423, "sentence_idx": 1058, "label": "unlabeled"}, {"type": "text", "expr": "Computer Vision\u2013ECCV 2012", "word_idx": 50438, "sentence_idx": 1059, "label": "unlabeled"}, {"type": "text", "expr": ", pages 201\u2013214", "word_idx": 50463, "sentence_idx": 1060, "label": "unlabeled"}, {"type": "text", "expr": " Springer, 2012", "word_idx": 50478, "sentence_idx": 1061, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Leal-Taix\u00e9, M", "word_idx": 50493, "sentence_idx": 1062, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fenzi, A", "word_idx": 50507, "sentence_idx": 1063, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kuznetsova, B", "word_idx": 50516, "sentence_idx": 1064, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rosenhahn, and S", "word_idx": 50530, "sentence_idx": 1065, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Savarese", "word_idx": 50547, "sentence_idx": 1066, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Leal-Taix\u00e9, M", "word_idx": 50556, "sentence_idx": 1067, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fenzi, A", "word_idx": 50570, "sentence_idx": 1068, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kuznetsova, B", "word_idx": 50579, "sentence_idx": 1069, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rosenhahn, and S", "word_idx": 50593, "sentence_idx": 1070, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Savarese", "word_idx": 50610, "sentence_idx": 1071, "label": "unlabeled"}, {"type": "text", "expr": "Learning an image-based motion context for multiple people tracking", "word_idx": 50619, "sentence_idx": 1072, "label": "unlabeled"}, {"type": "text", "expr": "Learning an image-based motion context for multiple people tracking", "word_idx": 50686, "sentence_idx": 1073, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , pages 3542\u20133549", "word_idx": 50753, "sentence_idx": 1074, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 2014", "word_idx": 50779, "sentence_idx": 1075, "label": "unlabeled"}, {"type": "text", "expr": ", pages 3542\u20133549", "word_idx": 50790, "sentence_idx": 1076, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 2014", "word_idx": 50807, "sentence_idx": 1077, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Leal-Taixe, G", "word_idx": 50818, "sentence_idx": 1078, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Pons-Moll, and B", "word_idx": 50832, "sentence_idx": 1079, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rosenhahn", "word_idx": 50849, "sentence_idx": 1080, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Leal-Taixe, G", "word_idx": 50859, "sentence_idx": 1081, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Pons-Moll, and B", "word_idx": 50873, "sentence_idx": 1082, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rosenhahn", "word_idx": 50890, "sentence_idx": 1083, "label": "unlabeled"}, {"type": "text", "expr": "Everybody needs somebody: Modeling social and grouping behavior on a\n linear programming multiple people tracker", "word_idx": 50900, "sentence_idx": 1084, "label": "unlabeled"}, {"type": "text", "expr": "Everybody needs somebody: Modeling social and grouping behavior on a", "word_idx": 51012, "sentence_idx": 1085, "label": "unlabeled"}, {"type": "text", "expr": "linear programming multiple people tracker", "word_idx": 51080, "sentence_idx": 1086, "label": "unlabeled"}, {"type": "text", "expr": "In  ICCV Workshops , 2011", "word_idx": 51122, "sentence_idx": 1087, "label": "unlabeled"}, {"type": "text", "expr": "ICCV Workshops", "word_idx": 51147, "sentence_idx": 1088, "label": "unlabeled"}, {"type": "text", "expr": ", 2011", "word_idx": 51161, "sentence_idx": 1089, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ledig, L", "word_idx": 51167, "sentence_idx": 1090, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Theis, F", "word_idx": 51176, "sentence_idx": 1091, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Husz\u00e1r, J", "word_idx": 51185, "sentence_idx": 1092, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Caballero, A", "word_idx": 51195, "sentence_idx": 1093, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Cunningham, A", "word_idx": 51208, "sentence_idx": 1094, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Acosta,\nA", "word_idx": 51222, "sentence_idx": 1095, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Aitken, A", "word_idx": 51232, "sentence_idx": 1096, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Tejani, J", "word_idx": 51242, "sentence_idx": 1097, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Totz, Z", "word_idx": 51252, "sentence_idx": 1098, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wang, et\u00a0al", "word_idx": 51260, "sentence_idx": 1099, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ledig, L", "word_idx": 51272, "sentence_idx": 1100, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Theis, F", "word_idx": 51281, "sentence_idx": 1101, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Husz\u00e1r, J", "word_idx": 51290, "sentence_idx": 1102, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Caballero, A", "word_idx": 51300, "sentence_idx": 1103, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Cunningham, A", "word_idx": 51313, "sentence_idx": 1104, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Acosta,\nA", "word_idx": 51327, "sentence_idx": 1105, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Aitken, A", "word_idx": 51337, "sentence_idx": 1106, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Tejani, J", "word_idx": 51347, "sentence_idx": 1107, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Totz, Z", "word_idx": 51357, "sentence_idx": 1108, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wang, et\u00a0al", "word_idx": 51365, "sentence_idx": 1109, "label": "unlabeled"}, {"type": "text", "expr": "Photo-realistic single image super-resolution using a generative\nadversarial network", "word_idx": 51377, "sentence_idx": 1110, "label": "unlabeled"}, {"type": "text", "expr": "Photo-realistic single image super-resolution using a generative\nadversarial network", "word_idx": 51461, "sentence_idx": 1111, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1609", "word_idx": 51545, "sentence_idx": 1112, "label": "unlabeled"}, {"type": "text", "expr": "04802 , 2016", "word_idx": 51570, "sentence_idx": 1113, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1609", "word_idx": 51582, "sentence_idx": 1114, "label": "unlabeled"}, {"type": "text", "expr": "04802", "word_idx": 51607, "sentence_idx": 1115, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 51612, "sentence_idx": 1116, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lee, W", "word_idx": 51618, "sentence_idx": 1117, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Choi, P", "word_idx": 51625, "sentence_idx": 1118, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Vernaza, C", "word_idx": 51633, "sentence_idx": 1119, "label": "unlabeled"}, {"type": "text", "expr": " Choy, P", "word_idx": 51644, "sentence_idx": 1120, "label": "unlabeled"}, {"type": "text", "expr": " Torr, and M", "word_idx": 51652, "sentence_idx": 1121, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chandraker", "word_idx": 51664, "sentence_idx": 1122, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lee, W", "word_idx": 51675, "sentence_idx": 1123, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Choi, P", "word_idx": 51682, "sentence_idx": 1124, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Vernaza, C", "word_idx": 51690, "sentence_idx": 1125, "label": "unlabeled"}, {"type": "text", "expr": " Choy, P", "word_idx": 51701, "sentence_idx": 1126, "label": "unlabeled"}, {"type": "text", "expr": " Torr, and M", "word_idx": 51709, "sentence_idx": 1127, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chandraker", "word_idx": 51721, "sentence_idx": 1128, "label": "unlabeled"}, {"type": "text", "expr": "Desire: Distant future prediction in dynamic scenes with interacting\nagents", "word_idx": 51732, "sentence_idx": 1129, "label": "unlabeled"}, {"type": "text", "expr": "Desire: Distant future prediction in dynamic scenes with interacting\nagents", "word_idx": 51807, "sentence_idx": 1130, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1704", "word_idx": 51882, "sentence_idx": 1131, "label": "unlabeled"}, {"type": "text", "expr": "04394 , 2017", "word_idx": 51907, "sentence_idx": 1132, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1704", "word_idx": 51919, "sentence_idx": 1133, "label": "unlabeled"}, {"type": "text", "expr": "04394", "word_idx": 51944, "sentence_idx": 1134, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 51949, "sentence_idx": 1135, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Liu, A", "word_idx": 51955, "sentence_idx": 1136, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Shahroudy, D", "word_idx": 51962, "sentence_idx": 1137, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xu, and G", "word_idx": 51975, "sentence_idx": 1138, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wang", "word_idx": 51985, "sentence_idx": 1139, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Liu, A", "word_idx": 51990, "sentence_idx": 1140, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Shahroudy, D", "word_idx": 51997, "sentence_idx": 1141, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xu, and G", "word_idx": 52010, "sentence_idx": 1142, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wang", "word_idx": 52020, "sentence_idx": 1143, "label": "unlabeled"}, {"type": "text", "expr": "Spatio-temporal lstm with trust gates for 3d human action\nrecognition", "word_idx": 52025, "sentence_idx": 1144, "label": "unlabeled"}, {"type": "text", "expr": "Spatio-temporal lstm with trust gates for 3d human action\nrecognition", "word_idx": 52094, "sentence_idx": 1145, "label": "unlabeled"}, {"type": "text", "expr": "In  European Conference on Computer Vision , pages 816\u2013833", "word_idx": 52163, "sentence_idx": 1146, "label": "unlabeled"}, {"type": "text", "expr": "\nSpringer, 2016", "word_idx": 52221, "sentence_idx": 1147, "label": "unlabeled"}, {"type": "text", "expr": "European Conference on Computer Vision", "word_idx": 52236, "sentence_idx": 1148, "label": "unlabeled"}, {"type": "text", "expr": ", pages 816\u2013833", "word_idx": 52274, "sentence_idx": 1149, "label": "unlabeled"}, {"type": "text", "expr": "\nSpringer, 2016", "word_idx": 52289, "sentence_idx": 1150, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Luber, J", "word_idx": 52304, "sentence_idx": 1151, "label": "unlabeled"}, {"type": "text", "expr": " Stork, G", "word_idx": 52313, "sentence_idx": 1152, "label": "unlabeled"}, {"type": "text", "expr": " Tipaldi, and K", "word_idx": 52322, "sentence_idx": 1153, "label": "unlabeled"}, {"type": "text", "expr": " Arras", "word_idx": 52337, "sentence_idx": 1154, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Luber, J", "word_idx": 52343, "sentence_idx": 1155, "label": "unlabeled"}, {"type": "text", "expr": " Stork, G", "word_idx": 52352, "sentence_idx": 1156, "label": "unlabeled"}, {"type": "text", "expr": " Tipaldi, and K", "word_idx": 52361, "sentence_idx": 1157, "label": "unlabeled"}, {"type": "text", "expr": " Arras", "word_idx": 52376, "sentence_idx": 1158, "label": "unlabeled"}, {"type": "text", "expr": "People tracking with human motion predictions from social forces", "word_idx": 52382, "sentence_idx": 1159, "label": "unlabeled"}, {"type": "text", "expr": "People tracking with human motion predictions from social forces", "word_idx": 52446, "sentence_idx": 1160, "label": "unlabeled"}, {"type": "text", "expr": "In  Robotics and Automation (ICRA), 2010 IEEE International\nConference on , pages 464\u2013469", "word_idx": 52510, "sentence_idx": 1161, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 2010", "word_idx": 52599, "sentence_idx": 1162, "label": "unlabeled"}, {"type": "text", "expr": "Robotics and Automation (ICRA), 2010 IEEE International\nConference on", "word_idx": 52610, "sentence_idx": 1163, "label": "unlabeled"}, {"type": "text", "expr": ", pages 464\u2013469", "word_idx": 52679, "sentence_idx": 1164, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 2010", "word_idx": 52694, "sentence_idx": 1165, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mehran, A", "word_idx": 52705, "sentence_idx": 1166, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Oyama, and M", "word_idx": 52715, "sentence_idx": 1167, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Shah", "word_idx": 52728, "sentence_idx": 1168, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mehran, A", "word_idx": 52733, "sentence_idx": 1169, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Oyama, and M", "word_idx": 52743, "sentence_idx": 1170, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Shah", "word_idx": 52756, "sentence_idx": 1171, "label": "unlabeled"}, {"type": "text", "expr": "Abnormal crowd behavior detection using social force model", "word_idx": 52761, "sentence_idx": 1172, "label": "unlabeled"}, {"type": "text", "expr": "Abnormal crowd behavior detection using social force model", "word_idx": 52819, "sentence_idx": 1173, "label": "unlabeled"}, {"type": "text", "expr": "In  Computer Vision and Pattern Recognition, 2009", "word_idx": 52877, "sentence_idx": 1174, "label": "unlabeled"}, {"type": "text", "expr": " CVPR 2009", "word_idx": 52926, "sentence_idx": 1175, "label": "unlabeled"}, {"type": "text", "expr": "\nIEEE Conference on , pages 935\u2013942", "word_idx": 52936, "sentence_idx": 1176, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 2009", "word_idx": 52971, "sentence_idx": 1177, "label": "unlabeled"}, {"type": "text", "expr": "Computer Vision and Pattern Recognition, 2009", "word_idx": 52982, "sentence_idx": 1178, "label": "unlabeled"}, {"type": "text", "expr": " CVPR 2009", "word_idx": 53027, "sentence_idx": 1179, "label": "unlabeled"}, {"type": "text", "expr": "\nIEEE Conference on", "word_idx": 53037, "sentence_idx": 1180, "label": "unlabeled"}, {"type": "text", "expr": ", pages 935\u2013942", "word_idx": 53056, "sentence_idx": 1181, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 2009", "word_idx": 53071, "sentence_idx": 1182, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mirza and S", "word_idx": 53082, "sentence_idx": 1183, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Osindero", "word_idx": 53094, "sentence_idx": 1184, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mirza and S", "word_idx": 53103, "sentence_idx": 1185, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Osindero", "word_idx": 53115, "sentence_idx": 1186, "label": "unlabeled"}, {"type": "text", "expr": "Conditional generative adversarial nets", "word_idx": 53124, "sentence_idx": 1187, "label": "unlabeled"}, {"type": "text", "expr": "Conditional generative adversarial nets", "word_idx": 53163, "sentence_idx": 1188, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1411", "word_idx": 53202, "sentence_idx": 1189, "label": "unlabeled"}, {"type": "text", "expr": "1784 , 2014", "word_idx": 53227, "sentence_idx": 1190, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1411", "word_idx": 53238, "sentence_idx": 1191, "label": "unlabeled"}, {"type": "text", "expr": ", 2014", "word_idx": 53263, "sentence_idx": 1192, "label": "unlabeled"}, {"type": "text", "expr": " Morris and M", "word_idx": 53269, "sentence_idx": 1193, "label": "unlabeled"}, {"type": "text", "expr": " Trivedi", "word_idx": 53282, "sentence_idx": 1194, "label": "unlabeled"}, {"type": "text", "expr": " Morris and M", "word_idx": 53290, "sentence_idx": 1195, "label": "unlabeled"}, {"type": "text", "expr": " Trivedi", "word_idx": 53303, "sentence_idx": 1196, "label": "unlabeled"}, {"type": "text", "expr": "Trajectory learning for activity understanding: Unsupervised,\nmultilevel, and long-term adaptive approach", "word_idx": 53311, "sentence_idx": 1197, "label": "unlabeled"}, {"type": "text", "expr": "Trajectory learning for activity understanding: Unsupervised,\nmultilevel, and long-term adaptive approach", "word_idx": 53416, "sentence_idx": 1198, "label": "unlabeled"}, {"type": "text", "expr": "Pattern Analysis and Machine Intelligence, IEEE Transactions\non , 33(11):2287\u20132301, 2011", "word_idx": 53521, "sentence_idx": 1199, "label": "unlabeled"}, {"type": "text", "expr": "Pattern Analysis and Machine Intelligence, IEEE Transactions\non", "word_idx": 53609, "sentence_idx": 1200, "label": "unlabeled"}, {"type": "text", "expr": ", 33(11):2287\u20132301, 2011", "word_idx": 53672, "sentence_idx": 1201, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Odena, C", "word_idx": 53696, "sentence_idx": 1202, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Olah, and J", "word_idx": 53705, "sentence_idx": 1203, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Shlens", "word_idx": 53717, "sentence_idx": 1204, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Odena, C", "word_idx": 53724, "sentence_idx": 1205, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Olah, and J", "word_idx": 53733, "sentence_idx": 1206, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Shlens", "word_idx": 53745, "sentence_idx": 1207, "label": "unlabeled"}, {"type": "text", "expr": "Conditional image synthesis with auxiliary classifier gans", "word_idx": 53752, "sentence_idx": 1208, "label": "unlabeled"}, {"type": "text", "expr": "Conditional image synthesis with auxiliary classifier gans", "word_idx": 53810, "sentence_idx": 1209, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1610", "word_idx": 53868, "sentence_idx": 1210, "label": "unlabeled"}, {"type": "text", "expr": "09585 , 2016", "word_idx": 53893, "sentence_idx": 1211, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1610", "word_idx": 53905, "sentence_idx": 1212, "label": "unlabeled"}, {"type": "text", "expr": "09585", "word_idx": 53930, "sentence_idx": 1213, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 53935, "sentence_idx": 1214, "label": "unlabeled"}, {"type": "text", "expr": " Park and J", "word_idx": 53941, "sentence_idx": 1215, "label": "unlabeled"}, {"type": "text", "expr": " Park and J", "word_idx": 53952, "sentence_idx": 1216, "label": "unlabeled"}, {"type": "text", "expr": "Social saliency prediction", "word_idx": 53963, "sentence_idx": 1217, "label": "unlabeled"}, {"type": "text", "expr": "Social saliency prediction", "word_idx": 53989, "sentence_idx": 1218, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Pellegrini, A", "word_idx": 54015, "sentence_idx": 1219, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ess, and L", "word_idx": 54029, "sentence_idx": 1220, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Van\u00a0Gool", "word_idx": 54040, "sentence_idx": 1221, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Pellegrini, A", "word_idx": 54049, "sentence_idx": 1222, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ess, and L", "word_idx": 54063, "sentence_idx": 1223, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Van\u00a0Gool", "word_idx": 54074, "sentence_idx": 1224, "label": "unlabeled"}, {"type": "text", "expr": "Improving data association by joint modeling of pedestrian\ntrajectories and groupings", "word_idx": 54083, "sentence_idx": 1225, "label": "unlabeled"}, {"type": "text", "expr": "Improving data association by joint modeling of pedestrian\ntrajectories and groupings", "word_idx": 54168, "sentence_idx": 1226, "label": "unlabeled"}, {"type": "text", "expr": "In  Computer Vision\u2013ECCV 2010 , pages 452\u2013465", "word_idx": 54253, "sentence_idx": 1227, "label": "unlabeled"}, {"type": "text", "expr": " Springer, 2010", "word_idx": 54298, "sentence_idx": 1228, "label": "unlabeled"}, {"type": "text", "expr": "Computer Vision\u2013ECCV 2010", "word_idx": 54313, "sentence_idx": 1229, "label": "unlabeled"}, {"type": "text", "expr": ", pages 452\u2013465", "word_idx": 54338, "sentence_idx": 1230, "label": "unlabeled"}, {"type": "text", "expr": " Springer, 2010", "word_idx": 54353, "sentence_idx": 1231, "label": "unlabeled"}, {"type": "text", "expr": " Qi, H", "word_idx": 54368, "sentence_idx": 1232, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Su, K", "word_idx": 54374, "sentence_idx": 1233, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mo, and L", "word_idx": 54380, "sentence_idx": 1234, "label": "unlabeled"}, {"type": "text", "expr": " Guibas", "word_idx": 54390, "sentence_idx": 1235, "label": "unlabeled"}, {"type": "text", "expr": " Qi, H", "word_idx": 54397, "sentence_idx": 1236, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Su, K", "word_idx": 54403, "sentence_idx": 1237, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mo, and L", "word_idx": 54409, "sentence_idx": 1238, "label": "unlabeled"}, {"type": "text", "expr": " Guibas", "word_idx": 54419, "sentence_idx": 1239, "label": "unlabeled"}, {"type": "text", "expr": "Pointnet: Deep learning on point sets for 3d classification and\nsegmentation", "word_idx": 54426, "sentence_idx": 1240, "label": "unlabeled"}, {"type": "text", "expr": "Pointnet: Deep learning on point sets for 3d classification and\nsegmentation", "word_idx": 54502, "sentence_idx": 1241, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1612", "word_idx": 54578, "sentence_idx": 1242, "label": "unlabeled"}, {"type": "text", "expr": "00593 , 2016", "word_idx": 54603, "sentence_idx": 1243, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1612", "word_idx": 54615, "sentence_idx": 1244, "label": "unlabeled"}, {"type": "text", "expr": "00593", "word_idx": 54640, "sentence_idx": 1245, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 54645, "sentence_idx": 1246, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Robicquet, A", "word_idx": 54651, "sentence_idx": 1247, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sadeghian, A", "word_idx": 54664, "sentence_idx": 1248, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Alahi, and S", "word_idx": 54677, "sentence_idx": 1249, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Savarese", "word_idx": 54690, "sentence_idx": 1250, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Robicquet, A", "word_idx": 54699, "sentence_idx": 1251, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sadeghian, A", "word_idx": 54712, "sentence_idx": 1252, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Alahi, and S", "word_idx": 54725, "sentence_idx": 1253, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Savarese", "word_idx": 54738, "sentence_idx": 1254, "label": "unlabeled"}, {"type": "text", "expr": "Learning social etiquette: Human trajectory understanding in crowded\nscenes", "word_idx": 54747, "sentence_idx": 1255, "label": "unlabeled"}, {"type": "text", "expr": "Learning social etiquette: Human trajectory understanding in crowded\nscenes", "word_idx": 54822, "sentence_idx": 1256, "label": "unlabeled"}, {"type": "text", "expr": "In  European conference on computer vision , pages 549\u2013565", "word_idx": 54897, "sentence_idx": 1257, "label": "unlabeled"}, {"type": "text", "expr": "\nSpringer, 2016", "word_idx": 54955, "sentence_idx": 1258, "label": "unlabeled"}, {"type": "text", "expr": "European conference on computer vision", "word_idx": 54970, "sentence_idx": 1259, "label": "unlabeled"}, {"type": "text", "expr": ", pages 549\u2013565", "word_idx": 55008, "sentence_idx": 1260, "label": "unlabeled"}, {"type": "text", "expr": "\nSpringer, 2016", "word_idx": 55023, "sentence_idx": 1261, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Shu, S", "word_idx": 55038, "sentence_idx": 1262, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Todorovic, and S", "word_idx": 55045, "sentence_idx": 1263, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Shu, S", "word_idx": 55062, "sentence_idx": 1264, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Todorovic, and S", "word_idx": 55069, "sentence_idx": 1265, "label": "unlabeled"}, {"type": "text", "expr": "Cern: confidence-energy recurrent network for group activity\nrecognition", "word_idx": 55086, "sentence_idx": 1266, "label": "unlabeled"}, {"type": "text", "expr": "Cern: confidence-energy recurrent network for group activity\nrecognition", "word_idx": 55158, "sentence_idx": 1267, "label": "unlabeled"}, {"type": "text", "expr": " of CVPR, Honolulu, Hawaii , 2017", "word_idx": 55230, "sentence_idx": 1268, "label": "unlabeled"}, {"type": "text", "expr": " of CVPR, Honolulu, Hawaii", "word_idx": 55263, "sentence_idx": 1269, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 55289, "sentence_idx": 1270, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Srivastava, E", "word_idx": 55295, "sentence_idx": 1271, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mansimov, and R", "word_idx": 55309, "sentence_idx": 1272, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Salakhudinov", "word_idx": 55325, "sentence_idx": 1273, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Srivastava, E", "word_idx": 55338, "sentence_idx": 1274, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mansimov, and R", "word_idx": 55352, "sentence_idx": 1275, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Salakhudinov", "word_idx": 55368, "sentence_idx": 1276, "label": "unlabeled"}, {"type": "text", "expr": "Unsupervised learning of video representations using lstms", "word_idx": 55381, "sentence_idx": 1277, "label": "unlabeled"}, {"type": "text", "expr": "Unsupervised learning of video representations using lstms", "word_idx": 55439, "sentence_idx": 1278, "label": "unlabeled"}, {"type": "text", "expr": "In  International Conference on Machine Learning , pages\n843\u2013852, 2015", "word_idx": 55497, "sentence_idx": 1279, "label": "unlabeled"}, {"type": "text", "expr": "International Conference on Machine Learning", "word_idx": 55567, "sentence_idx": 1280, "label": "unlabeled"}, {"type": "text", "expr": ", pages\n843\u2013852, 2015", "word_idx": 55611, "sentence_idx": 1281, "label": "unlabeled"}, {"type": "text", "expr": " Tay and C", "word_idx": 55632, "sentence_idx": 1282, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Laugier", "word_idx": 55642, "sentence_idx": 1283, "label": "unlabeled"}, {"type": "text", "expr": " Tay and C", "word_idx": 55650, "sentence_idx": 1284, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Laugier", "word_idx": 55660, "sentence_idx": 1285, "label": "unlabeled"}, {"type": "text", "expr": "Modelling smooth paths using gaussian processes", "word_idx": 55668, "sentence_idx": 1286, "label": "unlabeled"}, {"type": "text", "expr": "Modelling smooth paths using gaussian processes", "word_idx": 55715, "sentence_idx": 1287, "label": "unlabeled"}, {"type": "text", "expr": "In  Field and Service Robotics , pages 381\u2013390", "word_idx": 55762, "sentence_idx": 1288, "label": "unlabeled"}, {"type": "text", "expr": " Springer, 2008", "word_idx": 55808, "sentence_idx": 1289, "label": "unlabeled"}, {"type": "text", "expr": "Field and Service Robotics", "word_idx": 55823, "sentence_idx": 1290, "label": "unlabeled"}, {"type": "text", "expr": ", pages 381\u2013390", "word_idx": 55849, "sentence_idx": 1291, "label": "unlabeled"}, {"type": "text", "expr": " Springer, 2008", "word_idx": 55864, "sentence_idx": 1292, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Treuille, S", "word_idx": 55879, "sentence_idx": 1293, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Cooper, and Z", "word_idx": 55891, "sentence_idx": 1294, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Popovi\u0107", "word_idx": 55905, "sentence_idx": 1295, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Treuille, S", "word_idx": 55913, "sentence_idx": 1296, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Cooper, and Z", "word_idx": 55925, "sentence_idx": 1297, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Popovi\u0107", "word_idx": 55939, "sentence_idx": 1298, "label": "unlabeled"}, {"type": "text", "expr": "Continuum crowds", "word_idx": 55947, "sentence_idx": 1299, "label": "unlabeled"}, {"type": "text", "expr": "Continuum crowds", "word_idx": 55963, "sentence_idx": 1300, "label": "unlabeled"}, {"type": "text", "expr": "In  ACM Transactions on Graphics (TOG) , volume\u00a025, pages\n1160\u20131168", "word_idx": 55979, "sentence_idx": 1301, "label": "unlabeled"}, {"type": "text", "expr": " ACM, 2006", "word_idx": 56046, "sentence_idx": 1302, "label": "unlabeled"}, {"type": "text", "expr": "ACM Transactions on Graphics (TOG)", "word_idx": 56056, "sentence_idx": 1303, "label": "unlabeled"}, {"type": "text", "expr": ", volume\u00a025, pages\n1160\u20131168", "word_idx": 56090, "sentence_idx": 1304, "label": "unlabeled"}, {"type": "text", "expr": " ACM, 2006", "word_idx": 56118, "sentence_idx": 1305, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Vinyals, A", "word_idx": 56128, "sentence_idx": 1306, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Toshev, S", "word_idx": 56139, "sentence_idx": 1307, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bengio, and D", "word_idx": 56149, "sentence_idx": 1308, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Erhan", "word_idx": 56163, "sentence_idx": 1309, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Vinyals, A", "word_idx": 56169, "sentence_idx": 1310, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Toshev, S", "word_idx": 56180, "sentence_idx": 1311, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bengio, and D", "word_idx": 56190, "sentence_idx": 1312, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Erhan", "word_idx": 56204, "sentence_idx": 1313, "label": "unlabeled"}, {"type": "text", "expr": "Show and tell: A neural image caption generator", "word_idx": 56210, "sentence_idx": 1314, "label": "unlabeled"}, {"type": "text", "expr": "Show and tell: A neural image caption generator", "word_idx": 56257, "sentence_idx": 1315, "label": "unlabeled"}, {"type": "text", "expr": "In  Proceedings of the IEEE conference on computer vision and\npattern recognition , pages 3156\u20133164, 2015", "word_idx": 56304, "sentence_idx": 1316, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the IEEE conference on computer vision and\npattern recognition", "word_idx": 56409, "sentence_idx": 1317, "label": "unlabeled"}, {"type": "text", "expr": ", pages 3156\u20133164, 2015", "word_idx": 56486, "sentence_idx": 1318, "label": "unlabeled"}, {"type": "text", "expr": " Wang, D", "word_idx": 56509, "sentence_idx": 1319, "label": "unlabeled"}, {"type": "text", "expr": " Fleet, and A", "word_idx": 56517, "sentence_idx": 1320, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hertzmann", "word_idx": 56530, "sentence_idx": 1321, "label": "unlabeled"}, {"type": "text", "expr": " Wang, D", "word_idx": 56540, "sentence_idx": 1322, "label": "unlabeled"}, {"type": "text", "expr": " Fleet, and A", "word_idx": 56548, "sentence_idx": 1323, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hertzmann", "word_idx": 56561, "sentence_idx": 1324, "label": "unlabeled"}, {"type": "text", "expr": "Gaussian process dynamical models for human motion", "word_idx": 56571, "sentence_idx": 1325, "label": "unlabeled"}, {"type": "text", "expr": "Gaussian process dynamical models for human motion", "word_idx": 56621, "sentence_idx": 1326, "label": "unlabeled"}, {"type": "text", "expr": "Pattern Analysis and Machine Intelligence, IEEE Transactions\non , 30(2):283\u2013298, 2008", "word_idx": 56671, "sentence_idx": 1327, "label": "unlabeled"}, {"type": "text", "expr": "Pattern Analysis and Machine Intelligence, IEEE Transactions\non", "word_idx": 56756, "sentence_idx": 1328, "label": "unlabeled"}, {"type": "text", "expr": ", 30(2):283\u2013298, 2008", "word_idx": 56819, "sentence_idx": 1329, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xu, J", "word_idx": 56840, "sentence_idx": 1330, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ba, R", "word_idx": 56846, "sentence_idx": 1331, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kiros, K", "word_idx": 56852, "sentence_idx": 1332, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Cho, A", "word_idx": 56861, "sentence_idx": 1333, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Courville, R", "word_idx": 56868, "sentence_idx": 1334, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Salakhudinov, R", "word_idx": 56881, "sentence_idx": 1335, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zemel, and\nY", "word_idx": 56897, "sentence_idx": 1336, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bengio", "word_idx": 56910, "sentence_idx": 1337, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xu, J", "word_idx": 56917, "sentence_idx": 1338, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ba, R", "word_idx": 56923, "sentence_idx": 1339, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kiros, K", "word_idx": 56929, "sentence_idx": 1340, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Cho, A", "word_idx": 56938, "sentence_idx": 1341, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Courville, R", "word_idx": 56945, "sentence_idx": 1342, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Salakhudinov, R", "word_idx": 56958, "sentence_idx": 1343, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zemel, and\nY", "word_idx": 56974, "sentence_idx": 1344, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bengio", "word_idx": 56987, "sentence_idx": 1345, "label": "unlabeled"}, {"type": "text", "expr": "Show, attend and tell: Neural image caption generation with visual\nattention", "word_idx": 56994, "sentence_idx": 1346, "label": "unlabeled"}, {"type": "text", "expr": "Show, attend and tell: Neural image caption generation with visual\nattention", "word_idx": 57070, "sentence_idx": 1347, "label": "unlabeled"}, {"type": "text", "expr": "In  International Conference on Machine Learning , pages\n2048\u20132057, 2015", "word_idx": 57146, "sentence_idx": 1348, "label": "unlabeled"}, {"type": "text", "expr": "International Conference on Machine Learning", "word_idx": 57218, "sentence_idx": 1349, "label": "unlabeled"}, {"type": "text", "expr": ", pages\n2048\u20132057, 2015", "word_idx": 57262, "sentence_idx": 1350, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yamaguchi, A", "word_idx": 57285, "sentence_idx": 1351, "label": "unlabeled"}, {"type": "text", "expr": " Berg, L", "word_idx": 57298, "sentence_idx": 1352, "label": "unlabeled"}, {"type": "text", "expr": " Ortiz, and T", "word_idx": 57306, "sentence_idx": 1353, "label": "unlabeled"}, {"type": "text", "expr": " Berg", "word_idx": 57319, "sentence_idx": 1354, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yamaguchi, A", "word_idx": 57324, "sentence_idx": 1355, "label": "unlabeled"}, {"type": "text", "expr": " Berg, L", "word_idx": 57337, "sentence_idx": 1356, "label": "unlabeled"}, {"type": "text", "expr": " Ortiz, and T", "word_idx": 57345, "sentence_idx": 1357, "label": "unlabeled"}, {"type": "text", "expr": " Berg", "word_idx": 57358, "sentence_idx": 1358, "label": "unlabeled"}, {"type": "text", "expr": "Who are you with and where are you going?", "word_idx": 57363, "sentence_idx": 1359, "label": "unlabeled"}, {"type": "text", "expr": "Who are you with and where are you going?", "word_idx": 57404, "sentence_idx": 1360, "label": "unlabeled"}, {"type": "text", "expr": "In  Computer Vision and Pattern Recognition (CVPR), 2011 IEEE\nConference on , pages 1345\u20131352", "word_idx": 57445, "sentence_idx": 1361, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 2011", "word_idx": 57538, "sentence_idx": 1362, "label": "unlabeled"}, {"type": "text", "expr": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE\nConference on", "word_idx": 57549, "sentence_idx": 1363, "label": "unlabeled"}, {"type": "text", "expr": ", pages 1345\u20131352", "word_idx": 57620, "sentence_idx": 1364, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 2011", "word_idx": 57637, "sentence_idx": 1365, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yi, H", "word_idx": 57648, "sentence_idx": 1366, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Li, and X", "word_idx": 57654, "sentence_idx": 1367, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wang", "word_idx": 57664, "sentence_idx": 1368, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yi, H", "word_idx": 57669, "sentence_idx": 1369, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Li, and X", "word_idx": 57675, "sentence_idx": 1370, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wang", "word_idx": 57685, "sentence_idx": 1371, "label": "unlabeled"}, {"type": "text", "expr": "Understanding pedestrian behaviors from stationary crowd groups", "word_idx": 57690, "sentence_idx": 1372, "label": "unlabeled"}, {"type": "text", "expr": "Understanding pedestrian behaviors from stationary crowd groups", "word_idx": 57753, "sentence_idx": 1373, "label": "unlabeled"}, {"type": "text", "expr": "In  Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition , pages 3488\u20133496, 2015", "word_idx": 57816, "sentence_idx": 1374, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition", "word_idx": 57921, "sentence_idx": 1375, "label": "unlabeled"}, {"type": "text", "expr": ", pages 3488\u20133496, 2015", "word_idx": 57998, "sentence_idx": 1376, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang, T", "word_idx": 58021, "sentence_idx": 1377, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xu, H", "word_idx": 58030, "sentence_idx": 1378, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Li, S", "word_idx": 58036, "sentence_idx": 1379, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang, X", "word_idx": 58042, "sentence_idx": 1380, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Huang, X", "word_idx": 58051, "sentence_idx": 1381, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wang, and D", "word_idx": 58060, "sentence_idx": 1382, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Metaxas", "word_idx": 58072, "sentence_idx": 1383, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang, T", "word_idx": 58080, "sentence_idx": 1384, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xu, H", "word_idx": 58089, "sentence_idx": 1385, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Li, S", "word_idx": 58095, "sentence_idx": 1386, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang, X", "word_idx": 58101, "sentence_idx": 1387, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Huang, X", "word_idx": 58110, "sentence_idx": 1388, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wang, and D", "word_idx": 58119, "sentence_idx": 1389, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Metaxas", "word_idx": 58131, "sentence_idx": 1390, "label": "unlabeled"}, {"type": "text", "expr": "Stackgan: Text to photo-realistic image synthesis with stacked\ngenerative adversarial networks", "word_idx": 58139, "sentence_idx": 1391, "label": "unlabeled"}, {"type": "text", "expr": "Stackgan: Text to photo-realistic image synthesis with stacked\ngenerative adversarial networks", "word_idx": 58233, "sentence_idx": 1392, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1612", "word_idx": 58327, "sentence_idx": 1393, "label": "unlabeled"}, {"type": "text", "expr": "03242 , 2016", "word_idx": 58352, "sentence_idx": 1394, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1612", "word_idx": 58364, "sentence_idx": 1395, "label": "unlabeled"}, {"type": "text", "expr": "03242", "word_idx": 58389, "sentence_idx": 1396, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 58394, "sentence_idx": 1397, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhou, X", "word_idx": 58400, "sentence_idx": 1398, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wang, and X", "word_idx": 58408, "sentence_idx": 1399, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Tang", "word_idx": 58420, "sentence_idx": 1400, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhou, X", "word_idx": 58425, "sentence_idx": 1401, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wang, and X", "word_idx": 58433, "sentence_idx": 1402, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Tang", "word_idx": 58445, "sentence_idx": 1403, "label": "unlabeled"}, {"type": "text", "expr": "Random field topic model for semantic region analysis in crowded\nscenes from tracklets", "word_idx": 58450, "sentence_idx": 1404, "label": "unlabeled"}, {"type": "text", "expr": "Random field topic model for semantic region analysis in crowded\nscenes from tracklets", "word_idx": 58536, "sentence_idx": 1405, "label": "unlabeled"}, {"type": "text", "expr": "In  Computer Vision and Pattern Recognition (CVPR), 2011 IEEE\nConference on , pages 3441\u20133448", "word_idx": 58622, "sentence_idx": 1406, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 2011", "word_idx": 58715, "sentence_idx": 1407, "label": "unlabeled"}, {"type": "text", "expr": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE\nConference on", "word_idx": 58726, "sentence_idx": 1408, "label": "unlabeled"}, {"type": "text", "expr": ", pages 3441\u20133448", "word_idx": 58797, "sentence_idx": 1409, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 2011", "word_idx": 58814, "sentence_idx": 1410, "label": "unlabeled"}, {"type": "text", "expr": "Generated  on Tue Apr  3 11:30:01 2018 by", "word_idx": 58825, "sentence_idx": 1411, "label": "unlabeled"}, {"type": "text", "expr": "LaTeXML", "word_idx": 58866, "sentence_idx": 1412, "label": "unlabeled"}], "Iterative _Visual_Reasoning_Beyond_Convolutions": [{"type": "text", "expr": "Iterative Visual Reasoning Beyond Convolutions", "word_idx": 0, "sentence_idx": 0, "label": "unlabeled"}, {"type": "text", "expr": "Iterative Visual Reasoning Beyond Convolutions", "word_idx": 46, "sentence_idx": 1, "label": "unlabeled"}, {"type": "text", "expr": "Xinlei Chen 1  \u2003\u2003Li-Jia Li 2  \u2003\u2003Li Fei-Fei 2  \u2003\u2003Abhinav Gupta 1 1 Carnegie Mellon University \u2003\u2003 2 Google", "word_idx": 92, "sentence_idx": 2, "label": "unlabeled"}, {"type": "text", "expr": "Abstract", "word_idx": 196, "sentence_idx": 3, "label": "unlabeled"}, {"type": "text", "expr": "Abstract", "word_idx": 204, "sentence_idx": 4, "label": "unlabeled"}, {"type": "text", "expr": "We present a novel framework for iterative visual reasoning", "word_idx": 212, "sentence_idx": 5, "label": "unlabeled"}, {"type": "text", "expr": " Our framework goes beyond current recognition systems that lack the capability to reason beyond stack of convolutions", "word_idx": 271, "sentence_idx": 6, "label": "unlabeled"}, {"type": "text", "expr": " The framework consists of two core modules: a local module that uses spatial memory\u00a0  to store previous beliefs with parallel updates; and a global graph-reasoning module", "word_idx": 389, "sentence_idx": 7, "label": "unlabeled"}, {"type": "text", "expr": " Our graph module has three components: a) a knowledge graph where we represent classes as nodes and build edges to encode different types of semantic relationships between them; b) a region graph of the current image where regions in the image are nodes and spatial relationships between these regions are edges; c) an assignment graph that assigns regions to classes", "word_idx": 560, "sentence_idx": 8, "label": "unlabeled"}, {"type": "text", "expr": " Both the local module and the global module roll-out iteratively and cross-feed predictions to each other to refine estimates", "word_idx": 928, "sentence_idx": 9, "label": "unlabeled"}, {"type": "text", "expr": " The final predictions are made by combining the best of both modules with an attention mechanism", "word_idx": 1054, "sentence_idx": 10, "label": "unlabeled"}, {"type": "text", "expr": " We show strong performance over plain ConvNets,  \\eg achieving an  $84\\%$  absolute improvement on ADE\u00a0  measured by per-class average precision", "word_idx": 1151, "sentence_idx": 11, "label": "unlabeled"}, {"type": "text", "expr": " Analysis also shows that the framework is resilient to missing regions for reasoning", "word_idx": 1296, "sentence_idx": 12, "label": "unlabeled"}, {"type": "math", "expr": "$$8.4\\%$$", "word_idx": 1381, "sentence_idx": 13, "label": "unlabeled"}, {"type": "text", "expr": "\\cvprfinalcopy", "word_idx": 1386, "sentence_idx": 14, "label": "unlabeled"}, {"type": "text", "expr": "1  Introduction", "word_idx": 1400, "sentence_idx": 15, "label": "unlabeled"}, {"type": "text", "expr": "In recent years, we have made significant advances in standard recognition tasks such as image classification\u00a0 , detection\u00a0  or segmentation\u00a0 ", "word_idx": 1415, "sentence_idx": 16, "label": "unlabeled"}, {"type": "text", "expr": " Most of these gains are a result of using feed-forward end-to-end learned ConvNet models", "word_idx": 1557, "sentence_idx": 17, "label": "unlabeled"}, {"type": "text", "expr": " Unlike humans where visual reasoning about the space and semantics is crucial\u00a0 , our current visual systems lack any context reasoning beyond convolutions with large receptive fields", "word_idx": 1646, "sentence_idx": 18, "label": "unlabeled"}, {"type": "text", "expr": " Therefore, a critical question is how do we incorporate both  spatial  and  semantic  reasoning as we build next-generation vision systems", "word_idx": 1829, "sentence_idx": 19, "label": "unlabeled"}, {"type": "text", "expr": "spatial", "word_idx": 1968, "sentence_idx": 20, "label": "unlabeled"}, {"type": "text", "expr": "semantic", "word_idx": 1975, "sentence_idx": 21, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:", "word_idx": 1983, "sentence_idx": 22, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:", "word_idx": 1992, "sentence_idx": 23, "label": "unlabeled"}, {"type": "text", "expr": "Current recognition systems lack the reasoning power beyond convolutions with large receptive fields, whereas humans can explore the rich space of spatial and semantic relationships for reasoning:  \\eg inferring the fourth \u201cwindow\u201d even with occlusion, or the \u201cperson\u201d who drives the \u201ccar\u201d", "word_idx": 2001, "sentence_idx": 24, "label": "unlabeled"}, {"type": "text", "expr": " To close this gap, we present a generic framework that also uses relationships to iteratively reason and build up estimates", "word_idx": 2290, "sentence_idx": 25, "label": "unlabeled"}, {"type": "text", "expr": "Our goal is to build a system that can not only extract and utilize hierarchy of convolutional features, but also improve its estimates via spatial and semantic relationships", "word_idx": 2414, "sentence_idx": 26, "label": "unlabeled"}, {"type": "text", "expr": " But what are spatial and semantic relationships and how can they be used to improve recognition? Take a look at Fig", "word_idx": 2588, "sentence_idx": 27, "label": "unlabeled"}, {"type": "text", "expr": " An example of spatial reasoning (top-left) would be: if three regions out of four in a line are \u201cwindow\u201d, then the fourth is also likely to be \u201cwindow\u201d", "word_idx": 2704, "sentence_idx": 28, "label": "unlabeled"}, {"type": "text", "expr": " An example of semantic reasoning (bottom-right) would be to recognize \u201cschool bus\u201d even if we have seen few or no examples of it \u2013 just given examples of \u201cbus\u201d and knowing their connections", "word_idx": 2856, "sentence_idx": 29, "label": "unlabeled"}, {"type": "text", "expr": " Finally, an example of spatial-semantic reasoning could be: recognition of a \u201ccar\u201d on road should help in recognizing the \u201cperson\u201d inside \u201cdriving\u201d the \u201ccar\u201d", "word_idx": 3046, "sentence_idx": 30, "label": "unlabeled"}, {"type": "text", "expr": "A key recipe to reasoning with relationships is to  iteratively  build up estimates", "word_idx": 3204, "sentence_idx": 31, "label": "unlabeled"}, {"type": "text", "expr": "\nRecently, there have been efforts to incorporate such reasoning via top-down modules\u00a0  or using explicit memories\u00a0 ", "word_idx": 3287, "sentence_idx": 32, "label": "unlabeled"}, {"type": "text", "expr": " In the case of top-down modules, high-level features which have class-based information can be used in conjunction with low-level features to improve recognition performance", "word_idx": 3403, "sentence_idx": 33, "label": "unlabeled"}, {"type": "text", "expr": " An alternative architecture is to use explicit memory", "word_idx": 3577, "sentence_idx": 34, "label": "unlabeled"}, {"type": "text", "expr": " For example, Chen & Gupta\u00a0  performs sequential object detection, where a  spatial memory  is used to store previously detected objects, leveraging the power of ConvNets for extracting dense context patterns beneficial for follow-up detections", "word_idx": 3631, "sentence_idx": 35, "label": "unlabeled"}, {"type": "text", "expr": "iteratively", "word_idx": 3875, "sentence_idx": 36, "label": "unlabeled"}, {"type": "text", "expr": "spatial memory", "word_idx": 3886, "sentence_idx": 37, "label": "unlabeled"}, {"type": "text", "expr": "However, there are two problems with these approaches: a) both approaches use stack of convolutions to perform  local  pixel-level reasoning\u00a0 , which can lack a  global  reasoning power that also allows regions farther away to directly communicate information; b) more importantly, both approaches assume enough examples of relationships in the training data \u2013 so that the model can learn them from scratch, but as the relationships grow exponentially with increasing number of classes, there is not always enough data", "word_idx": 3900, "sentence_idx": 38, "label": "unlabeled"}, {"type": "text", "expr": " A lot of semantic reasoning requires learning from few or no examples\u00a0 ", "word_idx": 4418, "sentence_idx": 39, "label": "unlabeled"}, {"type": "text", "expr": " Therefore, we need ways to exploit additional structured information for visual reasoning", "word_idx": 4490, "sentence_idx": 40, "label": "unlabeled"}, {"type": "text", "expr": "local", "word_idx": 4580, "sentence_idx": 41, "label": "unlabeled"}, {"type": "text", "expr": "global", "word_idx": 4585, "sentence_idx": 42, "label": "unlabeled"}, {"type": "text", "expr": "In this paper, we put forward a generic framework for both spatial and semantic reasoning", "word_idx": 4591, "sentence_idx": 43, "label": "unlabeled"}, {"type": "text", "expr": " Different from current approaches that are just relying on convolutions, our framework can also learn from structured information in the form of knowledge bases\u00a0  for visual recognition", "word_idx": 4680, "sentence_idx": 44, "label": "unlabeled"}, {"type": "text", "expr": " The core of our algorithm consists of two modules: the local module, based on spatial memory\u00a0 , performs pixel-level reasoning using ConvNets", "word_idx": 4866, "sentence_idx": 45, "label": "unlabeled"}, {"type": "text", "expr": " We make major improvements on efficiency by parallel memory updates", "word_idx": 5008, "sentence_idx": 46, "label": "unlabeled"}, {"type": "text", "expr": " Additionally, we introduce a global module for reasoning beyond local regions", "word_idx": 5076, "sentence_idx": 47, "label": "unlabeled"}, {"type": "text", "expr": " In the global module, reasoning is based on a  graph  structure", "word_idx": 5154, "sentence_idx": 48, "label": "unlabeled"}, {"type": "text", "expr": " It has three components: a) a knowledge graph where we represent classes as nodes and build edges to encode different types of semantic relationships; b) a region graph of the current image where regions in the image are nodes and spatial relationships between these regions are edges; c) an assignment graph that assigns regions to classes", "word_idx": 5218, "sentence_idx": 49, "label": "unlabeled"}, {"type": "text", "expr": " Taking advantage of such a structure, we develop a reasoning module specifically designed to pass information on this graph", "word_idx": 5559, "sentence_idx": 50, "label": "unlabeled"}, {"type": "text", "expr": " Both the local module and the global module roll-out iteratively and cross-feed predictions to each other in order to refine estimates", "word_idx": 5683, "sentence_idx": 51, "label": "unlabeled"}, {"type": "text", "expr": " Note that, local and global reasoning are not isolated: a good image understanding is usually a compromise between background knowledge learned  a priori  and image-specific observations", "word_idx": 5818, "sentence_idx": 52, "label": "unlabeled"}, {"type": "text", "expr": " Therefore, our full pipeline joins force of the two modules by an attention\u00a0  mechanism allowing the model to rely on the most relevant features when making the final predictions", "word_idx": 6005, "sentence_idx": 53, "label": "unlabeled"}, {"type": "text", "expr": "graph", "word_idx": 6184, "sentence_idx": 54, "label": "unlabeled"}, {"type": "text", "expr": "a priori", "word_idx": 6189, "sentence_idx": 55, "label": "unlabeled"}, {"type": "text", "expr": "We show strong performance over plain ConvNets using our framework", "word_idx": 6197, "sentence_idx": 56, "label": "unlabeled"}, {"type": "text", "expr": " For example, we can achieve  $84\\%$  absolute improvements on ADE\u00a0  measured by per-class average precision, where by simply making the network deeper can only help  ${\\sim}1\\%$ ", "word_idx": 6263, "sentence_idx": 57, "label": "unlabeled"}, {"type": "math", "expr": "$$8.4\\%$$", "word_idx": 6442, "sentence_idx": 58, "label": "unlabeled"}, {"type": "math", "expr": "$${\\sim}1\\%$$", "word_idx": 6447, "sentence_idx": 59, "label": "unlabeled"}, {"type": "text", "expr": "2  Related Work", "word_idx": 6456, "sentence_idx": 60, "label": "unlabeled"}, {"type": "text", "expr": "Visual Knowledge Base", "word_idx": 6471, "sentence_idx": 61, "label": "unlabeled"}, {"type": "text", "expr": "  Whereas past five years in computer vision will probably be remembered as the successful resurgence of neural networks, acquiring visual knowledge at a large scale \u2013 the simplest form being labeled instances of objects\u00a0 , scenes\u00a0 , relationships\u00a0   \\etc \u2013 deserves at least half the credit, since ConvNets hinge on large datasets\u00a0 ", "word_idx": 6492, "sentence_idx": 62, "label": "unlabeled"}, {"type": "text", "expr": " Apart from providing labels using crowd-sourcing, attempts have also been made to accumulate structured knowledge ( \\eg relationships\u00a0 ,  $n$ -grams\u00a0 ) automatically from the web", "word_idx": 6825, "sentence_idx": 63, "label": "unlabeled"}, {"type": "text", "expr": " However, these works fixate on building knowledge bases rather than using knowledge for reasoning", "word_idx": 7004, "sentence_idx": 64, "label": "unlabeled"}, {"type": "text", "expr": " Our framework, while being more general, is along the line of research that applies visual knowledge base to end tasks, such as affordances\u00a0 , image classification\u00a0 , or question answering\u00a0 ", "word_idx": 7102, "sentence_idx": 65, "label": "unlabeled"}, {"type": "text", "expr": "Visual Knowledge Base", "word_idx": 7293, "sentence_idx": 66, "label": "unlabeled"}, {"type": "text", "expr": "Context Modeling", "word_idx": 7314, "sentence_idx": 67, "label": "unlabeled"}, {"type": "text", "expr": "  Modeling context, or the interplay between scenes, objects and parts is one of the central problems in computer vision", "word_idx": 7330, "sentence_idx": 68, "label": "unlabeled"}, {"type": "text", "expr": " While various previous work ( \\eg scene-level reasoning\u00a0 , attributes\u00a0 , structured prediction\u00a0 , relationship graph\u00a0 ) has approached this problem from different angles, the breakthrough comes from the idea of feature learning with ConvNets\u00a0 ", "word_idx": 7450, "sentence_idx": 69, "label": "unlabeled"}, {"type": "text", "expr": " On the surface, such models hardly use any explicit context module for reasoning, but it is generally accepted that ConvNets are extremely effective in aggregating local pixel-to-level context through its ever-growing receptive fields\u00a0 ", "word_idx": 7694, "sentence_idx": 70, "label": "unlabeled"}, {"type": "text", "expr": " Even the most recent developments such as top-down module\u00a0 , pairwise module\u00a0 , iterative feedback\u00a0 , attention\u00a0 , and memory\u00a0  are motivated to leverage such power and depend on variants of convolutions for reasoning", "word_idx": 7931, "sentence_idx": 71, "label": "unlabeled"}, {"type": "text", "expr": " Our work takes an important next step beyond those approaches in that it also incorporates learning from structured visual knowledge bases directly to reason with spatial and semantic relationships", "word_idx": 8149, "sentence_idx": 72, "label": "unlabeled"}, {"type": "text", "expr": "Context Modeling", "word_idx": 8347, "sentence_idx": 73, "label": "unlabeled"}, {"type": "text", "expr": "Relational Reasoning", "word_idx": 8363, "sentence_idx": 74, "label": "unlabeled"}, {"type": "text", "expr": "  The earliest form of reasoning in artificial intelligence dates back to symbolic approaches\u00a0 , where relations between abstract symbols are defined by the language of mathematics and logic, and reasoning takes place by deduction, abduction\u00a0 ,  \\etc ", "word_idx": 8383, "sentence_idx": 75, "label": "unlabeled"}, {"type": "text", "expr": " However, symbols need to be grounded\u00a0  before such systems are practically useful", "word_idx": 8634, "sentence_idx": 76, "label": "unlabeled"}, {"type": "text", "expr": " Modern approaches, such as path ranking algorithm\u00a0 , rely on statistical learning to extract useful patterns to perform relational reasoning on structured knowledge bases", "word_idx": 8716, "sentence_idx": 77, "label": "unlabeled"}, {"type": "text", "expr": " As an active research area, there are recent works also applying neural networks to the graph structured data\u00a0 , or attempting to regularize the output of networks with relationships\u00a0  and knowledge bases\u00a0 ", "word_idx": 8887, "sentence_idx": 78, "label": "unlabeled"}, {"type": "text", "expr": " However, we believe for visual data, reasoning should be both local and global: discarding the two-dimensional image structure is neither efficient nor effective for tasks that involve regions", "word_idx": 9094, "sentence_idx": 79, "label": "unlabeled"}, {"type": "text", "expr": "Relational Reasoning", "word_idx": 9287, "sentence_idx": 80, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:", "word_idx": 9307, "sentence_idx": 81, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:", "word_idx": 9316, "sentence_idx": 82, "label": "unlabeled"}, {"type": "text", "expr": "Overview of our reasoning framework", "word_idx": 9325, "sentence_idx": 83, "label": "unlabeled"}, {"type": "text", "expr": " Besides a plain ConvNet that gives predictions, the framework has two modules to perform reasoning: a local one (Sec", "word_idx": 9360, "sentence_idx": 84, "label": "unlabeled"}, {"type": "text", "expr": "1 ) that uses spatial memory  $\\mathcal{S}_{i}$ , and reasons with another ConvNet  $\\mathcal{C}$ ; and a global one (Sec", "word_idx": 9477, "sentence_idx": 85, "label": "unlabeled"}, {"type": "text", "expr": "2 ) that treats regions and classes as nodes in a graph and reasons by passing information among them", "word_idx": 9598, "sentence_idx": 86, "label": "unlabeled"}, {"type": "text", "expr": " Both modules receive combined high-level and mid-level features, and roll-out iteratively (Sec", "word_idx": 9699, "sentence_idx": 87, "label": "unlabeled"}, {"type": "text", "expr": "3 ) while cross-feeding beliefs", "word_idx": 9794, "sentence_idx": 88, "label": "unlabeled"}, {"type": "text", "expr": " The final prediction  $f$  is produced by combining all the predictions  $f_{i}$  with attentions  $a_{i}$  (Sec", "word_idx": 9825, "sentence_idx": 89, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{S}_{i}$$", "word_idx": 9938, "sentence_idx": 90, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{C}$$", "word_idx": 9953, "sentence_idx": 91, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{i}$$", "word_idx": 9964, "sentence_idx": 92, "label": "unlabeled"}, {"type": "math", "expr": "$$a_{i}$$", "word_idx": 9969, "sentence_idx": 93, "label": "unlabeled"}, {"type": "text", "expr": "3  Reasoning Framework", "word_idx": 9974, "sentence_idx": 94, "label": "unlabeled"}, {"type": "text", "expr": "In this section we build up our reasoning framework", "word_idx": 9996, "sentence_idx": 95, "label": "unlabeled"}, {"type": "text", "expr": " Besides plain predictions  $p_{0}$  from a ConvNet, it consists of two core modules that reason to predict", "word_idx": 10047, "sentence_idx": 96, "label": "unlabeled"}, {"type": "text", "expr": " The first one, local module, uses a spatial memory to store previous beliefs with parallel updates, and still falls within the regime of convolution based reasoning (Sec", "word_idx": 10154, "sentence_idx": 97, "label": "unlabeled"}, {"type": "text", "expr": " Beyond convolutions, we present our key contribution \u2013 a global module that reasons directly between regions and classes represented as nodes in a graph (Sec", "word_idx": 10324, "sentence_idx": 98, "label": "unlabeled"}, {"type": "text", "expr": " Both modules build up estimation iteratively (Sec", "word_idx": 10482, "sentence_idx": 99, "label": "unlabeled"}, {"type": "text", "expr": "3 ), with beliefs cross-fed to each other", "word_idx": 10532, "sentence_idx": 100, "label": "unlabeled"}, {"type": "text", "expr": " Finally taking advantage of both local and global, we combine predictions from all iterations with an attention mechanism (Sec", "word_idx": 10573, "sentence_idx": 101, "label": "unlabeled"}, {"type": "text", "expr": "4 ) and train the model with sample re-weighting (Sec", "word_idx": 10700, "sentence_idx": 102, "label": "unlabeled"}, {"type": "text", "expr": "5 ) that focuses on hard examples (See Fig", "word_idx": 10753, "sentence_idx": 103, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 2 )", "word_idx": 10795, "sentence_idx": 104, "label": "unlabeled"}, {"type": "math", "expr": "$$p_{0}$$", "word_idx": 10800, "sentence_idx": 105, "label": "unlabeled"}, {"type": "text", "expr": "1  Reasoning with Convolutions", "word_idx": 10805, "sentence_idx": 106, "label": "unlabeled"}, {"type": "text", "expr": "Our first building block, the local module, is inspired from\u00a0 ", "word_idx": 10835, "sentence_idx": 107, "label": "unlabeled"}, {"type": "text", "expr": " At a high level, the idea is to use a spatial memory  $\\mathcal{S}$  to store previously detected objects at the very location they have been found", "word_idx": 10897, "sentence_idx": 108, "label": "unlabeled"}, {"type": "text", "expr": "  $\\mathcal{S}$  is a tensor with three dimensions", "word_idx": 11045, "sentence_idx": 109, "label": "unlabeled"}, {"type": "text", "expr": " The first two, height  $H$  and width  $W$ , correspond to the reduced size ( $1/16$ ) of the image", "word_idx": 11095, "sentence_idx": 110, "label": "unlabeled"}, {"type": "text", "expr": " The third one, depth  $D$  ( ${=}512$ ), makes each cell of the memory  $c$  a vector that stores potentially useful information at that location", "word_idx": 11195, "sentence_idx": 111, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{S}$$", "word_idx": 11341, "sentence_idx": 112, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{S}$$", "word_idx": 11352, "sentence_idx": 113, "label": "unlabeled"}, {"type": "math", "expr": "$$1/16$$", "word_idx": 11363, "sentence_idx": 114, "label": "unlabeled"}, {"type": "math", "expr": "$${=}512$$", "word_idx": 11367, "sentence_idx": 115, "label": "unlabeled"}, {"type": "text", "expr": "$\\mathcal{S}$  is updated with both high-level and mid-level features", "word_idx": 11373, "sentence_idx": 116, "label": "unlabeled"}, {"type": "text", "expr": " For high-level, information regarding the estimated class label is stored", "word_idx": 11442, "sentence_idx": 117, "label": "unlabeled"}, {"type": "text", "expr": " However, just knowing the class may not be ideal \u2013 more details about the shape, pose  \\etc can also be useful for other objects", "word_idx": 11516, "sentence_idx": 118, "label": "unlabeled"}, {"type": "text", "expr": " For example, it would be nice to know the pose of a \u201cperson\u201d playing tennis to recognize the \u201cracket\u201d", "word_idx": 11645, "sentence_idx": 119, "label": "unlabeled"}, {"type": "text", "expr": " In this paper, we use the logits  $f$  before soft-max activation, in conjunction with feature maps from a bottom convolutional layer  $h$  to feed-in the memory", "word_idx": 11747, "sentence_idx": 120, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{S}$$", "word_idx": 11909, "sentence_idx": 121, "label": "unlabeled"}, {"type": "text", "expr": "Given an image region  $r$  to update, we first crop the corresponding features from the bottom layer, and resize it to a predefined square ( $7{\\times}7$ ) with bi-linear interpolation as  $h$ ", "word_idx": 11920, "sentence_idx": 122, "label": "unlabeled"}, {"type": "text", "expr": " Since high-level feature  $f$  is a vector covering the entire region, we append it to all the  $49$  locations", "word_idx": 12114, "sentence_idx": 123, "label": "unlabeled"}, {"type": "text", "expr": " Two  $1{\\times}1$  convolutions are used to fuse the information\u00a0  and form our input features  $f_{r}$  for  $r$ ", "word_idx": 12226, "sentence_idx": 124, "label": "unlabeled"}, {"type": "text", "expr": " The same region in the memory  $\\mathcal{S}$  is also cropped and resized to  $7{\\times}7$ , denoted as  $s_{r}$ ", "word_idx": 12341, "sentence_idx": 125, "label": "unlabeled"}, {"type": "text", "expr": " After this alignment, we use a convolutional gated recurrent unit (GRU)\u00a0  to write the memory:", "word_idx": 12455, "sentence_idx": 126, "label": "unlabeled"}, {"type": "math", "expr": "$$7{\\times}7$$", "word_idx": 12550, "sentence_idx": 127, "label": "unlabeled"}, {"type": "math", "expr": "$$1{\\times}1$$", "word_idx": 12560, "sentence_idx": 128, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{r}$$", "word_idx": 12570, "sentence_idx": 129, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{S}$$", "word_idx": 12575, "sentence_idx": 130, "label": "unlabeled"}, {"type": "math", "expr": "$$7{\\times}7$$", "word_idx": 12586, "sentence_idx": 131, "label": "unlabeled"}, {"type": "math", "expr": "$$s_{r}$$", "word_idx": 12596, "sentence_idx": 132, "label": "unlabeled"}, {"type": "text", "expr": "$s^{\\prime}_{r}=u\\circ s_{r}+(1-u)\\circ\\sigma(W_{f}f_{r}+W_{s}(z\\circ s_{r})+b),$", "word_idx": 12601, "sentence_idx": 133, "label": "unlabeled"}, {"type": "math", "expr": "$$s^{\\prime}_{r}=u\\circ s_{r}+(1-u)\\circ\\sigma(W_{f}f_{r}+W_{s}(z\\circ s_{r})+b),$$", "word_idx": 12682, "sentence_idx": 134, "label": "unlabeled"}, {"type": "text", "expr": "where  $s^{\\prime}_{r}$  is the updated memory for  $r$ ,  $u$  is update gate,  $z$  is reset gate,  $W_{f}$ ,  $W_{s}$  and  $b$  are convolutional weights and bias, and  $\\circ$  is entry-wise product", "word_idx": 12761, "sentence_idx": 135, "label": "unlabeled"}, {"type": "text", "expr": "  $\\sigma(\\cdot)$  is an activation function", "word_idx": 12964, "sentence_idx": 136, "label": "unlabeled"}, {"type": "text", "expr": " After the update,  $s^{\\prime}_{r}$  is placed back to  $\\mathcal{S}$  with another crop and resize operation ", "word_idx": 13008, "sentence_idx": 137, "label": "unlabeled"}, {"type": "math", "expr": "$$s^{\\prime}_{r}$$", "word_idx": 13119, "sentence_idx": 138, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{f}$$", "word_idx": 13133, "sentence_idx": 139, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{s}$$", "word_idx": 13138, "sentence_idx": 140, "label": "unlabeled"}, {"type": "math", "expr": "$$\\circ$$", "word_idx": 13143, "sentence_idx": 141, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma(\\cdot)$$", "word_idx": 13148, "sentence_idx": 142, "label": "unlabeled"}, {"type": "math", "expr": "$$s^{\\prime}_{r}$$", "word_idx": 13161, "sentence_idx": 143, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{S}$$", "word_idx": 13175, "sentence_idx": 144, "label": "unlabeled"}, {"type": "text", "expr": "1 Different from previous work\u00a0  that introduces an inverse operation to put the region back, we note that crop and resize  itself  with proper extrapolation can simply meet this requirement", "word_idx": 13186, "sentence_idx": 145, "label": "unlabeled"}, {"type": "text", "expr": "itself", "word_idx": 13376, "sentence_idx": 146, "label": "unlabeled"}, {"type": "text", "expr": "Parallel Updates", "word_idx": 13382, "sentence_idx": 147, "label": "unlabeled"}, {"type": "text", "expr": "  Previous work\u00a0  made sequential updates to memory", "word_idx": 13398, "sentence_idx": 148, "label": "unlabeled"}, {"type": "text", "expr": " However, sequential inference is inefficient and GPU-intensive \u2013 limiting it to only give ten outputs per image\u00a0 ", "word_idx": 13449, "sentence_idx": 149, "label": "unlabeled"}, {"type": "text", "expr": " In this paper we propose to update the regions in parallel as an approximation", "word_idx": 13563, "sentence_idx": 150, "label": "unlabeled"}, {"type": "text", "expr": " In overlapping cases, a cell can be covered multiple times from different regions", "word_idx": 13642, "sentence_idx": 151, "label": "unlabeled"}, {"type": "text", "expr": " When placing the regions back to  $\\mathcal{S}$ , we also calculate a weight matrix  $\\Gamma$  where each entry  $\\gamma_{r,c}{\\in}[0,1]$  keeps track of how much a region  $r$  has contributed to a memory cell  $c$ :  $1$  meaning the cell is fully covered by the region,  $0$  meaning not covered", "word_idx": 13724, "sentence_idx": 152, "label": "unlabeled"}, {"type": "text", "expr": " The final values of the updated cell is the weighted average of all regions", "word_idx": 14023, "sentence_idx": 153, "label": "unlabeled"}, {"type": "text", "expr": "Parallel Updates", "word_idx": 14099, "sentence_idx": 154, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{S}$$", "word_idx": 14115, "sentence_idx": 155, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma$$", "word_idx": 14126, "sentence_idx": 156, "label": "unlabeled"}, {"type": "math", "expr": "$$\\gamma_{r,c}{\\in}[0,1]$$", "word_idx": 14132, "sentence_idx": 157, "label": "unlabeled"}, {"type": "text", "expr": "The actual reasoning module, a ConvNet  $\\mathcal{C}$  of three  $3{\\times}3$  convolutions and two  $4096$ -D fully-connected layers, takes  $\\mathcal{S}$  as the input, and builds connections within the local window of its receptive fields to perform prediction", "word_idx": 14154, "sentence_idx": 158, "label": "unlabeled"}, {"type": "text", "expr": " Since the two-dimensional image structure and the location information is preserved in  $\\mathcal{S}$ , such an architecture is particularly useful for relationships with spatial reasoning", "word_idx": 14417, "sentence_idx": 159, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{C}$$", "word_idx": 14606, "sentence_idx": 160, "label": "unlabeled"}, {"type": "math", "expr": "$$3{\\times}3$$", "word_idx": 14617, "sentence_idx": 161, "label": "unlabeled"}, {"type": "math", "expr": "$$4096$$", "word_idx": 14627, "sentence_idx": 162, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{S}$$", "word_idx": 14631, "sentence_idx": 163, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{S}$$", "word_idx": 14642, "sentence_idx": 164, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:", "word_idx": 14653, "sentence_idx": 165, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:", "word_idx": 14662, "sentence_idx": 166, "label": "unlabeled"}, {"type": "text", "expr": "Illustration of directly passing information on a graph with multiple edge types", "word_idx": 14671, "sentence_idx": 167, "label": "unlabeled"}, {"type": "text", "expr": " Here four nodes are linked with two edge types", "word_idx": 14751, "sentence_idx": 168, "label": "unlabeled"}, {"type": "text", "expr": " Each node is represented as an input feature vector  $m_{i}$  (aggregated as  $M$ )", "word_idx": 14798, "sentence_idx": 169, "label": "unlabeled"}, {"type": "text", "expr": " Weight matrix  $W_{j}$  is learned for edge type  $j$  to transform inputs", "word_idx": 14882, "sentence_idx": 170, "label": "unlabeled"}, {"type": "text", "expr": " Then adjacency matrix  $A_{j}$  is applied to pass information to linked nodes", "word_idx": 14957, "sentence_idx": 171, "label": "unlabeled"}, {"type": "text", "expr": " Finally, output  $G$  is generated by accumulating all edge types and apply activation function", "word_idx": 15036, "sentence_idx": 172, "label": "unlabeled"}, {"type": "math", "expr": "$$m_{i}$$", "word_idx": 15132, "sentence_idx": 173, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{j}$$", "word_idx": 15137, "sentence_idx": 174, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{j}$$", "word_idx": 15142, "sentence_idx": 175, "label": "unlabeled"}, {"type": "text", "expr": "2  Beyond Convolutions", "word_idx": 15147, "sentence_idx": 176, "label": "unlabeled"}, {"type": "text", "expr": "Our second module goes beyond local regions and convolutions for global reasoning", "word_idx": 15169, "sentence_idx": 177, "label": "unlabeled"}, {"type": "text", "expr": " Here the meaning of  global  is two-fold", "word_idx": 15250, "sentence_idx": 178, "label": "unlabeled"}, {"type": "text", "expr": " First is  spatial , that is, we want to let the regions farther away to directly communicate information with each other, not confined by the receptive fields of the reasoning module  $\\mathcal{C}$ ", "word_idx": 15291, "sentence_idx": 179, "label": "unlabeled"}, {"type": "text", "expr": " Second is  semantic , meaning we want to take advantage of visual knowledge bases, which can provide relationships between classes that are globally true ( \\ie commonsense) across images", "word_idx": 15490, "sentence_idx": 180, "label": "unlabeled"}, {"type": "text", "expr": " To achieve both types of reasoning, we build a graph  $\\mathcal{G}=(\\mathcal{N},\\mathcal{E})$ , where  $\\mathcal{N}$  and  $\\mathcal{E}$  denote node sets and edge sets, respectively", "word_idx": 15677, "sentence_idx": 181, "label": "unlabeled"}, {"type": "text", "expr": " Two types of nodes are defined in  $\\mathcal{N}$ : region nodes  $\\mathcal{N}_{r}$  for  $R$  regions, and class nodes  $\\mathcal{N}_{c}$  for  $C$  classes", "word_idx": 15860, "sentence_idx": 182, "label": "unlabeled"}, {"type": "text", "expr": "global", "word_idx": 16017, "sentence_idx": 183, "label": "unlabeled"}, {"type": "text", "expr": "spatial", "word_idx": 16023, "sentence_idx": 184, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{C}$$", "word_idx": 16030, "sentence_idx": 185, "label": "unlabeled"}, {"type": "text", "expr": "semantic", "word_idx": 16041, "sentence_idx": 186, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{G}=(\\mathcal{N},\\mathcal{E})$$", "word_idx": 16049, "sentence_idx": 187, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{N}$$", "word_idx": 16086, "sentence_idx": 188, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{E}$$", "word_idx": 16097, "sentence_idx": 189, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{N}$$", "word_idx": 16108, "sentence_idx": 190, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{N}_{r}$$", "word_idx": 16119, "sentence_idx": 191, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{N}_{c}$$", "word_idx": 16134, "sentence_idx": 192, "label": "unlabeled"}, {"type": "text", "expr": "As for  $\\mathcal{E}$ , three groups of edges are defined between nodes", "word_idx": 16149, "sentence_idx": 193, "label": "unlabeled"}, {"type": "text", "expr": " First for  $\\mathcal{N}_{r}$ , a spatial graph is used to encode spatial relationships between regions ( $\\mathcal{E}_{r{\\rightarrow}r}$ )", "word_idx": 16220, "sentence_idx": 194, "label": "unlabeled"}, {"type": "text", "expr": " Multiple types of edges are designed to characterize the relative locations", "word_idx": 16359, "sentence_idx": 195, "label": "unlabeled"}, {"type": "text", "expr": " We begin with basic relationships such as \u201cleft/right\u201d, \u201ctop/bottom\u201d and we define edge weights by measuring the pixel-level distances between the two", "word_idx": 16435, "sentence_idx": 196, "label": "unlabeled"}, {"type": "text", "expr": " Note that we do not use the raw distance  $x$  directly, but instead normalizing it to  $[0,1]$  with a kernel  $\\kappa(x){=}\\exp(-x/\\Delta)$  (where  $\\Delta{=}50$  is the bandwidth), with the intuition that closer regions are more correlated", "word_idx": 16586, "sentence_idx": 197, "label": "unlabeled"}, {"type": "text", "expr": " The edge weights are then used directly in the adjacency matrix of the graph", "word_idx": 16830, "sentence_idx": 198, "label": "unlabeled"}, {"type": "text", "expr": " Additionally, we include edges to encode the coverage patterns ( \\eg intersection over union, IoU\u00a0 ), which can be especially helpful when two regions overlap", "word_idx": 16907, "sentence_idx": 199, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{E}$$", "word_idx": 17066, "sentence_idx": 200, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{N}_{r}$$", "word_idx": 17077, "sentence_idx": 201, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{E}_{r{\\rightarrow}r}$$", "word_idx": 17092, "sentence_idx": 202, "label": "unlabeled"}, {"type": "math", "expr": "$$[0,1]$$", "word_idx": 17121, "sentence_idx": 203, "label": "unlabeled"}, {"type": "math", "expr": "$$\\kappa(x){=}\\exp(-x/\\Delta)$$", "word_idx": 17126, "sentence_idx": 204, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Delta{=}50$$", "word_idx": 17153, "sentence_idx": 205, "label": "unlabeled"}, {"type": "text", "expr": "A second group of edges lie between regions and classes, where the assignment for a region to a class takes place", "word_idx": 17164, "sentence_idx": 206, "label": "unlabeled"}, {"type": "text", "expr": " Such edges shoulder the responsibility of propagating beliefs from region to class ( $e_{r{\\rightarrow}c}$ ) or backwards from class to region ( $e_{c{\\rightarrow}r}$ )", "word_idx": 17277, "sentence_idx": 207, "label": "unlabeled"}, {"type": "text", "expr": " Rather than only linking to the most confident class, we choose full soft-max score  $p$  to define the edge weights of connections to all classes", "word_idx": 17446, "sentence_idx": 208, "label": "unlabeled"}, {"type": "text", "expr": " The hope that it can deliver more information and thus is more robust to false assignments", "word_idx": 17593, "sentence_idx": 209, "label": "unlabeled"}, {"type": "math", "expr": "$$e_{r{\\rightarrow}c}$$", "word_idx": 17684, "sentence_idx": 210, "label": "unlabeled"}, {"type": "math", "expr": "$$e_{c{\\rightarrow}r}$$", "word_idx": 17703, "sentence_idx": 211, "label": "unlabeled"}, {"type": "text", "expr": "Semantic relationships from knowledge bases are used to construct the third group of edges between classes ( $\\mathcal{E}_{c{\\rightarrow}c}$ )", "word_idx": 17722, "sentence_idx": 212, "label": "unlabeled"}, {"type": "text", "expr": " Again, multiple types of edges can be included here", "word_idx": 17864, "sentence_idx": 213, "label": "unlabeled"}, {"type": "text", "expr": " Classical examples are \u201cis-kind-of\u201d ( \\eg between \u201ccake\u201d and \u201cfood\u201d), \u201cis-part-of\u201d ( \\eg between \u201cwheel\u201d and \u201ccar\u201d), \u201csimilarity\u201d ( \\eg between \u201cleopard\u201d and \u201ccheetah\u201d), many of which are universally true and are thus regarded as commonsense knowledge for humans", "word_idx": 17916, "sentence_idx": 214, "label": "unlabeled"}, {"type": "text", "expr": " Such commonsense can be either manually listed\u00a0  or automatically collected\u00a0 ", "word_idx": 18179, "sentence_idx": 215, "label": "unlabeled"}, {"type": "text", "expr": " Interestingly, even relationships beyond these ( \\eg actions, prepositions) can help recognition\u00a0 ", "word_idx": 18257, "sentence_idx": 216, "label": "unlabeled"}, {"type": "text", "expr": " Take \u201cperson ride bike\u201d as an example, which is apparantly more of an image-specific relationship", "word_idx": 18356, "sentence_idx": 217, "label": "unlabeled"}, {"type": "text", "expr": " However, given less confident predictions of \u201cperson\u201d and \u201cbike\u201d, knowing the relationship \u201cride\u201d along with the spatial configurations of the two can also help prune other spurious explanations", "word_idx": 18454, "sentence_idx": 218, "label": "unlabeled"}, {"type": "text", "expr": " To study both cases, we experimented with two knowledge graphs in this paper: one created in-house with mostly commonsense edges, and the other also includes more types of relationships accumulated at a large-scale", "word_idx": 18649, "sentence_idx": 219, "label": "unlabeled"}, {"type": "text", "expr": " For the actual graphs used in our experiments, please see Sec", "word_idx": 18864, "sentence_idx": 220, "label": "unlabeled"}, {"type": "text", "expr": "1  for more details", "word_idx": 18926, "sentence_idx": 221, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{E}_{c{\\rightarrow}c}$$", "word_idx": 18945, "sentence_idx": 222, "label": "unlabeled"}, {"type": "text", "expr": "Now we are ready to describe the graph-based reasoning module  $\\mathcal{R}$ ", "word_idx": 18974, "sentence_idx": 223, "label": "unlabeled"}, {"type": "text", "expr": " As the input to our graph, we use  $M_{r}{\\in}\\mathbb{R}^{R\\times D}$  to denote the features from all the region nodes  $\\mathcal{N}_{r}$  combined, where  $D$  ( ${=}512$ ) is the number of feature channels", "word_idx": 19051, "sentence_idx": 224, "label": "unlabeled"}, {"type": "text", "expr": " For each class node  $n_{c}$ , we choose off-the-shelf word vectors\u00a0  as a convenient representation, denoted as  $M_{c}{\\in}\\mathbb{R}^{C\\times D}$ ", "word_idx": 19260, "sentence_idx": 225, "label": "unlabeled"}, {"type": "text", "expr": " We then extend previous works\u00a0  and pass messages directly on  $\\mathcal{G}$  (See Fig", "word_idx": 19410, "sentence_idx": 226, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 3 )", "word_idx": 19497, "sentence_idx": 227, "label": "unlabeled"}, {"type": "text", "expr": " Note that, because our end-goal is to recognize regions better, all the class nodes should only be used as intermediate \u201chops\u201d for better region representations", "word_idx": 19502, "sentence_idx": 228, "label": "unlabeled"}, {"type": "text", "expr": " With this insight, we design two reasoning paths to learn the output features  $G_{r}$ : a  spatial  path on which only region nodes are involved:", "word_idx": 19663, "sentence_idx": 229, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{R}$$", "word_idx": 19810, "sentence_idx": 230, "label": "unlabeled"}, {"type": "math", "expr": "$$M_{r}{\\in}\\mathbb{R}^{R\\times D}$$", "word_idx": 19821, "sentence_idx": 231, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{N}_{r}$$", "word_idx": 19853, "sentence_idx": 232, "label": "unlabeled"}, {"type": "math", "expr": "$${=}512$$", "word_idx": 19868, "sentence_idx": 233, "label": "unlabeled"}, {"type": "math", "expr": "$$n_{c}$$", "word_idx": 19874, "sentence_idx": 234, "label": "unlabeled"}, {"type": "math", "expr": "$$M_{c}{\\in}\\mathbb{R}^{C\\times D}$$", "word_idx": 19879, "sentence_idx": 235, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{G}$$", "word_idx": 19911, "sentence_idx": 236, "label": "unlabeled"}, {"type": "math", "expr": "$$G_{r}$$", "word_idx": 19922, "sentence_idx": 237, "label": "unlabeled"}, {"type": "text", "expr": "spatial", "word_idx": 19927, "sentence_idx": 238, "label": "unlabeled"}, {"type": "text", "expr": "$G^{spatial}_{r}=\\sum_{e{\\in}\\mathcal{E}_{r{\\rightarrow}r}}{A_{e}M_{r}W_{e}},$", "word_idx": 19934, "sentence_idx": 239, "label": "unlabeled"}, {"type": "math", "expr": "$$G^{spatial}_{r}=\\sum_{e{\\in}\\mathcal{E}_{r{\\rightarrow}r}}{A_{e}M_{r}W_{e}},$$", "word_idx": 20012, "sentence_idx": 240, "label": "unlabeled"}, {"type": "text", "expr": "where  $A_{e}{\\in}\\mathbb{R}^{r\\times r}$  is the adjacency matrix of edge type  $e$ ,  $W_{e}{\\in}\\mathbb{R}^{d\\times d}$  is weight (bias is ignored for simplicity)", "word_idx": 20088, "sentence_idx": 241, "label": "unlabeled"}, {"type": "text", "expr": " The second reasoning path is a  semantic  one through class nodes:", "word_idx": 20254, "sentence_idx": 242, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{e}{\\in}\\mathbb{R}^{r\\times r}$$", "word_idx": 20321, "sentence_idx": 243, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{e}{\\in}\\mathbb{R}^{d\\times d}$$", "word_idx": 20353, "sentence_idx": 244, "label": "unlabeled"}, {"type": "text", "expr": "semantic", "word_idx": 20385, "sentence_idx": 245, "label": "unlabeled"}, {"type": "text", "expr": "$G^{semantic}_{c}=\\sum_{e{\\in}\\mathcal{E}_{c{\\rightarrow}c}}{A_{e}\\sigma(A_{e_{%\nr{\\rightarrow}c}}M_{r}W_{e_{r{\\rightarrow}c}}+M_{c}W_{c})W_{e}},$", "word_idx": 20393, "sentence_idx": 246, "label": "unlabeled"}, {"type": "math", "expr": "$$G^{semantic}_{c}=\\sum_{e{\\in}\\mathcal{E}_{c{\\rightarrow}c}}{A_{e}\\sigma(A_{e_{%\nr{\\rightarrow}c}}M_{r}W_{e_{r{\\rightarrow}c}}+M_{c}W_{c})W_{e}},$$", "word_idx": 20539, "sentence_idx": 247, "label": "unlabeled"}, {"type": "text", "expr": "where we first map regions to classes through  $A_{e_{r{\\rightarrow}c}}$  and  $W_{e_{r{\\rightarrow}c}}$ , combine the intermediate features with class features  $M_{c}$ , and again aggregate features from multiple types of edges between classes", "word_idx": 20683, "sentence_idx": 248, "label": "unlabeled"}, {"type": "text", "expr": "\nFinally, the output for regions  $G_{r}$  are computed by merging these two paths:", "word_idx": 20928, "sentence_idx": 249, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{e_{r{\\rightarrow}c}}$$", "word_idx": 21011, "sentence_idx": 250, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{e_{r{\\rightarrow}c}}$$", "word_idx": 21034, "sentence_idx": 251, "label": "unlabeled"}, {"type": "math", "expr": "$$M_{c}$$", "word_idx": 21057, "sentence_idx": 252, "label": "unlabeled"}, {"type": "math", "expr": "$$G_{r}$$", "word_idx": 21062, "sentence_idx": 253, "label": "unlabeled"}, {"type": "text", "expr": "$G_{r}=\\sigma(G^{spatial}_{r}+\\sigma(A_{e_{c{\\rightarrow}r}}G^{semantic}_{c}W_{%\ne_{c{\\rightarrow}r}})),$", "word_idx": 21067, "sentence_idx": 254, "label": "unlabeled"}, {"type": "math", "expr": "$$G_{r}=\\sigma(G^{spatial}_{r}+\\sigma(A_{e_{c{\\rightarrow}r}}G^{semantic}_{c}W_{%\ne_{c{\\rightarrow}r}})),$$", "word_idx": 21172, "sentence_idx": 255, "label": "unlabeled"}, {"type": "text", "expr": "which first propagates semantic information back to regions, and then applies non-linear activation (See Fig", "word_idx": 21275, "sentence_idx": 256, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 4 )", "word_idx": 21383, "sentence_idx": 257, "label": "unlabeled"}, {"type": "text", "expr": "Just like convolution filters, the above-described paths can also be stacked, where the output  $G_{r}$  can go through another set of graph operations \u2013 allowing the framework to perform joint spatial-semantic reasoning with deeper features", "word_idx": 21388, "sentence_idx": 258, "label": "unlabeled"}, {"type": "text", "expr": " We use three stacks of operations with residual connections\u00a0  in  $\\mathcal{R}$ , before the output is fed to predict", "word_idx": 21629, "sentence_idx": 259, "label": "unlabeled"}, {"type": "math", "expr": "$$G_{r}$$", "word_idx": 21747, "sentence_idx": 260, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{R}$$", "word_idx": 21752, "sentence_idx": 261, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:", "word_idx": 21763, "sentence_idx": 262, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:", "word_idx": 21772, "sentence_idx": 263, "label": "unlabeled"}, {"type": "text", "expr": "Two reasoning paths used in our global reasoning module  $\\mathcal{R}$ ", "word_idx": 21781, "sentence_idx": 264, "label": "unlabeled"}, {"type": "text", "expr": " Taking the region and class inputs  $M_{r}$  and  $M_{c}$ , the spatial path directly passes information in the region graph with region-to-region edges  $\\mathcal{E}_{r{\\rightarrow}r}$ , whereas the semantic path first assigns regions to classes with  $e_{r{\\rightarrow}c}$ , passes the information on to other classes with class-to-class edges  $\\mathcal{E}_{c{\\rightarrow}c}$ , and then propagates back", "word_idx": 21852, "sentence_idx": 265, "label": "unlabeled"}, {"type": "text", "expr": " Final outputs are combined to generate output region features  $G_{r}$ ", "word_idx": 22258, "sentence_idx": 266, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{R}$$", "word_idx": 22330, "sentence_idx": 267, "label": "unlabeled"}, {"type": "math", "expr": "$$M_{r}$$", "word_idx": 22341, "sentence_idx": 268, "label": "unlabeled"}, {"type": "math", "expr": "$$M_{c}$$", "word_idx": 22346, "sentence_idx": 269, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{E}_{r{\\rightarrow}r}$$", "word_idx": 22351, "sentence_idx": 270, "label": "unlabeled"}, {"type": "math", "expr": "$$e_{r{\\rightarrow}c}$$", "word_idx": 22380, "sentence_idx": 271, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{E}_{c{\\rightarrow}c}$$", "word_idx": 22399, "sentence_idx": 272, "label": "unlabeled"}, {"type": "math", "expr": "$$G_{r}$$", "word_idx": 22428, "sentence_idx": 273, "label": "unlabeled"}, {"type": "text", "expr": "3  Iterative Reasoning", "word_idx": 22433, "sentence_idx": 274, "label": "unlabeled"}, {"type": "text", "expr": "A key ingredient of reasoning is to iteratively build up estimates", "word_idx": 22455, "sentence_idx": 275, "label": "unlabeled"}, {"type": "text", "expr": " But how does information pass from one iteration to another? Our answer is  explicit  memory, which stores all the history from previous iterations", "word_idx": 22521, "sentence_idx": 276, "label": "unlabeled"}, {"type": "text", "expr": " The local module uses spatial memory  $\\mathcal{S}$ , and the global module uses another memory  $\\mathcal{M}$  but without spatial structures", "word_idx": 22669, "sentence_idx": 277, "label": "unlabeled"}, {"type": "text", "expr": " At iteration  $i$ ,  $\\mathcal{S}_{i}$  is followed by convolutional reasoning module  $\\mathcal{C}$  to generate new predictions  $f_{i}^{l}$  for each region", "word_idx": 22812, "sentence_idx": 278, "label": "unlabeled"}, {"type": "text", "expr": " Similarly, global module also gives new predictions  $f_{i}^{g}$  from  $\\mathcal{R}$ ", "word_idx": 22972, "sentence_idx": 279, "label": "unlabeled"}, {"type": "text", "expr": " These new predictions as high-level features can then be used to get the updated memories  $\\mathcal{S}_{i+1}$  and  $\\mathcal{M}_{i+1}$ ", "word_idx": 23059, "sentence_idx": 280, "label": "unlabeled"}, {"type": "text", "expr": " The new memories will lead to another round of updated  $f_{i+1}$ s and the iteration goes on", "word_idx": 23197, "sentence_idx": 281, "label": "unlabeled"}, {"type": "text", "expr": "explicit", "word_idx": 23291, "sentence_idx": 282, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{S}$$", "word_idx": 23299, "sentence_idx": 283, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{M}$$", "word_idx": 23310, "sentence_idx": 284, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{S}_{i}$$", "word_idx": 23321, "sentence_idx": 285, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{C}$$", "word_idx": 23336, "sentence_idx": 286, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{i}^{l}$$", "word_idx": 23347, "sentence_idx": 287, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{i}^{g}$$", "word_idx": 23356, "sentence_idx": 288, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{R}$$", "word_idx": 23365, "sentence_idx": 289, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{S}_{i+1}$$", "word_idx": 23376, "sentence_idx": 290, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{M}_{i+1}$$", "word_idx": 23393, "sentence_idx": 291, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{i+1}$$", "word_idx": 23410, "sentence_idx": 292, "label": "unlabeled"}, {"type": "text", "expr": "While one can do local and global reasoning in isolation, both the modules work best in conjunction", "word_idx": 23417, "sentence_idx": 293, "label": "unlabeled"}, {"type": "text", "expr": " Therefore, for our full pipeline we want to join force of both modules when generating the predictions", "word_idx": 23516, "sentence_idx": 294, "label": "unlabeled"}, {"type": "text", "expr": " To this end, we introduce  cross-feed  connections", "word_idx": 23619, "sentence_idx": 295, "label": "unlabeled"}, {"type": "text", "expr": " After reasoning, both the local and global features are then concatenated together to update the memories  $\\mathcal{S}_{i+1}$  and  $\\mathcal{M}_{i+1}$  using GRU", "word_idx": 23670, "sentence_idx": 296, "label": "unlabeled"}, {"type": "text", "expr": " In this way, spatial memory can benefit from global knowledge of spatial and semantic relationships, and graph can get a better sense of the local region layouts", "word_idx": 23834, "sentence_idx": 297, "label": "unlabeled"}, {"type": "text", "expr": "cross-feed", "word_idx": 23996, "sentence_idx": 298, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{S}_{i+1}$$", "word_idx": 24006, "sentence_idx": 299, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{M}_{i+1}$$", "word_idx": 24023, "sentence_idx": 300, "label": "unlabeled"}, {"type": "text", "expr": "4  Attention", "word_idx": 24040, "sentence_idx": 301, "label": "unlabeled"}, {"type": "text", "expr": "Inspired from the recent work on attention\u00a0 , we make another modification at the model output", "word_idx": 24052, "sentence_idx": 302, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, instead of only generating scores  $f$ , the model also has to produce an \u201cattention\u201d value  $a$  that denotes the relative confidence of the current prediction compared to the ones from other iterations or modules", "word_idx": 24146, "sentence_idx": 303, "label": "unlabeled"}, {"type": "text", "expr": " Then the fused output is a weighted version of all predictions using attentions", "word_idx": 24375, "sentence_idx": 304, "label": "unlabeled"}, {"type": "text", "expr": " Mathematically, if the model roll-outs  $I$  times, and outputs  $N{=}2I{+}1$  (including  $I$  local,  $I$  global and  $1$  from plain ConvNet) predictions  $f_{n}$ , using attentions  $a_{n}$ , the final output  $f$  is calculated as:", "word_idx": 24455, "sentence_idx": 305, "label": "unlabeled"}, {"type": "math", "expr": "$$N{=}2I{+}1$$", "word_idx": 24693, "sentence_idx": 306, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{n}$$", "word_idx": 24703, "sentence_idx": 307, "label": "unlabeled"}, {"type": "math", "expr": "$$a_{n}$$", "word_idx": 24708, "sentence_idx": 308, "label": "unlabeled"}, {"type": "text", "expr": "$f=\\sum_{n}{w_{n}f_{n}},\\quad\\mathrm{where}\\quad w_{n}=\\frac{\\exp(-a_{n})}{\\sum%\n_{n^{\\prime}}{\\exp(-a_{n^{\\prime}})}}$", "word_idx": 24713, "sentence_idx": 309, "label": "unlabeled"}, {"type": "math", "expr": "$$f=\\sum_{n}{w_{n}f_{n}},\\quad\\mathrm{where}\\quad w_{n}=\\frac{\\exp(-a_{n})}{\\sum%\n_{n^{\\prime}}{\\exp(-a_{n^{\\prime}})}}.$$", "word_idx": 24832, "sentence_idx": 310, "label": "unlabeled"}, {"type": "text", "expr": "Note again that here  $f_{n}$  is the logits before soft-max, which is then activated to produce  $p_{n}$ ", "word_idx": 24950, "sentence_idx": 311, "label": "unlabeled"}, {"type": "text", "expr": " The introduction of attention allows the model to intelligently choose feasible predictions from different modules and iterations", "word_idx": 25056, "sentence_idx": 312, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{n}$$", "word_idx": 25186, "sentence_idx": 313, "label": "unlabeled"}, {"type": "math", "expr": "$$p_{n}$$", "word_idx": 25191, "sentence_idx": 314, "label": "unlabeled"}, {"type": "text", "expr": "5  Training", "word_idx": 25196, "sentence_idx": 315, "label": "unlabeled"}, {"type": "text", "expr": "Finally, the overall framework is trained end-to-end, with a total loss function consists of: a) plain ConvNet loss  $\\mathcal{L}_{0}$ ; b) local module loss  $\\mathcal{L}^{l}_{i}$ ; c) global module loss  $\\mathcal{L}^{g}_{i}$ ; and d) the final prediction loss with attentions  $\\mathcal{L}_{f}$ ", "word_idx": 25207, "sentence_idx": 316, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}_{0}$$", "word_idx": 25505, "sentence_idx": 317, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}^{l}_{i}$$", "word_idx": 25520, "sentence_idx": 318, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}^{g}_{i}$$", "word_idx": 25539, "sentence_idx": 319, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}_{f}$$", "word_idx": 25558, "sentence_idx": 320, "label": "unlabeled"}, {"type": "text", "expr": "Since we want our reasoning modules to focus more on the harder examples, we propose to simply  re-weight  the examples in the loss, based on predictions from previous iterations", "word_idx": 25573, "sentence_idx": 321, "label": "unlabeled"}, {"type": "text", "expr": " Formally, for region  $r$  at iteration  $i{\\geq}1$ , the cross-entropy loss for both modules is computed as:", "word_idx": 25751, "sentence_idx": 322, "label": "unlabeled"}, {"type": "text", "expr": "re-weight", "word_idx": 25861, "sentence_idx": 323, "label": "unlabeled"}, {"type": "math", "expr": "$$i{\\geq}1$$", "word_idx": 25870, "sentence_idx": 324, "label": "unlabeled"}, {"type": "text", "expr": "$\\mathcal{L}_{i}(r)=\\frac{\\max(1-p_{i-1}(r),\\beta)}{\\sum_{r^{\\prime}}\\max(1-p%\n_{i-1}(r^{\\prime}),\\beta)}\\log(p_{i}(r)),$", "word_idx": 25878, "sentence_idx": 325, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}_{i}(r)=\\frac{\\max(1.-p_{i-1}(r),\\beta)}{\\sum_{r^{\\prime}}\\max(1.-p%\n_{i-1}(r^{\\prime}),\\beta)}\\log(p_{i}(r)),$$", "word_idx": 25999, "sentence_idx": 326, "label": "unlabeled"}, {"type": "text", "expr": "where  $p_{i}(r)$  is the soft-max output of the ground-truth class, and  $\\beta{\\in}[0,1]$  controls the entropy of the weight distribution: when  $\\beta{=}1$ , it is uniform distribution; and when  $\\beta{=}0$ , entropy is minimized", "word_idx": 26120, "sentence_idx": 327, "label": "unlabeled"}, {"type": "text", "expr": " In our experiments,  $\\beta$  is set to  $05$ ", "word_idx": 26354, "sentence_idx": 328, "label": "unlabeled"}, {"type": "text", "expr": "  $p_{i-1}(r)$  is used as features without back-propagation", "word_idx": 26401, "sentence_idx": 329, "label": "unlabeled"}, {"type": "text", "expr": " For both local and global,  $p_{0}(r)$  is the output from the plain ConvNet", "word_idx": 26461, "sentence_idx": 330, "label": "unlabeled"}, {"type": "math", "expr": "$$p_{i}(r)$$", "word_idx": 26538, "sentence_idx": 331, "label": "unlabeled"}, {"type": "math", "expr": "$$\\beta{\\in}[0,1]$$", "word_idx": 26546, "sentence_idx": 332, "label": "unlabeled"}, {"type": "math", "expr": "$$\\beta{=}1$$", "word_idx": 26561, "sentence_idx": 333, "label": "unlabeled"}, {"type": "math", "expr": "$$\\beta{=}0$$", "word_idx": 26570, "sentence_idx": 334, "label": "unlabeled"}, {"type": "math", "expr": "$$\\beta$$", "word_idx": 26579, "sentence_idx": 335, "label": "unlabeled"}, {"type": "math", "expr": "$$0.5$$", "word_idx": 26584, "sentence_idx": 336, "label": "unlabeled"}, {"type": "math", "expr": "$$p_{i-1}(r)$$", "word_idx": 26587, "sentence_idx": 337, "label": "unlabeled"}, {"type": "math", "expr": "$$p_{0}(r)$$", "word_idx": 26597, "sentence_idx": 338, "label": "unlabeled"}, {"type": "text", "expr": "4  Experiments", "word_idx": 26605, "sentence_idx": 339, "label": "unlabeled"}, {"type": "text", "expr": "In this section we evaluate the effectiveness of our framework", "word_idx": 26619, "sentence_idx": 340, "label": "unlabeled"}, {"type": "text", "expr": " We begin with our experimental setups, which includes the datasets to work with (Sec", "word_idx": 26681, "sentence_idx": 341, "label": "unlabeled"}, {"type": "text", "expr": "1 ), the task to evaluate on (Sec", "word_idx": 26766, "sentence_idx": 342, "label": "unlabeled"}, {"type": "text", "expr": "2 ) and details of our implementation (Sec", "word_idx": 26799, "sentence_idx": 343, "label": "unlabeled"}, {"type": "text", "expr": " We discuss our results and analyze them in Sec", "word_idx": 26841, "sentence_idx": 344, "label": "unlabeled"}, {"type": "text", "expr": "4  and Sec", "word_idx": 26888, "sentence_idx": 345, "label": "unlabeled"}, {"type": "text", "expr": "5  respectively", "word_idx": 26898, "sentence_idx": 346, "label": "unlabeled"}, {"type": "text", "expr": "1  Datasets and Graphs", "word_idx": 26913, "sentence_idx": 347, "label": "unlabeled"}, {"type": "text", "expr": "Datasets are biased\u00a0 ", "word_idx": 26935, "sentence_idx": 348, "label": "unlabeled"}, {"type": "text", "expr": " For context reasoning we would naturally like to have scene-focused datasets\u00a0  as opposed to object-focused ones\u00a0 ", "word_idx": 26956, "sentence_idx": 349, "label": "unlabeled"}, {"type": "text", "expr": " To showcase the capabilities of our system, we need densely labeled dataset with a large number of classes", "word_idx": 27071, "sentence_idx": 350, "label": "unlabeled"}, {"type": "text", "expr": " Finally, one benefit of using knowledge graph is to transfer across classes, therefore a dataset with  long-tail  distribution is an ideal test-bed", "word_idx": 27178, "sentence_idx": 351, "label": "unlabeled"}, {"type": "text", "expr": " Satisfying all these constraints, ADE\u00a0  and Visual Genome (VG)\u00a0  where regions are densely labeled in open vocabulary are the main picks of our study", "word_idx": 27326, "sentence_idx": 352, "label": "unlabeled"}, {"type": "text", "expr": "long-tail", "word_idx": 27476, "sentence_idx": 353, "label": "unlabeled"}, {"type": "text", "expr": "For ADE, we use the publicly released training set ( $20,210$ ) images for training, and split the validation set ( $2,000$  images) into  val-1k  and  test-1k  with  $1,000$  images each", "word_idx": 27485, "sentence_idx": 354, "label": "unlabeled"}, {"type": "text", "expr": " The original raw names are used due to a more detailed categorization\u00a0 ", "word_idx": 27672, "sentence_idx": 355, "label": "unlabeled"}, {"type": "text", "expr": " We filter out classes with less than five instances, which leaves us with  $1,484$  classes", "word_idx": 27744, "sentence_idx": 356, "label": "unlabeled"}, {"type": "text", "expr": " With the help of parts annotations in the dataset, a commonsense knowledge graph is created with five types of edges between classes: a) \u201cis-part-of\u201d ( \\eg \u201cleg\u201d and \u201cchair\u201d); b) \u201cis-kind-of\u201d ( \\eg \u201cjacket\u201d and \u201cclothes\u201d); c) \u201cplural-form\u201d ( \\eg \u201ctree\u201d and \u201ctrees\u201d); d) \u201chorizontal-symmetry\u201d ( \\eg \u201cleft-arm\u201d and \u201cright-arm\u201d); e) \u201csimilarity\u201d ( \\eg \u201chandle\u201d and \u201cknob\u201d)", "word_idx": 27836, "sentence_idx": 357, "label": "unlabeled"}, {"type": "text", "expr": " Notice that the first four types are directed edges, hence we also include their inverted versions", "word_idx": 28206, "sentence_idx": 358, "label": "unlabeled"}, {"type": "math", "expr": "$$20,210$$", "word_idx": 28305, "sentence_idx": 359, "label": "unlabeled"}, {"type": "math", "expr": "$$2,000$$", "word_idx": 28311, "sentence_idx": 360, "label": "unlabeled"}, {"type": "text", "expr": "val-1k", "word_idx": 28316, "sentence_idx": 361, "label": "unlabeled"}, {"type": "text", "expr": "test-1k", "word_idx": 28322, "sentence_idx": 362, "label": "unlabeled"}, {"type": "math", "expr": "$$1,000$$", "word_idx": 28329, "sentence_idx": 363, "label": "unlabeled"}, {"type": "math", "expr": "$$1,484$$", "word_idx": 28334, "sentence_idx": 364, "label": "unlabeled"}, {"type": "text", "expr": "For VG, the latest release (v $14$ ) is used", "word_idx": 28339, "sentence_idx": 365, "label": "unlabeled"}, {"type": "text", "expr": " We split the entire set of  $108,077$  images into  $100$ K,  $4,077$  and  $4$ K as  train ,  val  and  test  set", "word_idx": 28383, "sentence_idx": 366, "label": "unlabeled"}, {"type": "text", "expr": " Similar pre-processing is done on VG, except that we use synsets\u00a0  instead of raw names due to less consistent labels from multiple annotators", "word_idx": 28498, "sentence_idx": 367, "label": "unlabeled"}, {"type": "text", "expr": "  $3,993$  classes are used", "word_idx": 28641, "sentence_idx": 368, "label": "unlabeled"}, {"type": "text", "expr": " For knowledge graph between classes, we take advantage of the relationship annotations in the set, and select the top  $10$  most frequent relationships to automatically construct edges beyond commonsense relationships constructed for ADE", "word_idx": 28668, "sentence_idx": 369, "label": "unlabeled"}, {"type": "text", "expr": " For each type of relationships, the edge weights are normalized so that each row of the adjacency matrix is summed-up to one", "word_idx": 28907, "sentence_idx": 370, "label": "unlabeled"}, {"type": "text", "expr": " While this approach results in a noisier graph, it also allows us to demonstrate that our approach is scalable and robust to noise", "word_idx": 29032, "sentence_idx": 371, "label": "unlabeled"}, {"type": "math", "expr": "$$1.4$$", "word_idx": 29163, "sentence_idx": 372, "label": "unlabeled"}, {"type": "math", "expr": "$$108,077$$", "word_idx": 29166, "sentence_idx": 373, "label": "unlabeled"}, {"type": "math", "expr": "$$100$$", "word_idx": 29173, "sentence_idx": 374, "label": "unlabeled"}, {"type": "math", "expr": "$$4,077$$", "word_idx": 29176, "sentence_idx": 375, "label": "unlabeled"}, {"type": "text", "expr": "train", "word_idx": 29181, "sentence_idx": 376, "label": "unlabeled"}, {"type": "math", "expr": "$$3,993$$", "word_idx": 29186, "sentence_idx": 377, "label": "unlabeled"}, {"type": "text", "expr": "Finally, we also show experiments on COCO\u00a0 ", "word_idx": 29191, "sentence_idx": 378, "label": "unlabeled"}, {"type": "text", "expr": " However, since it is detection oriented \u2013 has only  $80$  classes picked to be mutually-exclusive, and covers less percentage of labeled pixels, we only report results a) without the knowledge graph and b) without a test split ( trainval35k \u00a0  for training and  minival  for evaluation)", "word_idx": 29234, "sentence_idx": 379, "label": "unlabeled"}, {"type": "text", "expr": " This setup is for analysis purposes only", "word_idx": 29521, "sentence_idx": 380, "label": "unlabeled"}, {"type": "text", "expr": "trainval35k", "word_idx": 29562, "sentence_idx": 381, "label": "unlabeled"}, {"type": "text", "expr": "minival", "word_idx": 29573, "sentence_idx": 382, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:  Main results on ADE  test-1k  and VG  test ", "word_idx": 29580, "sentence_idx": 383, "label": "unlabeled"}, {"type": "text", "expr": " AP is average precision, AC is classification accuracy", "word_idx": 29633, "sentence_idx": 384, "label": "unlabeled"}, {"type": "text", "expr": " Superscripts show the improvement  $\\nabla$  over the baseline", "word_idx": 29688, "sentence_idx": 385, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:", "word_idx": 29751, "sentence_idx": 386, "label": "unlabeled"}, {"type": "text", "expr": "test-1k", "word_idx": 29759, "sentence_idx": 387, "label": "unlabeled"}, {"type": "math", "expr": "$$\\nabla$$", "word_idx": 29766, "sentence_idx": 388, "label": "unlabeled"}, {"type": "text", "expr": "per-instance per-class", "word_idx": 29772, "sentence_idx": 389, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\Xhline 1pt\n %", "word_idx": 29794, "sentence_idx": 390, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 29824, "sentence_idx": 391, "label": "unlabeled"}, {"type": "text", "expr": "\\Xhline", "word_idx": 29839, "sentence_idx": 392, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash Method", "word_idx": 29846, "sentence_idx": 393, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 29868, "sentence_idx": 394, "label": "unlabeled"}, {"type": "text", "expr": "Method", "word_idx": 29883, "sentence_idx": 395, "label": "unlabeled"}, {"type": "text", "expr": "per-instance", "word_idx": 29889, "sentence_idx": 396, "label": "unlabeled"}, {"type": "text", "expr": "per-class", "word_idx": 29901, "sentence_idx": 397, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 29910, "sentence_idx": 398, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 29925, "sentence_idx": 399, "label": "unlabeled"}, {"type": "math", "expr": "$$\\nabla$$", "word_idx": 29940, "sentence_idx": 400, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 29946, "sentence_idx": 401, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 29961, "sentence_idx": 402, "label": "unlabeled"}, {"type": "math", "expr": "$$\\nabla$$", "word_idx": 29976, "sentence_idx": 403, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 29982, "sentence_idx": 404, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 29997, "sentence_idx": 405, "label": "unlabeled"}, {"type": "math", "expr": "$$\\nabla$$", "word_idx": 30012, "sentence_idx": 406, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30018, "sentence_idx": 407, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30033, "sentence_idx": 408, "label": "unlabeled"}, {"type": "math", "expr": "$$\\nabla$$", "word_idx": 30048, "sentence_idx": 409, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\Xhline 1pt", "word_idx": 30054, "sentence_idx": 410, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30081, "sentence_idx": 411, "label": "unlabeled"}, {"type": "text", "expr": "\\Xhline", "word_idx": 30096, "sentence_idx": 412, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash Baseline", "word_idx": 30103, "sentence_idx": 413, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30127, "sentence_idx": 414, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 67", "word_idx": 30142, "sentence_idx": 415, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30160, "sentence_idx": 416, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 67", "word_idx": 30175, "sentence_idx": 417, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30193, "sentence_idx": 418, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 40", "word_idx": 30208, "sentence_idx": 419, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30226, "sentence_idx": 420, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 33", "word_idx": 30241, "sentence_idx": 421, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30259, "sentence_idx": 422, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \u00a0\u00a0\u00a0\u00a0 w/ ResNet-101", "word_idx": 30274, "sentence_idx": 423, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30308, "sentence_idx": 424, "label": "unlabeled"}, {"type": "text", "expr": "w/ ResNet-101", "word_idx": 30323, "sentence_idx": 425, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 68", "word_idx": 30336, "sentence_idx": 426, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30354, "sentence_idx": 427, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 68", "word_idx": 30369, "sentence_idx": 428, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30387, "sentence_idx": 429, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 40", "word_idx": 30402, "sentence_idx": 430, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30420, "sentence_idx": 431, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 34", "word_idx": 30435, "sentence_idx": 432, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30453, "sentence_idx": 433, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30468, "sentence_idx": 434, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30483, "sentence_idx": 435, "label": "unlabeled"}, {"type": "text", "expr": "w/  $800$ -input", "word_idx": 30498, "sentence_idx": 436, "label": "unlabeled"}, {"type": "math", "expr": "$$800$$", "word_idx": 30514, "sentence_idx": 437, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 68", "word_idx": 30517, "sentence_idx": 438, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30535, "sentence_idx": 439, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 68", "word_idx": 30550, "sentence_idx": 440, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30568, "sentence_idx": 441, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 41", "word_idx": 30583, "sentence_idx": 442, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30601, "sentence_idx": 443, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 34", "word_idx": 30616, "sentence_idx": 444, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30634, "sentence_idx": 445, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \u00a0\u00a0\u00a0\u00a0 Ensemble", "word_idx": 30649, "sentence_idx": 446, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30678, "sentence_idx": 447, "label": "unlabeled"}, {"type": "text", "expr": "Ensemble", "word_idx": 30693, "sentence_idx": 448, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 68", "word_idx": 30701, "sentence_idx": 449, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30719, "sentence_idx": 450, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 68", "word_idx": 30734, "sentence_idx": 451, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30752, "sentence_idx": 452, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 42", "word_idx": 30767, "sentence_idx": 453, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30785, "sentence_idx": 454, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 35", "word_idx": 30800, "sentence_idx": 455, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30818, "sentence_idx": 456, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash Ours \\textsubscript -Local", "word_idx": 30833, "sentence_idx": 457, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30875, "sentence_idx": 458, "label": "unlabeled"}, {"type": "text", "expr": "\\textsubscript", "word_idx": 30890, "sentence_idx": 459, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30904, "sentence_idx": 460, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30919, "sentence_idx": 461, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30934, "sentence_idx": 462, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30949, "sentence_idx": 463, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30964, "sentence_idx": 464, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30979, "sentence_idx": 465, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 30994, "sentence_idx": 466, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31009, "sentence_idx": 467, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash Ours \\textsubscript -Global", "word_idx": 31024, "sentence_idx": 468, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31067, "sentence_idx": 469, "label": "unlabeled"}, {"type": "text", "expr": "\\textsubscript", "word_idx": 31082, "sentence_idx": 470, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31096, "sentence_idx": 471, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31111, "sentence_idx": 472, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31126, "sentence_idx": 473, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31141, "sentence_idx": 474, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31156, "sentence_idx": 475, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31171, "sentence_idx": 476, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31186, "sentence_idx": 477, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31201, "sentence_idx": 478, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash Ours \\textsubscript -Final", "word_idx": 31216, "sentence_idx": 479, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31258, "sentence_idx": 480, "label": "unlabeled"}, {"type": "text", "expr": "\\textsubscript", "word_idx": 31273, "sentence_idx": 481, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31287, "sentence_idx": 482, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31302, "sentence_idx": 483, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31317, "sentence_idx": 484, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31332, "sentence_idx": 485, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31347, "sentence_idx": 486, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31362, "sentence_idx": 487, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31377, "sentence_idx": 488, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31392, "sentence_idx": 489, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\Xhline 0", "word_idx": 31407, "sentence_idx": 490, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31432, "sentence_idx": 491, "label": "unlabeled"}, {"type": "text", "expr": "\\Xhline", "word_idx": 31447, "sentence_idx": 492, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash Baseline", "word_idx": 31454, "sentence_idx": 493, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31478, "sentence_idx": 494, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 49", "word_idx": 31493, "sentence_idx": 495, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31511, "sentence_idx": 496, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 49", "word_idx": 31526, "sentence_idx": 497, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31544, "sentence_idx": 498, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 16", "word_idx": 31559, "sentence_idx": 499, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31577, "sentence_idx": 500, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 12", "word_idx": 31592, "sentence_idx": 501, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31610, "sentence_idx": 502, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \u00a0\u00a0\u00a0\u00a0 w/ ResNet-101", "word_idx": 31625, "sentence_idx": 503, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31659, "sentence_idx": 504, "label": "unlabeled"}, {"type": "text", "expr": "w/ ResNet-101", "word_idx": 31674, "sentence_idx": 505, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 50", "word_idx": 31687, "sentence_idx": 506, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31705, "sentence_idx": 507, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 50", "word_idx": 31720, "sentence_idx": 508, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31738, "sentence_idx": 509, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 18", "word_idx": 31753, "sentence_idx": 510, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31771, "sentence_idx": 511, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 13", "word_idx": 31786, "sentence_idx": 512, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31804, "sentence_idx": 513, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31819, "sentence_idx": 514, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31834, "sentence_idx": 515, "label": "unlabeled"}, {"type": "text", "expr": "w/  $800$ -input", "word_idx": 31849, "sentence_idx": 516, "label": "unlabeled"}, {"type": "math", "expr": "$$800$$", "word_idx": 31865, "sentence_idx": 517, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 49", "word_idx": 31868, "sentence_idx": 518, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31886, "sentence_idx": 519, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 50", "word_idx": 31901, "sentence_idx": 520, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31919, "sentence_idx": 521, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 17", "word_idx": 31934, "sentence_idx": 522, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31952, "sentence_idx": 523, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 12", "word_idx": 31967, "sentence_idx": 524, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 31985, "sentence_idx": 525, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \u00a0\u00a0\u00a0\u00a0 w/ Ensemble", "word_idx": 32000, "sentence_idx": 526, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32032, "sentence_idx": 527, "label": "unlabeled"}, {"type": "text", "expr": "w/ Ensemble", "word_idx": 32047, "sentence_idx": 528, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 50", "word_idx": 32058, "sentence_idx": 529, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32076, "sentence_idx": 530, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 50", "word_idx": 32091, "sentence_idx": 531, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32109, "sentence_idx": 532, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 17", "word_idx": 32124, "sentence_idx": 533, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32142, "sentence_idx": 534, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 12", "word_idx": 32157, "sentence_idx": 535, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32175, "sentence_idx": 536, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash Ours \\textsubscript -Local", "word_idx": 32190, "sentence_idx": 537, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32232, "sentence_idx": 538, "label": "unlabeled"}, {"type": "text", "expr": "\\textsubscript", "word_idx": 32247, "sentence_idx": 539, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32261, "sentence_idx": 540, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32276, "sentence_idx": 541, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32291, "sentence_idx": 542, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32306, "sentence_idx": 543, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32321, "sentence_idx": 544, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32336, "sentence_idx": 545, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32351, "sentence_idx": 546, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32366, "sentence_idx": 547, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash Ours \\textsubscript -Global", "word_idx": 32381, "sentence_idx": 548, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32424, "sentence_idx": 549, "label": "unlabeled"}, {"type": "text", "expr": "\\textsubscript", "word_idx": 32439, "sentence_idx": 550, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32453, "sentence_idx": 551, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32468, "sentence_idx": 552, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32483, "sentence_idx": 553, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32498, "sentence_idx": 554, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32513, "sentence_idx": 555, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32528, "sentence_idx": 556, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32543, "sentence_idx": 557, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32558, "sentence_idx": 558, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash Ours \\textsubscript -Final", "word_idx": 32573, "sentence_idx": 559, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32615, "sentence_idx": 560, "label": "unlabeled"}, {"type": "text", "expr": "\\textsubscript", "word_idx": 32630, "sentence_idx": 561, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32644, "sentence_idx": 562, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32659, "sentence_idx": 563, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32674, "sentence_idx": 564, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32689, "sentence_idx": 565, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32704, "sentence_idx": 566, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32719, "sentence_idx": 567, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32734, "sentence_idx": 568, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32749, "sentence_idx": 569, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\Xhline 1pt", "word_idx": 32764, "sentence_idx": 570, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 32791, "sentence_idx": 571, "label": "unlabeled"}, {"type": "text", "expr": "\\Xhline", "word_idx": 32806, "sentence_idx": 572, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a05:", "word_idx": 32813, "sentence_idx": 573, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a05:", "word_idx": 32822, "sentence_idx": 574, "label": "unlabeled"}, {"type": "text", "expr": "Qualitative examples from ADE  test-1k  (best if zoomed-in)", "word_idx": 32831, "sentence_idx": 575, "label": "unlabeled"}, {"type": "text", "expr": " For regions highlighted in blue, the predictions from baseline and our model are compared", "word_idx": 32890, "sentence_idx": 576, "label": "unlabeled"}, {"type": "text", "expr": " Other regions are also listed to provide the context", "word_idx": 32980, "sentence_idx": 577, "label": "unlabeled"}, {"type": "text", "expr": " For example, the \u201cright-leg\u201d is less confused with \u201cleft-leg\u201d after reasoning (top-left); the \u201cmouse\u201d on the \u201cdesk\u201d is predicted despite low resolution (top-third); and \u201cdetergent-dispenser\u201d is recognized given the context of \u201cwashing-machine\u201d (top-right)", "word_idx": 33033, "sentence_idx": 578, "label": "unlabeled"}, {"type": "text", "expr": " At bottom-right we show a failure case where context does not help \u201cremote-control\u201d, probably because it has never appeared on the \u201cnight-table\u201d before and no semantic relationship is there to help", "word_idx": 33289, "sentence_idx": 579, "label": "unlabeled"}, {"type": "text", "expr": "test-1k", "word_idx": 33487, "sentence_idx": 580, "label": "unlabeled"}, {"type": "text", "expr": "2  Task and Evaluation", "word_idx": 33494, "sentence_idx": 581, "label": "unlabeled"}, {"type": "text", "expr": "We evaluate our system on the task of region classification, where the goal is to assign labels to designated regions denoted by rectangular bounding boxes", "word_idx": 33516, "sentence_idx": 582, "label": "unlabeled"}, {"type": "text", "expr": " For both training and testing, we use provided ground-truth locations", "word_idx": 33671, "sentence_idx": 583, "label": "unlabeled"}, {"type": "text", "expr": " We picked this task for three reasons", "word_idx": 33741, "sentence_idx": 584, "label": "unlabeled"}, {"type": "text", "expr": " The  first  one is on evaluation", "word_idx": 33779, "sentence_idx": 585, "label": "unlabeled"}, {"type": "text", "expr": " As the number of classes increases in the vocabulary,  missing  labels are inevitable, which is especially severe for object parts ( \\eg \u201crim\u201d, \u201carm\u201d) and related classes ( \\eg \u201cshoes\u201d  \\vs \u201csneakers\u201d) where external knowledge is valuable", "word_idx": 33812, "sentence_idx": 586, "label": "unlabeled"}, {"type": "text", "expr": " If there are missing labels, fair evaluation becomes much more difficult since accuracy becomes impossible to evaluate \u2013 cannot tell if a prediction is wrong, or the label itself is missing", "word_idx": 34051, "sentence_idx": 587, "label": "unlabeled"}, {"type": "text", "expr": " Interestingly, such an issue also happens to other research areas ( \\eg recommendation systems\u00a0  and link prediction\u00a0 )", "word_idx": 34241, "sentence_idx": 588, "label": "unlabeled"}, {"type": "text", "expr": " Borrowing ideas from them, a practical solution is to evaluate  only  on what we already know \u2013 in our case ground-truth regions", "word_idx": 34361, "sentence_idx": 589, "label": "unlabeled"}, {"type": "text", "expr": "  Second , although region classification is a simplified version of object detection and semantic segmentation, it maintains a richer set of labels, especially including \u201cstuff\u201d classes like \u201croad\u201d, \u201csky\u201d, and object instances", "word_idx": 34490, "sentence_idx": 590, "label": "unlabeled"}, {"type": "text", "expr": " Modeling \u201cstuff-object\u201d and instance-level relationships is a crucial capability which would be missed in a pure detection/segmentation setting", "word_idx": 34717, "sentence_idx": 591, "label": "unlabeled"}, {"type": "text", "expr": "  Finally  as our experiment will show (Sec", "word_idx": 34861, "sentence_idx": 592, "label": "unlabeled"}, {"type": "text", "expr": "5 ), while object detectors can be used off-the-shelf, the additional manually defined parameters and components ( \\eg overlapping threshold for a region to be positive/negative, predefined scale/aspect ratio sets of anchors\u00a0 ) in its pipeline pose limitations on how much context can benefit", "word_idx": 34904, "sentence_idx": 593, "label": "unlabeled"}, {"type": "text", "expr": " For example, after non-maximal suppression (NMS), highly overlapping objects ( \\eg \u201cwindow\u201d and \u201cshutter\u201d) will be suppressed, and ironically this is exactly where context reasoning could have helped", "word_idx": 35196, "sentence_idx": 594, "label": "unlabeled"}, {"type": "text", "expr": " On the other hand, by feeding fixed regions directly for end-to-end learning, we can at least factorize the  recognition  error from the  localization  one\u00a0 , and get a clean focus on how context can help discriminating confusing classes", "word_idx": 35396, "sentence_idx": 595, "label": "unlabeled"}, {"type": "text", "expr": "first", "word_idx": 35634, "sentence_idx": 596, "label": "unlabeled"}, {"type": "text", "expr": "missing", "word_idx": 35639, "sentence_idx": 597, "label": "unlabeled"}, {"type": "text", "expr": "Second", "word_idx": 35646, "sentence_idx": 598, "label": "unlabeled"}, {"type": "text", "expr": "Finally", "word_idx": 35652, "sentence_idx": 599, "label": "unlabeled"}, {"type": "text", "expr": "recognition", "word_idx": 35659, "sentence_idx": 600, "label": "unlabeled"}, {"type": "text", "expr": "localization", "word_idx": 35670, "sentence_idx": 601, "label": "unlabeled"}, {"type": "text", "expr": "Since ADE is a segmentation dataset, we convert segmentation masks to bounding boxes", "word_idx": 35682, "sentence_idx": 602, "label": "unlabeled"}, {"type": "text", "expr": " For object classes ( \\eg \u201cperson\u201d), each instance is created a separate box", "word_idx": 35766, "sentence_idx": 603, "label": "unlabeled"}, {"type": "text", "expr": " Part ( \\eg \u201chead\u201d) and part-of-part ( \\eg \u201cnose\u201d) are also included", "word_idx": 35842, "sentence_idx": 604, "label": "unlabeled"}, {"type": "text", "expr": " For VG and COCO, boxes are directly used", "word_idx": 35910, "sentence_idx": 605, "label": "unlabeled"}, {"type": "text", "expr": "For evaluation, we use classification accuracy (AC) and average precision (AP)\u00a0 ", "word_idx": 35951, "sentence_idx": 606, "label": "unlabeled"}, {"type": "text", "expr": " Note that since all the regions are fixed with known labels, there is no need to set a region overlap threshold for AP", "word_idx": 36031, "sentence_idx": 607, "label": "unlabeled"}, {"type": "text", "expr": " Results can be aggregated in two ways: the first way (\u201cper-class\u201d) computes metrics separately for each class in the set, and take the mean; since the final scores are all taken from a calibrated soft-max output, a second way (\u201cper-instance\u201d) that computes metrics simultaneously for all classes", "word_idx": 36150, "sentence_idx": 608, "label": "unlabeled"}, {"type": "text", "expr": " Intuitively, \u201cper-class\u201d assigns more weights to instances from rare classes", "word_idx": 36446, "sentence_idx": 609, "label": "unlabeled"}, {"type": "text", "expr": "3  Implementation Details", "word_idx": 36523, "sentence_idx": 610, "label": "unlabeled"}, {"type": "text", "expr": "A simplified version of  tf-faster-rcnn  is used to implement our baseline for region classification, with region proposal branch and bounding box regression components removed", "word_idx": 36548, "sentence_idx": 611, "label": "unlabeled"}, {"type": "text", "expr": " Unless otherwise noted, ResNet-50\u00a0  pre-trained on ImageNet\u00a0  is used as our backbone image classifier, and images are enlarged to shorter size  $600$  pixels during both training and testing", "word_idx": 36724, "sentence_idx": 612, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, full-image shared convolutional feature maps are computed till the last  conv4  layer", "word_idx": 36916, "sentence_idx": 613, "label": "unlabeled"}, {"type": "text", "expr": " Then the ground-truth boxes are used as regions-of-interest to compute region-specific features (crop and resize to  $7{\\times}7$  without max-pool)", "word_idx": 37016, "sentence_idx": 614, "label": "unlabeled"}, {"type": "text", "expr": " All layers of  conv5  and up are then adopted to obtain the final feature for the baseline prediction  $p_{0}$ ", "word_idx": 37165, "sentence_idx": 615, "label": "unlabeled"}, {"type": "text", "expr": " Batch normalization parameters are fixed", "word_idx": 37277, "sentence_idx": 616, "label": "unlabeled"}, {"type": "text", "expr": "tf-faster-rcnn", "word_idx": 37318, "sentence_idx": 617, "label": "unlabeled"}, {"type": "text", "expr": "2 https://github", "word_idx": 37332, "sentence_idx": 618, "label": "unlabeled"}, {"type": "text", "expr": "com/endernewton/tf-faster-rcnn", "word_idx": 37348, "sentence_idx": 619, "label": "unlabeled"}, {"type": "text", "expr": "https://github", "word_idx": 37378, "sentence_idx": 620, "label": "unlabeled"}, {"type": "text", "expr": "com/endernewton/tf-faster-rcnn", "word_idx": 37392, "sentence_idx": 621, "label": "unlabeled"}, {"type": "math", "expr": "$$600$$", "word_idx": 37422, "sentence_idx": 622, "label": "unlabeled"}, {"type": "text", "expr": "conv4", "word_idx": 37425, "sentence_idx": 623, "label": "unlabeled"}, {"type": "math", "expr": "$$7{\\times}7$$", "word_idx": 37430, "sentence_idx": 624, "label": "unlabeled"}, {"type": "text", "expr": "conv5", "word_idx": 37440, "sentence_idx": 625, "label": "unlabeled"}, {"type": "math", "expr": "$$p_{0}$$", "word_idx": 37445, "sentence_idx": 626, "label": "unlabeled"}, {"type": "text", "expr": "For the local module, we use the last  conv4  layer as our mid-level features to feed the spatial memory  $\\mathcal{S}$ ", "word_idx": 37450, "sentence_idx": 627, "label": "unlabeled"}, {"type": "text", "expr": " For the global module, mid-level features are the final  conv5  ( $2048$ -D) layer after avg-pool", "word_idx": 37570, "sentence_idx": 628, "label": "unlabeled"}, {"type": "text", "expr": " Both features are fused with the logits before soft-max  $f$ , and then fed into the memory cells", "word_idx": 37668, "sentence_idx": 629, "label": "unlabeled"}, {"type": "text", "expr": " Word vectors from fastText\u00a0  are used to represent each class, which extracts sub-word information and generalizes well to out-of-vocabulary words", "word_idx": 37766, "sentence_idx": 630, "label": "unlabeled"}, {"type": "text", "expr": " ReLU is selected as the activation function", "word_idx": 37913, "sentence_idx": 631, "label": "unlabeled"}, {"type": "text", "expr": " We roll-out the reasoning modules  $3$  times and concurrently update all regions at each iteration, as more iterations do not offer more help", "word_idx": 37957, "sentence_idx": 632, "label": "unlabeled"}, {"type": "text", "expr": "conv4", "word_idx": 38100, "sentence_idx": 633, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{S}$$", "word_idx": 38105, "sentence_idx": 634, "label": "unlabeled"}, {"type": "text", "expr": "conv5", "word_idx": 38116, "sentence_idx": 635, "label": "unlabeled"}, {"type": "math", "expr": "$$2048$$", "word_idx": 38121, "sentence_idx": 636, "label": "unlabeled"}, {"type": "text", "expr": "We apply stochastic gradient descent with momentum to optimize all the models, and use the validation set to tune hyper-parameters", "word_idx": 38125, "sentence_idx": 637, "label": "unlabeled"}, {"type": "text", "expr": " Our final setups are:  $5e^{-4}$  as the initial learning rate, reduced once ( $01{\\times}$ ) during fine-tuning;  $1e^{-4}$  as weight decay;  $09$  as momentum", "word_idx": 38255, "sentence_idx": 638, "label": "unlabeled"}, {"type": "text", "expr": " For ADE, we train  $320$ K iterations and reduce learning rate at  $280$ K", "word_idx": 38417, "sentence_idx": 639, "label": "unlabeled"}, {"type": "text", "expr": " For VG and COCO the numbers are  $640$ K/ $500$ K and  $560$ K/ $320$ K, respectively ", "word_idx": 38492, "sentence_idx": 640, "label": "unlabeled"}, {"type": "text", "expr": " We use a single image per step, and the only data augmentation technique used during training is left-right flipping ", "word_idx": 38579, "sentence_idx": 641, "label": "unlabeled"}, {"type": "text", "expr": " No augmentation is used in testing", "word_idx": 38697, "sentence_idx": 642, "label": "unlabeled"}, {"type": "math", "expr": "$$5e^{-4}$$", "word_idx": 38732, "sentence_idx": 643, "label": "unlabeled"}, {"type": "math", "expr": "$$0.1{\\times}$$", "word_idx": 38739, "sentence_idx": 644, "label": "unlabeled"}, {"type": "math", "expr": "$$1e^{-4}$$", "word_idx": 38750, "sentence_idx": 645, "label": "unlabeled"}, {"type": "math", "expr": "$$0.9$$", "word_idx": 38757, "sentence_idx": 646, "label": "unlabeled"}, {"type": "math", "expr": "$$320$$", "word_idx": 38760, "sentence_idx": 647, "label": "unlabeled"}, {"type": "math", "expr": "$$280$$", "word_idx": 38763, "sentence_idx": 648, "label": "unlabeled"}, {"type": "math", "expr": "$$640$$", "word_idx": 38766, "sentence_idx": 649, "label": "unlabeled"}, {"type": "math", "expr": "$$500$$", "word_idx": 38769, "sentence_idx": 650, "label": "unlabeled"}, {"type": "math", "expr": "$$560$$", "word_idx": 38772, "sentence_idx": 651, "label": "unlabeled"}, {"type": "math", "expr": "$$320$$", "word_idx": 38775, "sentence_idx": 652, "label": "unlabeled"}, {"type": "text", "expr": "3 Training longer still reduces cross-entropy, but drops both AP and AC", "word_idx": 38778, "sentence_idx": 653, "label": "unlabeled"}, {"type": "text", "expr": "4 The labels for class pairs like \u201cleft-hand\u201d and \u201cright-hand\u201d are swapped for flipped images", "word_idx": 38849, "sentence_idx": 654, "label": "unlabeled"}, {"type": "text", "expr": "4  Main Results", "word_idx": 38942, "sentence_idx": 655, "label": "unlabeled"}, {"type": "text", "expr": "Quantitative results on ADE  test-1k  and VG  test  are shown in Tab", "word_idx": 38957, "sentence_idx": 656, "label": "unlabeled"}, {"type": "text", "expr": " Besides plain ConvNet  $p_{0}$ , we also add three more baselines", "word_idx": 39025, "sentence_idx": 657, "label": "unlabeled"}, {"type": "text", "expr": " First, we use ResNet-101 as the backbone to see the performance can benefit from deeper networks", "word_idx": 39091, "sentence_idx": 658, "label": "unlabeled"}, {"type": "text", "expr": " Second, we increase the input image size with a shorter side  $800$  pixels, which is shown helpful especially for small objects in context\u00a0 ", "word_idx": 39188, "sentence_idx": 659, "label": "unlabeled"}, {"type": "text", "expr": " Finally, to check whether our performance gain is a result of more parameters, we include model ensemble as the third baseline where the prediction of two separate baseline models are averaged", "word_idx": 39330, "sentence_idx": 660, "label": "unlabeled"}, {"type": "text", "expr": "test-1k", "word_idx": 39523, "sentence_idx": 661, "label": "unlabeled"}, {"type": "math", "expr": "$$p_{0}$$", "word_idx": 39530, "sentence_idx": 662, "label": "unlabeled"}, {"type": "math", "expr": "$$800$$", "word_idx": 39535, "sentence_idx": 663, "label": "unlabeled"}, {"type": "text", "expr": "As can be seen, our reasoning modules are performing much better than all the baselines on ADE", "word_idx": 39538, "sentence_idx": 664, "label": "unlabeled"}, {"type": "text", "expr": " The local module alone can increase per-class AP by  $78$  absolute points", "word_idx": 39632, "sentence_idx": 665, "label": "unlabeled"}, {"type": "text", "expr": " Although the global module alone is not as effective ( $44\\%$  improvement), the performance gain it offers is  complementary  to the local module, and combining both modules we arrive at an AP of  $485\\%$  compared to the baseline AP  $401\\%$ ", "word_idx": 39707, "sentence_idx": 666, "label": "unlabeled"}, {"type": "text", "expr": " On the other hand, deeper network and larger input size can only help  ${\\sim}1\\%$ , less than model ensembles", "word_idx": 39952, "sentence_idx": 667, "label": "unlabeled"}, {"type": "text", "expr": " Additionally, our models achieve higher per-class metric gains than per-instance ones, indicating that  rare  classes get helped more \u2013 a nice property for learning from few examples", "word_idx": 40063, "sentence_idx": 668, "label": "unlabeled"}, {"type": "text", "expr": " Some qualitative results are listed in Fig", "word_idx": 40246, "sentence_idx": 669, "label": "unlabeled"}, {"type": "math", "expr": "$$7.8$$", "word_idx": 40289, "sentence_idx": 670, "label": "unlabeled"}, {"type": "math", "expr": "$$4.4\\%$$", "word_idx": 40292, "sentence_idx": 671, "label": "unlabeled"}, {"type": "text", "expr": "complementary", "word_idx": 40297, "sentence_idx": 672, "label": "unlabeled"}, {"type": "math", "expr": "$$48.5\\%$$", "word_idx": 40310, "sentence_idx": 673, "label": "unlabeled"}, {"type": "math", "expr": "$$40.1\\%$$", "word_idx": 40316, "sentence_idx": 674, "label": "unlabeled"}, {"type": "math", "expr": "$${\\sim}1\\%$$", "word_idx": 40322, "sentence_idx": 675, "label": "unlabeled"}, {"type": "text", "expr": "We also report the speed for future reference", "word_idx": 40331, "sentence_idx": 676, "label": "unlabeled"}, {"type": "text", "expr": " On Titan Xp, the final model on ADE trains at 0", "word_idx": 40376, "sentence_idx": 677, "label": "unlabeled"}, {"type": "text", "expr": "344s per iteration, compared to the baseline ResNet-50 at  $0163$ s and ResNet-101 at  $0209$ s", "word_idx": 40424, "sentence_idx": 678, "label": "unlabeled"}, {"type": "text", "expr": " For testing, our model takes  $0165$ s, whereas ResNet-50  $0136$ s, ResNet-101  $0156$ s", "word_idx": 40519, "sentence_idx": 679, "label": "unlabeled"}, {"type": "text", "expr": " We believe the additional\ncost is minimal with regard to the extra accuracy", "word_idx": 40609, "sentence_idx": 680, "label": "unlabeled"}, {"type": "math", "expr": "$$0.163$$", "word_idx": 40685, "sentence_idx": 681, "label": "unlabeled"}, {"type": "math", "expr": "$$0.209$$", "word_idx": 40690, "sentence_idx": 682, "label": "unlabeled"}, {"type": "math", "expr": "$$0.165$$", "word_idx": 40695, "sentence_idx": 683, "label": "unlabeled"}, {"type": "math", "expr": "$$0.136$$", "word_idx": 40700, "sentence_idx": 684, "label": "unlabeled"}, {"type": "math", "expr": "$$0.156$$", "word_idx": 40705, "sentence_idx": 685, "label": "unlabeled"}, {"type": "text", "expr": "We see a similar but less significant trend on VG", "word_idx": 40710, "sentence_idx": 686, "label": "unlabeled"}, {"type": "text", "expr": " This can potentially be a result of  noisier  labels \u2013 for ADE (and COCO shown later), the per-instance AP and AC values are within  $01\\%$ , intuitively suggesting that  higher  scores usually correspond to correct classifications", "word_idx": 40759, "sentence_idx": 687, "label": "unlabeled"}, {"type": "text", "expr": " However, on VG the difference is at  ${\\sim}05\\%$ , meaning more of the highly confident predictions are not classified right, which are likely caused by missing ground-truths", "word_idx": 40991, "sentence_idx": 688, "label": "unlabeled"}, {"type": "text", "expr": "noisier", "word_idx": 41167, "sentence_idx": 689, "label": "unlabeled"}, {"type": "math", "expr": "$$0.1\\%$$", "word_idx": 41174, "sentence_idx": 690, "label": "unlabeled"}, {"type": "text", "expr": "higher", "word_idx": 41179, "sentence_idx": 691, "label": "unlabeled"}, {"type": "math", "expr": "$${\\sim}0.5\\%$$", "word_idx": 41185, "sentence_idx": 692, "label": "unlabeled"}, {"type": "text", "expr": "5  Analysis", "word_idx": 41196, "sentence_idx": 693, "label": "unlabeled"}, {"type": "text", "expr": "Our analysis is divided into two major parts", "word_idx": 41207, "sentence_idx": 694, "label": "unlabeled"}, {"type": "text", "expr": " In the first part, we conduct thorough ablative analysis on the framework we have built", "word_idx": 41251, "sentence_idx": 695, "label": "unlabeled"}, {"type": "text", "expr": " Due to space limitation, we only report results on ADE here at Tab", "word_idx": 41339, "sentence_idx": 696, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 2 , for more analysis on VG, please check our supplementary material", "word_idx": 41406, "sentence_idx": 697, "label": "unlabeled"}, {"type": "text", "expr": "As can be seen, re-weighting hard examples with Eq", "word_idx": 41476, "sentence_idx": 698, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 6  helps around  $05\\%$  regardless of reasoning modules", "word_idx": 41526, "sentence_idx": 699, "label": "unlabeled"}, {"type": "text", "expr": " Spatial memory  $\\mathcal{S}$  is critical in the local module \u2013 if replaced by feeding last  conv4  layer directly the performance drops almost to baseline", "word_idx": 41584, "sentence_idx": 700, "label": "unlabeled"}, {"type": "text", "expr": " Local context aggregator  $\\mathcal{C}$  is less influential for ADE since the regions including background are densely labeled", "word_idx": 41741, "sentence_idx": 701, "label": "unlabeled"}, {"type": "text", "expr": " A different story takes place at the global module: removing the reasoning module  $\\mathcal{R}$  steeply drops performance, whereas further removing memory  $\\mathcal{M}$  does not hurt much", "word_idx": 41869, "sentence_idx": 702, "label": "unlabeled"}, {"type": "text", "expr": " Finally, for our full pipeline, removing cross-feeding and dropping the number of iterations both result in worse performance", "word_idx": 42061, "sentence_idx": 703, "label": "unlabeled"}, {"type": "math", "expr": "$$0.5\\%$$", "word_idx": 42187, "sentence_idx": 704, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{S}$$", "word_idx": 42192, "sentence_idx": 705, "label": "unlabeled"}, {"type": "text", "expr": "conv4", "word_idx": 42203, "sentence_idx": 706, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{C}$$", "word_idx": 42208, "sentence_idx": 707, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{R}$$", "word_idx": 42219, "sentence_idx": 708, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{M}$$", "word_idx": 42230, "sentence_idx": 709, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:  Ablative analysis on ADE  test-1k ", "word_idx": 42241, "sentence_idx": 710, "label": "unlabeled"}, {"type": "text", "expr": " In the first row of each block we repeat Local, Global and Final results from Tab", "word_idx": 42285, "sentence_idx": 711, "label": "unlabeled"}, {"type": "text", "expr": " Others see Sec", "word_idx": 42367, "sentence_idx": 712, "label": "unlabeled"}, {"type": "text", "expr": "5  for details", "word_idx": 42382, "sentence_idx": 713, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:", "word_idx": 42396, "sentence_idx": 714, "label": "unlabeled"}, {"type": "text", "expr": "test-1k", "word_idx": 42404, "sentence_idx": 715, "label": "unlabeled"}, {"type": "text", "expr": "per-instance per-class", "word_idx": 42411, "sentence_idx": 716, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\Xhline 1pt\n %", "word_idx": 42433, "sentence_idx": 717, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 42463, "sentence_idx": 718, "label": "unlabeled"}, {"type": "text", "expr": "\\Xhline", "word_idx": 42478, "sentence_idx": 719, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash Analysis", "word_idx": 42485, "sentence_idx": 720, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 42509, "sentence_idx": 721, "label": "unlabeled"}, {"type": "text", "expr": "Analysis", "word_idx": 42524, "sentence_idx": 722, "label": "unlabeled"}, {"type": "text", "expr": "per-instance", "word_idx": 42532, "sentence_idx": 723, "label": "unlabeled"}, {"type": "text", "expr": "per-class", "word_idx": 42544, "sentence_idx": 724, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash AP", "word_idx": 42553, "sentence_idx": 725, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 42571, "sentence_idx": 726, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash AC", "word_idx": 42586, "sentence_idx": 727, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 42604, "sentence_idx": 728, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash AP", "word_idx": 42619, "sentence_idx": 729, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 42637, "sentence_idx": 730, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash AC", "word_idx": 42652, "sentence_idx": 731, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 42670, "sentence_idx": 732, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\Xhline 1pt", "word_idx": 42685, "sentence_idx": 733, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 42712, "sentence_idx": 734, "label": "unlabeled"}, {"type": "text", "expr": "\\Xhline", "word_idx": 42727, "sentence_idx": 735, "label": "unlabeled"}, {"type": "text", "expr": "Local", "word_idx": 42734, "sentence_idx": 736, "label": "unlabeled"}, {"type": "text", "expr": "Local", "word_idx": 42739, "sentence_idx": 737, "label": "unlabeled"}, {"type": "text", "expr": "Local", "word_idx": 42744, "sentence_idx": 738, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash Ours \\textsubscript -Local", "word_idx": 42749, "sentence_idx": 739, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 42791, "sentence_idx": 740, "label": "unlabeled"}, {"type": "text", "expr": "\\textsubscript", "word_idx": 42806, "sentence_idx": 741, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 71", "word_idx": 42820, "sentence_idx": 742, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 42838, "sentence_idx": 743, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 71", "word_idx": 42853, "sentence_idx": 744, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 42871, "sentence_idx": 745, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 47", "word_idx": 42886, "sentence_idx": 746, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 42904, "sentence_idx": 747, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 38", "word_idx": 42919, "sentence_idx": 748, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 42937, "sentence_idx": 749, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \u00a0\u00a0\u00a0\u00a0 w/o re-weight", "word_idx": 42952, "sentence_idx": 750, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 42986, "sentence_idx": 751, "label": "unlabeled"}, {"type": "text", "expr": "w/o re-weight", "word_idx": 43001, "sentence_idx": 752, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 71", "word_idx": 43014, "sentence_idx": 753, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 43032, "sentence_idx": 754, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 71", "word_idx": 43047, "sentence_idx": 755, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 43065, "sentence_idx": 756, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 46", "word_idx": 43080, "sentence_idx": 757, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 43098, "sentence_idx": 758, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 37", "word_idx": 43113, "sentence_idx": 759, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 43131, "sentence_idx": 760, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 43146, "sentence_idx": 761, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 43161, "sentence_idx": 762, "label": "unlabeled"}, {"type": "text", "expr": "w/o  $\\mathcal{C}$", "word_idx": 43176, "sentence_idx": 763, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{C}$$", "word_idx": 43194, "sentence_idx": 764, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 70", "word_idx": 43205, "sentence_idx": 765, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 43223, "sentence_idx": 766, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 71", "word_idx": 43238, "sentence_idx": 767, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 43256, "sentence_idx": 768, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 46", "word_idx": 43271, "sentence_idx": 769, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 43289, "sentence_idx": 770, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 37", "word_idx": 43304, "sentence_idx": 771, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 43322, "sentence_idx": 772, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 43337, "sentence_idx": 773, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 43352, "sentence_idx": 774, "label": "unlabeled"}, {"type": "text", "expr": "w/o  $\\mathcal{S}$", "word_idx": 43367, "sentence_idx": 775, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{S}$$", "word_idx": 43385, "sentence_idx": 776, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 67", "word_idx": 43396, "sentence_idx": 777, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 43414, "sentence_idx": 778, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 67", "word_idx": 43429, "sentence_idx": 779, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 43447, "sentence_idx": 780, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 42", "word_idx": 43462, "sentence_idx": 781, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 43480, "sentence_idx": 782, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 34", "word_idx": 43495, "sentence_idx": 783, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 43513, "sentence_idx": 784, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\Xhline 0", "word_idx": 43528, "sentence_idx": 785, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 43553, "sentence_idx": 786, "label": "unlabeled"}, {"type": "text", "expr": "\\Xhline", "word_idx": 43568, "sentence_idx": 787, "label": "unlabeled"}, {"type": "text", "expr": "Global", "word_idx": 43575, "sentence_idx": 788, "label": "unlabeled"}, {"type": "text", "expr": "Global", "word_idx": 43581, "sentence_idx": 789, "label": "unlabeled"}, {"type": "text", "expr": "Global", "word_idx": 43587, "sentence_idx": 790, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash Ours \\textsubscript -Global", "word_idx": 43593, "sentence_idx": 791, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 43636, "sentence_idx": 792, "label": "unlabeled"}, {"type": "text", "expr": "\\textsubscript", "word_idx": 43651, "sentence_idx": 793, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 69", "word_idx": 43665, "sentence_idx": 794, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 43683, "sentence_idx": 795, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 69", "word_idx": 43698, "sentence_idx": 796, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 43716, "sentence_idx": 797, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 44", "word_idx": 43731, "sentence_idx": 798, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 43749, "sentence_idx": 799, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 36", "word_idx": 43764, "sentence_idx": 800, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 43782, "sentence_idx": 801, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \u00a0\u00a0\u00a0\u00a0 w/o re-weight", "word_idx": 43797, "sentence_idx": 802, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 43831, "sentence_idx": 803, "label": "unlabeled"}, {"type": "text", "expr": "w/o re-weight", "word_idx": 43846, "sentence_idx": 804, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 69", "word_idx": 43859, "sentence_idx": 805, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 43877, "sentence_idx": 806, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 69", "word_idx": 43892, "sentence_idx": 807, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 43910, "sentence_idx": 808, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 43", "word_idx": 43925, "sentence_idx": 809, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 43943, "sentence_idx": 810, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 36", "word_idx": 43958, "sentence_idx": 811, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 43976, "sentence_idx": 812, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \u00a0\u00a0\u00a0\u00a0 w/o spatial", "word_idx": 43991, "sentence_idx": 813, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 44023, "sentence_idx": 814, "label": "unlabeled"}, {"type": "text", "expr": "w/o spatial", "word_idx": 44038, "sentence_idx": 815, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 67", "word_idx": 44049, "sentence_idx": 816, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 44067, "sentence_idx": 817, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 67", "word_idx": 44082, "sentence_idx": 818, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 44100, "sentence_idx": 819, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 41", "word_idx": 44115, "sentence_idx": 820, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 44133, "sentence_idx": 821, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 35", "word_idx": 44148, "sentence_idx": 822, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 44166, "sentence_idx": 823, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \u00a0\u00a0\u00a0\u00a0 w/o semantic", "word_idx": 44181, "sentence_idx": 824, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 44214, "sentence_idx": 825, "label": "unlabeled"}, {"type": "text", "expr": "w/o semantic", "word_idx": 44229, "sentence_idx": 826, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 69", "word_idx": 44241, "sentence_idx": 827, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 44259, "sentence_idx": 828, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 69", "word_idx": 44274, "sentence_idx": 829, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 44292, "sentence_idx": 830, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 43", "word_idx": 44307, "sentence_idx": 831, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 44325, "sentence_idx": 832, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 35", "word_idx": 44340, "sentence_idx": 833, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 44358, "sentence_idx": 834, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 44373, "sentence_idx": 835, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 44388, "sentence_idx": 836, "label": "unlabeled"}, {"type": "text", "expr": "w/o  $\\mathcal{R}$", "word_idx": 44403, "sentence_idx": 837, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{R}$$", "word_idx": 44421, "sentence_idx": 838, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 67", "word_idx": 44432, "sentence_idx": 839, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 44450, "sentence_idx": 840, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 67", "word_idx": 44465, "sentence_idx": 841, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 44483, "sentence_idx": 842, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 41", "word_idx": 44498, "sentence_idx": 843, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 44516, "sentence_idx": 844, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 34", "word_idx": 44531, "sentence_idx": 845, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 44549, "sentence_idx": 846, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 44564, "sentence_idx": 847, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 44579, "sentence_idx": 848, "label": "unlabeled"}, {"type": "text", "expr": "w/o  $\\mathcal{M}$  &  $\\mathcal{R}$", "word_idx": 44594, "sentence_idx": 849, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{M}$$", "word_idx": 44630, "sentence_idx": 850, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{R}$$", "word_idx": 44641, "sentence_idx": 851, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 67", "word_idx": 44652, "sentence_idx": 852, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 44670, "sentence_idx": 853, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 67", "word_idx": 44685, "sentence_idx": 854, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 44703, "sentence_idx": 855, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 41", "word_idx": 44718, "sentence_idx": 856, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 44736, "sentence_idx": 857, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 34", "word_idx": 44751, "sentence_idx": 858, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 44769, "sentence_idx": 859, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\Xhline 0", "word_idx": 44784, "sentence_idx": 860, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 44809, "sentence_idx": 861, "label": "unlabeled"}, {"type": "text", "expr": "\\Xhline", "word_idx": 44824, "sentence_idx": 862, "label": "unlabeled"}, {"type": "text", "expr": "Final", "word_idx": 44831, "sentence_idx": 863, "label": "unlabeled"}, {"type": "text", "expr": "Final", "word_idx": 44836, "sentence_idx": 864, "label": "unlabeled"}, {"type": "text", "expr": "Final", "word_idx": 44841, "sentence_idx": 865, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash Ours \\textsubscript -Final", "word_idx": 44846, "sentence_idx": 866, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 44888, "sentence_idx": 867, "label": "unlabeled"}, {"type": "text", "expr": "\\textsubscript", "word_idx": 44903, "sentence_idx": 868, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 72", "word_idx": 44917, "sentence_idx": 869, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 44935, "sentence_idx": 870, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 72", "word_idx": 44950, "sentence_idx": 871, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 44968, "sentence_idx": 872, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 48", "word_idx": 44983, "sentence_idx": 873, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 45001, "sentence_idx": 874, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 39", "word_idx": 45016, "sentence_idx": 875, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 45034, "sentence_idx": 876, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \u00a0\u00a0\u00a0\u00a0 w/o re-weight", "word_idx": 45049, "sentence_idx": 877, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 45083, "sentence_idx": 878, "label": "unlabeled"}, {"type": "text", "expr": "w/o re-weight", "word_idx": 45098, "sentence_idx": 879, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 72", "word_idx": 45111, "sentence_idx": 880, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 45129, "sentence_idx": 881, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 72", "word_idx": 45144, "sentence_idx": 882, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 45162, "sentence_idx": 883, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 47", "word_idx": 45177, "sentence_idx": 884, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 45195, "sentence_idx": 885, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 38", "word_idx": 45210, "sentence_idx": 886, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 45228, "sentence_idx": 887, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \u00a0\u00a0\u00a0\u00a0 w/o cross-feed", "word_idx": 45243, "sentence_idx": 888, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 45278, "sentence_idx": 889, "label": "unlabeled"}, {"type": "text", "expr": "w/o cross-feed", "word_idx": 45293, "sentence_idx": 890, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 72", "word_idx": 45307, "sentence_idx": 891, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 45325, "sentence_idx": 892, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 72", "word_idx": 45340, "sentence_idx": 893, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 45358, "sentence_idx": 894, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 47", "word_idx": 45373, "sentence_idx": 895, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 45391, "sentence_idx": 896, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 39", "word_idx": 45406, "sentence_idx": 897, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 45424, "sentence_idx": 898, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \u00a0\u00a0\u00a0\u00a0 $2$  iterations", "word_idx": 45439, "sentence_idx": 899, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 45475, "sentence_idx": 900, "label": "unlabeled"}, {"type": "text", "expr": "iterations", "word_idx": 45490, "sentence_idx": 901, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 71", "word_idx": 45500, "sentence_idx": 902, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 45518, "sentence_idx": 903, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 72", "word_idx": 45533, "sentence_idx": 904, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 45551, "sentence_idx": 905, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 48", "word_idx": 45566, "sentence_idx": 906, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 45584, "sentence_idx": 907, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 39", "word_idx": 45599, "sentence_idx": 908, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 45617, "sentence_idx": 909, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\Xhline 1pt", "word_idx": 45632, "sentence_idx": 910, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 45659, "sentence_idx": 911, "label": "unlabeled"}, {"type": "text", "expr": "\\Xhline", "word_idx": 45674, "sentence_idx": 912, "label": "unlabeled"}, {"type": "text", "expr": "Missing Regions", "word_idx": 45681, "sentence_idx": 913, "label": "unlabeled"}, {"type": "text", "expr": "  So far we have shown results when all the regions are present", "word_idx": 45696, "sentence_idx": 914, "label": "unlabeled"}, {"type": "text", "expr": " Next, we want to analyze if our framework is robust to missing regions: if some percentage of regions are not used for reasoning", "word_idx": 45759, "sentence_idx": 915, "label": "unlabeled"}, {"type": "text", "expr": " This will be a common scenario if we use our framework in the detection setting \u2013 the underlying region proposal network\u00a0  may itself miss some regions", "word_idx": 45888, "sentence_idx": 916, "label": "unlabeled"}, {"type": "text", "expr": " We perform this set of experiments on COCO, since its regions are object-focused", "word_idx": 46040, "sentence_idx": 917, "label": "unlabeled"}, {"type": "text", "expr": "Missing Regions", "word_idx": 46121, "sentence_idx": 918, "label": "unlabeled"}, {"type": "text", "expr": "We test three variations", "word_idx": 46136, "sentence_idx": 919, "label": "unlabeled"}, {"type": "text", "expr": " In the first variation, the same region classification pipeline is applied as-is", "word_idx": 46160, "sentence_idx": 920, "label": "unlabeled"}, {"type": "text", "expr": " In the other two, we drop regions", "word_idx": 46241, "sentence_idx": 921, "label": "unlabeled"}, {"type": "text", "expr": " While we could have done it randomly, we simulate the real-world scenario by using region proposals from faster R-CNN\u00a0  ( $1190$ K/ $900$ K,  minival  detection mAP  $324\\%$ ) for testing, where  $300$  region proposals after NMS are applied to filter the ground-truth regions (max IoU ${>}\\delta$ )", "word_idx": 46275, "sentence_idx": 922, "label": "unlabeled"}, {"type": "text", "expr": " Evaluation is only done on the remaining regions", "word_idx": 46575, "sentence_idx": 923, "label": "unlabeled"}, {"type": "text", "expr": " Here we choose not to use region proposals directly, since the model has seen ground truth regions only", "word_idx": 46624, "sentence_idx": 924, "label": "unlabeled"}, {"type": "text", "expr": " We test two variations: a) \u201cpre\u201d, where the regions are filtered before inference,  \\ie only the remaining ground-truths are fed for reasoning; \u201cpost\u201d, where regions are filtered after inference", "word_idx": 46728, "sentence_idx": 925, "label": "unlabeled"}, {"type": "text", "expr": " Note that for the baseline, \u201cpre\u201d and \u201cpost\u201d makes no difference performance-wise", "word_idx": 46923, "sentence_idx": 926, "label": "unlabeled"}, {"type": "math", "expr": "$$1190$$", "word_idx": 47005, "sentence_idx": 927, "label": "unlabeled"}, {"type": "math", "expr": "$$900$$", "word_idx": 47009, "sentence_idx": 928, "label": "unlabeled"}, {"type": "text", "expr": "minival", "word_idx": 47012, "sentence_idx": 929, "label": "unlabeled"}, {"type": "math", "expr": "$$32.4\\%$$", "word_idx": 47019, "sentence_idx": 930, "label": "unlabeled"}, {"type": "math", "expr": "$$300$$", "word_idx": 47025, "sentence_idx": 931, "label": "unlabeled"}, {"type": "math", "expr": "$${>}\\delta$$", "word_idx": 47028, "sentence_idx": 932, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a03:  Results with missing regions when region proposals are used", "word_idx": 47037, "sentence_idx": 933, "label": "unlabeled"}, {"type": "text", "expr": " COCO  minival  is used since it is more detection oriented", "word_idx": 47106, "sentence_idx": 934, "label": "unlabeled"}, {"type": "text", "expr": "  pre  filters regions before inference, and  post  filters after inference", "word_idx": 47165, "sentence_idx": 935, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a03:", "word_idx": 47240, "sentence_idx": 936, "label": "unlabeled"}, {"type": "text", "expr": "minival", "word_idx": 47248, "sentence_idx": 937, "label": "unlabeled"}, {"type": "text", "expr": "per-instance per-class", "word_idx": 47255, "sentence_idx": 938, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\Xhline 1pt\n Method", "word_idx": 47277, "sentence_idx": 939, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 47312, "sentence_idx": 940, "label": "unlabeled"}, {"type": "text", "expr": "\\Xhline", "word_idx": 47327, "sentence_idx": 941, "label": "unlabeled"}, {"type": "text", "expr": "Method", "word_idx": 47334, "sentence_idx": 942, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash pre", "word_idx": 47340, "sentence_idx": 943, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 47359, "sentence_idx": 944, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash post", "word_idx": 47374, "sentence_idx": 945, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 47394, "sentence_idx": 946, "label": "unlabeled"}, {"type": "text", "expr": "per-instance", "word_idx": 47409, "sentence_idx": 947, "label": "unlabeled"}, {"type": "text", "expr": "per-class", "word_idx": 47421, "sentence_idx": 948, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 47430, "sentence_idx": 949, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 47445, "sentence_idx": 950, "label": "unlabeled"}, {"type": "math", "expr": "$$\\nabla$$", "word_idx": 47460, "sentence_idx": 951, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 47466, "sentence_idx": 952, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 47481, "sentence_idx": 953, "label": "unlabeled"}, {"type": "math", "expr": "$$\\nabla$$", "word_idx": 47496, "sentence_idx": 954, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 47502, "sentence_idx": 955, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 47517, "sentence_idx": 956, "label": "unlabeled"}, {"type": "math", "expr": "$$\\nabla$$", "word_idx": 47532, "sentence_idx": 957, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 47538, "sentence_idx": 958, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 47553, "sentence_idx": 959, "label": "unlabeled"}, {"type": "math", "expr": "$$\\nabla$$", "word_idx": 47568, "sentence_idx": 960, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\arraybackslash", "word_idx": 47574, "sentence_idx": 961, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\Xhline 1pt\nBaseline", "word_idx": 47605, "sentence_idx": 962, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 47641, "sentence_idx": 963, "label": "unlabeled"}, {"type": "text", "expr": "\\Xhline", "word_idx": 47656, "sentence_idx": 964, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 47663, "sentence_idx": 965, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 47678, "sentence_idx": 966, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 47693, "sentence_idx": 967, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 47708, "sentence_idx": 968, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 83", "word_idx": 47723, "sentence_idx": 969, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 47741, "sentence_idx": 970, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 83", "word_idx": 47756, "sentence_idx": 971, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 47774, "sentence_idx": 972, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 83", "word_idx": 47789, "sentence_idx": 973, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 47807, "sentence_idx": 974, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 75", "word_idx": 47822, "sentence_idx": 975, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 47840, "sentence_idx": 976, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\arraybackslash", "word_idx": 47855, "sentence_idx": 977, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash Ours \\textsubscript -Local", "word_idx": 47886, "sentence_idx": 978, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 47928, "sentence_idx": 979, "label": "unlabeled"}, {"type": "text", "expr": "\\textsubscript", "word_idx": 47943, "sentence_idx": 980, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 47957, "sentence_idx": 981, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 47972, "sentence_idx": 982, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 47987, "sentence_idx": 983, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48002, "sentence_idx": 984, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48017, "sentence_idx": 985, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48032, "sentence_idx": 986, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48047, "sentence_idx": 987, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48062, "sentence_idx": 988, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48077, "sentence_idx": 989, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48092, "sentence_idx": 990, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48107, "sentence_idx": 991, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48122, "sentence_idx": 992, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\arraybackslash", "word_idx": 48137, "sentence_idx": 993, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash Ours \\textsubscript -Global", "word_idx": 48168, "sentence_idx": 994, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48211, "sentence_idx": 995, "label": "unlabeled"}, {"type": "text", "expr": "\\textsubscript", "word_idx": 48226, "sentence_idx": 996, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48240, "sentence_idx": 997, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48255, "sentence_idx": 998, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48270, "sentence_idx": 999, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48285, "sentence_idx": 1000, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48300, "sentence_idx": 1001, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48315, "sentence_idx": 1002, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48330, "sentence_idx": 1003, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48345, "sentence_idx": 1004, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48360, "sentence_idx": 1005, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48375, "sentence_idx": 1006, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48390, "sentence_idx": 1007, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48405, "sentence_idx": 1008, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\arraybackslash", "word_idx": 48420, "sentence_idx": 1009, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash Ours \\textsubscript -Final", "word_idx": 48451, "sentence_idx": 1010, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48493, "sentence_idx": 1011, "label": "unlabeled"}, {"type": "text", "expr": "\\textsubscript", "word_idx": 48508, "sentence_idx": 1012, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48522, "sentence_idx": 1013, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48537, "sentence_idx": 1014, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48552, "sentence_idx": 1015, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48567, "sentence_idx": 1016, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48582, "sentence_idx": 1017, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48597, "sentence_idx": 1018, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48612, "sentence_idx": 1019, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48627, "sentence_idx": 1020, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48642, "sentence_idx": 1021, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48657, "sentence_idx": 1022, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48672, "sentence_idx": 1023, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48687, "sentence_idx": 1024, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\Xhline 0", "word_idx": 48702, "sentence_idx": 1025, "label": "unlabeled"}, {"type": "text", "expr": "5pt\nBaseline", "word_idx": 48727, "sentence_idx": 1026, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48739, "sentence_idx": 1027, "label": "unlabeled"}, {"type": "text", "expr": "\\Xhline", "word_idx": 48754, "sentence_idx": 1028, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash -", "word_idx": 48761, "sentence_idx": 1029, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48778, "sentence_idx": 1030, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash -", "word_idx": 48793, "sentence_idx": 1031, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48810, "sentence_idx": 1032, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 87", "word_idx": 48825, "sentence_idx": 1033, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48843, "sentence_idx": 1034, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 87", "word_idx": 48858, "sentence_idx": 1035, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48876, "sentence_idx": 1036, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 87", "word_idx": 48891, "sentence_idx": 1037, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48909, "sentence_idx": 1038, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 80", "word_idx": 48924, "sentence_idx": 1039, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48942, "sentence_idx": 1040, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 48957, "sentence_idx": 1041, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash Ours \\textsubscript -Final", "word_idx": 48972, "sentence_idx": 1042, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 49014, "sentence_idx": 1043, "label": "unlabeled"}, {"type": "text", "expr": "\\textsubscript", "word_idx": 49029, "sentence_idx": 1044, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \u2733", "word_idx": 49043, "sentence_idx": 1045, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 49060, "sentence_idx": 1046, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 49075, "sentence_idx": 1047, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 49090, "sentence_idx": 1048, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 49105, "sentence_idx": 1049, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 49120, "sentence_idx": 1050, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 49135, "sentence_idx": 1051, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 49150, "sentence_idx": 1052, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 49165, "sentence_idx": 1053, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 49180, "sentence_idx": 1054, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 49195, "sentence_idx": 1055, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 49210, "sentence_idx": 1056, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 49225, "sentence_idx": 1057, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash Ours \\textsubscript -Final", "word_idx": 49240, "sentence_idx": 1058, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 49282, "sentence_idx": 1059, "label": "unlabeled"}, {"type": "text", "expr": "\\textsubscript", "word_idx": 49297, "sentence_idx": 1060, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 49311, "sentence_idx": 1061, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 49326, "sentence_idx": 1062, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \u2733", "word_idx": 49341, "sentence_idx": 1063, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 49358, "sentence_idx": 1064, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 49373, "sentence_idx": 1065, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 49388, "sentence_idx": 1066, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 49403, "sentence_idx": 1067, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 49418, "sentence_idx": 1068, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 49433, "sentence_idx": 1069, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 49448, "sentence_idx": 1070, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 49463, "sentence_idx": 1071, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 49478, "sentence_idx": 1072, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\Xhline 1pt", "word_idx": 49493, "sentence_idx": 1073, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 49520, "sentence_idx": 1074, "label": "unlabeled"}, {"type": "text", "expr": "\\Xhline", "word_idx": 49535, "sentence_idx": 1075, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a06:", "word_idx": 49542, "sentence_idx": 1076, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a06:", "word_idx": 49551, "sentence_idx": 1077, "label": "unlabeled"}, {"type": "text", "expr": "Trends of recall and per-class AP when varying IoU threshold  $\\delta$  from  $0$  to  $9$  to drop regions", "word_idx": 49560, "sentence_idx": 1078, "label": "unlabeled"}, {"type": "text", "expr": " See text for details", "word_idx": 49667, "sentence_idx": 1079, "label": "unlabeled"}, {"type": "math", "expr": "$$\\delta$$", "word_idx": 49688, "sentence_idx": 1080, "label": "unlabeled"}, {"type": "text", "expr": "The results are summarized in Tab", "word_idx": 49694, "sentence_idx": 1081, "label": "unlabeled"}, {"type": "text", "expr": " Interestingly, despite lacking a knowledge graph, our global module works better than the local module with the region graph alone, likely due to its power that allows direct region-to-region communication even for farther-away pairs", "word_idx": 49727, "sentence_idx": 1082, "label": "unlabeled"}, {"type": "text", "expr": " Combining the two, we report  $37\\%$  absolute advantage on per-class AP over the baseline even with all classes being objects \u2013 no \u201cstuff\u201d classes involved", "word_idx": 49961, "sentence_idx": 1083, "label": "unlabeled"}, {"type": "math", "expr": "$$3.7\\%$$", "word_idx": 50118, "sentence_idx": 1084, "label": "unlabeled"}, {"type": "text", "expr": "In Fig", "word_idx": 50123, "sentence_idx": 1085, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 6 , we vary  $\\delta$  from  $0$  to  $9$ : with  $0$  keeping all regions and  $09$  dropping the most", "word_idx": 50129, "sentence_idx": 1086, "label": "unlabeled"}, {"type": "text", "expr": " As the trend shows, while the reasoning module suffers when regions are dropped, it is quiet resilient and the performance degradation is smooth", "word_idx": 50234, "sentence_idx": 1087, "label": "unlabeled"}, {"type": "text", "expr": " For example (listed in Tab", "word_idx": 50379, "sentence_idx": 1088, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 3 ), with an IoU threshold  $\\delta$  of  $05$  that recalls  $781\\%$  of the ground truth boxes, we still outperform the baseline by  $24\\%$  in the \u201cpost\u201d setting, and  $22\\%$  in \u201cpre\u201d where not all regions can be fed for reasoning", "word_idx": 50406, "sentence_idx": 1089, "label": "unlabeled"}, {"type": "text", "expr": " The lower gap implies a) region proposals are usually corresponding to easy examples where less context is needed, and b) context reasoning frameworks like ours benefit from more known regions", "word_idx": 50642, "sentence_idx": 1090, "label": "unlabeled"}, {"type": "text", "expr": " At  $\\delta{=}8$  the recall ( $305\\%$ ) is so small that it cannot afford much reasoning, and at  $\\delta{=}9$  (recall  $39\\%$ ), reasoning even hurts the performance", "word_idx": 50835, "sentence_idx": 1091, "label": "unlabeled"}, {"type": "math", "expr": "$$\\delta$$", "word_idx": 51004, "sentence_idx": 1092, "label": "unlabeled"}, {"type": "math", "expr": "$$0.9$$", "word_idx": 51010, "sentence_idx": 1093, "label": "unlabeled"}, {"type": "math", "expr": "$$\\delta$$", "word_idx": 51013, "sentence_idx": 1094, "label": "unlabeled"}, {"type": "math", "expr": "$$0.5$$", "word_idx": 51019, "sentence_idx": 1095, "label": "unlabeled"}, {"type": "math", "expr": "$$78.1\\%$$", "word_idx": 51022, "sentence_idx": 1096, "label": "unlabeled"}, {"type": "math", "expr": "$$2.4\\%$$", "word_idx": 51028, "sentence_idx": 1097, "label": "unlabeled"}, {"type": "math", "expr": "$$2.2\\%$$", "word_idx": 51033, "sentence_idx": 1098, "label": "unlabeled"}, {"type": "math", "expr": "$$\\delta{=}.8$$", "word_idx": 51038, "sentence_idx": 1099, "label": "unlabeled"}, {"type": "math", "expr": "$$30.5\\%$$", "word_idx": 51049, "sentence_idx": 1100, "label": "unlabeled"}, {"type": "math", "expr": "$$\\delta{=}.9$$", "word_idx": 51055, "sentence_idx": 1101, "label": "unlabeled"}, {"type": "math", "expr": "$$3.9\\%$$", "word_idx": 51066, "sentence_idx": 1102, "label": "unlabeled"}, {"type": "text", "expr": "5  Conclusion", "word_idx": 51071, "sentence_idx": 1103, "label": "unlabeled"}, {"type": "text", "expr": "We presented a novel framework for iterative visual reasoning", "word_idx": 51084, "sentence_idx": 1104, "label": "unlabeled"}, {"type": "text", "expr": " Beyond convolutions, it uses a graph to encode spatial and semantic relationships between regions and classes and passes message on the graph", "word_idx": 51145, "sentence_idx": 1105, "label": "unlabeled"}, {"type": "text", "expr": " We show strong performance over plain ConvNets,  \\eg achieving an  $84\\%$  absolute gain on ADE and  $37\\%$  on COCO", "word_idx": 51287, "sentence_idx": 1106, "label": "unlabeled"}, {"type": "text", "expr": " Analysis also shows that our reasoning framework is resilient to missing regions caused by current region proposal approaches", "word_idx": 51404, "sentence_idx": 1107, "label": "unlabeled"}, {"type": "math", "expr": "$$8.4\\%$$", "word_idx": 51530, "sentence_idx": 1108, "label": "unlabeled"}, {"type": "math", "expr": "$$3.7\\%$$", "word_idx": 51535, "sentence_idx": 1109, "label": "unlabeled"}, {"type": "text", "expr": "Acknowledgements : This work was supported in part by ONR MURI N000141612007", "word_idx": 51540, "sentence_idx": 1110, "label": "unlabeled"}, {"type": "text", "expr": " XC would also like to thank Shengyang Dai and Google Cloud AI team for support during the internship", "word_idx": 51616, "sentence_idx": 1111, "label": "unlabeled"}, {"type": "text", "expr": "Acknowledgements", "word_idx": 51717, "sentence_idx": 1112, "label": "unlabeled"}, {"type": "text", "expr": "References", "word_idx": 51733, "sentence_idx": 1113, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Biederman, R", "word_idx": 51743, "sentence_idx": 1114, "label": "unlabeled"}, {"type": "text", "expr": " Mezzanotte, and J", "word_idx": 51756, "sentence_idx": 1115, "label": "unlabeled"}, {"type": "text", "expr": " Rabinowitz", "word_idx": 51774, "sentence_idx": 1116, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Biederman, R", "word_idx": 51785, "sentence_idx": 1117, "label": "unlabeled"}, {"type": "text", "expr": " Mezzanotte, and J", "word_idx": 51798, "sentence_idx": 1118, "label": "unlabeled"}, {"type": "text", "expr": " Rabinowitz", "word_idx": 51816, "sentence_idx": 1119, "label": "unlabeled"}, {"type": "text", "expr": "Scene perception: Detecting and judging objects undergoing relational\nviolations", "word_idx": 51827, "sentence_idx": 1120, "label": "unlabeled"}, {"type": "text", "expr": "Scene perception: Detecting and judging objects undergoing relational\nviolations", "word_idx": 51907, "sentence_idx": 1121, "label": "unlabeled"}, {"type": "text", "expr": "Cognitive psychology , 14(2):143\u2013177, 1982", "word_idx": 51987, "sentence_idx": 1122, "label": "unlabeled"}, {"type": "text", "expr": "Cognitive psychology", "word_idx": 52029, "sentence_idx": 1123, "label": "unlabeled"}, {"type": "text", "expr": ", 14(2):143\u2013177, 1982", "word_idx": 52049, "sentence_idx": 1124, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Carreira, P", "word_idx": 52070, "sentence_idx": 1125, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Agrawal, K", "word_idx": 52082, "sentence_idx": 1126, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fragkiadaki, and J", "word_idx": 52093, "sentence_idx": 1127, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Malik", "word_idx": 52112, "sentence_idx": 1128, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Carreira, P", "word_idx": 52118, "sentence_idx": 1129, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Agrawal, K", "word_idx": 52130, "sentence_idx": 1130, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fragkiadaki, and J", "word_idx": 52141, "sentence_idx": 1131, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Malik", "word_idx": 52160, "sentence_idx": 1132, "label": "unlabeled"}, {"type": "text", "expr": "Human pose estimation with iterative error feedback", "word_idx": 52166, "sentence_idx": 1133, "label": "unlabeled"}, {"type": "text", "expr": "Human pose estimation with iterative error feedback", "word_idx": 52217, "sentence_idx": 1134, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2016", "word_idx": 52268, "sentence_idx": 1135, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 52283, "sentence_idx": 1136, "label": "unlabeled"}, {"type": "text", "expr": " Chen, Y", "word_idx": 52289, "sentence_idx": 1137, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yang, J", "word_idx": 52297, "sentence_idx": 1138, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wang, W", "word_idx": 52305, "sentence_idx": 1139, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xu, and A", "word_idx": 52313, "sentence_idx": 1140, "label": "unlabeled"}, {"type": "text", "expr": " Yuille", "word_idx": 52323, "sentence_idx": 1141, "label": "unlabeled"}, {"type": "text", "expr": " Chen, Y", "word_idx": 52330, "sentence_idx": 1142, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yang, J", "word_idx": 52338, "sentence_idx": 1143, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wang, W", "word_idx": 52346, "sentence_idx": 1144, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xu, and A", "word_idx": 52354, "sentence_idx": 1145, "label": "unlabeled"}, {"type": "text", "expr": " Yuille", "word_idx": 52364, "sentence_idx": 1146, "label": "unlabeled"}, {"type": "text", "expr": "Attention to scale: Scale-aware semantic image segmentation", "word_idx": 52371, "sentence_idx": 1147, "label": "unlabeled"}, {"type": "text", "expr": "Attention to scale: Scale-aware semantic image segmentation", "word_idx": 52430, "sentence_idx": 1148, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2016", "word_idx": 52489, "sentence_idx": 1149, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 52504, "sentence_idx": 1150, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chen and A", "word_idx": 52510, "sentence_idx": 1151, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gupta", "word_idx": 52521, "sentence_idx": 1152, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chen and A", "word_idx": 52527, "sentence_idx": 1153, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gupta", "word_idx": 52538, "sentence_idx": 1154, "label": "unlabeled"}, {"type": "text", "expr": "Spatial memory for context reasoning in object detection", "word_idx": 52544, "sentence_idx": 1155, "label": "unlabeled"}, {"type": "text", "expr": "Spatial memory for context reasoning in object detection", "word_idx": 52600, "sentence_idx": 1156, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1704", "word_idx": 52656, "sentence_idx": 1157, "label": "unlabeled"}, {"type": "text", "expr": "04224 , 2017", "word_idx": 52681, "sentence_idx": 1158, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1704", "word_idx": 52693, "sentence_idx": 1159, "label": "unlabeled"}, {"type": "text", "expr": "04224", "word_idx": 52718, "sentence_idx": 1160, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 52723, "sentence_idx": 1161, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chen, A", "word_idx": 52729, "sentence_idx": 1162, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Shrivastava, and A", "word_idx": 52737, "sentence_idx": 1163, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gupta", "word_idx": 52756, "sentence_idx": 1164, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chen, A", "word_idx": 52762, "sentence_idx": 1165, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Shrivastava, and A", "word_idx": 52770, "sentence_idx": 1166, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gupta", "word_idx": 52789, "sentence_idx": 1167, "label": "unlabeled"}, {"type": "text", "expr": "Neil: Extracting visual knowledge from web data", "word_idx": 52795, "sentence_idx": 1168, "label": "unlabeled"}, {"type": "text", "expr": "Neil: Extracting visual knowledge from web data", "word_idx": 52842, "sentence_idx": 1169, "label": "unlabeled"}, {"type": "text", "expr": "In  ICCV , 2013", "word_idx": 52889, "sentence_idx": 1170, "label": "unlabeled"}, {"type": "text", "expr": ", 2013", "word_idx": 52904, "sentence_idx": 1171, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chung, C", "word_idx": 52910, "sentence_idx": 1172, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gulcehre, K", "word_idx": 52919, "sentence_idx": 1173, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Cho, and Y", "word_idx": 52931, "sentence_idx": 1174, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bengio", "word_idx": 52942, "sentence_idx": 1175, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chung, C", "word_idx": 52949, "sentence_idx": 1176, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gulcehre, K", "word_idx": 52958, "sentence_idx": 1177, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Cho, and Y", "word_idx": 52970, "sentence_idx": 1178, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bengio", "word_idx": 52981, "sentence_idx": 1179, "label": "unlabeled"}, {"type": "text", "expr": "Empirical evaluation of gated recurrent neural networks on sequence\nmodeling", "word_idx": 52988, "sentence_idx": 1180, "label": "unlabeled"}, {"type": "text", "expr": "Empirical evaluation of gated recurrent neural networks on sequence\nmodeling", "word_idx": 53064, "sentence_idx": 1181, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1412", "word_idx": 53140, "sentence_idx": 1182, "label": "unlabeled"}, {"type": "text", "expr": "3555 , 2014", "word_idx": 53150, "sentence_idx": 1183, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1412", "word_idx": 53161, "sentence_idx": 1184, "label": "unlabeled"}, {"type": "text", "expr": ", 2014", "word_idx": 53171, "sentence_idx": 1185, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Das, A", "word_idx": 53177, "sentence_idx": 1186, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Neelakantan, D", "word_idx": 53184, "sentence_idx": 1187, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Belanger, and A", "word_idx": 53199, "sentence_idx": 1188, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0McCallum", "word_idx": 53215, "sentence_idx": 1189, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Das, A", "word_idx": 53224, "sentence_idx": 1190, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Neelakantan, D", "word_idx": 53231, "sentence_idx": 1191, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Belanger, and A", "word_idx": 53246, "sentence_idx": 1192, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0McCallum", "word_idx": 53262, "sentence_idx": 1193, "label": "unlabeled"}, {"type": "text", "expr": "Chains of reasoning over entities, relations, and text using\nrecurrent neural networks", "word_idx": 53271, "sentence_idx": 1194, "label": "unlabeled"}, {"type": "text", "expr": "Chains of reasoning over entities, relations, and text using\nrecurrent neural networks", "word_idx": 53357, "sentence_idx": 1195, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1607", "word_idx": 53443, "sentence_idx": 1196, "label": "unlabeled"}, {"type": "text", "expr": "01426 , 2016", "word_idx": 53468, "sentence_idx": 1197, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1607", "word_idx": 53480, "sentence_idx": 1198, "label": "unlabeled"}, {"type": "text", "expr": "01426", "word_idx": 53505, "sentence_idx": 1199, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 53510, "sentence_idx": 1200, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Deng, N", "word_idx": 53516, "sentence_idx": 1201, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ding, Y", "word_idx": 53524, "sentence_idx": 1202, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Jia, A", "word_idx": 53532, "sentence_idx": 1203, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Frome, K", "word_idx": 53539, "sentence_idx": 1204, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Murphy, S", "word_idx": 53548, "sentence_idx": 1205, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bengio, Y", "word_idx": 53558, "sentence_idx": 1206, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Li, H", "word_idx": 53568, "sentence_idx": 1207, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Neven, and\nH", "word_idx": 53574, "sentence_idx": 1208, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Adam", "word_idx": 53587, "sentence_idx": 1209, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Deng, N", "word_idx": 53592, "sentence_idx": 1210, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ding, Y", "word_idx": 53600, "sentence_idx": 1211, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Jia, A", "word_idx": 53608, "sentence_idx": 1212, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Frome, K", "word_idx": 53615, "sentence_idx": 1213, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Murphy, S", "word_idx": 53624, "sentence_idx": 1214, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bengio, Y", "word_idx": 53634, "sentence_idx": 1215, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Li, H", "word_idx": 53644, "sentence_idx": 1216, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Neven, and\nH", "word_idx": 53650, "sentence_idx": 1217, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Adam", "word_idx": 53663, "sentence_idx": 1218, "label": "unlabeled"}, {"type": "text", "expr": "Large-scale object classification using label relation graphs", "word_idx": 53668, "sentence_idx": 1219, "label": "unlabeled"}, {"type": "text", "expr": "Large-scale object classification using label relation graphs", "word_idx": 53729, "sentence_idx": 1220, "label": "unlabeled"}, {"type": "text", "expr": "In  ECCV , 2014", "word_idx": 53790, "sentence_idx": 1221, "label": "unlabeled"}, {"type": "text", "expr": ", 2014", "word_idx": 53805, "sentence_idx": 1222, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Desai, D", "word_idx": 53811, "sentence_idx": 1223, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ramanan, and C", "word_idx": 53820, "sentence_idx": 1224, "label": "unlabeled"}, {"type": "text", "expr": " Fowlkes", "word_idx": 53835, "sentence_idx": 1225, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Desai, D", "word_idx": 53843, "sentence_idx": 1226, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ramanan, and C", "word_idx": 53852, "sentence_idx": 1227, "label": "unlabeled"}, {"type": "text", "expr": " Fowlkes", "word_idx": 53867, "sentence_idx": 1228, "label": "unlabeled"}, {"type": "text", "expr": "Discriminative models for multi-class object layout", "word_idx": 53875, "sentence_idx": 1229, "label": "unlabeled"}, {"type": "text", "expr": "Discriminative models for multi-class object layout", "word_idx": 53926, "sentence_idx": 1230, "label": "unlabeled"}, {"type": "text", "expr": "IJCV , 95(1):1\u201312, 2011", "word_idx": 53977, "sentence_idx": 1231, "label": "unlabeled"}, {"type": "text", "expr": ", 95(1):1\u201312, 2011", "word_idx": 54000, "sentence_idx": 1232, "label": "unlabeled"}, {"type": "text", "expr": " Divvala, A", "word_idx": 54018, "sentence_idx": 1233, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Farhadi, and C", "word_idx": 54029, "sentence_idx": 1234, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Guestrin", "word_idx": 54044, "sentence_idx": 1235, "label": "unlabeled"}, {"type": "text", "expr": " Divvala, A", "word_idx": 54053, "sentence_idx": 1236, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Farhadi, and C", "word_idx": 54064, "sentence_idx": 1237, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Guestrin", "word_idx": 54079, "sentence_idx": 1238, "label": "unlabeled"}, {"type": "text", "expr": "Learning everything about anything: Webly-supervised visual concept\nlearning", "word_idx": 54088, "sentence_idx": 1239, "label": "unlabeled"}, {"type": "text", "expr": "Learning everything about anything: Webly-supervised visual concept\nlearning", "word_idx": 54164, "sentence_idx": 1240, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2014", "word_idx": 54240, "sentence_idx": 1241, "label": "unlabeled"}, {"type": "text", "expr": ", 2014", "word_idx": 54255, "sentence_idx": 1242, "label": "unlabeled"}, {"type": "text", "expr": " Divvala, D", "word_idx": 54261, "sentence_idx": 1243, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hoiem, J", "word_idx": 54272, "sentence_idx": 1244, "label": "unlabeled"}, {"type": "text", "expr": " Hays, A", "word_idx": 54281, "sentence_idx": 1245, "label": "unlabeled"}, {"type": "text", "expr": " Efros, and M", "word_idx": 54289, "sentence_idx": 1246, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hebert", "word_idx": 54302, "sentence_idx": 1247, "label": "unlabeled"}, {"type": "text", "expr": " Divvala, D", "word_idx": 54309, "sentence_idx": 1248, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hoiem, J", "word_idx": 54320, "sentence_idx": 1249, "label": "unlabeled"}, {"type": "text", "expr": " Hays, A", "word_idx": 54329, "sentence_idx": 1250, "label": "unlabeled"}, {"type": "text", "expr": " Efros, and M", "word_idx": 54337, "sentence_idx": 1251, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hebert", "word_idx": 54350, "sentence_idx": 1252, "label": "unlabeled"}, {"type": "text", "expr": "An empirical study of context in object detection", "word_idx": 54357, "sentence_idx": 1253, "label": "unlabeled"}, {"type": "text", "expr": "An empirical study of context in object detection", "word_idx": 54406, "sentence_idx": 1254, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2009", "word_idx": 54455, "sentence_idx": 1255, "label": "unlabeled"}, {"type": "text", "expr": ", 2009", "word_idx": 54470, "sentence_idx": 1256, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Everingham, L", "word_idx": 54476, "sentence_idx": 1257, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Van\u00a0Gool, C", "word_idx": 54490, "sentence_idx": 1258, "label": "unlabeled"}, {"type": "text", "expr": " Williams, J", "word_idx": 54502, "sentence_idx": 1259, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Winn, and A", "word_idx": 54514, "sentence_idx": 1260, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zisserman", "word_idx": 54526, "sentence_idx": 1261, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Everingham, L", "word_idx": 54536, "sentence_idx": 1262, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Van\u00a0Gool, C", "word_idx": 54550, "sentence_idx": 1263, "label": "unlabeled"}, {"type": "text", "expr": " Williams, J", "word_idx": 54562, "sentence_idx": 1264, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Winn, and A", "word_idx": 54574, "sentence_idx": 1265, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zisserman", "word_idx": 54586, "sentence_idx": 1266, "label": "unlabeled"}, {"type": "text", "expr": "The pascal visual object classes (voc) challenge", "word_idx": 54596, "sentence_idx": 1267, "label": "unlabeled"}, {"type": "text", "expr": "The pascal visual object classes (voc) challenge", "word_idx": 54644, "sentence_idx": 1268, "label": "unlabeled"}, {"type": "text", "expr": "IJCV , 88(2):303\u2013338, 2010", "word_idx": 54692, "sentence_idx": 1269, "label": "unlabeled"}, {"type": "text", "expr": ", 88(2):303\u2013338, 2010", "word_idx": 54718, "sentence_idx": 1270, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Farhadi, I", "word_idx": 54739, "sentence_idx": 1271, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Endres, D", "word_idx": 54750, "sentence_idx": 1272, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hoiem, and D", "word_idx": 54760, "sentence_idx": 1273, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Forsyth", "word_idx": 54773, "sentence_idx": 1274, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Farhadi, I", "word_idx": 54781, "sentence_idx": 1275, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Endres, D", "word_idx": 54792, "sentence_idx": 1276, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hoiem, and D", "word_idx": 54802, "sentence_idx": 1277, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Forsyth", "word_idx": 54815, "sentence_idx": 1278, "label": "unlabeled"}, {"type": "text", "expr": "Describing objects by their attributes", "word_idx": 54823, "sentence_idx": 1279, "label": "unlabeled"}, {"type": "text", "expr": "Describing objects by their attributes", "word_idx": 54861, "sentence_idx": 1280, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2009", "word_idx": 54899, "sentence_idx": 1281, "label": "unlabeled"}, {"type": "text", "expr": ", 2009", "word_idx": 54914, "sentence_idx": 1282, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fei-Fei, R", "word_idx": 54920, "sentence_idx": 1283, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fergus, and P", "word_idx": 54931, "sentence_idx": 1284, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Perona", "word_idx": 54945, "sentence_idx": 1285, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fei-Fei, R", "word_idx": 54952, "sentence_idx": 1286, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fergus, and P", "word_idx": 54963, "sentence_idx": 1287, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Perona", "word_idx": 54977, "sentence_idx": 1288, "label": "unlabeled"}, {"type": "text", "expr": "One-shot learning of object categories", "word_idx": 54984, "sentence_idx": 1289, "label": "unlabeled"}, {"type": "text", "expr": "One-shot learning of object categories", "word_idx": 55022, "sentence_idx": 1290, "label": "unlabeled"}, {"type": "text", "expr": "TPAMI , 28(4):594\u2013611, 2006", "word_idx": 55060, "sentence_idx": 1291, "label": "unlabeled"}, {"type": "text", "expr": "TPAMI", "word_idx": 55087, "sentence_idx": 1292, "label": "unlabeled"}, {"type": "text", "expr": ", 28(4):594\u2013611, 2006", "word_idx": 55092, "sentence_idx": 1293, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Harnad", "word_idx": 55113, "sentence_idx": 1294, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Harnad", "word_idx": 55120, "sentence_idx": 1295, "label": "unlabeled"}, {"type": "text", "expr": "The symbol grounding problem", "word_idx": 55127, "sentence_idx": 1296, "label": "unlabeled"}, {"type": "text", "expr": "The symbol grounding problem", "word_idx": 55155, "sentence_idx": 1297, "label": "unlabeled"}, {"type": "text", "expr": "Physica D: Nonlinear Phenomena , 42(1-3):335\u2013346, 1990", "word_idx": 55183, "sentence_idx": 1298, "label": "unlabeled"}, {"type": "text", "expr": "Physica D: Nonlinear Phenomena", "word_idx": 55237, "sentence_idx": 1299, "label": "unlabeled"}, {"type": "text", "expr": ", 42(1-3):335\u2013346, 1990", "word_idx": 55267, "sentence_idx": 1300, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, X", "word_idx": 55290, "sentence_idx": 1301, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang, S", "word_idx": 55296, "sentence_idx": 1302, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ren, and J", "word_idx": 55305, "sentence_idx": 1303, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, X", "word_idx": 55316, "sentence_idx": 1304, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang, S", "word_idx": 55322, "sentence_idx": 1305, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ren, and J", "word_idx": 55331, "sentence_idx": 1306, "label": "unlabeled"}, {"type": "text", "expr": "Deep residual learning for image recognition", "word_idx": 55342, "sentence_idx": 1307, "label": "unlabeled"}, {"type": "text", "expr": "Deep residual learning for image recognition", "word_idx": 55386, "sentence_idx": 1308, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2016", "word_idx": 55430, "sentence_idx": 1309, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 55445, "sentence_idx": 1310, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Henaff, J", "word_idx": 55451, "sentence_idx": 1311, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bruna, and Y", "word_idx": 55461, "sentence_idx": 1312, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0LeCun", "word_idx": 55474, "sentence_idx": 1313, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Henaff, J", "word_idx": 55480, "sentence_idx": 1314, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bruna, and Y", "word_idx": 55490, "sentence_idx": 1315, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0LeCun", "word_idx": 55503, "sentence_idx": 1316, "label": "unlabeled"}, {"type": "text", "expr": "Deep convolutional networks on graph-structured data", "word_idx": 55509, "sentence_idx": 1317, "label": "unlabeled"}, {"type": "text", "expr": "Deep convolutional networks on graph-structured data", "word_idx": 55561, "sentence_idx": 1318, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1506", "word_idx": 55613, "sentence_idx": 1319, "label": "unlabeled"}, {"type": "text", "expr": "05163 , 2015", "word_idx": 55638, "sentence_idx": 1320, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1506", "word_idx": 55650, "sentence_idx": 1321, "label": "unlabeled"}, {"type": "text", "expr": "05163", "word_idx": 55675, "sentence_idx": 1322, "label": "unlabeled"}, {"type": "text", "expr": ", 2015", "word_idx": 55680, "sentence_idx": 1323, "label": "unlabeled"}, {"type": "text", "expr": " Hobbs, M", "word_idx": 55686, "sentence_idx": 1324, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Stickel, P", "word_idx": 55695, "sentence_idx": 1325, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Martin, and D", "word_idx": 55706, "sentence_idx": 1326, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Edwards", "word_idx": 55720, "sentence_idx": 1327, "label": "unlabeled"}, {"type": "text", "expr": " Hobbs, M", "word_idx": 55728, "sentence_idx": 1328, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Stickel, P", "word_idx": 55737, "sentence_idx": 1329, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Martin, and D", "word_idx": 55748, "sentence_idx": 1330, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Edwards", "word_idx": 55762, "sentence_idx": 1331, "label": "unlabeled"}, {"type": "text", "expr": "Interpretation as abduction", "word_idx": 55770, "sentence_idx": 1332, "label": "unlabeled"}, {"type": "text", "expr": "Interpretation as abduction", "word_idx": 55797, "sentence_idx": 1333, "label": "unlabeled"}, {"type": "text", "expr": "In  ACL , 1988", "word_idx": 55824, "sentence_idx": 1334, "label": "unlabeled"}, {"type": "text", "expr": ", 1988", "word_idx": 55838, "sentence_idx": 1335, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hoiem, Y", "word_idx": 55844, "sentence_idx": 1336, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chodpathumwan, and Q", "word_idx": 55853, "sentence_idx": 1337, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hoiem, Y", "word_idx": 55874, "sentence_idx": 1338, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chodpathumwan, and Q", "word_idx": 55883, "sentence_idx": 1339, "label": "unlabeled"}, {"type": "text", "expr": "Diagnosing error in object detectors", "word_idx": 55904, "sentence_idx": 1340, "label": "unlabeled"}, {"type": "text", "expr": "Diagnosing error in object detectors", "word_idx": 55940, "sentence_idx": 1341, "label": "unlabeled"}, {"type": "text", "expr": "ECCV , 2012", "word_idx": 55976, "sentence_idx": 1342, "label": "unlabeled"}, {"type": "text", "expr": ", 2012", "word_idx": 55987, "sentence_idx": 1343, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hu, Z", "word_idx": 55993, "sentence_idx": 1344, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yang, R", "word_idx": 55999, "sentence_idx": 1345, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Salakhutdinov, and E", "word_idx": 56007, "sentence_idx": 1346, "label": "unlabeled"}, {"type": "text", "expr": " Xing", "word_idx": 56028, "sentence_idx": 1347, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hu, Z", "word_idx": 56033, "sentence_idx": 1348, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yang, R", "word_idx": 56039, "sentence_idx": 1349, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Salakhutdinov, and E", "word_idx": 56047, "sentence_idx": 1350, "label": "unlabeled"}, {"type": "text", "expr": " Xing", "word_idx": 56068, "sentence_idx": 1351, "label": "unlabeled"}, {"type": "text", "expr": "Deep neural networks with massive learned knowledge", "word_idx": 56073, "sentence_idx": 1352, "label": "unlabeled"}, {"type": "text", "expr": "Deep neural networks with massive learned knowledge", "word_idx": 56124, "sentence_idx": 1353, "label": "unlabeled"}, {"type": "text", "expr": "In  EMNLP , 2016", "word_idx": 56175, "sentence_idx": 1354, "label": "unlabeled"}, {"type": "text", "expr": "EMNLP", "word_idx": 56191, "sentence_idx": 1355, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 56196, "sentence_idx": 1356, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Johnson, R", "word_idx": 56202, "sentence_idx": 1357, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Krishna, M", "word_idx": 56213, "sentence_idx": 1358, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Stark, L", "word_idx": 56224, "sentence_idx": 1359, "label": "unlabeled"}, {"type": "text", "expr": " Li, D", "word_idx": 56233, "sentence_idx": 1360, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Shamma, M", "word_idx": 56239, "sentence_idx": 1361, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bernstein, and\nL", "word_idx": 56249, "sentence_idx": 1362, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fei-Fei", "word_idx": 56266, "sentence_idx": 1363, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Johnson, R", "word_idx": 56274, "sentence_idx": 1364, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Krishna, M", "word_idx": 56285, "sentence_idx": 1365, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Stark, L", "word_idx": 56296, "sentence_idx": 1366, "label": "unlabeled"}, {"type": "text", "expr": " Li, D", "word_idx": 56305, "sentence_idx": 1367, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Shamma, M", "word_idx": 56311, "sentence_idx": 1368, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bernstein, and\nL", "word_idx": 56321, "sentence_idx": 1369, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fei-Fei", "word_idx": 56338, "sentence_idx": 1370, "label": "unlabeled"}, {"type": "text", "expr": "Image retrieval using scene graphs", "word_idx": 56346, "sentence_idx": 1371, "label": "unlabeled"}, {"type": "text", "expr": "Image retrieval using scene graphs", "word_idx": 56380, "sentence_idx": 1372, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2015", "word_idx": 56414, "sentence_idx": 1373, "label": "unlabeled"}, {"type": "text", "expr": ", 2015", "word_idx": 56429, "sentence_idx": 1374, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Joulin, E", "word_idx": 56435, "sentence_idx": 1375, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Grave, P", "word_idx": 56445, "sentence_idx": 1376, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bojanowski, M", "word_idx": 56454, "sentence_idx": 1377, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Douze, H", "word_idx": 56468, "sentence_idx": 1378, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0J\u00e9gou, and T", "word_idx": 56477, "sentence_idx": 1379, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mikolov", "word_idx": 56490, "sentence_idx": 1380, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Joulin, E", "word_idx": 56498, "sentence_idx": 1381, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Grave, P", "word_idx": 56508, "sentence_idx": 1382, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bojanowski, M", "word_idx": 56517, "sentence_idx": 1383, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Douze, H", "word_idx": 56531, "sentence_idx": 1384, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0J\u00e9gou, and T", "word_idx": 56540, "sentence_idx": 1385, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mikolov", "word_idx": 56553, "sentence_idx": 1386, "label": "unlabeled"}, {"type": "text", "expr": "Fasttext", "word_idx": 56561, "sentence_idx": 1387, "label": "unlabeled"}, {"type": "text", "expr": "zip: Compressing text classification models", "word_idx": 56569, "sentence_idx": 1388, "label": "unlabeled"}, {"type": "text", "expr": "Fasttext", "word_idx": 56612, "sentence_idx": 1389, "label": "unlabeled"}, {"type": "text", "expr": "zip: Compressing text classification models", "word_idx": 56620, "sentence_idx": 1390, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1612", "word_idx": 56663, "sentence_idx": 1391, "label": "unlabeled"}, {"type": "text", "expr": "03651 , 2016", "word_idx": 56688, "sentence_idx": 1392, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1612", "word_idx": 56700, "sentence_idx": 1393, "label": "unlabeled"}, {"type": "text", "expr": "03651", "word_idx": 56725, "sentence_idx": 1394, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 56730, "sentence_idx": 1395, "label": "unlabeled"}, {"type": "text", "expr": " Kipf and M", "word_idx": 56736, "sentence_idx": 1396, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Welling", "word_idx": 56747, "sentence_idx": 1397, "label": "unlabeled"}, {"type": "text", "expr": " Kipf and M", "word_idx": 56755, "sentence_idx": 1398, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Welling", "word_idx": 56766, "sentence_idx": 1399, "label": "unlabeled"}, {"type": "text", "expr": "Semi-supervised classification with graph convolutional networks", "word_idx": 56774, "sentence_idx": 1400, "label": "unlabeled"}, {"type": "text", "expr": "Semi-supervised classification with graph convolutional networks", "word_idx": 56838, "sentence_idx": 1401, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1609", "word_idx": 56902, "sentence_idx": 1402, "label": "unlabeled"}, {"type": "text", "expr": "02907 , 2016", "word_idx": 56927, "sentence_idx": 1403, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1609", "word_idx": 56939, "sentence_idx": 1404, "label": "unlabeled"}, {"type": "text", "expr": "02907", "word_idx": 56964, "sentence_idx": 1405, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 56969, "sentence_idx": 1406, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kr\u00e4henb\u00fchl and V", "word_idx": 56975, "sentence_idx": 1407, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Koltun", "word_idx": 56992, "sentence_idx": 1408, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kr\u00e4henb\u00fchl and V", "word_idx": 56999, "sentence_idx": 1409, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Koltun", "word_idx": 57016, "sentence_idx": 1410, "label": "unlabeled"}, {"type": "text", "expr": "Efficient inference in fully connected crfs with gaussian edge\npotentials", "word_idx": 57023, "sentence_idx": 1411, "label": "unlabeled"}, {"type": "text", "expr": "Efficient inference in fully connected crfs with gaussian edge\npotentials", "word_idx": 57096, "sentence_idx": 1412, "label": "unlabeled"}, {"type": "text", "expr": "In  NIPS , 2011", "word_idx": 57169, "sentence_idx": 1413, "label": "unlabeled"}, {"type": "text", "expr": ", 2011", "word_idx": 57184, "sentence_idx": 1414, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Krishna, Y", "word_idx": 57190, "sentence_idx": 1415, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhu, O", "word_idx": 57201, "sentence_idx": 1416, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Groth, J", "word_idx": 57208, "sentence_idx": 1417, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Johnson, K", "word_idx": 57217, "sentence_idx": 1418, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hata, J", "word_idx": 57228, "sentence_idx": 1419, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kravitz, S", "word_idx": 57236, "sentence_idx": 1420, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chen,\nY", "word_idx": 57247, "sentence_idx": 1421, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kalantidis, L", "word_idx": 57255, "sentence_idx": 1422, "label": "unlabeled"}, {"type": "text", "expr": " Li, D", "word_idx": 57269, "sentence_idx": 1423, "label": "unlabeled"}, {"type": "text", "expr": " Shamma, et\u00a0al", "word_idx": 57275, "sentence_idx": 1424, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Krishna, Y", "word_idx": 57289, "sentence_idx": 1425, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhu, O", "word_idx": 57300, "sentence_idx": 1426, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Groth, J", "word_idx": 57307, "sentence_idx": 1427, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Johnson, K", "word_idx": 57316, "sentence_idx": 1428, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hata, J", "word_idx": 57327, "sentence_idx": 1429, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kravitz, S", "word_idx": 57335, "sentence_idx": 1430, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chen,\nY", "word_idx": 57346, "sentence_idx": 1431, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kalantidis, L", "word_idx": 57354, "sentence_idx": 1432, "label": "unlabeled"}, {"type": "text", "expr": " Li, D", "word_idx": 57368, "sentence_idx": 1433, "label": "unlabeled"}, {"type": "text", "expr": " Shamma, et\u00a0al", "word_idx": 57374, "sentence_idx": 1434, "label": "unlabeled"}, {"type": "text", "expr": "Visual genome: Connecting language and vision using crowdsourced\ndense image annotations", "word_idx": 57388, "sentence_idx": 1435, "label": "unlabeled"}, {"type": "text", "expr": "Visual genome: Connecting language and vision using crowdsourced\ndense image annotations", "word_idx": 57476, "sentence_idx": 1436, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1602", "word_idx": 57564, "sentence_idx": 1437, "label": "unlabeled"}, {"type": "text", "expr": "07332 , 2016", "word_idx": 57574, "sentence_idx": 1438, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1602", "word_idx": 57586, "sentence_idx": 1439, "label": "unlabeled"}, {"type": "text", "expr": "07332", "word_idx": 57596, "sentence_idx": 1440, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 57601, "sentence_idx": 1441, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lao, T", "word_idx": 57607, "sentence_idx": 1442, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mitchell, and W", "word_idx": 57614, "sentence_idx": 1443, "label": "unlabeled"}, {"type": "text", "expr": " Cohen", "word_idx": 57630, "sentence_idx": 1444, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lao, T", "word_idx": 57636, "sentence_idx": 1445, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mitchell, and W", "word_idx": 57643, "sentence_idx": 1446, "label": "unlabeled"}, {"type": "text", "expr": " Cohen", "word_idx": 57659, "sentence_idx": 1447, "label": "unlabeled"}, {"type": "text", "expr": "Random walk inference and learning in a large scale knowledge base", "word_idx": 57665, "sentence_idx": 1448, "label": "unlabeled"}, {"type": "text", "expr": "Random walk inference and learning in a large scale knowledge base", "word_idx": 57731, "sentence_idx": 1449, "label": "unlabeled"}, {"type": "text", "expr": "In  EMNLP , 2011", "word_idx": 57797, "sentence_idx": 1450, "label": "unlabeled"}, {"type": "text", "expr": "EMNLP", "word_idx": 57813, "sentence_idx": 1451, "label": "unlabeled"}, {"type": "text", "expr": ", 2011", "word_idx": 57818, "sentence_idx": 1452, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Li, D", "word_idx": 57824, "sentence_idx": 1453, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Tarlow, M", "word_idx": 57830, "sentence_idx": 1454, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Brockschmidt, and R", "word_idx": 57840, "sentence_idx": 1455, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zemel", "word_idx": 57860, "sentence_idx": 1456, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Li, D", "word_idx": 57866, "sentence_idx": 1457, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Tarlow, M", "word_idx": 57872, "sentence_idx": 1458, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Brockschmidt, and R", "word_idx": 57882, "sentence_idx": 1459, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zemel", "word_idx": 57902, "sentence_idx": 1460, "label": "unlabeled"}, {"type": "text", "expr": "Gated graph sequence neural networks", "word_idx": 57908, "sentence_idx": 1461, "label": "unlabeled"}, {"type": "text", "expr": "Gated graph sequence neural networks", "word_idx": 57944, "sentence_idx": 1462, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1511", "word_idx": 57980, "sentence_idx": 1463, "label": "unlabeled"}, {"type": "text", "expr": "05493 , 2015", "word_idx": 58005, "sentence_idx": 1464, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1511", "word_idx": 58017, "sentence_idx": 1465, "label": "unlabeled"}, {"type": "text", "expr": "05493", "word_idx": 58042, "sentence_idx": 1466, "label": "unlabeled"}, {"type": "text", "expr": ", 2015", "word_idx": 58047, "sentence_idx": 1467, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Liben-Nowell and J", "word_idx": 58053, "sentence_idx": 1468, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kleinberg", "word_idx": 58072, "sentence_idx": 1469, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Liben-Nowell and J", "word_idx": 58082, "sentence_idx": 1470, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kleinberg", "word_idx": 58101, "sentence_idx": 1471, "label": "unlabeled"}, {"type": "text", "expr": "The link-prediction problem for social networks", "word_idx": 58111, "sentence_idx": 1472, "label": "unlabeled"}, {"type": "text", "expr": "The link-prediction problem for social networks", "word_idx": 58158, "sentence_idx": 1473, "label": "unlabeled"}, {"type": "text", "expr": "Journal of the Association for Information Science and\nTechnology , 58(7):1019\u20131031, 2007", "word_idx": 58205, "sentence_idx": 1474, "label": "unlabeled"}, {"type": "text", "expr": "Journal of the Association for Information Science and\nTechnology", "word_idx": 58294, "sentence_idx": 1475, "label": "unlabeled"}, {"type": "text", "expr": ", 58(7):1019\u20131031, 2007", "word_idx": 58359, "sentence_idx": 1476, "label": "unlabeled"}, {"type": "text", "expr": " Lin, P", "word_idx": 58382, "sentence_idx": 1477, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Doll\u00e1r, R", "word_idx": 58389, "sentence_idx": 1478, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Girshick, K", "word_idx": 58399, "sentence_idx": 1479, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, B", "word_idx": 58411, "sentence_idx": 1480, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hariharan, and S", "word_idx": 58417, "sentence_idx": 1481, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Belongie", "word_idx": 58434, "sentence_idx": 1482, "label": "unlabeled"}, {"type": "text", "expr": " Lin, P", "word_idx": 58443, "sentence_idx": 1483, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Doll\u00e1r, R", "word_idx": 58450, "sentence_idx": 1484, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Girshick, K", "word_idx": 58460, "sentence_idx": 1485, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, B", "word_idx": 58472, "sentence_idx": 1486, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hariharan, and S", "word_idx": 58478, "sentence_idx": 1487, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Belongie", "word_idx": 58495, "sentence_idx": 1488, "label": "unlabeled"}, {"type": "text", "expr": "Feature pyramid networks for object detection", "word_idx": 58504, "sentence_idx": 1489, "label": "unlabeled"}, {"type": "text", "expr": "Feature pyramid networks for object detection", "word_idx": 58549, "sentence_idx": 1490, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1612", "word_idx": 58594, "sentence_idx": 1491, "label": "unlabeled"}, {"type": "text", "expr": "03144 , 2016", "word_idx": 58604, "sentence_idx": 1492, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1612", "word_idx": 58616, "sentence_idx": 1493, "label": "unlabeled"}, {"type": "text", "expr": "03144", "word_idx": 58626, "sentence_idx": 1494, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 58631, "sentence_idx": 1495, "label": "unlabeled"}, {"type": "text", "expr": " Lin, M", "word_idx": 58637, "sentence_idx": 1496, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Maire, S", "word_idx": 58644, "sentence_idx": 1497, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Belongie, J", "word_idx": 58653, "sentence_idx": 1498, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hays, P", "word_idx": 58665, "sentence_idx": 1499, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Perona, D", "word_idx": 58673, "sentence_idx": 1500, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ramanan,\nP", "word_idx": 58683, "sentence_idx": 1501, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Doll\u00e1r, and C", "word_idx": 58694, "sentence_idx": 1502, "label": "unlabeled"}, {"type": "text", "expr": " Zitnick", "word_idx": 58708, "sentence_idx": 1503, "label": "unlabeled"}, {"type": "text", "expr": " Lin, M", "word_idx": 58716, "sentence_idx": 1504, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Maire, S", "word_idx": 58723, "sentence_idx": 1505, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Belongie, J", "word_idx": 58732, "sentence_idx": 1506, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hays, P", "word_idx": 58744, "sentence_idx": 1507, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Perona, D", "word_idx": 58752, "sentence_idx": 1508, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ramanan,\nP", "word_idx": 58762, "sentence_idx": 1509, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Doll\u00e1r, and C", "word_idx": 58773, "sentence_idx": 1510, "label": "unlabeled"}, {"type": "text", "expr": " Zitnick", "word_idx": 58787, "sentence_idx": 1511, "label": "unlabeled"}, {"type": "text", "expr": "Microsoft coco: Common objects in context", "word_idx": 58795, "sentence_idx": 1512, "label": "unlabeled"}, {"type": "text", "expr": "Microsoft coco: Common objects in context", "word_idx": 58836, "sentence_idx": 1513, "label": "unlabeled"}, {"type": "text", "expr": "In  ECCV , 2014", "word_idx": 58877, "sentence_idx": 1514, "label": "unlabeled"}, {"type": "text", "expr": ", 2014", "word_idx": 58892, "sentence_idx": 1515, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lu, R", "word_idx": 58898, "sentence_idx": 1516, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Krishna, M", "word_idx": 58904, "sentence_idx": 1517, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bernstein, and L", "word_idx": 58915, "sentence_idx": 1518, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fei-Fei", "word_idx": 58932, "sentence_idx": 1519, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lu, R", "word_idx": 58940, "sentence_idx": 1520, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Krishna, M", "word_idx": 58946, "sentence_idx": 1521, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bernstein, and L", "word_idx": 58957, "sentence_idx": 1522, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fei-Fei", "word_idx": 58974, "sentence_idx": 1523, "label": "unlabeled"}, {"type": "text", "expr": "Visual relationship detection with language priors", "word_idx": 58982, "sentence_idx": 1524, "label": "unlabeled"}, {"type": "text", "expr": "Visual relationship detection with language priors", "word_idx": 59032, "sentence_idx": 1525, "label": "unlabeled"}, {"type": "text", "expr": "In  ECCV , 2016", "word_idx": 59082, "sentence_idx": 1526, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 59097, "sentence_idx": 1527, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Marino, R", "word_idx": 59103, "sentence_idx": 1528, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Salakhutdinov, and A", "word_idx": 59113, "sentence_idx": 1529, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gupta", "word_idx": 59134, "sentence_idx": 1530, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Marino, R", "word_idx": 59140, "sentence_idx": 1531, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Salakhutdinov, and A", "word_idx": 59150, "sentence_idx": 1532, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gupta", "word_idx": 59171, "sentence_idx": 1533, "label": "unlabeled"}, {"type": "text", "expr": "The more you know: Using knowledge graphs for image classification", "word_idx": 59177, "sentence_idx": 1534, "label": "unlabeled"}, {"type": "text", "expr": "The more you know: Using knowledge graphs for image classification", "word_idx": 59243, "sentence_idx": 1535, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1612", "word_idx": 59309, "sentence_idx": 1536, "label": "unlabeled"}, {"type": "text", "expr": "04844 , 2016", "word_idx": 59334, "sentence_idx": 1537, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1612", "word_idx": 59346, "sentence_idx": 1538, "label": "unlabeled"}, {"type": "text", "expr": "04844", "word_idx": 59371, "sentence_idx": 1539, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 59376, "sentence_idx": 1540, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Newell", "word_idx": 59382, "sentence_idx": 1541, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Newell", "word_idx": 59389, "sentence_idx": 1542, "label": "unlabeled"}, {"type": "text", "expr": "Physical symbol systems", "word_idx": 59396, "sentence_idx": 1543, "label": "unlabeled"}, {"type": "text", "expr": "Physical symbol systems", "word_idx": 59419, "sentence_idx": 1544, "label": "unlabeled"}, {"type": "text", "expr": "Cognitive science , 4(2):135\u2013183, 1980", "word_idx": 59442, "sentence_idx": 1545, "label": "unlabeled"}, {"type": "text", "expr": "Cognitive science", "word_idx": 59480, "sentence_idx": 1546, "label": "unlabeled"}, {"type": "text", "expr": ", 4(2):135\u2013183, 1980", "word_idx": 59497, "sentence_idx": 1547, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Newell, K", "word_idx": 59517, "sentence_idx": 1548, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yang, and J", "word_idx": 59527, "sentence_idx": 1549, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Deng", "word_idx": 59539, "sentence_idx": 1550, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Newell, K", "word_idx": 59544, "sentence_idx": 1551, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yang, and J", "word_idx": 59554, "sentence_idx": 1552, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Deng", "word_idx": 59566, "sentence_idx": 1553, "label": "unlabeled"}, {"type": "text", "expr": "Stacked hourglass networks for human pose estimation", "word_idx": 59571, "sentence_idx": 1554, "label": "unlabeled"}, {"type": "text", "expr": "Stacked hourglass networks for human pose estimation", "word_idx": 59623, "sentence_idx": 1555, "label": "unlabeled"}, {"type": "text", "expr": "In  ECCV , 2016", "word_idx": 59675, "sentence_idx": 1556, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 59690, "sentence_idx": 1557, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Niepert, M", "word_idx": 59696, "sentence_idx": 1558, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ahmed, and K", "word_idx": 59707, "sentence_idx": 1559, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kutzkov", "word_idx": 59720, "sentence_idx": 1560, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Niepert, M", "word_idx": 59728, "sentence_idx": 1561, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ahmed, and K", "word_idx": 59739, "sentence_idx": 1562, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kutzkov", "word_idx": 59752, "sentence_idx": 1563, "label": "unlabeled"}, {"type": "text", "expr": "Learning convolutional neural networks for graphs", "word_idx": 59760, "sentence_idx": 1564, "label": "unlabeled"}, {"type": "text", "expr": "Learning convolutional neural networks for graphs", "word_idx": 59809, "sentence_idx": 1565, "label": "unlabeled"}, {"type": "text", "expr": "In  ICML , 2016", "word_idx": 59858, "sentence_idx": 1566, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 59873, "sentence_idx": 1567, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Parikh and K", "word_idx": 59879, "sentence_idx": 1568, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Grauman", "word_idx": 59892, "sentence_idx": 1569, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Parikh and K", "word_idx": 59900, "sentence_idx": 1570, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Grauman", "word_idx": 59913, "sentence_idx": 1571, "label": "unlabeled"}, {"type": "text", "expr": "Relative attributes", "word_idx": 59921, "sentence_idx": 1572, "label": "unlabeled"}, {"type": "text", "expr": "Relative attributes", "word_idx": 59940, "sentence_idx": 1573, "label": "unlabeled"}, {"type": "text", "expr": "In  ICCV , 2011", "word_idx": 59959, "sentence_idx": 1574, "label": "unlabeled"}, {"type": "text", "expr": ", 2011", "word_idx": 59974, "sentence_idx": 1575, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ren, K", "word_idx": 59980, "sentence_idx": 1576, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, R", "word_idx": 59987, "sentence_idx": 1577, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Girshick, and J", "word_idx": 59993, "sentence_idx": 1578, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ren, K", "word_idx": 60009, "sentence_idx": 1579, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, R", "word_idx": 60016, "sentence_idx": 1580, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Girshick, and J", "word_idx": 60022, "sentence_idx": 1581, "label": "unlabeled"}, {"type": "text", "expr": "Faster r-cnn: Towards real-time object detection with region proposal\nnetworks", "word_idx": 60038, "sentence_idx": 1582, "label": "unlabeled"}, {"type": "text", "expr": "Faster r-cnn: Towards real-time object detection with region proposal\nnetworks", "word_idx": 60116, "sentence_idx": 1583, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1506", "word_idx": 60194, "sentence_idx": 1584, "label": "unlabeled"}, {"type": "text", "expr": "01497 , 2015", "word_idx": 60204, "sentence_idx": 1585, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1506", "word_idx": 60216, "sentence_idx": 1586, "label": "unlabeled"}, {"type": "text", "expr": "01497", "word_idx": 60226, "sentence_idx": 1587, "label": "unlabeled"}, {"type": "text", "expr": ", 2015", "word_idx": 60231, "sentence_idx": 1588, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ronneberger, P", "word_idx": 60237, "sentence_idx": 1589, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fischer, and T", "word_idx": 60252, "sentence_idx": 1590, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Brox", "word_idx": 60267, "sentence_idx": 1591, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ronneberger, P", "word_idx": 60272, "sentence_idx": 1592, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fischer, and T", "word_idx": 60287, "sentence_idx": 1593, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Brox", "word_idx": 60302, "sentence_idx": 1594, "label": "unlabeled"}, {"type": "text", "expr": "U-net: Convolutional networks for biomedical image segmentation", "word_idx": 60307, "sentence_idx": 1595, "label": "unlabeled"}, {"type": "text", "expr": "U-net: Convolutional networks for biomedical image segmentation", "word_idx": 60370, "sentence_idx": 1596, "label": "unlabeled"}, {"type": "text", "expr": "In  MICCAI , 2015", "word_idx": 60433, "sentence_idx": 1597, "label": "unlabeled"}, {"type": "text", "expr": "MICCAI", "word_idx": 60450, "sentence_idx": 1598, "label": "unlabeled"}, {"type": "text", "expr": ", 2015", "word_idx": 60456, "sentence_idx": 1599, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Russakovsky, J", "word_idx": 60462, "sentence_idx": 1600, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Deng, H", "word_idx": 60477, "sentence_idx": 1601, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Su, J", "word_idx": 60485, "sentence_idx": 1602, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Krause, S", "word_idx": 60491, "sentence_idx": 1603, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Satheesh, S", "word_idx": 60501, "sentence_idx": 1604, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ma, Z", "word_idx": 60513, "sentence_idx": 1605, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Huang,\nA", "word_idx": 60519, "sentence_idx": 1606, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Karpathy, A", "word_idx": 60528, "sentence_idx": 1607, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Khosla, M", "word_idx": 60540, "sentence_idx": 1608, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bernstein, et\u00a0al", "word_idx": 60550, "sentence_idx": 1609, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Russakovsky, J", "word_idx": 60567, "sentence_idx": 1610, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Deng, H", "word_idx": 60582, "sentence_idx": 1611, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Su, J", "word_idx": 60590, "sentence_idx": 1612, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Krause, S", "word_idx": 60596, "sentence_idx": 1613, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Satheesh, S", "word_idx": 60606, "sentence_idx": 1614, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ma, Z", "word_idx": 60618, "sentence_idx": 1615, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Huang,\nA", "word_idx": 60624, "sentence_idx": 1616, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Karpathy, A", "word_idx": 60633, "sentence_idx": 1617, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Khosla, M", "word_idx": 60645, "sentence_idx": 1618, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bernstein, et\u00a0al", "word_idx": 60655, "sentence_idx": 1619, "label": "unlabeled"}, {"type": "text", "expr": "Imagenet large scale visual recognition challenge", "word_idx": 60672, "sentence_idx": 1620, "label": "unlabeled"}, {"type": "text", "expr": "Imagenet large scale visual recognition challenge", "word_idx": 60721, "sentence_idx": 1621, "label": "unlabeled"}, {"type": "text", "expr": "IJCV , 115(3):211\u2013252, 2015", "word_idx": 60770, "sentence_idx": 1622, "label": "unlabeled"}, {"type": "text", "expr": ", 115(3):211\u2013252, 2015", "word_idx": 60797, "sentence_idx": 1623, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Santoro, D", "word_idx": 60819, "sentence_idx": 1624, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Raposo, D", "word_idx": 60830, "sentence_idx": 1625, "label": "unlabeled"}, {"type": "text", "expr": " Barrett, M", "word_idx": 60840, "sentence_idx": 1626, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Malinowski, R", "word_idx": 60851, "sentence_idx": 1627, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Pascanu, P", "word_idx": 60865, "sentence_idx": 1628, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Battaglia,\nand T", "word_idx": 60876, "sentence_idx": 1629, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lillicrap", "word_idx": 60893, "sentence_idx": 1630, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Santoro, D", "word_idx": 60903, "sentence_idx": 1631, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Raposo, D", "word_idx": 60914, "sentence_idx": 1632, "label": "unlabeled"}, {"type": "text", "expr": " Barrett, M", "word_idx": 60924, "sentence_idx": 1633, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Malinowski, R", "word_idx": 60935, "sentence_idx": 1634, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Pascanu, P", "word_idx": 60949, "sentence_idx": 1635, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Battaglia,\nand T", "word_idx": 60960, "sentence_idx": 1636, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lillicrap", "word_idx": 60977, "sentence_idx": 1637, "label": "unlabeled"}, {"type": "text", "expr": "A simple neural network module for relational reasoning", "word_idx": 60987, "sentence_idx": 1638, "label": "unlabeled"}, {"type": "text", "expr": "A simple neural network module for relational reasoning", "word_idx": 61042, "sentence_idx": 1639, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1706", "word_idx": 61097, "sentence_idx": 1640, "label": "unlabeled"}, {"type": "text", "expr": "01427 , 2017", "word_idx": 61122, "sentence_idx": 1641, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1706", "word_idx": 61134, "sentence_idx": 1642, "label": "unlabeled"}, {"type": "text", "expr": "01427", "word_idx": 61159, "sentence_idx": 1643, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 61164, "sentence_idx": 1644, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sarwar, G", "word_idx": 61170, "sentence_idx": 1645, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Karypis, J", "word_idx": 61180, "sentence_idx": 1646, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Konstan, and J", "word_idx": 61191, "sentence_idx": 1647, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Riedl", "word_idx": 61206, "sentence_idx": 1648, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sarwar, G", "word_idx": 61212, "sentence_idx": 1649, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Karypis, J", "word_idx": 61222, "sentence_idx": 1650, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Konstan, and J", "word_idx": 61233, "sentence_idx": 1651, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Riedl", "word_idx": 61248, "sentence_idx": 1652, "label": "unlabeled"}, {"type": "text", "expr": "Item-based collaborative filtering recommendation algorithms", "word_idx": 61254, "sentence_idx": 1653, "label": "unlabeled"}, {"type": "text", "expr": "Item-based collaborative filtering recommendation algorithms", "word_idx": 61314, "sentence_idx": 1654, "label": "unlabeled"}, {"type": "text", "expr": "In  WWW , 2001", "word_idx": 61374, "sentence_idx": 1655, "label": "unlabeled"}, {"type": "text", "expr": ", 2001", "word_idx": 61388, "sentence_idx": 1656, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Scarselli, M", "word_idx": 61394, "sentence_idx": 1657, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gori, A", "word_idx": 61407, "sentence_idx": 1658, "label": "unlabeled"}, {"type": "text", "expr": " Tsoi, M", "word_idx": 61415, "sentence_idx": 1659, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hagenbuchner, and G", "word_idx": 61423, "sentence_idx": 1660, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Monfardini", "word_idx": 61443, "sentence_idx": 1661, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Scarselli, M", "word_idx": 61454, "sentence_idx": 1662, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gori, A", "word_idx": 61467, "sentence_idx": 1663, "label": "unlabeled"}, {"type": "text", "expr": " Tsoi, M", "word_idx": 61475, "sentence_idx": 1664, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hagenbuchner, and G", "word_idx": 61483, "sentence_idx": 1665, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Monfardini", "word_idx": 61503, "sentence_idx": 1666, "label": "unlabeled"}, {"type": "text", "expr": "The graph neural network model", "word_idx": 61514, "sentence_idx": 1667, "label": "unlabeled"}, {"type": "text", "expr": "The graph neural network model", "word_idx": 61544, "sentence_idx": 1668, "label": "unlabeled"}, {"type": "text", "expr": "TNN , 20(1):61\u201380, 2009", "word_idx": 61574, "sentence_idx": 1669, "label": "unlabeled"}, {"type": "text", "expr": ", 20(1):61\u201380, 2009", "word_idx": 61597, "sentence_idx": 1670, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Shrivastava, R", "word_idx": 61616, "sentence_idx": 1671, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sukthankar, J", "word_idx": 61631, "sentence_idx": 1672, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Malik, and A", "word_idx": 61645, "sentence_idx": 1673, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gupta", "word_idx": 61658, "sentence_idx": 1674, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Shrivastava, R", "word_idx": 61664, "sentence_idx": 1675, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sukthankar, J", "word_idx": 61679, "sentence_idx": 1676, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Malik, and A", "word_idx": 61693, "sentence_idx": 1677, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gupta", "word_idx": 61706, "sentence_idx": 1678, "label": "unlabeled"}, {"type": "text", "expr": "Beyond Skip Connections: Top-Down Modulation for Object Detection", "word_idx": 61712, "sentence_idx": 1679, "label": "unlabeled"}, {"type": "text", "expr": "Beyond Skip Connections: Top-Down Modulation for Object Detection", "word_idx": 61777, "sentence_idx": 1680, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1612", "word_idx": 61842, "sentence_idx": 1681, "label": "unlabeled"}, {"type": "text", "expr": "06851 , 2016", "word_idx": 61852, "sentence_idx": 1682, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1612", "word_idx": 61864, "sentence_idx": 1683, "label": "unlabeled"}, {"type": "text", "expr": "06851", "word_idx": 61874, "sentence_idx": 1684, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 61879, "sentence_idx": 1685, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sun, A", "word_idx": 61885, "sentence_idx": 1686, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Shrivastava, S", "word_idx": 61892, "sentence_idx": 1687, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Singh, and A", "word_idx": 61907, "sentence_idx": 1688, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gupta", "word_idx": 61920, "sentence_idx": 1689, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sun, A", "word_idx": 61926, "sentence_idx": 1690, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Shrivastava, S", "word_idx": 61933, "sentence_idx": 1691, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Singh, and A", "word_idx": 61948, "sentence_idx": 1692, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gupta", "word_idx": 61961, "sentence_idx": 1693, "label": "unlabeled"}, {"type": "text", "expr": "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era", "word_idx": 61967, "sentence_idx": 1694, "label": "unlabeled"}, {"type": "text", "expr": "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era", "word_idx": 62033, "sentence_idx": 1695, "label": "unlabeled"}, {"type": "text", "expr": "In  ICCV , 2017", "word_idx": 62099, "sentence_idx": 1696, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 62114, "sentence_idx": 1697, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Torralba and A", "word_idx": 62120, "sentence_idx": 1698, "label": "unlabeled"}, {"type": "text", "expr": " Efros", "word_idx": 62135, "sentence_idx": 1699, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Torralba and A", "word_idx": 62141, "sentence_idx": 1700, "label": "unlabeled"}, {"type": "text", "expr": " Efros", "word_idx": 62156, "sentence_idx": 1701, "label": "unlabeled"}, {"type": "text", "expr": "Unbiased look at dataset bias", "word_idx": 62162, "sentence_idx": 1702, "label": "unlabeled"}, {"type": "text", "expr": "Unbiased look at dataset bias", "word_idx": 62191, "sentence_idx": 1703, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2011", "word_idx": 62220, "sentence_idx": 1704, "label": "unlabeled"}, {"type": "text", "expr": ", 2011", "word_idx": 62235, "sentence_idx": 1705, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Torralba, K", "word_idx": 62241, "sentence_idx": 1706, "label": "unlabeled"}, {"type": "text", "expr": " Murphy, W", "word_idx": 62253, "sentence_idx": 1707, "label": "unlabeled"}, {"type": "text", "expr": " Freeman, M", "word_idx": 62263, "sentence_idx": 1708, "label": "unlabeled"}, {"type": "text", "expr": " Rubin, et\u00a0al", "word_idx": 62274, "sentence_idx": 1709, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Torralba, K", "word_idx": 62287, "sentence_idx": 1710, "label": "unlabeled"}, {"type": "text", "expr": " Murphy, W", "word_idx": 62299, "sentence_idx": 1711, "label": "unlabeled"}, {"type": "text", "expr": " Freeman, M", "word_idx": 62309, "sentence_idx": 1712, "label": "unlabeled"}, {"type": "text", "expr": " Rubin, et\u00a0al", "word_idx": 62320, "sentence_idx": 1713, "label": "unlabeled"}, {"type": "text", "expr": "Context-based vision system for place and object recognition", "word_idx": 62333, "sentence_idx": 1714, "label": "unlabeled"}, {"type": "text", "expr": "Context-based vision system for place and object recognition", "word_idx": 62393, "sentence_idx": 1715, "label": "unlabeled"}, {"type": "text", "expr": "In  ICCV , 2003", "word_idx": 62453, "sentence_idx": 1716, "label": "unlabeled"}, {"type": "text", "expr": ", 2003", "word_idx": 62468, "sentence_idx": 1717, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Tu and X", "word_idx": 62474, "sentence_idx": 1718, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Tu and X", "word_idx": 62483, "sentence_idx": 1719, "label": "unlabeled"}, {"type": "text", "expr": "Auto-context and its application to high-level vision tasks and 3d\nbrain image segmentation", "word_idx": 62492, "sentence_idx": 1720, "label": "unlabeled"}, {"type": "text", "expr": "Auto-context and its application to high-level vision tasks and 3d\nbrain image segmentation", "word_idx": 62583, "sentence_idx": 1721, "label": "unlabeled"}, {"type": "text", "expr": "TPAMI , 32(10):1744\u20131757, 2010", "word_idx": 62674, "sentence_idx": 1722, "label": "unlabeled"}, {"type": "text", "expr": "TPAMI", "word_idx": 62704, "sentence_idx": 1723, "label": "unlabeled"}, {"type": "text", "expr": ", 32(10):1744\u20131757, 2010", "word_idx": 62709, "sentence_idx": 1724, "label": "unlabeled"}, {"type": "text", "expr": " Wei, V", "word_idx": 62733, "sentence_idx": 1725, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ramakrishna, T", "word_idx": 62740, "sentence_idx": 1726, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kanade, and Y", "word_idx": 62755, "sentence_idx": 1727, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sheikh", "word_idx": 62769, "sentence_idx": 1728, "label": "unlabeled"}, {"type": "text", "expr": " Wei, V", "word_idx": 62776, "sentence_idx": 1729, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ramakrishna, T", "word_idx": 62783, "sentence_idx": 1730, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kanade, and Y", "word_idx": 62798, "sentence_idx": 1731, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sheikh", "word_idx": 62812, "sentence_idx": 1732, "label": "unlabeled"}, {"type": "text", "expr": "Convolutional pose machines", "word_idx": 62819, "sentence_idx": 1733, "label": "unlabeled"}, {"type": "text", "expr": "Convolutional pose machines", "word_idx": 62846, "sentence_idx": 1734, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2016", "word_idx": 62873, "sentence_idx": 1735, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 62888, "sentence_idx": 1736, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wu, P", "word_idx": 62894, "sentence_idx": 1737, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wang, C", "word_idx": 62900, "sentence_idx": 1738, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Shen, A", "word_idx": 62908, "sentence_idx": 1739, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dick, and A", "word_idx": 62916, "sentence_idx": 1740, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0van\u00a0den Hengel", "word_idx": 62928, "sentence_idx": 1741, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wu, P", "word_idx": 62943, "sentence_idx": 1742, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Wang, C", "word_idx": 62949, "sentence_idx": 1743, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Shen, A", "word_idx": 62957, "sentence_idx": 1744, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dick, and A", "word_idx": 62965, "sentence_idx": 1745, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0van\u00a0den Hengel", "word_idx": 62977, "sentence_idx": 1746, "label": "unlabeled"}, {"type": "text", "expr": "Ask me anything: Free-form visual question answering based on\nknowledge from external sources", "word_idx": 62992, "sentence_idx": 1747, "label": "unlabeled"}, {"type": "text", "expr": "Ask me anything: Free-form visual question answering based on\nknowledge from external sources", "word_idx": 63085, "sentence_idx": 1748, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2016", "word_idx": 63178, "sentence_idx": 1749, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 63193, "sentence_idx": 1750, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xie, X", "word_idx": 63199, "sentence_idx": 1751, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Huang, and Z", "word_idx": 63206, "sentence_idx": 1752, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xie, X", "word_idx": 63219, "sentence_idx": 1753, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Huang, and Z", "word_idx": 63226, "sentence_idx": 1754, "label": "unlabeled"}, {"type": "text", "expr": "Top-down learning for structured labeling with convolutional\n pseudoprior", "word_idx": 63239, "sentence_idx": 1755, "label": "unlabeled"}, {"type": "text", "expr": "Top-down learning for structured labeling with convolutional", "word_idx": 63312, "sentence_idx": 1756, "label": "unlabeled"}, {"type": "text", "expr": "pseudoprior", "word_idx": 63372, "sentence_idx": 1757, "label": "unlabeled"}, {"type": "text", "expr": "In  ECCV , 2016", "word_idx": 63383, "sentence_idx": 1758, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 63398, "sentence_idx": 1759, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xiong, S", "word_idx": 63404, "sentence_idx": 1760, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Merity, and R", "word_idx": 63413, "sentence_idx": 1761, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Socher", "word_idx": 63427, "sentence_idx": 1762, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xiong, S", "word_idx": 63434, "sentence_idx": 1763, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Merity, and R", "word_idx": 63443, "sentence_idx": 1764, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Socher", "word_idx": 63457, "sentence_idx": 1765, "label": "unlabeled"}, {"type": "text", "expr": "Dynamic memory networks for visual and textual question answering", "word_idx": 63464, "sentence_idx": 1766, "label": "unlabeled"}, {"type": "text", "expr": "Dynamic memory networks for visual and textual question answering", "word_idx": 63529, "sentence_idx": 1767, "label": "unlabeled"}, {"type": "text", "expr": "arXiv , 1603, 2016", "word_idx": 63594, "sentence_idx": 1768, "label": "unlabeled"}, {"type": "text", "expr": "arXiv", "word_idx": 63612, "sentence_idx": 1769, "label": "unlabeled"}, {"type": "text", "expr": ", 1603, 2016", "word_idx": 63617, "sentence_idx": 1770, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xu, Y", "word_idx": 63629, "sentence_idx": 1771, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhu, C", "word_idx": 63635, "sentence_idx": 1772, "label": "unlabeled"}, {"type": "text", "expr": " Choy, and L", "word_idx": 63642, "sentence_idx": 1773, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fei-Fei", "word_idx": 63654, "sentence_idx": 1774, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xu, Y", "word_idx": 63662, "sentence_idx": 1775, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhu, C", "word_idx": 63668, "sentence_idx": 1776, "label": "unlabeled"}, {"type": "text", "expr": " Choy, and L", "word_idx": 63675, "sentence_idx": 1777, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fei-Fei", "word_idx": 63687, "sentence_idx": 1778, "label": "unlabeled"}, {"type": "text", "expr": "Scene graph generation by iterative message passing", "word_idx": 63695, "sentence_idx": 1779, "label": "unlabeled"}, {"type": "text", "expr": "Scene graph generation by iterative message passing", "word_idx": 63746, "sentence_idx": 1780, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1701", "word_idx": 63797, "sentence_idx": 1781, "label": "unlabeled"}, {"type": "text", "expr": "02426 , 2017", "word_idx": 63822, "sentence_idx": 1782, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1701", "word_idx": 63834, "sentence_idx": 1783, "label": "unlabeled"}, {"type": "text", "expr": "02426", "word_idx": 63859, "sentence_idx": 1784, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 63864, "sentence_idx": 1785, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yang, X", "word_idx": 63870, "sentence_idx": 1786, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, J", "word_idx": 63878, "sentence_idx": 1787, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gao, L", "word_idx": 63884, "sentence_idx": 1788, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Deng, and A", "word_idx": 63891, "sentence_idx": 1789, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Smola", "word_idx": 63903, "sentence_idx": 1790, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yang, X", "word_idx": 63909, "sentence_idx": 1791, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, J", "word_idx": 63917, "sentence_idx": 1792, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gao, L", "word_idx": 63923, "sentence_idx": 1793, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Deng, and A", "word_idx": 63930, "sentence_idx": 1794, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Smola", "word_idx": 63942, "sentence_idx": 1795, "label": "unlabeled"}, {"type": "text", "expr": "Stacked attention networks for image question answering", "word_idx": 63948, "sentence_idx": 1796, "label": "unlabeled"}, {"type": "text", "expr": "Stacked attention networks for image question answering", "word_idx": 64003, "sentence_idx": 1797, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2016", "word_idx": 64058, "sentence_idx": 1798, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 64073, "sentence_idx": 1799, "label": "unlabeled"}, {"type": "text", "expr": " Zeiler and R", "word_idx": 64079, "sentence_idx": 1800, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fergus", "word_idx": 64092, "sentence_idx": 1801, "label": "unlabeled"}, {"type": "text", "expr": " Zeiler and R", "word_idx": 64099, "sentence_idx": 1802, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fergus", "word_idx": 64112, "sentence_idx": 1803, "label": "unlabeled"}, {"type": "text", "expr": "Visualizing and understanding convolutional networks", "word_idx": 64119, "sentence_idx": 1804, "label": "unlabeled"}, {"type": "text", "expr": "Visualizing and understanding convolutional networks", "word_idx": 64171, "sentence_idx": 1805, "label": "unlabeled"}, {"type": "text", "expr": "In  ECCV , 2014", "word_idx": 64223, "sentence_idx": 1806, "label": "unlabeled"}, {"type": "text", "expr": ", 2014", "word_idx": 64238, "sentence_idx": 1807, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhou, H", "word_idx": 64244, "sentence_idx": 1808, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhao, X", "word_idx": 64252, "sentence_idx": 1809, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Puig, S", "word_idx": 64260, "sentence_idx": 1810, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fidler, A", "word_idx": 64268, "sentence_idx": 1811, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Barriuso, and A", "word_idx": 64278, "sentence_idx": 1812, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Torralba", "word_idx": 64294, "sentence_idx": 1813, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhou, H", "word_idx": 64303, "sentence_idx": 1814, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhao, X", "word_idx": 64311, "sentence_idx": 1815, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Puig, S", "word_idx": 64319, "sentence_idx": 1816, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fidler, A", "word_idx": 64327, "sentence_idx": 1817, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Barriuso, and A", "word_idx": 64337, "sentence_idx": 1818, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Torralba", "word_idx": 64353, "sentence_idx": 1819, "label": "unlabeled"}, {"type": "text", "expr": "Semantic understanding of scenes through the ade20k dataset", "word_idx": 64362, "sentence_idx": 1820, "label": "unlabeled"}, {"type": "text", "expr": "Semantic understanding of scenes through the ade20k dataset", "word_idx": 64421, "sentence_idx": 1821, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1608", "word_idx": 64480, "sentence_idx": 1822, "label": "unlabeled"}, {"type": "text", "expr": "05442 , 2016", "word_idx": 64505, "sentence_idx": 1823, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1608", "word_idx": 64517, "sentence_idx": 1824, "label": "unlabeled"}, {"type": "text", "expr": "05442", "word_idx": 64542, "sentence_idx": 1825, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 64547, "sentence_idx": 1826, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhu, C", "word_idx": 64553, "sentence_idx": 1827, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang, C", "word_idx": 64560, "sentence_idx": 1828, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0R\u00e9, and L", "word_idx": 64569, "sentence_idx": 1829, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fei-Fei", "word_idx": 64579, "sentence_idx": 1830, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhu, C", "word_idx": 64587, "sentence_idx": 1831, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang, C", "word_idx": 64594, "sentence_idx": 1832, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0R\u00e9, and L", "word_idx": 64603, "sentence_idx": 1833, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fei-Fei", "word_idx": 64613, "sentence_idx": 1834, "label": "unlabeled"}, {"type": "text", "expr": "Building a large-scale multimodal knowledge base system for answering\nvisual queries", "word_idx": 64621, "sentence_idx": 1835, "label": "unlabeled"}, {"type": "text", "expr": "Building a large-scale multimodal knowledge base system for answering\nvisual queries", "word_idx": 64705, "sentence_idx": 1836, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1507", "word_idx": 64789, "sentence_idx": 1837, "label": "unlabeled"}, {"type": "text", "expr": "05670 , 2015", "word_idx": 64799, "sentence_idx": 1838, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1507", "word_idx": 64811, "sentence_idx": 1839, "label": "unlabeled"}, {"type": "text", "expr": "05670", "word_idx": 64821, "sentence_idx": 1840, "label": "unlabeled"}, {"type": "text", "expr": ", 2015", "word_idx": 64826, "sentence_idx": 1841, "label": "unlabeled"}, {"type": "text", "expr": "Generated  on Tue Apr  3 11:30:19 2018 by", "word_idx": 64832, "sentence_idx": 1842, "label": "unlabeled"}, {"type": "text", "expr": "LaTeXML", "word_idx": 64873, "sentence_idx": 1843, "label": "unlabeled"}], "sparse_tensor_generation": [{"type": "text", "expr": "Fredrik Kjolstad", "word_idx": 0, "sentence_idx": 0, "label": "unlabeled"}, {"type": "text", "expr": "Fredrik Kjolstad", "word_idx": 16, "sentence_idx": 1, "label": "unlabeled"}, {"type": "text", "expr": "fred@csail", "word_idx": 32, "sentence_idx": 2, "label": "unlabeled"}, {"type": "text", "expr": "fred@csail", "word_idx": 42, "sentence_idx": 3, "label": "unlabeled"}, {"type": "text", "expr": "Shoaib Kamil", "word_idx": 52, "sentence_idx": 4, "label": "unlabeled"}, {"type": "text", "expr": "Shoaib Kamil", "word_idx": 64, "sentence_idx": 5, "label": "unlabeled"}, {"type": "text", "expr": "Adobe Research, USA", "word_idx": 76, "sentence_idx": 6, "label": "unlabeled"}, {"type": "text", "expr": "Adobe Research, USA", "word_idx": 95, "sentence_idx": 7, "label": "unlabeled"}, {"type": "text", "expr": "kamil@adobe", "word_idx": 114, "sentence_idx": 8, "label": "unlabeled"}, {"type": "text", "expr": "kamil@adobe", "word_idx": 125, "sentence_idx": 9, "label": "unlabeled"}, {"type": "text", "expr": "Saman Amarasinghe", "word_idx": 136, "sentence_idx": 10, "label": "unlabeled"}, {"type": "text", "expr": "Saman Amarasinghe", "word_idx": 153, "sentence_idx": 11, "label": "unlabeled"}, {"type": "text", "expr": "saman@csail", "word_idx": 170, "sentence_idx": 12, "label": "unlabeled"}, {"type": "text", "expr": "saman@csail", "word_idx": 181, "sentence_idx": 13, "label": "unlabeled"}, {"type": "text", "expr": "Abstract Recent advances in compiler theory describe how to compile sparse tensor\nalgebra", "word_idx": 192, "sentence_idx": 14, "label": "unlabeled"}, {"type": "text", "expr": "\nPrior work, however, does not describe how to generate efficient code that\ntakes advantage of temporary workspaces", "word_idx": 281, "sentence_idx": 15, "label": "unlabeled"}, {"type": "text", "expr": " These are often used to\nhand-optimize important kernels such as sparse matrix multiplication and the\nmatricized tensor times Khatri-Rao product", "word_idx": 396, "sentence_idx": 16, "label": "unlabeled"}, {"type": "text", "expr": "\nWithout this capability, compilers and code generators cannot automatically\ngenerate efficient kernels for many important tensor algebra expressions", "word_idx": 540, "sentence_idx": 17, "label": "unlabeled"}, {"type": "text", "expr": "\nWe describe a compiler optimization called operator splitting that breaks up\ntensor sub-computations by introducing workspaces", "word_idx": 689, "sentence_idx": 18, "label": "unlabeled"}, {"type": "text", "expr": "\nOur case studies demonstrate that operator splitting is surprisingly general,\nand our results show that it increases the performance of important generated\ntensor kernels to match hand-optimized code", "word_idx": 816, "sentence_idx": 19, "label": "unlabeled"}, {"type": "text", "expr": "Abstract", "word_idx": 1016, "sentence_idx": 20, "label": "unlabeled"}, {"type": "text", "expr": "Recent advances in compiler theory describe how to compile sparse tensor\nalgebra", "word_idx": 1024, "sentence_idx": 21, "label": "unlabeled"}, {"type": "text", "expr": "\nPrior work, however, does not describe how to generate efficient code that\ntakes advantage of temporary workspaces", "word_idx": 1104, "sentence_idx": 22, "label": "unlabeled"}, {"type": "text", "expr": " These are often used to\nhand-optimize important kernels such as sparse matrix multiplication and the\nmatricized tensor times Khatri-Rao product", "word_idx": 1219, "sentence_idx": 23, "label": "unlabeled"}, {"type": "text", "expr": "\nWithout this capability, compilers and code generators cannot automatically\ngenerate efficient kernels for many important tensor algebra expressions", "word_idx": 1363, "sentence_idx": 24, "label": "unlabeled"}, {"type": "text", "expr": "\nWe describe a compiler optimization called operator splitting that breaks up\ntensor sub-computations by introducing workspaces", "word_idx": 1512, "sentence_idx": 25, "label": "unlabeled"}, {"type": "text", "expr": "\nOur case studies demonstrate that operator splitting is surprisingly general,\nand our results show that it increases the performance of important generated\ntensor kernels to match hand-optimized code", "word_idx": 1639, "sentence_idx": 26, "label": "unlabeled"}, {"type": "text", "expr": "sparse tensors, tensor algebra, linear algebra, optimization, operator split, workspaces, performance", "word_idx": 1839, "sentence_idx": 27, "label": "unlabeled"}, {"type": "text", "expr": "\\settopmatter", "word_idx": 1940, "sentence_idx": 28, "label": "unlabeled"}, {"type": "text", "expr": "printfolios=true,printccs=false,printacmref=false \\startPage 1 \\setcopyright none \\definecolor todocolorrgb0", "word_idx": 1953, "sentence_idx": 29, "label": "unlabeled"}, {"type": "text", "expr": "8,0,0 \\definecolor keywordcolorrgb0", "word_idx": 2061, "sentence_idx": 30, "label": "unlabeled"}, {"type": "text", "expr": "5,0,0", "word_idx": 2096, "sentence_idx": 31, "label": "unlabeled"}, {"type": "text", "expr": "5 \\definecolor textgraygray0", "word_idx": 2101, "sentence_idx": 32, "label": "unlabeled"}, {"type": "text", "expr": "\\startPage", "word_idx": 2129, "sentence_idx": 33, "label": "unlabeled"}, {"type": "text", "expr": "\\setcopyright", "word_idx": 2139, "sentence_idx": 34, "label": "unlabeled"}, {"type": "text", "expr": "\\definecolor", "word_idx": 2152, "sentence_idx": 35, "label": "unlabeled"}, {"type": "text", "expr": "\\definecolor", "word_idx": 2164, "sentence_idx": 36, "label": "unlabeled"}, {"type": "text", "expr": "\\definecolor", "word_idx": 2176, "sentence_idx": 37, "label": "unlabeled"}, {"type": "text", "expr": "Automatic Generation of Sparse Tensor Kernels with Workspaces]Automatic Generation of Sparse Tensor Kernels\n with Workspaces", "word_idx": 2188, "sentence_idx": 38, "label": "unlabeled"}, {"type": "text", "expr": "1  Introduction", "word_idx": 2312, "sentence_idx": 39, "label": "unlabeled"}, {"type": "text", "expr": "Tensor algebra is an important tool for computing on multi-mode data in domains\nsuch as machine learning\u00a0 , data\nanalytics\u00a0 , engineering\u00a0 ,\nand science\u00a0 ", "word_idx": 2327, "sentence_idx": 40, "label": "unlabeled"}, {"type": "text", "expr": " Tensors generalize matrices to\nany number of dimensions and can model both linear and multilinear\nrelationships", "word_idx": 2481, "sentence_idx": 41, "label": "unlabeled"}, {"type": "text", "expr": " Interesting tensors are often sparse, which means most\ncomponents are zero", "word_idx": 2593, "sentence_idx": 42, "label": "unlabeled"}, {"type": "text", "expr": " Compressing these tensors is often necessary", "word_idx": 2668, "sentence_idx": 43, "label": "unlabeled"}, {"type": "text", "expr": " For\nexample, a tensor encoding Amazon reviews\u00a0  used to predict user\nresponse to a new product\u00a0  contains 15 quintillion\ncomponents where only 1", "word_idx": 2713, "sentence_idx": 44, "label": "unlabeled"}, {"type": "text", "expr": "7 billion are nonzeros", "word_idx": 2858, "sentence_idx": 45, "label": "unlabeled"}, {"type": "text", "expr": "Abadi et\u00a0al", "word_idx": 2880, "sentence_idx": 46, "label": "unlabeled"}, {"type": "text", "expr": " (2016)", "word_idx": 2891, "sentence_idx": 47, "label": "unlabeled"}, {"type": "text", "expr": "Bader\net\u00a0al", "word_idx": 2898, "sentence_idx": 48, "label": "unlabeled"}, {"type": "text", "expr": " (2008)", "word_idx": 2909, "sentence_idx": 49, "label": "unlabeled"}, {"type": "text", "expr": "Anandkumar et\u00a0al", "word_idx": 2916, "sentence_idx": 50, "label": "unlabeled"}, {"type": "text", "expr": " (2014)", "word_idx": 2932, "sentence_idx": 51, "label": "unlabeled"}, {"type": "text", "expr": "Kolecki (2002)", "word_idx": 2939, "sentence_idx": 52, "label": "unlabeled"}, {"type": "text", "expr": "Einstein (1916)", "word_idx": 2953, "sentence_idx": 53, "label": "unlabeled"}, {"type": "text", "expr": "Feynman\net\u00a0al", "word_idx": 2968, "sentence_idx": 54, "label": "unlabeled"}, {"type": "text", "expr": " (1963)", "word_idx": 2981, "sentence_idx": 55, "label": "unlabeled"}, {"type": "text", "expr": "Smith et\u00a0al", "word_idx": 2988, "sentence_idx": 56, "label": "unlabeled"}, {"type": "text", "expr": " (2017b)", "word_idx": 2999, "sentence_idx": 57, "label": "unlabeled"}, {"type": "text", "expr": "McAuley and\nLeskovec (2013)", "word_idx": 3007, "sentence_idx": 58, "label": "unlabeled"}, {"type": "text", "expr": "Leskovec and\nKrevl (2014)", "word_idx": 3034, "sentence_idx": 59, "label": "unlabeled"}, {"type": "text", "expr": "We recently proposed a compiler theory that shows us how to automatically\ngenerate kernels for tensor algebra expressions with both dense and sparse\ntensors\u00a0 ", "word_idx": 3059, "sentence_idx": 60, "label": "unlabeled"}, {"type": "text", "expr": " We implemented the theory in a library and showed\nthat the performance of many generated kernels is competitive with the\nperformance of hand-optimized kernels in existing libraries", "word_idx": 3217, "sentence_idx": 61, "label": "unlabeled"}, {"type": "text", "expr": "Kjolstad et\u00a0al", "word_idx": 3398, "sentence_idx": 62, "label": "unlabeled"}, {"type": "text", "expr": " (2017b)", "word_idx": 3412, "sentence_idx": 63, "label": "unlabeled"}, {"type": "text", "expr": "Our previous approach, however, did not generate kernels that take advantage of\ndense temporary workspace arrays to accumulate temporary values", "word_idx": 3420, "sentence_idx": 64, "label": "unlabeled"}, {"type": "text", "expr": " Such arrays\nare a common optimization technique in sparse linear algebra algorithms, such\nas Gustavson\u2019s sparse matrix multiplication (SpMM)\u00a0 , and was applied to the matricized tensor times\nKhatri-Rao product (MTTKRP) by\n Smith et\u00a0al", "word_idx": 3563, "sentence_idx": 65, "label": "unlabeled"}, {"type": "text", "expr": " Without workspaces,\nkernels underperform due to expensive insertions into sparse tensors, branches\nfrom code to merge sparse tensors, and redundant loop-invariant work", "word_idx": 3798, "sentence_idx": 66, "label": "unlabeled"}, {"type": "text", "expr": "Gustavson (1978)", "word_idx": 3966, "sentence_idx": 67, "label": "unlabeled"}, {"type": "text", "expr": "Pissanetzky (1984)", "word_idx": 3982, "sentence_idx": 68, "label": "unlabeled"}, {"type": "text", "expr": "Gilbert\net\u00a0al", "word_idx": 4000, "sentence_idx": 69, "label": "unlabeled"}, {"type": "text", "expr": " (1992)", "word_idx": 4013, "sentence_idx": 70, "label": "unlabeled"}, {"type": "text", "expr": "Smith et\u00a0al", "word_idx": 4020, "sentence_idx": 71, "label": "unlabeled"}, {"type": "text", "expr": "2017a", "word_idx": 4031, "sentence_idx": 72, "label": "unlabeled"}, {"type": "text", "expr": "This paper introduces a novel compiler optimization called operator splitting\non our iteration graph tensor algebra intermediate\nrepresentation\u00a0 ", "word_idx": 4036, "sentence_idx": 73, "label": "unlabeled"}, {"type": "text", "expr": " An operator split breaks up a tensor\nsub-expression with respect to an index variable and introduces a dense\nworkspace to store intermediate results", "word_idx": 4181, "sentence_idx": 74, "label": "unlabeled"}, {"type": "text", "expr": " Operator splitting reveals that\nprevious proposed algorithms that use workspaces for different purposes are the\nresults of the same transformation", "word_idx": 4330, "sentence_idx": 75, "label": "unlabeled"}, {"type": "text", "expr": " Our case studies show that operator\nsplitting is a general optimization that applies to many important tensor\nexpressions, including sparse matrix multiplication with the linear combination\nalgorithm, sparse matrix addition, and the matricized tensor times Khatri-Rao\nproduct", "word_idx": 4477, "sentence_idx": 76, "label": "unlabeled"}, {"type": "text", "expr": "2017b", "word_idx": 4753, "sentence_idx": 77, "label": "unlabeled"}, {"type": "text", "expr": "The key contributions are:", "word_idx": 4758, "sentence_idx": 78, "label": "unlabeled"}, {"type": "text", "expr": "The key contributions are:", "word_idx": 4784, "sentence_idx": 79, "label": "unlabeled"}, {"type": "text", "expr": "Formulation", "word_idx": 4810, "sentence_idx": 80, "label": "unlabeled"}, {"type": "text", "expr": "We identify the lack of workspaces in the tensor\ncompilation literature", "word_idx": 4821, "sentence_idx": 81, "label": "unlabeled"}, {"type": "text", "expr": "We identify the lack of workspaces in the tensor\ncompilation literature", "word_idx": 4892, "sentence_idx": 82, "label": "unlabeled"}, {"type": "text", "expr": "Operator Split", "word_idx": 4963, "sentence_idx": 83, "label": "unlabeled"}, {"type": "text", "expr": "We introduce a compiler optimization that removes\nexpensive inserts into sparse results, eliminates merge code, and hoists\nloop invariant code", "word_idx": 4977, "sentence_idx": 84, "label": "unlabeled"}, {"type": "text", "expr": "We introduce a compiler optimization that removes\nexpensive inserts into sparse results, eliminates merge code, and hoists\nloop invariant code", "word_idx": 5119, "sentence_idx": 85, "label": "unlabeled"}, {"type": "text", "expr": "Applicability", "word_idx": 5261, "sentence_idx": 86, "label": "unlabeled"}, {"type": "text", "expr": "We define preconditions for applying operator splits and\ndiscuss their trade-offs", "word_idx": 5274, "sentence_idx": 87, "label": "unlabeled"}, {"type": "text", "expr": "We define preconditions for applying operator splits and\ndiscuss their trade-offs", "word_idx": 5355, "sentence_idx": 88, "label": "unlabeled"}, {"type": "text", "expr": "Case Studies", "word_idx": 5436, "sentence_idx": 89, "label": "unlabeled"}, {"type": "text", "expr": "We show that operator splits recreate several important\nalgorithms with workspaces from the literature and generalizes to important\nnew kernels", "word_idx": 5448, "sentence_idx": 90, "label": "unlabeled"}, {"type": "text", "expr": "We show that operator splits recreate several important\nalgorithms with workspaces from the literature and generalizes to important\nnew kernels", "word_idx": 5591, "sentence_idx": 91, "label": "unlabeled"}, {"type": "text", "expr": "Finally, we evaluate operator splitting by showing that the performance of the\nresulting code is competitive with hand-optimized implementations with\nworkspaces in the MKL, Eigen, and SPLATT high-performance\nlibraries\u00a0 ", "word_idx": 5734, "sentence_idx": 92, "label": "unlabeled"}, {"type": "text", "expr": "2  Motivating Example", "word_idx": 5953, "sentence_idx": 93, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 5974, "sentence_idx": 94, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 5985, "sentence_idx": 95, "label": "unlabeled"}, {"type": "text", "expr": "Sparse CSR index of  $B$", "word_idx": 5996, "sentence_idx": 96, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 6020, "sentence_idx": 97, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 6031, "sentence_idx": 98, "label": "unlabeled"}, {"type": "text", "expr": "Dense  $m\\times o$  matrix  $B$", "word_idx": 6042, "sentence_idx": 99, "label": "unlabeled"}, {"type": "math", "expr": "$$m\\times o$$", "word_idx": 6073, "sentence_idx": 100, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 6082, "sentence_idx": 101, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 6093, "sentence_idx": 102, "label": "unlabeled"}, {"type": "text", "expr": "$A_{ij}=\\sum_{k}B_{ik}C_{kj}$  (sparse  $B$ ,  $C$ )", "word_idx": 6104, "sentence_idx": 103, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ij}=\\sum_{k}B_{ik}C_{kj}$$", "word_idx": 6156, "sentence_idx": 104, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 6183, "sentence_idx": 105, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 6194, "sentence_idx": 106, "label": "unlabeled"}, {"type": "text", "expr": "$A_{ij}=\\sum_{k}B_{ik}C_{kj}$  (sparse  $A$ ,  $B$ ,  $C$ )", "word_idx": 6205, "sentence_idx": 107, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ij}=\\sum_{k}B_{ik}C_{kj}$$", "word_idx": 6264, "sentence_idx": 108, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:  \n(a) The CSR sparse matrix index structure of a matrix  $B$ , (b) the matrix\n $B$ , and (c-d) two sparse matrix multiplication kernels written in C", "word_idx": 6291, "sentence_idx": 109, "label": "unlabeled"}, {"type": "text", "expr": " The\nfirst kernel (c) operators on matrices stored in the CSR format and its\nresult is dense", "word_idx": 6449, "sentence_idx": 110, "label": "unlabeled"}, {"type": "text", "expr": " The second kernel (d) also operates on CSR matrices, but\nits result is also a CSR matrix", "word_idx": 6541, "sentence_idx": 111, "label": "unlabeled"}, {"type": "text", "expr": " The second kernel uses a workspace", "word_idx": 6630, "sentence_idx": 112, "label": "unlabeled"}, {"type": "text", "expr": " The\ncode to zero  $A$  is omitted and result indices have been pre-assembled", "word_idx": 6665, "sentence_idx": 113, "label": "unlabeled"}, {"type": "text", "expr": "\nSection\u00a0 6  discusses the code that assembles result\nindices", "word_idx": 6742, "sentence_idx": 114, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:", "word_idx": 6803, "sentence_idx": 115, "label": "unlabeled"}, {"type": "text", "expr": "We will use sparse matrix multiplication to introduce sparse tensor data\nstructures, sparse kernels, and the need for workspaces", "word_idx": 6812, "sentence_idx": 116, "label": "unlabeled"}, {"type": "text", "expr": " The ideas, however,\ngeneralize to higher-order tensor kernels", "word_idx": 6940, "sentence_idx": 117, "label": "unlabeled"}, {"type": "text", "expr": " Matrix multiplication in linear\nalgebra notation is  $A=BC$  and in tensor index notation it is", "word_idx": 7002, "sentence_idx": 118, "label": "unlabeled"}, {"type": "math", "expr": "$$A=BC$$", "word_idx": 7098, "sentence_idx": 119, "label": "unlabeled"}, {"type": "text", "expr": "$A_{ij}=\\sum_{k}B_{ik}C_{kj}$", "word_idx": 7102, "sentence_idx": 120, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ij}=\\sum_{k}B_{ik}C_{kj}.$$", "word_idx": 7131, "sentence_idx": 121, "label": "unlabeled"}, {"type": "text", "expr": "The code in a matrix multiplication kernel depends on the storage formats of\nthe operands and the result", "word_idx": 7159, "sentence_idx": 122, "label": "unlabeled"}, {"type": "text", "expr": " Many matrix storage formats have been proposed in\nthe literature", "word_idx": 7263, "sentence_idx": 123, "label": "unlabeled"}, {"type": "text", "expr": " They can be classified as dense formats that store every\nmatrix component and sparse formats that store only the components that are\nnonzero", "word_idx": 7328, "sentence_idx": 124, "label": "unlabeled"}, {"type": "text", "expr": " Figure\u00a0 1  shows two linear combination of rows matrix\nmultiplication kernels", "word_idx": 7469, "sentence_idx": 125, "label": "unlabeled"}, {"type": "text", "expr": " We will study this algorithm, instead of the inner\nproduct algorithm, because its sparse variant has better asymptotic\ncomplexity\u00a0  and because the inputs are conveniently all\nrow major", "word_idx": 7547, "sentence_idx": 126, "label": "unlabeled"}, {"type": "text", "expr": "Sparse kernels are more involved than dense kernels because they iterate over\nsparse data structures", "word_idx": 7733, "sentence_idx": 127, "label": "unlabeled"}, {"type": "text", "expr": " Figure\u00a0 1  shows a sparse matrix\nmultiplication kernel where the result matrix is stored dense row-major and the\noperand matrices are stored with the compressed sparse row format\n(CSR)\u00a0 ", "word_idx": 7833, "sentence_idx": 128, "label": "unlabeled"}, {"type": "text", "expr": "The CSR format and its column-major CSC sibling are ubiquitous in sparse linear\nalgebra libraries\u00a0  due to their generality and\nperformance", "word_idx": 8020, "sentence_idx": 129, "label": "unlabeled"}, {"type": "text", "expr": " In the CSR format, each matrix row is compressed (only nonzero\ncomponents are stored)", "word_idx": 8159, "sentence_idx": 130, "label": "unlabeled"}, {"type": "text", "expr": " This requires two index arrays to describe the matrix\ncoordinates and positions of the nonzeros", "word_idx": 8245, "sentence_idx": 131, "label": "unlabeled"}, {"type": "text", "expr": " Figure\u00a0 1  shows the CSR\ndata structure of matrix  $B$  in Figure\u00a0 1 ", "word_idx": 8341, "sentence_idx": 132, "label": "unlabeled"}, {"type": "text", "expr": " It consists of the\nindex arrays  B_pos  and  B_idx  and a value array  B ", "word_idx": 8411, "sentence_idx": 133, "label": "unlabeled"}, {"type": "text", "expr": " The\narray  B_idx  contains the column coordinate of each nonzero value in the\ncorresponding position in  B ", "word_idx": 8485, "sentence_idx": 134, "label": "unlabeled"}, {"type": "text", "expr": " The array  B_pos  stores the position\nof the first column coordinate of each row in  B_idx , as well as a\nsentinel with the number of nonzeros ( nnz ) in the matrix", "word_idx": 8593, "sentence_idx": 135, "label": "unlabeled"}, {"type": "text", "expr": " Thus,\ncontiguous values in  B_pos  store the beginning and end\n[inclusive-exclusive) of a row in the arrays  B_idx  and  B ", "word_idx": 8758, "sentence_idx": 136, "label": "unlabeled"}, {"type": "text", "expr": " For\nexample, the column coordinates of the third row are stored in  B_idx  at\npositions  $[$ ,  $)$ ", "word_idx": 8882, "sentence_idx": 137, "label": "unlabeled"}, {"type": "text", "expr": " Moreover, some\nlibraries require the entries within each row to be sorted in order of\nascending coordinate value, which results in faster performance for some\nalgorithms", "word_idx": 8983, "sentence_idx": 138, "label": "unlabeled"}, {"type": "text", "expr": "B_pos", "word_idx": 9153, "sentence_idx": 139, "label": "unlabeled"}, {"type": "text", "expr": "B_idx", "word_idx": 9158, "sentence_idx": 140, "label": "unlabeled"}, {"type": "text", "expr": "B_idx", "word_idx": 9163, "sentence_idx": 141, "label": "unlabeled"}, {"type": "text", "expr": "B_pos", "word_idx": 9168, "sentence_idx": 142, "label": "unlabeled"}, {"type": "text", "expr": "B_idx", "word_idx": 9173, "sentence_idx": 143, "label": "unlabeled"}, {"type": "text", "expr": "B_pos", "word_idx": 9178, "sentence_idx": 144, "label": "unlabeled"}, {"type": "text", "expr": "B_idx", "word_idx": 9183, "sentence_idx": 145, "label": "unlabeled"}, {"type": "text", "expr": "B_idx", "word_idx": 9188, "sentence_idx": 146, "label": "unlabeled"}, {"type": "text", "expr": "B_pos [2]", "word_idx": 9193, "sentence_idx": 147, "label": "unlabeled"}, {"type": "text", "expr": "B_pos", "word_idx": 9202, "sentence_idx": 148, "label": "unlabeled"}, {"type": "text", "expr": "B_pos [3]", "word_idx": 9207, "sentence_idx": 149, "label": "unlabeled"}, {"type": "text", "expr": "B_pos", "word_idx": 9216, "sentence_idx": 150, "label": "unlabeled"}, {"type": "text", "expr": "Because matrix multiplication contains the sub-expression  $B_{ik}$ , the kernel\nin\u00a0Figure\u00a0 1  iterates over the matrix  $B$  with the loops over  $i$ \n(line\u00a01) and  $k$  (lines\u00a02\u20133)", "word_idx": 9221, "sentence_idx": 151, "label": "unlabeled"}, {"type": "text", "expr": " The loop over  $i$  is dense because the CSR\nformat stores every row", "word_idx": 9403, "sentence_idx": 152, "label": "unlabeled"}, {"type": "text", "expr": " The loop over  $k$ , however, is sparse because each\nrow is compressed", "word_idx": 9472, "sentence_idx": 153, "label": "unlabeled"}, {"type": "text", "expr": " To iterate over the column coordinates of the  $i$ th row,\nthe  $k$  loop iterates over  $[$ ,  $)$  in\n B_idx ", "word_idx": 9543, "sentence_idx": 154, "label": "unlabeled"}, {"type": "text", "expr": " We have highlighted  $B$ \u2019s index arrays in the code in\nFigure\u00a0 1 ", "word_idx": 9655, "sentence_idx": 155, "label": "unlabeled"}, {"type": "math", "expr": "$$B_{ik}$$", "word_idx": 9722, "sentence_idx": 156, "label": "unlabeled"}, {"type": "text", "expr": "B_pos", "word_idx": 9728, "sentence_idx": 157, "label": "unlabeled"}, {"type": "text", "expr": "B_pos", "word_idx": 9733, "sentence_idx": 158, "label": "unlabeled"}, {"type": "text", "expr": "[ i ]", "word_idx": 9738, "sentence_idx": 159, "label": "unlabeled"}, {"type": "text", "expr": "B_pos", "word_idx": 9743, "sentence_idx": 160, "label": "unlabeled"}, {"type": "text", "expr": "B_pos", "word_idx": 9748, "sentence_idx": 161, "label": "unlabeled"}, {"type": "text", "expr": "[ i +1]", "word_idx": 9753, "sentence_idx": 162, "label": "unlabeled"}, {"type": "text", "expr": "B_idx", "word_idx": 9760, "sentence_idx": 163, "label": "unlabeled"}, {"type": "text", "expr": "The kernel is further complicated when the result matrix  $A$  is sparse, because\nthe assignment to  $A$  (line\u00a06) is nested inside the reduction loop  $k$ ", "word_idx": 9765, "sentence_idx": 164, "label": "unlabeled"}, {"type": "text", "expr": " This\ncauses the inner loop  $j$  to iterate over and insert into each row of  $A$ \nseveral times", "word_idx": 9921, "sentence_idx": 165, "label": "unlabeled"}, {"type": "text", "expr": " Sparse data structures, however, do not support fast random\ninserts (only appends)", "word_idx": 10018, "sentence_idx": 166, "label": "unlabeled"}, {"type": "text", "expr": " Inserting into the middle of a CSR matrix costs\n $\\Theta{}(\\textrm{nnz})$  because the new value must be inserted into the middle\nof an array", "word_idx": 10101, "sentence_idx": 167, "label": "unlabeled"}, {"type": "text", "expr": " To get the  $\\Theta{}(1)$  insertion cost of dense formats, the\nkernel in\u00a0Figure\u00a0 1  introduces a dense workspace", "word_idx": 10243, "sentence_idx": 168, "label": "unlabeled"}, {"type": "text", "expr": " The\nworkspace and accompanying loop transformations is the purpose of the operator\nsplit optimization", "word_idx": 10357, "sentence_idx": 169, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Theta{}(\\textrm{nnz})$$", "word_idx": 10459, "sentence_idx": 170, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Theta{}(1)$$", "word_idx": 10481, "sentence_idx": 171, "label": "unlabeled"}, {"type": "text", "expr": "A workspace is a temporary tensor that is typically dense with fast insertion\nand random access", "word_idx": 10492, "sentence_idx": 172, "label": "unlabeled"}, {"type": "text", "expr": " Because values can be scattered efficiently into a dense\nworkspace, the loop nest  $k,j$  (lines\u00a02\u20138) in\u00a0Figure\u00a0 1  looks\nsimilar to the kernel in\u00a0Figure\u00a0 1 ", "word_idx": 10587, "sentence_idx": 173, "label": "unlabeled"}, {"type": "text", "expr": " Instead of assigning values\nto the result matrix\u00a0 $A$ , however, it assigns them to a dense workspace vector", "word_idx": 10745, "sentence_idx": 174, "label": "unlabeled"}, {"type": "text", "expr": "\nWhen a row of the result is fully computed in the workspace, it is appended to\n $A$  in a second loop over  $j$  (lines\u00a010\u201314)", "word_idx": 10854, "sentence_idx": 175, "label": "unlabeled"}, {"type": "text", "expr": " This loop iterates over the row\nin  $A$ \u2019s sparse index structure, which assumes  $A$ \u2019s CSR index has been\npre-assembled", "word_idx": 10981, "sentence_idx": 176, "label": "unlabeled"}, {"type": "text", "expr": " Pre-assembling index structures increases performance when\nassembly can be moved out of inner loops, which is common in material\nsimulations\u00a0 ", "word_idx": 11103, "sentence_idx": 177, "label": "unlabeled"}, {"type": "text", "expr": " We will describe the code to assemble result\nindices, which keeps track of the nonzero coordinates in the workspace,\nin\u00a0Section\u00a0 6 ", "word_idx": 11246, "sentence_idx": 178, "label": "unlabeled"}, {"type": "math", "expr": "$$k,j$$", "word_idx": 11378, "sentence_idx": 179, "label": "unlabeled"}, {"type": "text", "expr": "3  Tensor Algebra Compilation Background", "word_idx": 11381, "sentence_idx": 180, "label": "unlabeled"}, {"type": "text", "expr": "In this section we will review the iteration graph intermediate representation\nfor tensor algebra compilation\u00a0  that is the target for\noperator splits", "word_idx": 11421, "sentence_idx": 181, "label": "unlabeled"}, {"type": "text", "expr": " Iteration graphs are constructed from tensor index notation\nand describe the constraints imposed by sparse tensors on the iteration space\nof the index variables", "word_idx": 11571, "sentence_idx": 182, "label": "unlabeled"}, {"type": "text", "expr": " Sparse tensors provide an opportunity and a challenge", "word_idx": 11732, "sentence_idx": 183, "label": "unlabeled"}, {"type": "text", "expr": "\nThey store only nonzeros and loops therefore avoid iterating over zeros, but\nthey also enforce a particular iteration order because they encode tensor\ncoordinates hierarchically", "word_idx": 11786, "sentence_idx": 184, "label": "unlabeled"}, {"type": "text", "expr": "2017b", "word_idx": 11964, "sentence_idx": 185, "label": "unlabeled"}, {"type": "text", "expr": "An iteration graph is constructed from tensor index notation, such as\n $A_{ij}=\\sum_{lk}B_{ilk}C_{lj}D_{kj}$  or\n $a_{i}=\\sum_{j}B_{ij}c_{j}+d_{i}$ ", "word_idx": 11969, "sentence_idx": 186, "label": "unlabeled"}, {"type": "text", "expr": " In index notation, index variables range\nover the dimensions they index and computations happen at each point in the\niteration space", "word_idx": 12117, "sentence_idx": 187, "label": "unlabeled"}, {"type": "text", "expr": " Index variables are nodes in iteration graphs and each\noperand access, such as  $B_{ij}$ , becomes a path through index variables\ninvolved", "word_idx": 12250, "sentence_idx": 188, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ij}=\\sum_{lk}B_{ilk}C_{lj}D_{kj}$$", "word_idx": 12389, "sentence_idx": 189, "label": "unlabeled"}, {"type": "math", "expr": "$$a_{i}=\\sum_{j}B_{ij}c_{j}+d_{i}$$", "word_idx": 12424, "sentence_idx": 190, "label": "unlabeled"}, {"type": "math", "expr": "$$B_{ij}$$", "word_idx": 12455, "sentence_idx": 191, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 12461, "sentence_idx": 192, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 12472, "sentence_idx": 193, "label": "unlabeled"}, {"type": "text", "expr": "$A_{ij}=\\sum_{k}B_{ik}C_{kj}$", "word_idx": 12483, "sentence_idx": 194, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ij}=\\sum_{k}B_{ik}C_{kj}$$", "word_idx": 12512, "sentence_idx": 195, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 12539, "sentence_idx": 196, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 12550, "sentence_idx": 197, "label": "unlabeled"}, {"type": "text", "expr": "$A_{ijk}=B_{ijk}+C_{ijk}$", "word_idx": 12561, "sentence_idx": 198, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ijk}=B_{ijk}+C_{ijk}$$", "word_idx": 12586, "sentence_idx": 199, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 12609, "sentence_idx": 200, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 12620, "sentence_idx": 201, "label": "unlabeled"}, {"type": "text", "expr": "$A_{ij}=\\sum_{k}B_{ijk}c_{k}$", "word_idx": 12631, "sentence_idx": 202, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ij}=\\sum_{k}B_{ijk}c_{k}$$", "word_idx": 12660, "sentence_idx": 203, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 12687, "sentence_idx": 204, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 12698, "sentence_idx": 205, "label": "unlabeled"}, {"type": "text", "expr": "$A_{ij}=\\sum_{k}B_{ij}C_{ik}D_{kj}$", "word_idx": 12709, "sentence_idx": 206, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ij}=\\sum_{k}B_{ij}C_{ik}D_{kj}$$", "word_idx": 12744, "sentence_idx": 207, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 12777, "sentence_idx": 208, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 12788, "sentence_idx": 209, "label": "unlabeled"}, {"type": "text", "expr": "$A_{ij}=\\sum_{lk}B_{ikl}C_{lj}D_{kj}$", "word_idx": 12799, "sentence_idx": 210, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ij}=\\sum_{lk}B_{ikl}C_{lj}D_{kj}$$", "word_idx": 12836, "sentence_idx": 211, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:  \nFive iteration graphs:\n( \\subref fig:iteration-graphs-spmm)\u00a0matrix multiplication,\n( \\subref fig:iteration-graphs-tadd)\u00a0tensor addition,\n( \\subref fig:iteration-graphs-ttv) tensor-vector multiplication,\n( \\subref fig:iteration-graphs-sddmm) sampled dense-dense matrix multiplication (SDDMM), and\n( \\subref fig:iteration-graphs-mttkrp) matricized tensor times Khatri-Rao\nproduct (MTTKRP)", "word_idx": 12871, "sentence_idx": 212, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:", "word_idx": 13269, "sentence_idx": 213, "label": "unlabeled"}, {"type": "text", "expr": "\\subref", "word_idx": 13278, "sentence_idx": 214, "label": "unlabeled"}, {"type": "text", "expr": "\\subref", "word_idx": 13285, "sentence_idx": 215, "label": "unlabeled"}, {"type": "text", "expr": "\\subref", "word_idx": 13292, "sentence_idx": 216, "label": "unlabeled"}, {"type": "text", "expr": "\\subref", "word_idx": 13299, "sentence_idx": 217, "label": "unlabeled"}, {"type": "text", "expr": "\\subref", "word_idx": 13306, "sentence_idx": 218, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a0 2  shows several iteration graphs, including matrix\nmultiplication, sampled dense-dense matrix multiplication from machine\nlearning\u00a0 , and the matricized tensor times Khatri-Rao product used\nto factorize tensors\u00a0 ", "word_idx": 13313, "sentence_idx": 219, "label": "unlabeled"}, {"type": "text", "expr": " The index notation for tensor-vector\nmultiplication is", "word_idx": 13534, "sentence_idx": 220, "label": "unlabeled"}, {"type": "text", "expr": "$A_{ij}=\\sum_{k}B_{ijk}c_{k}$", "word_idx": 13589, "sentence_idx": 221, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ij}=\\sum_{k}B_{ijk}c_{k}.$$", "word_idx": 13618, "sentence_idx": 222, "label": "unlabeled"}, {"type": "text", "expr": "The corresponding\niteration graph in Figure\u00a0 2  has a node for each index\nvariable  $i$ ,  $j$ , and  $k$  and a a path for each of the three tensor accesses\n $B_{ijk}$  (blue),  $c_{k}$  (purple), and  $A_{ij}$  (stippled green)", "word_idx": 13646, "sentence_idx": 223, "label": "unlabeled"}, {"type": "text", "expr": " We draw\nstippled paths for results", "word_idx": 13875, "sentence_idx": 224, "label": "unlabeled"}, {"type": "text", "expr": " Figure\u00a0 3  shows code generated from this\niteration graph when  $B$  and  $c$  are sparse", "word_idx": 13910, "sentence_idx": 225, "label": "unlabeled"}, {"type": "text", "expr": " Each index variable node becomes\na loop that iterates over the sparse tensor indices belonging to the incoming\nedges", "word_idx": 14000, "sentence_idx": 226, "label": "unlabeled"}, {"type": "math", "expr": "$$B_{ijk}$$", "word_idx": 14117, "sentence_idx": 227, "label": "unlabeled"}, {"type": "math", "expr": "$$c_{k}$$", "word_idx": 14124, "sentence_idx": 228, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ij}$$", "word_idx": 14129, "sentence_idx": 229, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:  \nCode generated from the iteration graph in\u00a0Figure\u00a0 2 \nwhen  $A$  is dense while  $B$  and  $c$  are sparse", "word_idx": 14135, "sentence_idx": 230, "label": "unlabeled"}, {"type": "text", "expr": " Each index variable\nbecomes a loop that iterates over the sparse tensor indices of its incoming\npaths", "word_idx": 14253, "sentence_idx": 231, "label": "unlabeled"}, {"type": "text", "expr": " The  $k$  loop iterates over the intersection of the last dimension\nof  $B$  and  $c$ ", "word_idx": 14355, "sentence_idx": 232, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:", "word_idx": 14442, "sentence_idx": 233, "label": "unlabeled"}, {"type": "text", "expr": "Two or more input paths meet at the same index variable when it was used to\nindex into two or more tensors", "word_idx": 14451, "sentence_idx": 234, "label": "unlabeled"}, {"type": "text", "expr": " The iteration space of the tensor dimensions\nthe variable indexes must be merged in the generated code", "word_idx": 14557, "sentence_idx": 235, "label": "unlabeled"}, {"type": "text", "expr": " The index variables\nare annotated with operators that tell the code generator what kind of merge\ncode to generate", "word_idx": 14660, "sentence_idx": 236, "label": "unlabeled"}, {"type": "text", "expr": " If the tensors are multiplied, then the generated code\niterates over the intersection of the indexed tensor dimensions\n(Figure\u00a0 5 )", "word_idx": 14774, "sentence_idx": 237, "label": "unlabeled"}, {"type": "text", "expr": " If they are added, then it iterates over their\nunion (Figure\u00a0 6 )", "word_idx": 14906, "sentence_idx": 238, "label": "unlabeled"}, {"type": "text", "expr": " If more than two tensors are indexed by the\nsame index variable, then code is generated to iterate over a mix of\nintersections and unions of tensor dimensions", "word_idx": 14972, "sentence_idx": 239, "label": "unlabeled"}, {"type": "text", "expr": "In prior work\u00a0  we described an algorithm to generate code\nfrom iteration graphs, including a mechanism called merge lattices to generate\ncode to co-iterate over tensor dimensions", "word_idx": 15131, "sentence_idx": 240, "label": "unlabeled"}, {"type": "text", "expr": " Understanding our workspace\noptimization does not require understanding the details of the code generation\nalgorithm or merge lattices", "word_idx": 15310, "sentence_idx": 241, "label": "unlabeled"}, {"type": "text", "expr": " We should note, however, that the performance of\ncode that merges sparse tensors may suffer from many conditionals", "word_idx": 15445, "sentence_idx": 242, "label": "unlabeled"}, {"type": "text", "expr": " Code to\nco-iterate over a combination of a single sparse and one or more dense tensors,\non the other hand, does not require conditionals\n(e", "word_idx": 15560, "sentence_idx": 243, "label": "unlabeled"}, {"type": "text", "expr": ",\u00a0Figure\u00a0 5  and Figure\u00a0 6 )", "word_idx": 15700, "sentence_idx": 244, "label": "unlabeled"}, {"type": "text", "expr": " One of the\nbenefits of introducing a workspace is to improve performance by turning\nsparse-sparse iteration into sparse-dense iteration", "word_idx": 15728, "sentence_idx": 245, "label": "unlabeled"}, {"type": "text", "expr": "2017b", "word_idx": 15864, "sentence_idx": 246, "label": "unlabeled"}, {"type": "text", "expr": "4  Operator Split", "word_idx": 15869, "sentence_idx": 247, "label": "unlabeled"}, {"type": "text", "expr": "Operator splitting is an optimization that applies to a binary operator at an\nindex variable in an index notation expression", "word_idx": 15886, "sentence_idx": 248, "label": "unlabeled"}, {"type": "text", "expr": " It splits the index variable\nin two and peels off the operator\u2019s left sub-expression and stores it to a\ntemporary dense workspace", "word_idx": 16010, "sentence_idx": 249, "label": "unlabeled"}, {"type": "text", "expr": " The original expression is then rewritten to\nreplace the left sub-expression with the workspace", "word_idx": 16140, "sentence_idx": 250, "label": "unlabeled"}, {"type": "text", "expr": " Operator splits have two\nadvantages:", "word_idx": 16236, "sentence_idx": 251, "label": "unlabeled"}, {"type": "text", "expr": "Operator splitting is an optimization that applies to a binary operator at an\nindex variable in an index notation expression", "word_idx": 16273, "sentence_idx": 252, "label": "unlabeled"}, {"type": "text", "expr": " It splits the index variable\nin two and peels off the operator\u2019s left sub-expression and stores it to a\ntemporary dense workspace", "word_idx": 16397, "sentence_idx": 253, "label": "unlabeled"}, {"type": "text", "expr": " The original expression is then rewritten to\nreplace the left sub-expression with the workspace", "word_idx": 16527, "sentence_idx": 254, "label": "unlabeled"}, {"type": "text", "expr": " Operator splits have two\nadvantages:", "word_idx": 16623, "sentence_idx": 255, "label": "unlabeled"}, {"type": "text", "expr": "Simplifies merges", "word_idx": 16660, "sentence_idx": 256, "label": "unlabeled"}, {"type": "text", "expr": "Splitting a binary operator that has sparse\noperands in both sub-expressions replaces a sparse-sparse merge with a\npotentially cheaper sparse-dense merge", "word_idx": 16677, "sentence_idx": 257, "label": "unlabeled"}, {"type": "text", "expr": " By removing sparse-sparse merges\nwe eliminate conditionals and loops and may also remove expensive random\ninserts into sparse data structures", "word_idx": 16830, "sentence_idx": 258, "label": "unlabeled"}, {"type": "text", "expr": "Splitting a binary operator that has sparse\noperands in both sub-expressions replaces a sparse-sparse merge with a\npotentially cheaper sparse-dense merge", "word_idx": 16972, "sentence_idx": 259, "label": "unlabeled"}, {"type": "text", "expr": " By removing sparse-sparse merges\nwe eliminate conditionals and loops and may also remove expensive random\ninserts into sparse data structures", "word_idx": 17125, "sentence_idx": 260, "label": "unlabeled"}, {"type": "text", "expr": "Hoists loop invariant computations", "word_idx": 17267, "sentence_idx": 261, "label": "unlabeled"}, {"type": "text", "expr": "Splitting an operator may result\nin the loop for one of the new index variables being emitted in a higher\nloop nest", "word_idx": 17301, "sentence_idx": 262, "label": "unlabeled"}, {"type": "text", "expr": " That is, it is revealed as loop-invariant\nand hoisted out of inner loops, which removes redundant computation", "word_idx": 17416, "sentence_idx": 263, "label": "unlabeled"}, {"type": "text", "expr": "Splitting an operator may result\nin the loop for one of the new index variables being emitted in a higher\nloop nest", "word_idx": 17526, "sentence_idx": 264, "label": "unlabeled"}, {"type": "text", "expr": " That is, it is revealed as loop-invariant\nand hoisted out of inner loops, which removes redundant computation", "word_idx": 17641, "sentence_idx": 265, "label": "unlabeled"}, {"type": "text", "expr": "Many important kernels benefit from operator splits, including sparse matrix\nmultiplication, matrix addition, and the matricized tensor times Khatri-Rao\nproduct", "word_idx": 17751, "sentence_idx": 266, "label": "unlabeled"}, {"type": "text", "expr": " In this section we describe the optimization with simple examples,\nbut we will explore its application to sophisticated real-world kernels\nin\u00a0Section\u00a0 5 ", "word_idx": 17911, "sentence_idx": 267, "label": "unlabeled"}, {"type": "text", "expr": "Splitting an operator in an iteration graph causes the associated index\nvariable  $i$  to be divided in two: one for the sub-expression left of the\noperator ( $i_{\\textrm{left}}$ ) and one for the whole expression\n( $i_{\\textrm{right}}$ )", "word_idx": 18065, "sentence_idx": 268, "label": "unlabeled"}, {"type": "text", "expr": " The incoming arrows are divided between the two new\nindex variables", "word_idx": 18303, "sentence_idx": 269, "label": "unlabeled"}, {"type": "text", "expr": " An operator split also introduces a dense workspace that is\nthe result of the first index variable and an operand of the second", "word_idx": 18371, "sentence_idx": 270, "label": "unlabeled"}, {"type": "text", "expr": " In\neffect, the operator\u2019s left sub-expression is stored to the workspace, and the\nworkspace takes the place of the left sub-expression at the second index\nvariable", "word_idx": 18499, "sentence_idx": 271, "label": "unlabeled"}, {"type": "text", "expr": " For example, splitting the multiplication operator in a\ncomponent-wise multiplication  $a_{i}=b_{i}c_{i}$  results in", "word_idx": 18663, "sentence_idx": 272, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{\\textrm{left}}$$", "word_idx": 18781, "sentence_idx": 273, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{\\textrm{right}}$$", "word_idx": 18798, "sentence_idx": 274, "label": "unlabeled"}, {"type": "math", "expr": "$$a_{i}=b_{i}c_{i}$$", "word_idx": 18816, "sentence_idx": 275, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle w_{i_{\\textrm{left}}}$", "word_idx": 18832, "sentence_idx": 276, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle w_{i_{\\textrm{left}}}$$", "word_idx": 18869, "sentence_idx": 277, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=b_{i_{\\textrm{left}}}$", "word_idx": 18904, "sentence_idx": 278, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=b_{i_{\\textrm{left}}}$$", "word_idx": 18941, "sentence_idx": 279, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle a_{i_{\\textrm{right}}}$", "word_idx": 18976, "sentence_idx": 280, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle a_{i_{\\textrm{right}}}$$", "word_idx": 19014, "sentence_idx": 281, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=w_{i_{\\textrm{right}}}c_{i_{\\textrm{right}}}$", "word_idx": 19050, "sentence_idx": 282, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=w_{i_{\\textrm{right}}}c_{i_{\\textrm{right}}}.$$", "word_idx": 19110, "sentence_idx": 283, "label": "unlabeled"}, {"type": "text", "expr": "The rationale for this transformation is that  w  may be a dense tensor\nand that the resulting multiplication therefore needs fewer conditional checks,\nas demonstrated in\u00a0Figure\u00a0 5 ", "word_idx": 19169, "sentence_idx": 284, "label": "unlabeled"}, {"type": "text", "expr": " Furthermore, when the\ntransformation is applied inside a loop nest then it can lead to loop invariant\ncode motion as shown in\u00a0Section\u00a0 5", "word_idx": 19350, "sentence_idx": 285, "label": "unlabeled"}, {"type": "text", "expr": " Finally, when applied to an\nassignment it facilitates scattering values into a sparse result", "word_idx": 19487, "sentence_idx": 286, "label": "unlabeled"}, {"type": "text", "expr": " Operator\nsplits can be applied to more than one operator in sequence and each resulting\nindex variable becomes a loop in the generated code", "word_idx": 19580, "sentence_idx": 287, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:  \nGeneric operator split", "word_idx": 19720, "sentence_idx": 288, "label": "unlabeled"}, {"type": "text", "expr": " The arrows to the left of the operator are\npeeled off to the left index variable, and a workspace is the result of\nthe left and an operand of the right index variable", "word_idx": 19754, "sentence_idx": 289, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:", "word_idx": 19921, "sentence_idx": 290, "label": "unlabeled"}, {"type": "text", "expr": "Operator Split Definition: \n\nLet  $\\textrm{expr}_{\\textrm{left}}\\oplus_{i}\\textrm{expr}_{\\textrm{right}}$  be the index expression at index variable\n $i$ , where  $\\oplus$  is any operator including assignment from left to right\n $\\stackrel{\\mathclap{\\tiny\\mbox{$\\rightarrow$}}}{=}$ ", "word_idx": 19930, "sentence_idx": 291, "label": "unlabeled"}, {"type": "text", "expr": " Moreover, let  $\\textrm{expr}_{\\textrm{left}}$  include all\noperands  $l_{1},\\ldots,l_{m}$  to the left of the operator and let\n $\\textrm{expr}_{\\textrm{right}}$  include all operands  $r_{1},\\ldots,r_{n}$  to the\nright", "word_idx": 20213, "sentence_idx": 292, "label": "unlabeled"}, {"type": "text", "expr": " An operator split of  $\\oplus_{i}$  creates two new index variable\n $i_{\\textrm{left}}$  and  $i_{\\textrm{right}}$ ", "word_idx": 20433, "sentence_idx": 293, "label": "unlabeled"}, {"type": "text", "expr": " The index variable\n $i_{\\textrm{left}}$  is given the expression\n $\\textrm{expr}_{\\textrm{left}}\\stackrel{\\mathclap{\\tiny\\mbox{$\\rightarrow$}}}{=%\n}w_{i_{\\textrm{left}}}$ , and\nthe expression of  $i_{\\textrm{right}}$  is rewritten to\n $w_{i_{\\textrm{right}}}\\oplus\\textrm{expr}_{\\textrm{right}}$ ", "word_idx": 20549, "sentence_idx": 294, "label": "unlabeled"}, {"type": "text", "expr": "Operator Split Definition:", "word_idx": 20846, "sentence_idx": 295, "label": "unlabeled"}, {"type": "math", "expr": "$$\\textrm{expr}_{\\textrm{left}}\\oplus_{i}\\textrm{expr}_{\\textrm{right}}$$", "word_idx": 20872, "sentence_idx": 296, "label": "unlabeled"}, {"type": "math", "expr": "$$\\oplus$$", "word_idx": 20941, "sentence_idx": 297, "label": "unlabeled"}, {"type": "math", "expr": "$$\\stackrel{\\mathclap{\\tiny\\mbox{$\\rightarrow$}}}{=}$$", "word_idx": 20947, "sentence_idx": 298, "label": "unlabeled"}, {"type": "math", "expr": "$$\\textrm{expr}_{\\textrm{left}}$$", "word_idx": 20997, "sentence_idx": 299, "label": "unlabeled"}, {"type": "math", "expr": "$$l_{1},\\ldots,l_{m}$$", "word_idx": 21026, "sentence_idx": 300, "label": "unlabeled"}, {"type": "math", "expr": "$$\\textrm{expr}_{\\textrm{right}}$$", "word_idx": 21044, "sentence_idx": 301, "label": "unlabeled"}, {"type": "math", "expr": "$$r_{1},\\ldots,r_{n}$$", "word_idx": 21074, "sentence_idx": 302, "label": "unlabeled"}, {"type": "math", "expr": "$$\\oplus_{i}$$", "word_idx": 21092, "sentence_idx": 303, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{\\textrm{left}}$$", "word_idx": 21102, "sentence_idx": 304, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{\\textrm{right}}$$", "word_idx": 21119, "sentence_idx": 305, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{\\textrm{left}}$$", "word_idx": 21137, "sentence_idx": 306, "label": "unlabeled"}, {"type": "math", "expr": "$$\\textrm{expr}_{\\textrm{left}}\\stackrel{\\mathclap{\\tiny\\mbox{$\\rightarrow$}}}{=%\n}w_{i_{\\textrm{left}}}$$", "word_idx": 21154, "sentence_idx": 307, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{\\textrm{right}}$$", "word_idx": 21256, "sentence_idx": 308, "label": "unlabeled"}, {"type": "math", "expr": "$$w_{i_{\\textrm{right}}}\\oplus\\textrm{expr}_{\\textrm{right}}$$", "word_idx": 21274, "sentence_idx": 309, "label": "unlabeled"}, {"type": "text", "expr": "An operator split is shown graphically in\u00a0Figure\u00a0 4 ", "word_idx": 21332, "sentence_idx": 310, "label": "unlabeled"}, {"type": "text", "expr": " The index\nvariables  $i_{\\textrm{left}}$  and  $i_{\\textrm{right}}$  divide the incoming\noperator arrows:  $i_{\\textrm{left}}$  has the arrows from the operator\u2019s left\nside and  $i_{\\textrm{right}}$  has the arrows from the operator\u2019s right side", "word_idx": 21384, "sentence_idx": 311, "label": "unlabeled"}, {"type": "text", "expr": "\nFurthermore,  $i_{\\textrm{right}}$  retains the result arrow  $a$ , while the\nresult of  $i_{\\textrm{left}}$  is stored to the workspace  $w$  that also replaces\nthe left expression in  $i_{\\textrm{right}}$ ", "word_idx": 21630, "sentence_idx": 312, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{\\textrm{left}}$$", "word_idx": 21838, "sentence_idx": 313, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{\\textrm{right}}$$", "word_idx": 21855, "sentence_idx": 314, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{\\textrm{left}}$$", "word_idx": 21873, "sentence_idx": 315, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{\\textrm{right}}$$", "word_idx": 21890, "sentence_idx": 316, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{\\textrm{right}}$$", "word_idx": 21908, "sentence_idx": 317, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{\\textrm{left}}$$", "word_idx": 21926, "sentence_idx": 318, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{\\textrm{right}}$$", "word_idx": 21943, "sentence_idx": 319, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a05:  \nSparse vector inner product  $a=\\sum_{i}b_{i}c_{i}$ ", "word_idx": 21961, "sentence_idx": 320, "label": "unlabeled"}, {"type": "text", "expr": " The top shows the\niteration graph and associated code that iterates over the intersection of\nthe sparse vectors", "word_idx": 22025, "sentence_idx": 321, "label": "unlabeled"}, {"type": "text", "expr": " The bottom shows the iteration graph and code after\nsplitting the multiplication", "word_idx": 22137, "sentence_idx": 322, "label": "unlabeled"}, {"type": "text", "expr": " The sparse-sparse merge code is replaced\nwith a copy to a dense workspace followed by a sparse-dense merge", "word_idx": 22218, "sentence_idx": 323, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a05:", "word_idx": 22325, "sentence_idx": 324, "label": "unlabeled"}, {"type": "math", "expr": "$$a=\\sum_{i}b_{i}c_{i}$$", "word_idx": 22334, "sentence_idx": 325, "label": "unlabeled"}, {"type": "text", "expr": "Consider two examples, vector product and addition, that show operator splits\nfor iteration graphs with a single variable", "word_idx": 22354, "sentence_idx": 326, "label": "unlabeled"}, {"type": "text", "expr": " In\u00a0Section\u00a0 5  we will\nexplore operator splits applied to deep iteration graphs that result from\nhigher-dimensional tensor expressions", "word_idx": 22475, "sentence_idx": 327, "label": "unlabeled"}, {"type": "text", "expr": " Figure\u00a0 5  shows the\niteration graphs for a vector inner product before and after splitting the\nmultiplication", "word_idx": 22610, "sentence_idx": 328, "label": "unlabeled"}, {"type": "text", "expr": " The generated code before the split iterates over the\nintersection of the coordinates in  $b$  and  $c$  that have nonzero component\nvalues", "word_idx": 22721, "sentence_idx": 329, "label": "unlabeled"}, {"type": "text", "expr": " Thus, the loop iterates while both  $c$  and  $b$  have values left and\nadds to  $a$  if both have a value at a coordinate", "word_idx": 22861, "sentence_idx": 330, "label": "unlabeled"}, {"type": "text", "expr": " After the split the\niteration graph has two index variables", "word_idx": 22984, "sentence_idx": 331, "label": "unlabeled"}, {"type": "text", "expr": " The first stores to a dense workspace\nand the second reads from it", "word_idx": 23044, "sentence_idx": 332, "label": "unlabeled"}, {"type": "text", "expr": " Multiplying a dense and sparse vector does not\nrequire merge code since you can iterate over the sparse vector and retrieve\nvalues from the dense vector", "word_idx": 23111, "sentence_idx": 333, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a06:  \nSparse vector addition  $a_{i}=b_{i}+c_{i}$ ", "word_idx": 23264, "sentence_idx": 334, "label": "unlabeled"}, {"type": "text", "expr": " The top shows the iteration\ngraph and generated code that iterates over the union of the sparse\nvectors", "word_idx": 23320, "sentence_idx": 335, "label": "unlabeled"}, {"type": "text", "expr": " The bottom shows the iteration graph and code after splitting the\naddition and assignment", "word_idx": 23424, "sentence_idx": 336, "label": "unlabeled"}, {"type": "text", "expr": " The sparse merge code is replaced with loops that\nadd each operand to the workspace and one that copies it to the result", "word_idx": 23514, "sentence_idx": 337, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a06:", "word_idx": 23635, "sentence_idx": 338, "label": "unlabeled"}, {"type": "math", "expr": "$$a_{i}=b_{i}+c_{i}$$", "word_idx": 23644, "sentence_idx": 339, "label": "unlabeled"}, {"type": "text", "expr": "The iteration graphs for a vector addition before and after splitting the\naddition and the assignment operators are shown in\u00a0Figure\u00a0 6 ", "word_idx": 23661, "sentence_idx": 340, "label": "unlabeled"}, {"type": "text", "expr": "\nCode generated from the iteration graph before splitting has three while loops\nthat together iterate over the union of the operands", "word_idx": 23796, "sentence_idx": 341, "label": "unlabeled"}, {"type": "text", "expr": " The first loop iterates\nwhile both  $b$  and  $c$  have coordinates left and computes results if either\nhave a nonzero at a coordinate; the remaining two loops add the rest of the\noperand with nonzeros left", "word_idx": 23928, "sentence_idx": 342, "label": "unlabeled"}, {"type": "text", "expr": " We split both the addition and the assignment to\nshow the effect of an assignment split", "word_idx": 24135, "sentence_idx": 343, "label": "unlabeled"}, {"type": "text", "expr": " The resulting code has two loops that\nstore each operand into a workspace and one that copies the nonzeros to  $a$ ", "word_idx": 24223, "sentence_idx": 344, "label": "unlabeled"}, {"type": "text", "expr": "\nThese loops have fewer conditionals and simpler loop bounds, at the cost of\nreduced temporal data locality as the reuse distance in  $w$  can be large", "word_idx": 24339, "sentence_idx": 345, "label": "unlabeled"}, {"type": "text", "expr": "\nWhich code performs better depends on the machine and on the particular\nsparsity of the inputs", "word_idx": 24490, "sentence_idx": 346, "label": "unlabeled"}, {"type": "text", "expr": "The final loop on lines 9\u201313 in\u00a0Figure\u00a0 6  after the split\niterates over the result  $a$ ", "word_idx": 24585, "sentence_idx": 347, "label": "unlabeled"}, {"type": "text", "expr": " This assumes that the index structure of  $a$  has\nbeen assembled prior to this code executing", "word_idx": 24674, "sentence_idx": 348, "label": "unlabeled"}, {"type": "text", "expr": " Our code generation can emit\nseparate assembly and compute kernels, and in pure compute kernels it assumes\nthat the result index structure has been pre-assembled", "word_idx": 24769, "sentence_idx": 349, "label": "unlabeled"}, {"type": "text", "expr": " We will discuss the\nassembly code that builds  $a$ \u2019s index structure in\u00a0Section\u00a0 6 ", "word_idx": 24931, "sentence_idx": 350, "label": "unlabeled"}, {"type": "text", "expr": "1  Precondition", "word_idx": 25016, "sentence_idx": 351, "label": "unlabeled"}, {"type": "text", "expr": "Operator splits do not always apply", "word_idx": 25031, "sentence_idx": 352, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, an operator can be split\nonly if its index variable does not have a reduction variable predecessor\nin the iteration graph that the operator does not distribute across", "word_idx": 25066, "sentence_idx": 353, "label": "unlabeled"}, {"type": "text", "expr": "Operator splits do not always apply", "word_idx": 25247, "sentence_idx": 354, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, an operator can be split\nonly if its index variable does not have a reduction variable predecessor\nin the iteration graph that the operator does not distribute across", "word_idx": 25282, "sentence_idx": 355, "label": "unlabeled"}, {"type": "text", "expr": "Reduction Precondition: \nLet  $j$  be an index variable with a predecessor reduction variable  $k$  with\nreduction operator  $\\oplus$ ", "word_idx": 25463, "sentence_idx": 356, "label": "unlabeled"}, {"type": "text", "expr": " Furthermore, let  $j$  and  $k$  have a shared\npredecessor  $i$  and let the incoming arrows on  $j$  from  $k$  and  $i$  be merged\nwith the operator  $\\otimes$ ", "word_idx": 25597, "sentence_idx": 357, "label": "unlabeled"}, {"type": "text", "expr": " Then  $j$  can be split on the operator  $\\otimes$ \nif and only if  $\\otimes$  distributes over  $\\oplus$  (i", "word_idx": 25760, "sentence_idx": 358, "label": "unlabeled"}, {"type": "text", "expr": ",  $b\\otimes(c\\oplus d)=(b\\otimes c)\\oplus(b\\otimes d)$ )", "word_idx": 25870, "sentence_idx": 359, "label": "unlabeled"}, {"type": "text", "expr": "Reduction Precondition:", "word_idx": 25927, "sentence_idx": 360, "label": "unlabeled"}, {"type": "math", "expr": "$$\\oplus$$", "word_idx": 25950, "sentence_idx": 361, "label": "unlabeled"}, {"type": "math", "expr": "$$\\otimes$$", "word_idx": 25956, "sentence_idx": 362, "label": "unlabeled"}, {"type": "math", "expr": "$$\\otimes$$", "word_idx": 25963, "sentence_idx": 363, "label": "unlabeled"}, {"type": "math", "expr": "$$\\otimes$$", "word_idx": 25970, "sentence_idx": 364, "label": "unlabeled"}, {"type": "math", "expr": "$$\\oplus$$", "word_idx": 25977, "sentence_idx": 365, "label": "unlabeled"}, {"type": "math", "expr": "$$b\\otimes(c\\oplus d)=(b\\otimes c)\\oplus(b\\otimes d)$$", "word_idx": 25983, "sentence_idx": 366, "label": "unlabeled"}, {"type": "text", "expr": "The reason for the precondition is that after splitting  $\\otimes$  at  $j$  the\nindex variable associated with the arrow from  $i$  will no longer be dominated\nby  $k$ ", "word_idx": 26033, "sentence_idx": 367, "label": "unlabeled"}, {"type": "text", "expr": " The loop resulting from this  $j$  will therefore be hoisted out of the\n $k$  reduction loop and will be added in only once", "word_idx": 26202, "sentence_idx": 368, "label": "unlabeled"}, {"type": "math", "expr": "$$\\otimes$$", "word_idx": 26326, "sentence_idx": 369, "label": "unlabeled"}, {"type": "text", "expr": "2  The Result Tensor as Workspace", "word_idx": 26333, "sentence_idx": 370, "label": "unlabeled"}, {"type": "text", "expr": "If the result is dense then it pays to use it to accumulate the results of both\nsub-expressions resulting from an operator, instead of introducing a workspace", "word_idx": 26366, "sentence_idx": 371, "label": "unlabeled"}, {"type": "text", "expr": "\nThus the result is used as a temporary workspace", "word_idx": 26524, "sentence_idx": 372, "label": "unlabeled"}, {"type": "text", "expr": " This is useful in sparse\nvector addition when the result is dense", "word_idx": 26573, "sentence_idx": 373, "label": "unlabeled"}, {"type": "text", "expr": " Since the result is dense, there is\nno need for a temporary workspace", "word_idx": 26639, "sentence_idx": 374, "label": "unlabeled"}, {"type": "text", "expr": " This optimization introduces another\nprecondition to ensure results are not overwritten", "word_idx": 26709, "sentence_idx": 375, "label": "unlabeled"}, {"type": "text", "expr": "If the result is dense then it pays to use it to accumulate the results of both\nsub-expressions resulting from an operator, instead of introducing a workspace", "word_idx": 26797, "sentence_idx": 376, "label": "unlabeled"}, {"type": "text", "expr": "\nThus the result is used as a temporary workspace", "word_idx": 26955, "sentence_idx": 377, "label": "unlabeled"}, {"type": "text", "expr": " This is useful in sparse\nvector addition when the result is dense", "word_idx": 27004, "sentence_idx": 378, "label": "unlabeled"}, {"type": "text", "expr": " Since the result is dense, there is\nno need for a temporary workspace", "word_idx": 27070, "sentence_idx": 379, "label": "unlabeled"}, {"type": "text", "expr": " This optimization introduces another\nprecondition to ensure results are not overwritten", "word_idx": 27140, "sentence_idx": 380, "label": "unlabeled"}, {"type": "text", "expr": "Reuse Precondition: \nLet  $j$  be an index variable that has a predecessor  $k$  through two paths that\nare merged with operator  $\\otimes_{j}$ ", "word_idx": 27228, "sentence_idx": 381, "label": "unlabeled"}, {"type": "text", "expr": " Furthermore, let  $j$  and  $k$  have a\nshared predecessor  $i$ , and let the arrow from  $i$  to  $k$  be a result arrow", "word_idx": 27372, "sentence_idx": 382, "label": "unlabeled"}, {"type": "text", "expr": "\nThen splitting  $\\otimes_{j}$  must introduce a new workspace", "word_idx": 27494, "sentence_idx": 383, "label": "unlabeled"}, {"type": "text", "expr": "Reuse Precondition:", "word_idx": 27556, "sentence_idx": 384, "label": "unlabeled"}, {"type": "math", "expr": "$$\\otimes_{j}$$", "word_idx": 27575, "sentence_idx": 385, "label": "unlabeled"}, {"type": "math", "expr": "$$\\otimes_{j}$$", "word_idx": 27586, "sentence_idx": 386, "label": "unlabeled"}, {"type": "text", "expr": "This precondition is, for example, not satisfied by the operator split to\nremove redundant computations in the MTTKRP kernel in\u00a0Section\u00a0 5", "word_idx": 27597, "sentence_idx": 387, "label": "unlabeled"}, {"type": "text", "expr": "\nTherefore, a temporary workspace is necessary to optimize that kernel", "word_idx": 27735, "sentence_idx": 388, "label": "unlabeled"}, {"type": "text", "expr": "3  Applying Operator Splits", "word_idx": 27805, "sentence_idx": 389, "label": "unlabeled"}, {"type": "text", "expr": "Operator splitting increases the performance of many important kernels because\nit removes inserts into sparse results, expensive merge code, and loop\ninvariant code", "word_idx": 27832, "sentence_idx": 390, "label": "unlabeled"}, {"type": "text", "expr": " It does, however, impose costs from constructing, maintaining,\nand using workspaces", "word_idx": 27996, "sentence_idx": 391, "label": "unlabeled"}, {"type": "text", "expr": " Constructing a workspace requires a  malloc \nfollowed by a  memset  to zero its values and it must be reinitialized\nbetween uses", "word_idx": 28080, "sentence_idx": 392, "label": "unlabeled"}, {"type": "text", "expr": " Furthermore, a workspace reduces temporal locality due to the\nincreased reuse distance from storing values to the workspace and later reading\nthem back to store to the result", "word_idx": 28209, "sentence_idx": 393, "label": "unlabeled"}, {"type": "text", "expr": "malloc", "word_idx": 28384, "sentence_idx": 394, "label": "unlabeled"}, {"type": "text", "expr": "memset", "word_idx": 28390, "sentence_idx": 395, "label": "unlabeled"}, {"type": "text", "expr": "A system design is more flexible if it separates mechanism (what to do) from\npolicy (how to do it)\u00a0 ", "word_idx": 28396, "sentence_idx": 396, "label": "unlabeled"}, {"type": "text", "expr": " Performance is a key design\ncriteria in a tensor algebra system", "word_idx": 28496, "sentence_idx": 397, "label": "unlabeled"}, {"type": "text", "expr": " A good system design should therefore\nseparate policy decisions of how to optimize generated code from the mechanisms\nthat carry out the optimization", "word_idx": 28560, "sentence_idx": 398, "label": "unlabeled"}, {"type": "text", "expr": " This paper provides the mechanism for\noperator splitting", "word_idx": 28710, "sentence_idx": 399, "label": "unlabeled"}, {"type": "text", "expr": "We imagine many fruitful policy approaches such as user-specified policy,\nheuristics, mathematical optimization, machine learning, and autotuning", "word_idx": 28767, "sentence_idx": 400, "label": "unlabeled"}, {"type": "text", "expr": " We\nleave the design of automated policy systems as future work", "word_idx": 28912, "sentence_idx": 401, "label": "unlabeled"}, {"type": "text", "expr": " To facilitate\npolicy research, we see operator splits as a key tensor algebra scheduling\nconstruct", "word_idx": 28975, "sentence_idx": 402, "label": "unlabeled"}, {"type": "text", "expr": " The Halide system\u00a0  shows that a scheduling language\nis effective at separating mechanism from policy", "word_idx": 29074, "sentence_idx": 403, "label": "unlabeled"}, {"type": "text", "expr": " Scheduling languages leave\nusers in control of performance, while freeing them from low level code\ntransformations", "word_idx": 29176, "sentence_idx": 404, "label": "unlabeled"}, {"type": "text", "expr": " The goal, of course, is a fully automated system where users\nare freed from performance decisions as well", "word_idx": 29291, "sentence_idx": 405, "label": "unlabeled"}, {"type": "text", "expr": " Such a system, however, also\nprofits from a well-design scheduling language because it it lets researchers\nexplore different policy approaches without re-implementing mechanisms", "word_idx": 29397, "sentence_idx": 406, "label": "unlabeled"}, {"type": "text", "expr": " For\ntensor algebra, a scheduling language should include formats, operator splits,\nand loop optimizations", "word_idx": 29575, "sentence_idx": 407, "label": "unlabeled"}, {"type": "text", "expr": "4  Dimensionality and Choice of Workspaces", "word_idx": 29681, "sentence_idx": 408, "label": "unlabeled"}, {"type": "text", "expr": "The examples in this paper use vector workspaces", "word_idx": 29723, "sentence_idx": 409, "label": "unlabeled"}, {"type": "text", "expr": " The operator split\noptimization, however, applies to kernels where higher-dimensional workspaces,\nsuch as a matrix or a tensor, are needed", "word_idx": 29771, "sentence_idx": 410, "label": "unlabeled"}, {"type": "text", "expr": " The dimensionality of a workspace is\ndetermined by counting the number of index variables above the second index\nvariable after the split, and the sizes of dimensions are determined by the\nranges of the index variables", "word_idx": 29910, "sentence_idx": 411, "label": "unlabeled"}, {"type": "text", "expr": "The examples in this paper use vector workspaces", "word_idx": 30129, "sentence_idx": 412, "label": "unlabeled"}, {"type": "text", "expr": " The operator split\noptimization, however, applies to kernels where higher-dimensional workspaces,\nsuch as a matrix or a tensor, are needed", "word_idx": 30177, "sentence_idx": 413, "label": "unlabeled"}, {"type": "text", "expr": " The dimensionality of a workspace is\ndetermined by counting the number of index variables above the second index\nvariable after the split, and the sizes of dimensions are determined by the\nranges of the index variables", "word_idx": 30316, "sentence_idx": 414, "label": "unlabeled"}, {"type": "text", "expr": "Furthermore, dense arrays are not the only choice for workspaces; a tensor of\nany format will do", "word_idx": 30535, "sentence_idx": 415, "label": "unlabeled"}, {"type": "text", "expr": " The format, however, affects the generated code and its\nperformance", "word_idx": 30631, "sentence_idx": 416, "label": "unlabeled"}, {"type": "text", "expr": " Operator splits are often used to remove expensive sparse-sparse\nmerge code, and dense workspaces are attractive because they result in cheaper\nsparse-dense merges", "word_idx": 30699, "sentence_idx": 417, "label": "unlabeled"}, {"type": "text", "expr": "\nAn alternative is another format with random access such as a hash map", "word_idx": 30863, "sentence_idx": 418, "label": "unlabeled"}, {"type": "text", "expr": " These\nresult in slower execution\u00a0 , but uses only memory\nproportional to the number of nonzeros", "word_idx": 30934, "sentence_idx": 419, "label": "unlabeled"}, {"type": "text", "expr": "5  Case Studies", "word_idx": 31030, "sentence_idx": 420, "label": "unlabeled"}, {"type": "text", "expr": "In this section we will study three important linear and tensor algebra\nexpressions that can be optimized with operator splits", "word_idx": 31045, "sentence_idx": 421, "label": "unlabeled"}, {"type": "text", "expr": " The resulting kernels\nare competitive with hand-optimized kernels from the\nliterature\u00a0 ", "word_idx": 31171, "sentence_idx": 422, "label": "unlabeled"}, {"type": "text", "expr": " The optimization, however,\ngeneralizes to an uncountable number of kernels that have not been implemented\nbefore", "word_idx": 31259, "sentence_idx": 423, "label": "unlabeled"}, {"type": "text", "expr": " We will show one example, MTTKRP with sparse matrices,\nin\u00a0Section\u00a0 5", "word_idx": 31372, "sentence_idx": 424, "label": "unlabeled"}, {"type": "text", "expr": "1  Matrix Multiplication", "word_idx": 31441, "sentence_idx": 425, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption Iteration graph before split", "word_idx": 31465, "sentence_idx": 426, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 31505, "sentence_idx": 427, "label": "unlabeled"}, {"type": "text", "expr": "Iteration graph before split", "word_idx": 31516, "sentence_idx": 428, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption Iteration graph after split", "word_idx": 31544, "sentence_idx": 429, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 31583, "sentence_idx": 430, "label": "unlabeled"}, {"type": "text", "expr": "Iteration graph after split", "word_idx": 31594, "sentence_idx": 431, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a07:  \nMatrix multiplication  $A_{ij}=\\sum_{k}B_{ik}C_{kj}$ , using the\nlinear combination of rows algorithm where all matrices are in the CSR\nformat", "word_idx": 31621, "sentence_idx": 432, "label": "unlabeled"}, {"type": "text", "expr": " Splitting the assignment operator yields\n Gustavson \u2019s algorithm\u00a0 , which we\nshowed in\u00a0Figure\u00a0 1 ", "word_idx": 31775, "sentence_idx": 433, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a07:", "word_idx": 31873, "sentence_idx": 434, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ij}=\\sum_{k}B_{ik}C_{kj}$$", "word_idx": 31882, "sentence_idx": 435, "label": "unlabeled"}, {"type": "text", "expr": "Gustavson", "word_idx": 31909, "sentence_idx": 436, "label": "unlabeled"}, {"type": "text", "expr": "The preferred algorithm for multiplying sparse matrices is to compute the\nlinear combinations of rows or columns\u00a0 ", "word_idx": 31918, "sentence_idx": 437, "label": "unlabeled"}, {"type": "text", "expr": " This\nalgorithm was introduced by  Gustavson \u00a0 ,\nwho showed that it is asymptotically superior to computing inner products when\nthe matrices are sparse", "word_idx": 32032, "sentence_idx": 438, "label": "unlabeled"}, {"type": "text", "expr": " Furthermore, both operands and the result are the\nsame format", "word_idx": 32183, "sentence_idx": 439, "label": "unlabeled"}, {"type": "text", "expr": " A sparse inner product algorithm inconveniently needs the first\noperand to be row major (CSR) and the second column major (CSC)", "word_idx": 32245, "sentence_idx": 440, "label": "unlabeled"}, {"type": "text", "expr": "Gustavson", "word_idx": 32373, "sentence_idx": 441, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a0 7  shows the iteration graph for a linear combination\nof rows algorithm, where the matrices are stored in the CSR format", "word_idx": 32382, "sentence_idx": 442, "label": "unlabeled"}, {"type": "text", "expr": " The\niteration graph has an issue at index variable  $j$ ", "word_idx": 32510, "sentence_idx": 443, "label": "unlabeled"}, {"type": "text", "expr": " Because the assignment to\n $A$  at  $j$  is dominated by the summation index variable  $k$ , the generated code\nmust repeatedly add new values into  $A$ ", "word_idx": 32567, "sentence_idx": 444, "label": "unlabeled"}, {"type": "text", "expr": " This is expensive when  $A$  is sparse\ndue to costly inserts into its sparse data structure", "word_idx": 32721, "sentence_idx": 445, "label": "unlabeled"}, {"type": "text", "expr": "In Figure\u00a0 7 , the assignment operator at  $j$  has been split to\nyield two new index variables  $j_{C}$  and  $j_{A}$ ", "word_idx": 32813, "sentence_idx": 446, "label": "unlabeled"}, {"type": "text", "expr": " The first index variable  $j_{C}$ \naccumulates values into a dense workspace  $w$ , while  $j_{A}$  copies the nonzero\nvalues from the workspace to  $A$ ", "word_idx": 32932, "sentence_idx": 447, "label": "unlabeled"}, {"type": "text", "expr": " Because the workspace is dense, the merge\nwith  $C$  at  $j_{C}$  is trivial: the kernel iterates over  $C$  and scatters values\ninto  $w$ ", "word_idx": 33086, "sentence_idx": 448, "label": "unlabeled"}, {"type": "text", "expr": " Furthermore, the second index variable  $j_{A}$  is not dominated by the\nsummation variable  $k$  and values are therefore appended to  $A$ ", "word_idx": 33226, "sentence_idx": 449, "label": "unlabeled"}, {"type": "math", "expr": "$$j_{C}$$", "word_idx": 33367, "sentence_idx": 450, "label": "unlabeled"}, {"type": "math", "expr": "$$j_{A}$$", "word_idx": 33372, "sentence_idx": 451, "label": "unlabeled"}, {"type": "math", "expr": "$$j_{C}$$", "word_idx": 33377, "sentence_idx": 452, "label": "unlabeled"}, {"type": "math", "expr": "$$j_{A}$$", "word_idx": 33382, "sentence_idx": 453, "label": "unlabeled"}, {"type": "math", "expr": "$$j_{C}$$", "word_idx": 33387, "sentence_idx": 454, "label": "unlabeled"}, {"type": "math", "expr": "$$j_{A}$$", "word_idx": 33392, "sentence_idx": 455, "label": "unlabeled"}, {"type": "text", "expr": "The code listing in Figure\u00a0 1  showed the code generated from a\nmatrix multiplication iteration graph where the assignment operator has been\nsplit", "word_idx": 33397, "sentence_idx": 456, "label": "unlabeled"}, {"type": "text", "expr": " Each index variable results in a loop, loops generated from index\nvariables connected by an arrow are nested, and loops generated from index\nvariables that share a direct predecessor are sequenced", "word_idx": 33543, "sentence_idx": 457, "label": "unlabeled"}, {"type": "text", "expr": " The loop of  $j_{A}$ \ncopies values from the workspace to  $A$ , so it can either iterate over the\nnonzeros of the workspace or the index structure of  $A$ ", "word_idx": 33740, "sentence_idx": 458, "label": "unlabeled"}, {"type": "text", "expr": " The loop on\nlines\u00a010\u201314 in the code listing iterates over the index structure of  $A$ ,\nmeaning it must be pre-assembled before this code is executed", "word_idx": 33897, "sentence_idx": 459, "label": "unlabeled"}, {"type": "text", "expr": " The alternative\nis to emit code that tracks the nonzeros inserted into the workspace, but this\nis more expensive", "word_idx": 34047, "sentence_idx": 460, "label": "unlabeled"}, {"type": "text", "expr": " It is often more efficient to separate the code that\nassembles  $A$ \u2019s index structure from the code that computes its\nvalues\u00a0 ", "word_idx": 34160, "sentence_idx": 461, "label": "unlabeled"}, {"type": "text", "expr": " For ease of exposition we choose to show pure\ncompute kernels in most code listings", "word_idx": 34288, "sentence_idx": 462, "label": "unlabeled"}, {"type": "text", "expr": " We will, however, discuss code\ngeneration for pure assembly and fused assembly-and-compute kernels in\nSection\u00a0 6 ", "word_idx": 34372, "sentence_idx": 463, "label": "unlabeled"}, {"type": "text", "expr": " These kernels cannot assume the results have been\npre-assembled and must maintain and iterate over a workspace index", "word_idx": 34486, "sentence_idx": 464, "label": "unlabeled"}, {"type": "math", "expr": "$$j_{A}$$", "word_idx": 34603, "sentence_idx": 465, "label": "unlabeled"}, {"type": "text", "expr": "2  Matrix Addition", "word_idx": 34608, "sentence_idx": 466, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption Iteration graph before split", "word_idx": 34626, "sentence_idx": 467, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 34666, "sentence_idx": 468, "label": "unlabeled"}, {"type": "text", "expr": "Iteration graph before split", "word_idx": 34677, "sentence_idx": 469, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption Iteration graph after split", "word_idx": 34705, "sentence_idx": 470, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 34744, "sentence_idx": 471, "label": "unlabeled"}, {"type": "text", "expr": "Iteration graph after split", "word_idx": 34755, "sentence_idx": 472, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a08:  \nSparse matrix addition  $A_{ij}=B_{ij}+C_{ij}$ ", "word_idx": 34782, "sentence_idx": 473, "label": "unlabeled"}, {"type": "text", "expr": " Splitting the addition\nand assignment operators removes expensive merge code at the cost of\nreduced temporal locality", "word_idx": 34841, "sentence_idx": 474, "label": "unlabeled"}, {"type": "text", "expr": " The resulting inner loop is similar to sparse\nvector addition after an operator split, which we showed\nin\u00a0Figure\u00a0 6 ", "word_idx": 34959, "sentence_idx": 475, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a08:", "word_idx": 35076, "sentence_idx": 476, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ij}=B_{ij}+C_{ij}$$", "word_idx": 35085, "sentence_idx": 477, "label": "unlabeled"}, {"type": "text", "expr": "Sparse matrix addition demonstrates operator splits for addition operators", "word_idx": 35105, "sentence_idx": 478, "label": "unlabeled"}, {"type": "text", "expr": "\nSparse additions result in involved code to iterate over the union of the\nnonzeros of the two operands, as a multi-way merge with three loops\n\u00a0 ", "word_idx": 35179, "sentence_idx": 479, "label": "unlabeled"}, {"type": "text", "expr": " Figure\u00a0 8  shows the iteration graph for\na sparse matrix addition", "word_idx": 35324, "sentence_idx": 480, "label": "unlabeled"}, {"type": "text", "expr": " When the matrices are stored in the CSR format,\nwhich is sparse in the second dimension, the compiler must emit code to merge\n $B$  and  $C$  at the  $j$  index variable", "word_idx": 35390, "sentence_idx": 481, "label": "unlabeled"}, {"type": "text", "expr": " Such merge code contains many if\nstatements that are expensive on modern processors", "word_idx": 35560, "sentence_idx": 482, "label": "unlabeled"}, {"type": "text", "expr": " Merge code also grows\nexponential with the number of addition, so if many matrices are added it is\nnecessary to either split the input expression or, better, to use an operator\nsplit on the inner index variable so that the outer loop can still be shared", "word_idx": 35644, "sentence_idx": 483, "label": "unlabeled"}, {"type": "text", "expr": "Splitting the addition and assignment operators at the  $j$  index variable\nintroduces a dense row workspace that  $B$  and  $C$  are in turn are added into,\nand that is then copied over to  $A$ ", "word_idx": 35898, "sentence_idx": 484, "label": "unlabeled"}, {"type": "text", "expr": " The resulting code, whose inner loop is\nsimilar to\u00a0Figure\u00a0 6 , has decreased temporal locality, as the\nworkspace reuse distance is can be large, but avoids expensive merges", "word_idx": 36093, "sentence_idx": 485, "label": "unlabeled"}, {"type": "text", "expr": " Whether\nthis results in an overall performance gain depends on the machine and the\nnumber of operands that are merged", "word_idx": 36266, "sentence_idx": 486, "label": "unlabeled"}, {"type": "text", "expr": " We show results for one machine\nin\u00a0Section\u00a0 7 ", "word_idx": 36384, "sentence_idx": 487, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption Iteration graph before split", "word_idx": 36431, "sentence_idx": 488, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 36471, "sentence_idx": 489, "label": "unlabeled"}, {"type": "text", "expr": "Iteration graph before split", "word_idx": 36482, "sentence_idx": 490, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 36510, "sentence_idx": 491, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 36521, "sentence_idx": 492, "label": "unlabeled"}, {"type": "text", "expr": "Iteration graph after  $*$ -split", "word_idx": 36532, "sentence_idx": 493, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 36565, "sentence_idx": 494, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 36576, "sentence_idx": 495, "label": "unlabeled"}, {"type": "text", "expr": "Code diff showing the effect of the  $*$ -split", "word_idx": 36587, "sentence_idx": 496, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 36634, "sentence_idx": 497, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 36645, "sentence_idx": 498, "label": "unlabeled"}, {"type": "text", "expr": "Iteration graph after  $*$  and  $=$  splits", "word_idx": 36656, "sentence_idx": 499, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 36700, "sentence_idx": 500, "label": "unlabeled"}, {"type": "text", "expr": "\\subcaption", "word_idx": 36711, "sentence_idx": 501, "label": "unlabeled"}, {"type": "text", "expr": "Code diff showing the effect of the  $=$ -split", "word_idx": 36722, "sentence_idx": 502, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a09:  \nIteration graphs for the matricized tensor times Khatri-Rao product\n(MTTKRP)  $A_{ij}=\\sum_{kl}B_{ikl}C_{lj}D_{kj}$ ", "word_idx": 36769, "sentence_idx": 503, "label": "unlabeled"}, {"type": "text", "expr": " Splitting the\nmultiplication at  $j$  hoists the multiplication with  $D$  out of the inner\nloop, which removes redundant work", "word_idx": 36897, "sentence_idx": 504, "label": "unlabeled"}, {"type": "text", "expr": " If the matrix  $A$  is sparse, then also\nsplitting the assignment introduces a random access workspace that removes\nthe need to insert into  $A$ ", "word_idx": 37024, "sentence_idx": 505, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a09:", "word_idx": 37170, "sentence_idx": 506, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ij}=\\sum_{kl}B_{ikl}C_{lj}D_{kj}$$", "word_idx": 37179, "sentence_idx": 507, "label": "unlabeled"}, {"type": "text", "expr": "3  Matricized Tensor Times Khatri-Rao Product", "word_idx": 37214, "sentence_idx": 508, "label": "unlabeled"}, {"type": "text", "expr": "The matricized tensor times Khatri-Rao product (MTTKRP) is a critical kernel in\nthe alternating least squares algorithms to compute the canonical polyadic\ndecomposition of tensors\u00a0 ", "word_idx": 37259, "sentence_idx": 509, "label": "unlabeled"}, {"type": "text", "expr": " It generalizes the singular\nvalue decomposition to higher-order tensors, and has applications in data\nanalytics\u00a0 , machine learning\u00a0 ,\nneuroscience\u00a0 , image classification and\ncompression\u00a0 , and other fields\u00a0 ", "word_idx": 37440, "sentence_idx": 510, "label": "unlabeled"}, {"type": "text", "expr": "The MTTKRP can be expressed with tensor index notation as  $A_{ij}=\\sum_{kl}B_{ikl}C_{lj}D_{kj}$ ", "word_idx": 37650, "sentence_idx": 511, "label": "unlabeled"}, {"type": "text", "expr": " That is, we multiply a three-dimensional\ntensor by two matrices in the  $l$  and  $k$  dimensions", "word_idx": 37747, "sentence_idx": 512, "label": "unlabeled"}, {"type": "text", "expr": " These simultaneous\nmultiplications require four nested loops", "word_idx": 37845, "sentence_idx": 513, "label": "unlabeled"}, {"type": "text", "expr": " Figure\u00a0 9  shows\nthe iteration graph before optimization, where the matrices are stored\nrow-major", "word_idx": 37906, "sentence_idx": 514, "label": "unlabeled"}, {"type": "text", "expr": " The iteration graph results in four nested loops", "word_idx": 38004, "sentence_idx": 515, "label": "unlabeled"}, {"type": "text", "expr": " The three\noutermost loops iterate over the sparse data structure of  $B$ , while the\ninnermost loop iterates over the range of the  $j$  index variable", "word_idx": 38053, "sentence_idx": 516, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{ij}=\\sum_{kl}B_{ikl}C_{lj}D_{kj}$$", "word_idx": 38205, "sentence_idx": 517, "label": "unlabeled"}, {"type": "text", "expr": "After splitting the multiplication operator at  $j$  we get the iteration graph\nin Figure\u00a0 9 ", "word_idx": 38240, "sentence_idx": 518, "label": "unlabeled"}, {"type": "text", "expr": " The index variable  $j$  has been split in two", "word_idx": 38333, "sentence_idx": 519, "label": "unlabeled"}, {"type": "text", "expr": "\nThe second variable,  $j_{D}$ , is no longer dominated by  $l$ , which means it is\nevaluated higher up in the resulting loop nest", "word_idx": 38380, "sentence_idx": 520, "label": "unlabeled"}, {"type": "text", "expr": " Furthermore, if the matrices\n $C$  and  $D$  were sparse in the second dimension, the split also removes the\nneed to merge their sparse data structures", "word_idx": 38510, "sentence_idx": 521, "label": "unlabeled"}, {"type": "text", "expr": " The code listing in\nFigure\u00a0 9  shows a code diff of the effect of the operator\nsplit on the code when the matrices are dense", "word_idx": 38662, "sentence_idx": 522, "label": "unlabeled"}, {"type": "text", "expr": " The code specific to the\niteration graph before the  $*$ -split is colored red, and the code specific to\nthe iteration graph after the split is colored green", "word_idx": 38787, "sentence_idx": 523, "label": "unlabeled"}, {"type": "text", "expr": " Shared code is not\ncolored", "word_idx": 38945, "sentence_idx": 524, "label": "unlabeled"}, {"type": "text", "expr": " The  $*$ -split results in code where the loop over  $j$ , that\nmultiplies  $B$  with  $D$ , has been lifted out of the  $l$  loop, resulting in\nfewer total multiplication", "word_idx": 38972, "sentence_idx": 525, "label": "unlabeled"}, {"type": "text", "expr": " The drawback is that the workspace reduces\ntemporal locality, as the reuse distance between writing values to it and\nreading them back can be large", "word_idx": 39144, "sentence_idx": 526, "label": "unlabeled"}, {"type": "text", "expr": " Our evaluation in\u00a0Section\u00a0 7  shows\nthat this optimization can result in significant gains on large data sets", "word_idx": 39292, "sentence_idx": 527, "label": "unlabeled"}, {"type": "math", "expr": "$$j_{D}$$", "word_idx": 39402, "sentence_idx": 528, "label": "unlabeled"}, {"type": "text", "expr": "The MTTKRP kernel does two simultaneous matrix multiplications", "word_idx": 39407, "sentence_idx": 529, "label": "unlabeled"}, {"type": "text", "expr": " Like the\nsparse matrix multiplication kernel in\u00a0Section\u00a0 5", "word_idx": 39469, "sentence_idx": 530, "label": "unlabeled"}, {"type": "text", "expr": "1 , therefore, it needs\nto scatter values into the middle of the result matrix  $A$ ", "word_idx": 39528, "sentence_idx": 531, "label": "unlabeled"}, {"type": "text", "expr": " The reason is that\nthe  $j$  and  $j_{D}$  index variables are dominated by reduction variables", "word_idx": 39612, "sentence_idx": 532, "label": "unlabeled"}, {"type": "text", "expr": " If the\nmatrix  $A$  is sparse then inserts are expensive, and the code benefits from\nsplitting the assignment at  $j_{D}$ , as shown in\u00a0Figure\u00a0 9 ", "word_idx": 39708, "sentence_idx": 533, "label": "unlabeled"}, {"type": "text", "expr": "\nThe effect is that values are scattered into a dense workspace with random\naccess, and copied to the result after a full row of the result has been\ncomputed", "word_idx": 39855, "sentence_idx": 534, "label": "unlabeled"}, {"type": "text", "expr": " Figure\u00a0 9  shows a code diff of the effect of\nmaking the result matrix  $A$  sparse and splitting the assignment operator", "word_idx": 40012, "sentence_idx": 535, "label": "unlabeled"}, {"type": "text", "expr": "\nBoth the code from before the split (red) and the code after (green) assumes\nthe operand matrices  $C$  and  $D$  are sparse, as opposed to\nFigure\u00a0 9  where  $C$  and  $D$  were dense", "word_idx": 40134, "sentence_idx": 536, "label": "unlabeled"}, {"type": "text", "expr": " As in the sparse\nmatrix multiplication code, the code after assignment split scatters into a\ndense workspace and, when a full row has been computed, appends the workspace\nnonzeros to the result", "word_idx": 40318, "sentence_idx": 537, "label": "unlabeled"}, {"type": "math", "expr": "$$j_{D}$$", "word_idx": 40512, "sentence_idx": 538, "label": "unlabeled"}, {"type": "math", "expr": "$$j_{D}$$", "word_idx": 40517, "sentence_idx": 539, "label": "unlabeled"}, {"type": "text", "expr": "6  Workspace Assembly", "word_idx": 40522, "sentence_idx": 540, "label": "unlabeled"}, {"type": "text", "expr": "In code listings that compute sparse results, we have so far shown only kernels\nthat compute results without assembling sparse index structures\n(Figures\u00a0 1 ,  6 ,\nand\u00a0 9 )", "word_idx": 40543, "sentence_idx": 541, "label": "unlabeled"}, {"type": "text", "expr": " This let us focus on the loop structures\nwithout the added complexity of workspace assembly", "word_idx": 40714, "sentence_idx": 542, "label": "unlabeled"}, {"type": "text", "expr": " Moreover, it is common in\nnumerical code to separate the kernel that assembles index structures (often\ncalled symbolic computation) from the kernel that computes values (numeric\ncomputation)\u00a0 ", "word_idx": 40806, "sentence_idx": 543, "label": "unlabeled"}, {"type": "text", "expr": " The code generation algorithm\nfor iteration graphs can be used to emit either kernel or a kernel that\nsimultaneously assembles the result index structures and computes its\nvalues\u00a0 ", "word_idx": 40999, "sentence_idx": 544, "label": "unlabeled"}, {"type": "text", "expr": "2017b", "word_idx": 41180, "sentence_idx": 545, "label": "unlabeled"}, {"type": "text", "expr": "When generating assembly kernels from iteration graphs, a workspace consists of\ntwo arrays that together track its nonzero index structure", "word_idx": 41185, "sentence_idx": 546, "label": "unlabeled"}, {"type": "text", "expr": " The first array\n wlist  is a list of coordinates that have been inserted into the\nworkspace, and the second array ( w ) is a boolean array that guards\nagainst redundant inserts into the coordinate list", "word_idx": 41323, "sentence_idx": 547, "label": "unlabeled"}, {"type": "text", "expr": "wlist", "word_idx": 41525, "sentence_idx": 548, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a010:  \nA sparse matrix multiplication assembly kernel (the compute kernel is given\nin\u00a0Figure\u00a0 1 )", "word_idx": 41530, "sentence_idx": 549, "label": "unlabeled"}, {"type": "text", "expr": " The coordinates of row  $i$  are inserted\ninto  wlist  on line\u00a011 and copied to  A  on line 29", "word_idx": 41633, "sentence_idx": 550, "label": "unlabeled"}, {"type": "text", "expr": " The\narray  w  guards against redundant inserts", "word_idx": 41728, "sentence_idx": 551, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a010:", "word_idx": 41775, "sentence_idx": 552, "label": "unlabeled"}, {"type": "text", "expr": "wlist", "word_idx": 41785, "sentence_idx": 553, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a0 10  shows assembly code for sparse matrix\nmultiplication generated from the iteration graph in\u00a0Figure\u00a0 7 ", "word_idx": 41790, "sentence_idx": 554, "label": "unlabeled"}, {"type": "text", "expr": "\nIt is generated from the same iteration graph as the compute kernel\nin\u00a0Figure\u00a0 1 , so the loop structure is the same except for the\nloop to copy the workspace to  A  on line\u00a027", "word_idx": 41903, "sentence_idx": 555, "label": "unlabeled"}, {"type": "text", "expr": " In compute kernels, the\nindex structure of  A  must be pre-assembled, so the code generation\nalgorithm emits a loop to iterate over  A ", "word_idx": 42080, "sentence_idx": 556, "label": "unlabeled"}, {"type": "text", "expr": " In an assembly kernel,\nhowever, it emits code to iterate over the index structure of the workspace", "word_idx": 42216, "sentence_idx": 557, "label": "unlabeled"}, {"type": "text", "expr": "\nFurthermore, the assembly kernel inserts into the workspace index\n( wlist ), on lines\u00a010\u201313, instead of computing a result, and sorts the\nindex list on line\u00a018 so that the new row of  A  is ordered", "word_idx": 42315, "sentence_idx": 558, "label": "unlabeled"}, {"type": "text", "expr": " Note that\nthe sort is optional and only needed if the result must be ordered", "word_idx": 42513, "sentence_idx": 559, "label": "unlabeled"}, {"type": "text", "expr": " For\nexample, one of the MKL matrix multiplication kernels does not sort", "word_idx": 42590, "sentence_idx": 560, "label": "unlabeled"}, {"type": "text", "expr": " Finally,\nthe assembly kernel allocates memory on lines\u00a01\u20132, 21\u201324 (by repeated\ndoubling), and 35", "word_idx": 42662, "sentence_idx": 561, "label": "unlabeled"}, {"type": "text", "expr": "wlist", "word_idx": 42759, "sentence_idx": 562, "label": "unlabeled"}, {"type": "text", "expr": "7  Evaluation", "word_idx": 42764, "sentence_idx": 563, "label": "unlabeled"}, {"type": "text", "expr": "In this section, we evaluate the effectiveness of the workspace optimization by\ncomparing performance against hand-written state-of-the-art sparse libraries", "word_idx": 42777, "sentence_idx": 564, "label": "unlabeled"}, {"type": "text", "expr": "In this section, we evaluate the effectiveness of the workspace optimization by\ncomparing performance against hand-written state-of-the-art sparse libraries", "word_idx": 42933, "sentence_idx": 565, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:  \nTest matrices and tensors from the SuiteSparse Matrix\nCollection\u00a0  and the FROSTT Tensor\nCollection\u00a0 ", "word_idx": 43089, "sentence_idx": 566, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:", "word_idx": 43201, "sentence_idx": 567, "label": "unlabeled"}, {"type": "text", "expr": "2017b", "word_idx": 43209, "sentence_idx": 568, "label": "unlabeled"}, {"type": "text", "expr": "Tensor Domain NNZ Density", "word_idx": 43214, "sentence_idx": 569, "label": "unlabeled"}, {"type": "text", "expr": "Tensor", "word_idx": 43239, "sentence_idx": 570, "label": "unlabeled"}, {"type": "text", "expr": "Tensor", "word_idx": 43245, "sentence_idx": 571, "label": "unlabeled"}, {"type": "text", "expr": "Domain", "word_idx": 43251, "sentence_idx": 572, "label": "unlabeled"}, {"type": "text", "expr": "Domain", "word_idx": 43257, "sentence_idx": 573, "label": "unlabeled"}, {"type": "text", "expr": "Density", "word_idx": 43263, "sentence_idx": 574, "label": "unlabeled"}, {"type": "text", "expr": "Density", "word_idx": 43270, "sentence_idx": 575, "label": "unlabeled"}, {"type": "text", "expr": "bcsstk17 Structural 428,650", "word_idx": 43277, "sentence_idx": 576, "label": "unlabeled"}, {"type": "text", "expr": "bcsstk17", "word_idx": 43304, "sentence_idx": 577, "label": "unlabeled"}, {"type": "text", "expr": "Structural", "word_idx": 43312, "sentence_idx": 578, "label": "unlabeled"}, {"type": "text", "expr": "428,650", "word_idx": 43322, "sentence_idx": 579, "label": "unlabeled"}, {"type": "text", "expr": "$4\\times 10^{-3}$", "word_idx": 43329, "sentence_idx": 580, "label": "unlabeled"}, {"type": "math", "expr": "$$4\\times 10^{-3}$$", "word_idx": 43346, "sentence_idx": 581, "label": "unlabeled"}, {"type": "text", "expr": "pdb1HYS Protein data base 4,344,765", "word_idx": 43361, "sentence_idx": 582, "label": "unlabeled"}, {"type": "text", "expr": "pdb1HYS", "word_idx": 43396, "sentence_idx": 583, "label": "unlabeled"}, {"type": "text", "expr": "Protein data base", "word_idx": 43403, "sentence_idx": 584, "label": "unlabeled"}, {"type": "text", "expr": "4,344,765", "word_idx": 43420, "sentence_idx": 585, "label": "unlabeled"}, {"type": "text", "expr": "$3\\times 10^{-3}$", "word_idx": 43429, "sentence_idx": 586, "label": "unlabeled"}, {"type": "math", "expr": "$$3\\times 10^{-3}$$", "word_idx": 43446, "sentence_idx": 587, "label": "unlabeled"}, {"type": "text", "expr": "rma10 3D CFD 2,329,092", "word_idx": 43461, "sentence_idx": 588, "label": "unlabeled"}, {"type": "text", "expr": "rma10", "word_idx": 43483, "sentence_idx": 589, "label": "unlabeled"}, {"type": "text", "expr": "3D CFD", "word_idx": 43488, "sentence_idx": 590, "label": "unlabeled"}, {"type": "text", "expr": "2,329,092", "word_idx": 43494, "sentence_idx": 591, "label": "unlabeled"}, {"type": "text", "expr": "$1\\times 10^{-3}$", "word_idx": 43503, "sentence_idx": 592, "label": "unlabeled"}, {"type": "math", "expr": "$$1\\times 10^{-3}$$", "word_idx": 43520, "sentence_idx": 593, "label": "unlabeled"}, {"type": "text", "expr": "cant FEM/Cantilever 4,007,383", "word_idx": 43535, "sentence_idx": 594, "label": "unlabeled"}, {"type": "text", "expr": "FEM/Cantilever", "word_idx": 43564, "sentence_idx": 595, "label": "unlabeled"}, {"type": "text", "expr": "4,007,383", "word_idx": 43578, "sentence_idx": 596, "label": "unlabeled"}, {"type": "text", "expr": "$1\\times 10^{-3}$", "word_idx": 43587, "sentence_idx": 597, "label": "unlabeled"}, {"type": "math", "expr": "$$1\\times 10^{-3}$$", "word_idx": 43604, "sentence_idx": 598, "label": "unlabeled"}, {"type": "text", "expr": "consph FEM/Spheres 6,010,480", "word_idx": 43619, "sentence_idx": 599, "label": "unlabeled"}, {"type": "text", "expr": "consph", "word_idx": 43647, "sentence_idx": 600, "label": "unlabeled"}, {"type": "text", "expr": "FEM/Spheres", "word_idx": 43653, "sentence_idx": 601, "label": "unlabeled"}, {"type": "text", "expr": "6,010,480", "word_idx": 43664, "sentence_idx": 602, "label": "unlabeled"}, {"type": "text", "expr": "$9\\times 10^{-4}$", "word_idx": 43673, "sentence_idx": 603, "label": "unlabeled"}, {"type": "math", "expr": "$$9\\times 10^{-4}$$", "word_idx": 43690, "sentence_idx": 604, "label": "unlabeled"}, {"type": "text", "expr": "Facebook Social Media 737,934", "word_idx": 43705, "sentence_idx": 605, "label": "unlabeled"}, {"type": "text", "expr": "Facebook", "word_idx": 43734, "sentence_idx": 606, "label": "unlabeled"}, {"type": "text", "expr": "Social Media", "word_idx": 43742, "sentence_idx": 607, "label": "unlabeled"}, {"type": "text", "expr": "737,934", "word_idx": 43754, "sentence_idx": 608, "label": "unlabeled"}, {"type": "text", "expr": "$1\\times 10^{-7}$", "word_idx": 43761, "sentence_idx": 609, "label": "unlabeled"}, {"type": "math", "expr": "$$1\\times 10^{-7}$$", "word_idx": 43778, "sentence_idx": 610, "label": "unlabeled"}, {"type": "text", "expr": "NELL-2 Machine learning 76,879,419", "word_idx": 43793, "sentence_idx": 611, "label": "unlabeled"}, {"type": "text", "expr": "NELL-2", "word_idx": 43827, "sentence_idx": 612, "label": "unlabeled"}, {"type": "text", "expr": "Machine learning", "word_idx": 43833, "sentence_idx": 613, "label": "unlabeled"}, {"type": "text", "expr": "76,879,419", "word_idx": 43849, "sentence_idx": 614, "label": "unlabeled"}, {"type": "text", "expr": "$2\\times 10^{-5}$", "word_idx": 43859, "sentence_idx": 615, "label": "unlabeled"}, {"type": "math", "expr": "$$2\\times 10^{-5}$$", "word_idx": 43876, "sentence_idx": 616, "label": "unlabeled"}, {"type": "text", "expr": "NELL-1 Machine learning 143,599,552", "word_idx": 43891, "sentence_idx": 617, "label": "unlabeled"}, {"type": "text", "expr": "NELL-1", "word_idx": 43926, "sentence_idx": 618, "label": "unlabeled"}, {"type": "text", "expr": "Machine learning", "word_idx": 43932, "sentence_idx": 619, "label": "unlabeled"}, {"type": "text", "expr": "143,599,552", "word_idx": 43948, "sentence_idx": 620, "label": "unlabeled"}, {"type": "text", "expr": "$9\\times 10^{-13}$", "word_idx": 43959, "sentence_idx": 621, "label": "unlabeled"}, {"type": "math", "expr": "$$9\\times 10^{-13}$$", "word_idx": 43977, "sentence_idx": 622, "label": "unlabeled"}, {"type": "text", "expr": "1  Methodology", "word_idx": 43993, "sentence_idx": 623, "label": "unlabeled"}, {"type": "text", "expr": "All experiments are run on a dual-socket 12-core/24-thread 2", "word_idx": 44007, "sentence_idx": 624, "label": "unlabeled"}, {"type": "text", "expr": "5 GHz Intel Xeon\nE5-2680v3 machine with 30 MB of L3 cache per socket, running Ubuntu 14", "word_idx": 44067, "sentence_idx": 625, "label": "unlabeled"}, {"type": "text", "expr": "5\nLTS", "word_idx": 44154, "sentence_idx": 626, "label": "unlabeled"}, {"type": "text", "expr": " The machine contains 128\u00a0GB of memory and runs Linux kernel version\n3", "word_idx": 44159, "sentence_idx": 627, "label": "unlabeled"}, {"type": "text", "expr": "0 and GCC 5", "word_idx": 44229, "sentence_idx": 628, "label": "unlabeled"}, {"type": "text", "expr": " For all experiments, we ensure the machine is otherwise\nidle and report average cold cache performance, without counting the first run,\nwhich often incurs overheads due to dynamic loading and other first-run\noverheads", "word_idx": 44240, "sentence_idx": 629, "label": "unlabeled"}, {"type": "text", "expr": " Unless otherwise noted, all experiments are single-threaded", "word_idx": 44458, "sentence_idx": 630, "label": "unlabeled"}, {"type": "text", "expr": "All experiments are run on a dual-socket 12-core/24-thread 2", "word_idx": 44518, "sentence_idx": 631, "label": "unlabeled"}, {"type": "text", "expr": "5 GHz Intel Xeon\nE5-2680v3 machine with 30 MB of L3 cache per socket, running Ubuntu 14", "word_idx": 44578, "sentence_idx": 632, "label": "unlabeled"}, {"type": "text", "expr": "5\nLTS", "word_idx": 44665, "sentence_idx": 633, "label": "unlabeled"}, {"type": "text", "expr": " The machine contains 128\u00a0GB of memory and runs Linux kernel version\n3", "word_idx": 44670, "sentence_idx": 634, "label": "unlabeled"}, {"type": "text", "expr": "0 and GCC 5", "word_idx": 44740, "sentence_idx": 635, "label": "unlabeled"}, {"type": "text", "expr": " For all experiments, we ensure the machine is otherwise\nidle and report average cold cache performance, without counting the first run,\nwhich often incurs overheads due to dynamic loading and other first-run\noverheads", "word_idx": 44751, "sentence_idx": 636, "label": "unlabeled"}, {"type": "text", "expr": " Unless otherwise noted, all experiments are single-threaded", "word_idx": 44969, "sentence_idx": 637, "label": "unlabeled"}, {"type": "text", "expr": "We evaluate our approach by comparing performance on linear algebra kernels\nwith Eigen\u00a0  and Intel MKL\u00a0  2018", "word_idx": 45029, "sentence_idx": 638, "label": "unlabeled"}, {"type": "text", "expr": " For tensor algebra,\nwe compare against the Matlab Tensor Toolbox\u00a0  and against\nSPLATT\u00a0 , a high-performance C++ library for sparse tensor\nfactorization", "word_idx": 45138, "sentence_idx": 639, "label": "unlabeled"}, {"type": "text", "expr": " We obtained the real-world inputs for the experiments in\nSections\u00a0 7", "word_idx": 45290, "sentence_idx": 640, "label": "unlabeled"}, {"type": "text", "expr": "2  and\u00a0 7", "word_idx": 45359, "sentence_idx": 641, "label": "unlabeled"}, {"type": "text", "expr": "3  from the SuiteSparse Matrix\nCollection\u00a0  and the FROSTT Tensor Collection\u00a0 \nrespectively", "word_idx": 45368, "sentence_idx": 642, "label": "unlabeled"}, {"type": "text", "expr": " Details of the matrices and tensors used in the experiments are\nshown in Table\u00a0 1 ", "word_idx": 45459, "sentence_idx": 643, "label": "unlabeled"}, {"type": "text", "expr": " We constructed the synthetic sparse inputs\nusing the random matrix generator in  taco \u00a0 , which places\nnonzeros randomly to reach a target sparsity", "word_idx": 45542, "sentence_idx": 644, "label": "unlabeled"}, {"type": "text", "expr": " All sparse matrices are in\ncompressed sparse row (CSR) format", "word_idx": 45690, "sentence_idx": 645, "label": "unlabeled"}, {"type": "text", "expr": "2017b", "word_idx": 45752, "sentence_idx": 646, "label": "unlabeled"}, {"type": "text", "expr": "2017a", "word_idx": 45757, "sentence_idx": 647, "label": "unlabeled"}, {"type": "text", "expr": "2  Sparse Matrix-Matrix Multiplication", "word_idx": 45762, "sentence_idx": 648, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a011:   Sparse matrix multiplication results for\nthe matrices in Table\u00a0 1 ", "word_idx": 45800, "sentence_idx": 649, "label": "unlabeled"}, {"type": "text", "expr": " We show performance for both\nsorted and unsorted column entries; Eigen\u2019s algorithm sorts them while MKL\u2019s\n mkl_sparse_spmm  function leaves them unsorted", "word_idx": 45879, "sentence_idx": 650, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a011:", "word_idx": 46033, "sentence_idx": 651, "label": "unlabeled"}, {"type": "text", "expr": "mkl_sparse_spmm", "word_idx": 46043, "sentence_idx": 652, "label": "unlabeled"}, {"type": "text", "expr": "\\newcolumntype", "word_idx": 46058, "sentence_idx": 653, "label": "unlabeled"}, {"type": "text", "expr": "R[2]\u00bf \\adjustbox angle=#1,lap=0pt-(#2)l\u00a1", "word_idx": 46072, "sentence_idx": 654, "label": "unlabeled"}, {"type": "text", "expr": "\\adjustbox", "word_idx": 46112, "sentence_idx": 655, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:  \nBreakdown of time, in milliseconds (with 3 significant digits), to multiply\nthe test matrices in Table\u00a0 1  with a random operand of\ndensity 0", "word_idx": 46122, "sentence_idx": 656, "label": "unlabeled"}, {"type": "text", "expr": " Running time is given separately for the workspace assemble\nand compute kernels, as well as the variant that assembles and computes in one\nkernel (fused)", "word_idx": 46274, "sentence_idx": 657, "label": "unlabeled"}, {"type": "text", "expr": " Times are compared to the total time spent by Eigen and\nMKL, which do not support separate assembly and compute", "word_idx": 46428, "sentence_idx": 658, "label": "unlabeled"}, {"type": "text", "expr": " For MKL, we use\n mkl_sparse_spmm , which does not sort rows of the output matrix", "word_idx": 46540, "sentence_idx": 659, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:", "word_idx": 46621, "sentence_idx": 660, "label": "unlabeled"}, {"type": "text", "expr": "mkl_sparse_spmm", "word_idx": 46629, "sentence_idx": 661, "label": "unlabeled"}, {"type": "text", "expr": "bcsstk17 bcsstk17 rma10 cant consph", "word_idx": 46644, "sentence_idx": 662, "label": "unlabeled"}, {"type": "text", "expr": "bcsstk17", "word_idx": 46679, "sentence_idx": 663, "label": "unlabeled"}, {"type": "text", "expr": "rma10", "word_idx": 46687, "sentence_idx": 664, "label": "unlabeled"}, {"type": "text", "expr": "consph", "word_idx": 46692, "sentence_idx": 665, "label": "unlabeled"}, {"type": "text", "expr": "Sorted (ms)", "word_idx": 46698, "sentence_idx": 666, "label": "unlabeled"}, {"type": "text", "expr": "Sorted (ms)", "word_idx": 46709, "sentence_idx": 667, "label": "unlabeled"}, {"type": "text", "expr": "assembly 331", "word_idx": 46720, "sentence_idx": 668, "label": "unlabeled"}, {"type": "text", "expr": "8 13204 8758 19979 41284", "word_idx": 46732, "sentence_idx": 669, "label": "unlabeled"}, {"type": "text", "expr": "13204", "word_idx": 46756, "sentence_idx": 670, "label": "unlabeled"}, {"type": "text", "expr": "19979", "word_idx": 46761, "sentence_idx": 671, "label": "unlabeled"}, {"type": "text", "expr": "41284", "word_idx": 46766, "sentence_idx": 672, "label": "unlabeled"}, {"type": "text", "expr": "compute 58", "word_idx": 46771, "sentence_idx": 673, "label": "unlabeled"}, {"type": "text", "expr": "07 2398 1742 4184 8480", "word_idx": 46781, "sentence_idx": 674, "label": "unlabeled"}, {"type": "text", "expr": "assembly+compute 389", "word_idx": 46803, "sentence_idx": 675, "label": "unlabeled"}, {"type": "text", "expr": "9 15602 10500 24163 49764", "word_idx": 46823, "sentence_idx": 676, "label": "unlabeled"}, {"type": "text", "expr": "15602", "word_idx": 46848, "sentence_idx": 677, "label": "unlabeled"}, {"type": "text", "expr": "10500", "word_idx": 46853, "sentence_idx": 678, "label": "unlabeled"}, {"type": "text", "expr": "24163", "word_idx": 46858, "sentence_idx": 679, "label": "unlabeled"}, {"type": "text", "expr": "49764", "word_idx": 46863, "sentence_idx": 680, "label": "unlabeled"}, {"type": "text", "expr": "fused 380", "word_idx": 46868, "sentence_idx": 681, "label": "unlabeled"}, {"type": "text", "expr": "5 15141 10279 23398 48106", "word_idx": 46877, "sentence_idx": 682, "label": "unlabeled"}, {"type": "text", "expr": "15141", "word_idx": 46902, "sentence_idx": 683, "label": "unlabeled"}, {"type": "text", "expr": "10279", "word_idx": 46907, "sentence_idx": 684, "label": "unlabeled"}, {"type": "text", "expr": "23398", "word_idx": 46912, "sentence_idx": 685, "label": "unlabeled"}, {"type": "text", "expr": "48106", "word_idx": 46917, "sentence_idx": 686, "label": "unlabeled"}, {"type": "text", "expr": "Eigen 1015 36555 28585 64706 230695", "word_idx": 46922, "sentence_idx": 687, "label": "unlabeled"}, {"type": "text", "expr": "36555", "word_idx": 46957, "sentence_idx": 688, "label": "unlabeled"}, {"type": "text", "expr": "28585", "word_idx": 46962, "sentence_idx": 689, "label": "unlabeled"}, {"type": "text", "expr": "64706", "word_idx": 46967, "sentence_idx": 690, "label": "unlabeled"}, {"type": "text", "expr": "230695", "word_idx": 46972, "sentence_idx": 691, "label": "unlabeled"}, {"type": "text", "expr": "Unsorted (ms)", "word_idx": 46978, "sentence_idx": 692, "label": "unlabeled"}, {"type": "text", "expr": "Unsorted (ms)", "word_idx": 46991, "sentence_idx": 693, "label": "unlabeled"}, {"type": "text", "expr": "assembly 34", "word_idx": 47004, "sentence_idx": 694, "label": "unlabeled"}, {"type": "text", "expr": "79 1510 949", "word_idx": 47015, "sentence_idx": 695, "label": "unlabeled"}, {"type": "text", "expr": "9 2126 4412", "word_idx": 47026, "sentence_idx": 696, "label": "unlabeled"}, {"type": "text", "expr": "compute 58", "word_idx": 47037, "sentence_idx": 697, "label": "unlabeled"}, {"type": "text", "expr": "02 2328 1849 4459 9624", "word_idx": 47047, "sentence_idx": 698, "label": "unlabeled"}, {"type": "text", "expr": "assembly+compute 92", "word_idx": 47069, "sentence_idx": 699, "label": "unlabeled"}, {"type": "text", "expr": "81 3838 2798", "word_idx": 47088, "sentence_idx": 700, "label": "unlabeled"}, {"type": "text", "expr": "9 6585 14036", "word_idx": 47100, "sentence_idx": 701, "label": "unlabeled"}, {"type": "text", "expr": "14036", "word_idx": 47112, "sentence_idx": 702, "label": "unlabeled"}, {"type": "text", "expr": "fused 76", "word_idx": 47117, "sentence_idx": 703, "label": "unlabeled"}, {"type": "text", "expr": "42 3369 2408 5652 12080", "word_idx": 47125, "sentence_idx": 704, "label": "unlabeled"}, {"type": "text", "expr": "12080", "word_idx": 47148, "sentence_idx": 705, "label": "unlabeled"}, {"type": "text", "expr": "MKL 76", "word_idx": 47153, "sentence_idx": 706, "label": "unlabeled"}, {"type": "text", "expr": "78 3300 2279 5507 13231", "word_idx": 47159, "sentence_idx": 707, "label": "unlabeled"}, {"type": "text", "expr": "13231", "word_idx": 47182, "sentence_idx": 708, "label": "unlabeled"}, {"type": "text", "expr": "Fast sparse matrix multiplication (SpMM) algorithms use workspaces to store\nintermediate values\u00a0 ", "word_idx": 47187, "sentence_idx": 709, "label": "unlabeled"}, {"type": "text", "expr": " We compare our generated workspace\nalgorithm to the SpMM implementations in MKL and Eigen", "word_idx": 47284, "sentence_idx": 710, "label": "unlabeled"}, {"type": "text", "expr": " The approach we\ndescribed in prior work\u00a0  can in theory handle sparse matrix\nmultiplication by inserting into sparse results", "word_idx": 47374, "sentence_idx": 711, "label": "unlabeled"}, {"type": "text", "expr": " The current implementation,\nhowever, does not support this, so we do not compare against its merge-based\napproach", "word_idx": 47499, "sentence_idx": 712, "label": "unlabeled"}, {"type": "text", "expr": " We compute SpMM with two operands: a real-world matrix from\nTable\u00a0 1  and a synthetic matrix generated with a specific\ntarget sparsity, with uniform random placement of nonzeros", "word_idx": 47613, "sentence_idx": 713, "label": "unlabeled"}, {"type": "text", "expr": " Eigen implements a\n sorted  algorithm, which sorts the column entries within each row so\nthey are ordered, while MKL\u2019s  mkl_sparse_spmm  implements an  unsorted  algorithm\u2014the\ncolumn entries may appear in any order", "word_idx": 47791, "sentence_idx": 714, "label": "unlabeled"}, {"type": "text", "expr": " Because these two algorithms have very\ndifferent costs, we implement a workspace variant of each", "word_idx": 48006, "sentence_idx": 715, "label": "unlabeled"}, {"type": "text", "expr": " In addition, we\nimplement two variants of workspace algorithm: one that separates assembly and\ncomputation, and one that fuses the two operations", "word_idx": 48103, "sentence_idx": 716, "label": "unlabeled"}, {"type": "text", "expr": "2017b", "word_idx": 48249, "sentence_idx": 717, "label": "unlabeled"}, {"type": "text", "expr": "sorted", "word_idx": 48254, "sentence_idx": 718, "label": "unlabeled"}, {"type": "text", "expr": "mkl_sparse_spmm", "word_idx": 48260, "sentence_idx": 719, "label": "unlabeled"}, {"type": "text", "expr": "unsorted", "word_idx": 48275, "sentence_idx": 720, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a0 11  shows the running times of matrix multiplication\nfor each matrix in Table\u00a0 1  multiplied by two matrices of\ndifferent densities (1E-4 and 2", "word_idx": 48283, "sentence_idx": 721, "label": "unlabeled"}, {"type": "text", "expr": "5E-3), using our fused workspace implementation", "word_idx": 48434, "sentence_idx": 722, "label": "unlabeled"}, {"type": "text", "expr": "\nOn average, Eigen is slower than our approach, which generates a variant of\nGustavson\u2019s matrix multiplication algorithm, by 2", "word_idx": 48481, "sentence_idx": 723, "label": "unlabeled"}, {"type": "text", "expr": "9 $\\times$  and 3", "word_idx": 48607, "sentence_idx": 724, "label": "unlabeled"}, {"type": "text", "expr": "1 $\\times$ \nrespectively for the two sparsity levels", "word_idx": 48624, "sentence_idx": 725, "label": "unlabeled"}, {"type": "text", "expr": " For the unsorted algorithm, we\ncompare against Intel MKL, and find that our performance is essentially the\nsame on average, with our algorithm being up to 22% faster in some cases", "word_idx": 48676, "sentence_idx": 726, "label": "unlabeled"}, {"type": "text", "expr": "\nOther than one outlier that is 32% slower than MKL, the workspace algorithm is\nnever more than 6% slower than MKL\u2019s hand-optimized SpMM implementation", "word_idx": 48856, "sentence_idx": 727, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 49007, "sentence_idx": 728, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 49013, "sentence_idx": 729, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a0 2  breaks down the running times for the different\ncodes for multiplying with a matrix of density 2", "word_idx": 49019, "sentence_idx": 730, "label": "unlabeled"}, {"type": "text", "expr": " Due to sorting, assembly\ntimes for the sorted algorithm are quite large; however, the compute time is\noccasionally faster than the unsorted compute time, due to improved locality\nwhen accumulating workspace entries into the result matrix", "word_idx": 49125, "sentence_idx": 731, "label": "unlabeled"}, {"type": "text", "expr": " The fused\nalgorithm is also faster when not using sorting, because otherwise the sort (we\nuse the standard C  qsort ) dominates the time", "word_idx": 49363, "sentence_idx": 732, "label": "unlabeled"}, {"type": "text", "expr": "qsort", "word_idx": 49500, "sentence_idx": 733, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a012:  \nMatricized tensor times Khatri-Rao product (MTTKRP) running times,\nnormalized to the workspace algorithm running time", "word_idx": 49505, "sentence_idx": 734, "label": "unlabeled"}, {"type": "text", "expr": " Only compute times are\nshown; assembly times are negligible because the outputs are dense", "word_idx": 49635, "sentence_idx": 735, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a012:", "word_idx": 49725, "sentence_idx": 736, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a013:  \nMTTKRP compute time as we vary the density of the matrix operands, for the\nthree test tensors", "word_idx": 49735, "sentence_idx": 737, "label": "unlabeled"}, {"type": "text", "expr": " We compare MTTKRP computed with a workspace when the\nmatrix operands are passed in as dense matrices against an implementation\nthat takes sparse matrices as inputs and outputs a sparse matrix", "word_idx": 49841, "sentence_idx": 738, "label": "unlabeled"}, {"type": "text", "expr": " In all\ncases, the tensor is passed in using a sparse format", "word_idx": 50033, "sentence_idx": 739, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a013:", "word_idx": 50093, "sentence_idx": 740, "label": "unlabeled"}, {"type": "text", "expr": "3  Matricized Tensor Times Khatri-Rao Product", "word_idx": 50103, "sentence_idx": 741, "label": "unlabeled"}, {"type": "text", "expr": "Matricized tensor times Khatri-Rao product (MTTKRP) is used to compute\ngeneralizations of SVD factorization for tensors in data analytics", "word_idx": 50148, "sentence_idx": 742, "label": "unlabeled"}, {"type": "text", "expr": " It takes\nas input a sparse 3-tensor and two matrices, and outputs a matrix", "word_idx": 50285, "sentence_idx": 743, "label": "unlabeled"}, {"type": "text", "expr": "\nFigure\u00a0 12  shows the results for our workspace algorithm\non three input tensors, compared to  taco  and the hand-coded SPLATT library", "word_idx": 50360, "sentence_idx": 744, "label": "unlabeled"}, {"type": "text", "expr": "\nWe show only compute times, as the assembly times are negligible because the\noutputs are dense", "word_idx": 50495, "sentence_idx": 745, "label": "unlabeled"}, {"type": "text", "expr": "For the NELL-1 and NELL-2 tensors, the workspace algorithm outperforms the\nmerge-based algorithm in  taco  and is within 16% and 12% of the hand-coded\nperformance of SPLATT", "word_idx": 50590, "sentence_idx": 746, "label": "unlabeled"}, {"type": "text", "expr": " On the smaller Facebook dataset, the merge algorithm is\nfaster than both our implementation and SPLATT\u2019s", "word_idx": 50762, "sentence_idx": 747, "label": "unlabeled"}, {"type": "text", "expr": " That is, different inputs\nperform better with different algorithms, which demonstrates the advantage of\nbeing able to generate both versions of the algorithm", "word_idx": 50867, "sentence_idx": 748, "label": "unlabeled"}, {"type": "text", "expr": "4  Matricized Tensor Times Khatri-Rao Product with Sparse Matrices", "word_idx": 51025, "sentence_idx": 749, "label": "unlabeled"}, {"type": "text", "expr": "It is useful to support MTTKRP where both the tensor and matrix operands are\nsparse\u00a0 ", "word_idx": 51091, "sentence_idx": 750, "label": "unlabeled"}, {"type": "text", "expr": " If the result is also sparse, then the MTTKRP can be\nmust faster since it only needs to iterate over nonzeros", "word_idx": 51176, "sentence_idx": 751, "label": "unlabeled"}, {"type": "text", "expr": " The code is tricky\nto write, however, and cannot be generated by the current version of  taco ,\nalthough the prior merge-based theory supports it", "word_idx": 51286, "sentence_idx": 752, "label": "unlabeled"}, {"type": "text", "expr": " In this section, we use a\nworkspace implementation of sparse MTTKRP enabled by the operator split\noptimization", "word_idx": 51432, "sentence_idx": 753, "label": "unlabeled"}, {"type": "text", "expr": " As far as we are aware, ours is the first implementation of an\nMTTKRP algorithm where all operands are sparse and the output is a sparse\nmatrix", "word_idx": 51543, "sentence_idx": 754, "label": "unlabeled"}, {"type": "text", "expr": "2017a", "word_idx": 51687, "sentence_idx": 755, "label": "unlabeled"}, {"type": "text", "expr": "Which version is faster depends on the density of the sparse operands", "word_idx": 51692, "sentence_idx": 756, "label": "unlabeled"}, {"type": "text", "expr": "\nFigure\u00a0 13  shows experiments that compares the compute\ntimes for MTTKRP with sparse matrices against MTTKRP with dense matrices, as we\nvary the density of the randomly generated input matrices", "word_idx": 51761, "sentence_idx": 757, "label": "unlabeled"}, {"type": "text", "expr": " Note that the dense\nmatrix version should have the same performance regardless of sparsity and any\nvariation is likely due to system noise", "word_idx": 51955, "sentence_idx": 758, "label": "unlabeled"}, {"type": "text", "expr": " For each of the tensors, the\ncrossover point is at about 50% nonzero values, showing that such a sparse\nalgorithm can be faster even with only a modest amount of sparsity in the\ninputs", "word_idx": 52094, "sentence_idx": 759, "label": "unlabeled"}, {"type": "text", "expr": " At the extreme, matrix operands that are 99", "word_idx": 52279, "sentence_idx": 760, "label": "unlabeled"}, {"type": "text", "expr": "99% sparse can result in\nspeedups of 4", "word_idx": 52323, "sentence_idx": 761, "label": "unlabeled"}, {"type": "text", "expr": "5\u201311 $\\times$  for our three test tensors", "word_idx": 52361, "sentence_idx": 762, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 52402, "sentence_idx": 763, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a014:  Scaling plot showing the time to assemble and compute  $n$  matrix additions with Eigen,\nMKL, taco binary operations, a single multi-operand taco function, and workspaces", "word_idx": 52408, "sentence_idx": 764, "label": "unlabeled"}, {"type": "text", "expr": "\nThe matrices are described in Table\u00a0 3 ", "word_idx": 52590, "sentence_idx": 765, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a014:", "word_idx": 52630, "sentence_idx": 766, "label": "unlabeled"}, {"type": "text", "expr": "5  Sparse Matrix Addition", "word_idx": 52640, "sentence_idx": 767, "label": "unlabeled"}, {"type": "text", "expr": "To demonstrate the utility of workspaces for sparse matrix addition (SpAdd), we\nshow that the algorithm scales as we increase the number of operands", "word_idx": 52665, "sentence_idx": 768, "label": "unlabeled"}, {"type": "text", "expr": " In\nFigure\u00a0 14 , we compare the workspace algorithm to  taco \nusing binary operations (as a library would be implemented),  taco  generating\na single function for the additions, Intel MKL (using its inspector-executor\nSpAdd implementation), and Eigen", "word_idx": 52813, "sentence_idx": 769, "label": "unlabeled"}, {"type": "text", "expr": " We pre-generate  $k$  matrices with the target\nsparsities chosen uniformly randomly from the range  $1E^{-4}$ \u20130", "word_idx": 53063, "sentence_idx": 770, "label": "unlabeled"}, {"type": "text", "expr": "04 and always add in\nthe same order and with the same matrices for each library", "word_idx": 53176, "sentence_idx": 771, "label": "unlabeled"}, {"type": "math", "expr": "$$1E^{-4}$$", "word_idx": 53255, "sentence_idx": 772, "label": "unlabeled"}, {"type": "text", "expr": "The results of this experiment show two things", "word_idx": 53262, "sentence_idx": 773, "label": "unlabeled"}, {"type": "text", "expr": " First, that the libraries are\nhampered by the restriction that they perform addition two operands at a time,\nhaving to construct and compute multiple temporaries, resulting in less\nperformance than is possible using code generation", "word_idx": 53308, "sentence_idx": 774, "label": "unlabeled"}, {"type": "text", "expr": " Even given this approach,\n taco  is faster than Intel MKL by 2", "word_idx": 53540, "sentence_idx": 775, "label": "unlabeled"}, {"type": "text", "expr": "8 $\\times$  on average, while Eigen and\n taco  show competitive performance", "word_idx": 53603, "sentence_idx": 776, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 53678, "sentence_idx": 777, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a03:   Breakdown of sparse matrix addition time\nin ms for adding 7 matrices, for all codes\nThe operands are randomly-generated sparse matrices of density\n2", "word_idx": 53684, "sentence_idx": 778, "label": "unlabeled"}, {"type": "text", "expr": "56E-02,\n1", "word_idx": 53843, "sentence_idx": 779, "label": "unlabeled"}, {"type": "text", "expr": "68E-03,\n2", "word_idx": 53852, "sentence_idx": 780, "label": "unlabeled"}, {"type": "text", "expr": "89E-04,\n2", "word_idx": 53861, "sentence_idx": 781, "label": "unlabeled"}, {"type": "text", "expr": "50E-03,\n2", "word_idx": 53870, "sentence_idx": 782, "label": "unlabeled"}, {"type": "text", "expr": "92E-03,\n2", "word_idx": 53879, "sentence_idx": 783, "label": "unlabeled"}, {"type": "text", "expr": "96E-02,\n1", "word_idx": 53888, "sentence_idx": 784, "label": "unlabeled"}, {"type": "text", "expr": "06E-02,\nrespectively", "word_idx": 53897, "sentence_idx": 785, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a03:", "word_idx": 53917, "sentence_idx": 786, "label": "unlabeled"}, {"type": "text", "expr": "Code Assembly Compute", "word_idx": 53925, "sentence_idx": 787, "label": "unlabeled"}, {"type": "text", "expr": "Assembly", "word_idx": 53946, "sentence_idx": 788, "label": "unlabeled"}, {"type": "text", "expr": "Compute", "word_idx": 53954, "sentence_idx": 789, "label": "unlabeled"}, {"type": "text", "expr": "taco binop 247 211", "word_idx": 53961, "sentence_idx": 790, "label": "unlabeled"}, {"type": "text", "expr": "taco 190 182", "word_idx": 53979, "sentence_idx": 791, "label": "unlabeled"}, {"type": "text", "expr": "workspace 190 93", "word_idx": 53991, "sentence_idx": 792, "label": "unlabeled"}, {"type": "text", "expr": "Eigen 436", "word_idx": 54007, "sentence_idx": 793, "label": "unlabeled"}, {"type": "text", "expr": "MKL 1141", "word_idx": 54016, "sentence_idx": 794, "label": "unlabeled"}, {"type": "text", "expr": "Secondly, the experiment shows the value of being able to produce both\nmerge-based and workspace-based implementations of SpAdd", "word_idx": 54024, "sentence_idx": 795, "label": "unlabeled"}, {"type": "text", "expr": " At up to four\nadditions, the two versions are competitive, with the merge-based code being\nslightly faster", "word_idx": 54151, "sentence_idx": 796, "label": "unlabeled"}, {"type": "text", "expr": " However, with increasing numbers of additions, the workspace\ncode begins to outperform the  taco  implementation, showing an increasing gap\nas more operands are added", "word_idx": 54258, "sentence_idx": 797, "label": "unlabeled"}, {"type": "text", "expr": " Table\u00a0 3  breaks down the\nperformance of adding 7 operands, separating out assembly time for the\n taco -based and workspace implementations", "word_idx": 54425, "sentence_idx": 798, "label": "unlabeled"}, {"type": "text", "expr": " For this experiment, we reuse the\nmatrix assembly code produced by taco to assemble the output, but compute\nusing a workspace", "word_idx": 54565, "sentence_idx": 799, "label": "unlabeled"}, {"type": "text", "expr": " Most of the time is spent in assembly, which is unsurprising,\ngiven that assembly requires memory allocations, while the computation performs only\npoint-wise work without the kinds of reductions found in MTTKRP and SpMM", "word_idx": 54691, "sentence_idx": 800, "label": "unlabeled"}, {"type": "text", "expr": "8  Related Work", "word_idx": 54911, "sentence_idx": 801, "label": "unlabeled"}, {"type": "text", "expr": "Related work is divided into work on tensor algebra compilation, work on manual\nworkspace optimizations of matrix and tensor kernels, and work on general loop\noptimization", "word_idx": 54926, "sentence_idx": 802, "label": "unlabeled"}, {"type": "text", "expr": "Related work is divided into work on tensor algebra compilation, work on manual\nworkspace optimizations of matrix and tensor kernels, and work on general loop\noptimization", "word_idx": 55097, "sentence_idx": 803, "label": "unlabeled"}, {"type": "text", "expr": "There have been much work on optimizing dense matrix and tensor\ncomputations\u00a0 ", "word_idx": 55268, "sentence_idx": 804, "label": "unlabeled"}, {"type": "text", "expr": " Researchers have also\nworked on compilation and code generation of sparse matrix computations,\nstarting with the work of  Bik and Wijshoff \u00a0  and the Bernoulli\nsystem\u00a0 ", "word_idx": 55346, "sentence_idx": 805, "label": "unlabeled"}, {"type": "text", "expr": " Recently, we proposed a tensor algebra compilation\ntheory built on an intermediate representation called iteration\ngraphs\u00a0  that are constructed from tensor index notation", "word_idx": 55515, "sentence_idx": 806, "label": "unlabeled"}, {"type": "text", "expr": " We\ngave an algorithm to generate code, but did not introduce any optimization\npasses", "word_idx": 55687, "sentence_idx": 807, "label": "unlabeled"}, {"type": "text", "expr": " The operator split optimization presented in this paper is, to the\nbest of our knowledge, the first optimization on iteration graphs", "word_idx": 55772, "sentence_idx": 808, "label": "unlabeled"}, {"type": "text", "expr": " As we have\nshown, it improves the performance of the kernels generated from many iteration\ngraphs by removing merges, hoisting loop invariant code, and handling\nscattering to sparse result tensors", "word_idx": 55905, "sentence_idx": 809, "label": "unlabeled"}, {"type": "text", "expr": "Bik and Wijshoff", "word_idx": 56102, "sentence_idx": 810, "label": "unlabeled"}, {"type": "text", "expr": "2017b", "word_idx": 56118, "sentence_idx": 811, "label": "unlabeled"}, {"type": "text", "expr": "The first use of dense workspaces for sparse matrix computations is\n Gustavson \u2019s sparse matrix multiplication implementation, that\nwe recreate with an operator split in\u00a0Figure\u00a0 7  to produce the code in\nand\u00a0Figure\u00a0 1 \u00a0 ", "word_idx": 56123, "sentence_idx": 812, "label": "unlabeled"}, {"type": "text", "expr": " A workspace used for\naccumulating temporary values is referred to as an expanded real accumulator\nin\u00a0  and as an abstract sparse accumulator data structure\nin\u00a0 ", "word_idx": 56343, "sentence_idx": 813, "label": "unlabeled"}, {"type": "text", "expr": " Dense workspaces and blocking are used to produce fast\nparallel code by\u00a0 Patwary et\u00a0al", "word_idx": 56504, "sentence_idx": 814, "label": "unlabeled"}, {"type": "text", "expr": " They also tried\na hash map workspace, but report that it did not have good performance for\ntheir use", "word_idx": 56591, "sentence_idx": 815, "label": "unlabeled"}, {"type": "text", "expr": " Furthermore,  Bulu\u00e7 et\u00a0al", "word_idx": 56692, "sentence_idx": 816, "label": "unlabeled"}, {"type": "text", "expr": "  use blocking and workspaces to\ndevelop sparse matrix-vector multiplication algorithms for the CSB data\nstructure that are equally fast for  $Ax$  and  $A^{T}x$ \u00a0 ", "word_idx": 56718, "sentence_idx": 817, "label": "unlabeled"}, {"type": "text", "expr": "\nFinally,  Smith et\u00a0al", "word_idx": 56882, "sentence_idx": 818, "label": "unlabeled"}, {"type": "text", "expr": "  uses a workspace to hoist loop-invariant code\nin their implementation of MTTKRP in the SPLATT library\u00a0 ", "word_idx": 56904, "sentence_idx": 819, "label": "unlabeled"}, {"type": "text", "expr": " We\nre-create this optimization with an operator split\nin\u00a0Figure\u00a0 9  and show the resulting source code\nin\u00a0Figure\u00a0 9 ", "word_idx": 57009, "sentence_idx": 820, "label": "unlabeled"}, {"type": "text", "expr": "Gustavson", "word_idx": 57126, "sentence_idx": 821, "label": "unlabeled"}, {"type": "text", "expr": "Patwary et\u00a0al", "word_idx": 57135, "sentence_idx": 822, "label": "unlabeled"}, {"type": "text", "expr": "Bulu\u00e7 et\u00a0al", "word_idx": 57148, "sentence_idx": 823, "label": "unlabeled"}, {"type": "math", "expr": "$$A^{T}x$$", "word_idx": 57159, "sentence_idx": 824, "label": "unlabeled"}, {"type": "text", "expr": "Smith et\u00a0al", "word_idx": 57165, "sentence_idx": 825, "label": "unlabeled"}, {"type": "text", "expr": "One use of operator splitting in loop nests, in addition to removing multi-way\nmerge code and scatters into sparse results, is to split apart computation that\nmay take place at different loop levels", "word_idx": 57176, "sentence_idx": 826, "label": "unlabeled"}, {"type": "text", "expr": " This results in operations being\nhoisted to a higher loop nest", "word_idx": 57374, "sentence_idx": 827, "label": "unlabeled"}, {"type": "text", "expr": " Loop invariant code motion has a long history\nin compilers, going back to the first FORTRAN compiler in\n1957\u00a0 ", "word_idx": 57437, "sentence_idx": 828, "label": "unlabeled"}, {"type": "text", "expr": " Recently, researchers have found new opportunities for\nremoving redundancy in loops by taking advantage of high-level algebraic\nknowledge\u00a0 ", "word_idx": 57548, "sentence_idx": 829, "label": "unlabeled"}, {"type": "text", "expr": " The operator split optimization is done on the\nhigh-level iteration graph intermediate representation that expose sparse\ndependencies", "word_idx": 57688, "sentence_idx": 830, "label": "unlabeled"}, {"type": "text", "expr": " Since it is applied prior to code generation, it avoids the need\nto analyze low-level code", "word_idx": 57822, "sentence_idx": 831, "label": "unlabeled"}, {"type": "text", "expr": " It can therefore apply loop invariant code motion\nto loops that implement sparse computations with many branches", "word_idx": 57913, "sentence_idx": 832, "label": "unlabeled"}, {"type": "text", "expr": "9  Conclusion", "word_idx": 58026, "sentence_idx": 833, "label": "unlabeled"}, {"type": "text", "expr": "This paper presented the first compiler optimization on the iteration graph\nintermediate representation for tensor computations", "word_idx": 58039, "sentence_idx": 834, "label": "unlabeled"}, {"type": "text", "expr": " A single\noperation\u2014operator split\u2014generalizes several manual optimizations described\nin the literature on sparse matrix and sparse tensor codes", "word_idx": 58166, "sentence_idx": 835, "label": "unlabeled"}, {"type": "text", "expr": " These\noptimizations remove expensive merge code, avoid the need to scatter into\nsparse results, and hoist partial sparse tensor computations out of inner\nloops", "word_idx": 58310, "sentence_idx": 836, "label": "unlabeled"}, {"type": "text", "expr": " Future work includes exploring trade-offs for different types of\nworkspaces and automating the decision of when to split", "word_idx": 58470, "sentence_idx": 837, "label": "unlabeled"}, {"type": "text", "expr": "This paper presented the first compiler optimization on the iteration graph\nintermediate representation for tensor computations", "word_idx": 58591, "sentence_idx": 838, "label": "unlabeled"}, {"type": "text", "expr": " A single\noperation\u2014operator split\u2014generalizes several manual optimizations described\nin the literature on sparse matrix and sparse tensor codes", "word_idx": 58718, "sentence_idx": 839, "label": "unlabeled"}, {"type": "text", "expr": " These\noptimizations remove expensive merge code, avoid the need to scatter into\nsparse results, and hoist partial sparse tensor computations out of inner\nloops", "word_idx": 58862, "sentence_idx": 840, "label": "unlabeled"}, {"type": "text", "expr": " Future work includes exploring trade-offs for different types of\nworkspaces and automating the decision of when to split", "word_idx": 59022, "sentence_idx": 841, "label": "unlabeled"}, {"type": "text", "expr": "References", "word_idx": 59143, "sentence_idx": 842, "label": "unlabeled"}, {"type": "text", "expr": "Abadi et\u00a0al", "word_idx": 59153, "sentence_idx": 843, "label": "unlabeled"}, {"type": "text", "expr": " (2016) \nMart\u00edn Abadi, Paul\nBarham, Jianmin Chen, Zhifeng Chen,\nAndy Davis, Jeffrey Dean,\nMatthieu Devin, Sanjay Ghemawat,\nGeoffrey Irving, Michael Isard,\nManjunath Kudlur, Josh Levenberg,\nRajat Monga, Sherry Moore,\nDerek\u00a0G", "word_idx": 59164, "sentence_idx": 844, "label": "unlabeled"}, {"type": "text", "expr": " Murray, Benoit Steiner,\nPaul Tucker, Vijay Vasudevan,\nPete Warden, Martin Wicke,\nYuan Yu, and Xiaoqiang Zheng", "word_idx": 59387, "sentence_idx": 845, "label": "unlabeled"}, {"type": "text", "expr": "\n2016", "word_idx": 59497, "sentence_idx": 846, "label": "unlabeled"}, {"type": "text", "expr": "Abadi et\u00a0al", "word_idx": 59502, "sentence_idx": 847, "label": "unlabeled"}, {"type": "text", "expr": " (2016)", "word_idx": 59513, "sentence_idx": 848, "label": "unlabeled"}, {"type": "text", "expr": "Mart\u00edn Abadi, Paul\nBarham, Jianmin Chen, Zhifeng Chen,\nAndy Davis, Jeffrey Dean,\nMatthieu Devin, Sanjay Ghemawat,\nGeoffrey Irving, Michael Isard,\nManjunath Kudlur, Josh Levenberg,\nRajat Monga, Sherry Moore,\nDerek\u00a0G", "word_idx": 59520, "sentence_idx": 849, "label": "unlabeled"}, {"type": "text", "expr": " Murray, Benoit Steiner,\nPaul Tucker, Vijay Vasudevan,\nPete Warden, Martin Wicke,\nYuan Yu, and Xiaoqiang Zheng", "word_idx": 59734, "sentence_idx": 850, "label": "unlabeled"}, {"type": "text", "expr": "\n2016", "word_idx": 59844, "sentence_idx": 851, "label": "unlabeled"}, {"type": "text", "expr": "TensorFlow: A System for Large-scale Machine\nLearning", "word_idx": 59849, "sentence_idx": 852, "label": "unlabeled"}, {"type": "text", "expr": " In  Proceedings of the 12th USENIX\nConference on Operating Systems Design and Implementation (OSDI\u201916) ", "word_idx": 59902, "sentence_idx": 853, "label": "unlabeled"}, {"type": "text", "expr": " USENIX Association,\nBerkeley, CA, USA, 265\u2013283", "word_idx": 60006, "sentence_idx": 854, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the 12th USENIX\nConference on Operating Systems Design and Implementation", "word_idx": 60053, "sentence_idx": 855, "label": "unlabeled"}, {"type": "text", "expr": "(OSDI\u201916)", "word_idx": 60141, "sentence_idx": 856, "label": "unlabeled"}, {"type": "text", "expr": "http://dl", "word_idx": 60150, "sentence_idx": 857, "label": "unlabeled"}, {"type": "text", "expr": "org/citation", "word_idx": 60159, "sentence_idx": 858, "label": "unlabeled"}, {"type": "text", "expr": "cfm?id=3026877", "word_idx": 60171, "sentence_idx": 859, "label": "unlabeled"}, {"type": "text", "expr": "3026899", "word_idx": 60185, "sentence_idx": 860, "label": "unlabeled"}, {"type": "text", "expr": "http://dl", "word_idx": 60192, "sentence_idx": 861, "label": "unlabeled"}, {"type": "text", "expr": "org/citation", "word_idx": 60201, "sentence_idx": 862, "label": "unlabeled"}, {"type": "text", "expr": "cfm?id=3026877", "word_idx": 60213, "sentence_idx": 863, "label": "unlabeled"}, {"type": "text", "expr": "3026899", "word_idx": 60227, "sentence_idx": 864, "label": "unlabeled"}, {"type": "text", "expr": "Anandkumar et\u00a0al", "word_idx": 60234, "sentence_idx": 865, "label": "unlabeled"}, {"type": "text", "expr": " (2014) \nAnimashree Anandkumar,\nRong Ge, Daniel Hsu,\nSham\u00a0M", "word_idx": 60250, "sentence_idx": 866, "label": "unlabeled"}, {"type": "text", "expr": " Kakade, and Matus Telgarsky", "word_idx": 60309, "sentence_idx": 867, "label": "unlabeled"}, {"type": "text", "expr": "\n2014", "word_idx": 60337, "sentence_idx": 868, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Tensor Decompositions for Learning Latent Variable\nModels", "word_idx": 60342, "sentence_idx": 869, "label": "unlabeled"}, {"type": "text", "expr": "Anandkumar et\u00a0al", "word_idx": 60402, "sentence_idx": 870, "label": "unlabeled"}, {"type": "text", "expr": " (2014)", "word_idx": 60418, "sentence_idx": 871, "label": "unlabeled"}, {"type": "text", "expr": "Animashree Anandkumar,\nRong Ge, Daniel Hsu,\nSham\u00a0M", "word_idx": 60425, "sentence_idx": 872, "label": "unlabeled"}, {"type": "text", "expr": " Kakade, and Matus Telgarsky", "word_idx": 60475, "sentence_idx": 873, "label": "unlabeled"}, {"type": "text", "expr": "\n2014", "word_idx": 60503, "sentence_idx": 874, "label": "unlabeled"}, {"type": "text", "expr": "Tensor Decompositions for Learning Latent Variable\nModels", "word_idx": 60508, "sentence_idx": 875, "label": "unlabeled"}, {"type": "text", "expr": " Mach", "word_idx": 60565, "sentence_idx": 876, "label": "unlabeled"}, {"type": "text", "expr": " Learn", "word_idx": 60570, "sentence_idx": 877, "label": "unlabeled"}, {"type": "text", "expr": "  15,\nArticle 1 (Jan", "word_idx": 60576, "sentence_idx": 878, "label": "unlabeled"}, {"type": "text", "expr": " 2014),\n60\u00a0pages", "word_idx": 60596, "sentence_idx": 879, "label": "unlabeled"}, {"type": "text", "expr": " Mach", "word_idx": 60612, "sentence_idx": 880, "label": "unlabeled"}, {"type": "text", "expr": " Learn", "word_idx": 60617, "sentence_idx": 881, "label": "unlabeled"}, {"type": "text", "expr": "Auer\net\u00a0al", "word_idx": 60623, "sentence_idx": 882, "label": "unlabeled"}, {"type": "text", "expr": " (2006) \nAlexander\u00a0A", "word_idx": 60633, "sentence_idx": 883, "label": "unlabeled"}, {"type": "text", "expr": " Auer, Gerald\nBaumgartner, David\u00a0E", "word_idx": 60653, "sentence_idx": 884, "label": "unlabeled"}, {"type": "text", "expr": " Bernholdt, Alina\nBibireata, Venkatesh Choppella, Daniel\nCociorva, Xiaoyang Gao, Robert Harrison,\nSriram Krishnamoorthy, Sandhya Krishnan,\nChi-Chung Lam, Qingda Lu,\nMarcel Nooijen, Russell Pitzer,\nJ", "word_idx": 60687, "sentence_idx": 885, "label": "unlabeled"}, {"type": "text", "expr": " Ramanujam, P", "word_idx": 60885, "sentence_idx": 886, "label": "unlabeled"}, {"type": "text", "expr": " Sadayappan, and\nAlexander Sibiryakov", "word_idx": 60898, "sentence_idx": 887, "label": "unlabeled"}, {"type": "text", "expr": " 2006", "word_idx": 60935, "sentence_idx": 888, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Automatic code generation for many-body electronic\nstructure methods: the tensor contraction engine", "word_idx": 60940, "sentence_idx": 889, "label": "unlabeled"}, {"type": "text", "expr": "Auer\net\u00a0al", "word_idx": 61042, "sentence_idx": 890, "label": "unlabeled"}, {"type": "text", "expr": " (2006)", "word_idx": 61052, "sentence_idx": 891, "label": "unlabeled"}, {"type": "text", "expr": "Alexander\u00a0A", "word_idx": 61059, "sentence_idx": 892, "label": "unlabeled"}, {"type": "text", "expr": " Auer, Gerald\nBaumgartner, David\u00a0E", "word_idx": 61070, "sentence_idx": 893, "label": "unlabeled"}, {"type": "text", "expr": " Bernholdt, Alina\nBibireata, Venkatesh Choppella, Daniel\nCociorva, Xiaoyang Gao, Robert Harrison,\nSriram Krishnamoorthy, Sandhya Krishnan,\nChi-Chung Lam, Qingda Lu,\nMarcel Nooijen, Russell Pitzer,\nJ", "word_idx": 61104, "sentence_idx": 894, "label": "unlabeled"}, {"type": "text", "expr": " Ramanujam, P", "word_idx": 61302, "sentence_idx": 895, "label": "unlabeled"}, {"type": "text", "expr": " Sadayappan, and\nAlexander Sibiryakov", "word_idx": 61315, "sentence_idx": 896, "label": "unlabeled"}, {"type": "text", "expr": " 2006", "word_idx": 61352, "sentence_idx": 897, "label": "unlabeled"}, {"type": "text", "expr": "Automatic code generation for many-body electronic\nstructure methods: the tensor contraction engine", "word_idx": 61357, "sentence_idx": 898, "label": "unlabeled"}, {"type": "text", "expr": "Molecular Physics  104,\n2 (2006), 211\u2013228", "word_idx": 61456, "sentence_idx": 899, "label": "unlabeled"}, {"type": "text", "expr": "Molecular Physics", "word_idx": 61497, "sentence_idx": 900, "label": "unlabeled"}, {"type": "text", "expr": "Backus (1978) \nJohn Backus", "word_idx": 61514, "sentence_idx": 901, "label": "unlabeled"}, {"type": "text", "expr": "\n1978", "word_idx": 61540, "sentence_idx": 902, "label": "unlabeled"}, {"type": "text", "expr": "Backus (1978)", "word_idx": 61545, "sentence_idx": 903, "label": "unlabeled"}, {"type": "text", "expr": "John Backus", "word_idx": 61558, "sentence_idx": 904, "label": "unlabeled"}, {"type": "text", "expr": "\n1978", "word_idx": 61569, "sentence_idx": 905, "label": "unlabeled"}, {"type": "text", "expr": "The history of FORTRAN I, II, and III", "word_idx": 61574, "sentence_idx": 906, "label": "unlabeled"}, {"type": "text", "expr": " In\n History of programming languages I ", "word_idx": 61611, "sentence_idx": 907, "label": "unlabeled"}, {"type": "text", "expr": " ACM,\n25\u201374", "word_idx": 61651, "sentence_idx": 908, "label": "unlabeled"}, {"type": "text", "expr": "History of programming languages I", "word_idx": 61662, "sentence_idx": 909, "label": "unlabeled"}, {"type": "text", "expr": "Bader\net\u00a0al", "word_idx": 61696, "sentence_idx": 910, "label": "unlabeled"}, {"type": "text", "expr": " (2008) \nBrett\u00a0W", "word_idx": 61707, "sentence_idx": 911, "label": "unlabeled"}, {"type": "text", "expr": " Bader, Michael\u00a0W", "word_idx": 61723, "sentence_idx": 912, "label": "unlabeled"}, {"type": "text", "expr": "\nBerry, and Murray Browne", "word_idx": 61740, "sentence_idx": 913, "label": "unlabeled"}, {"type": "text", "expr": "\n2008", "word_idx": 61765, "sentence_idx": 914, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Springer London, 147\u2013163", "word_idx": 61770, "sentence_idx": 915, "label": "unlabeled"}, {"type": "text", "expr": "Bader\net\u00a0al", "word_idx": 61797, "sentence_idx": 916, "label": "unlabeled"}, {"type": "text", "expr": " (2008)", "word_idx": 61808, "sentence_idx": 917, "label": "unlabeled"}, {"type": "text", "expr": "Brett\u00a0W", "word_idx": 61815, "sentence_idx": 918, "label": "unlabeled"}, {"type": "text", "expr": " Bader, Michael\u00a0W", "word_idx": 61822, "sentence_idx": 919, "label": "unlabeled"}, {"type": "text", "expr": "\nBerry, and Murray Browne", "word_idx": 61839, "sentence_idx": 920, "label": "unlabeled"}, {"type": "text", "expr": "\n2008", "word_idx": 61864, "sentence_idx": 921, "label": "unlabeled"}, {"type": "text", "expr": "Discussion Tracking in Enron Email Using\nPARAFAC ", "word_idx": 61869, "sentence_idx": 922, "label": "unlabeled"}, {"type": "text", "expr": "Discussion Tracking in Enron Email Using\nPARAFAC", "word_idx": 61918, "sentence_idx": 923, "label": "unlabeled"}, {"type": "text", "expr": "Springer London, 147\u2013163", "word_idx": 61966, "sentence_idx": 924, "label": "unlabeled"}, {"type": "text", "expr": "Bader and Kolda (2007) \nBrett\u00a0W Bader and\nTamara\u00a0G Kolda", "word_idx": 61990, "sentence_idx": 925, "label": "unlabeled"}, {"type": "text", "expr": " 2007", "word_idx": 62046, "sentence_idx": 926, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Efficient MATLAB computations with sparse and\nfactored tensors", "word_idx": 62051, "sentence_idx": 927, "label": "unlabeled"}, {"type": "text", "expr": "Bader and Kolda (2007)", "word_idx": 62116, "sentence_idx": 928, "label": "unlabeled"}, {"type": "text", "expr": "Brett\u00a0W Bader and\nTamara\u00a0G Kolda", "word_idx": 62138, "sentence_idx": 929, "label": "unlabeled"}, {"type": "text", "expr": " 2007", "word_idx": 62170, "sentence_idx": 930, "label": "unlabeled"}, {"type": "text", "expr": "Efficient MATLAB computations with sparse and\nfactored tensors", "word_idx": 62175, "sentence_idx": 931, "label": "unlabeled"}, {"type": "text", "expr": "SIAM Journal on Scientific Computing \n30, 1 (2007),\n205\u2013231", "word_idx": 62237, "sentence_idx": 932, "label": "unlabeled"}, {"type": "text", "expr": "SIAM Journal on Scientific Computing", "word_idx": 62296, "sentence_idx": 933, "label": "unlabeled"}, {"type": "text", "expr": "Bezanson et\u00a0al", "word_idx": 62332, "sentence_idx": 934, "label": "unlabeled"}, {"type": "text", "expr": " (2012) \nJeff Bezanson, Stefan\nKarpinski, Viral\u00a0B", "word_idx": 62346, "sentence_idx": 935, "label": "unlabeled"}, {"type": "text", "expr": " Shah, and Alan\nEdelman", "word_idx": 62395, "sentence_idx": 936, "label": "unlabeled"}, {"type": "text", "expr": " 2012", "word_idx": 62418, "sentence_idx": 937, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Julia: A Fast Dynamic Language for Technical\nComputing", "word_idx": 62423, "sentence_idx": 938, "label": "unlabeled"}, {"type": "text", "expr": "\n\n (2012)", "word_idx": 62480, "sentence_idx": 939, "label": "unlabeled"}, {"type": "text", "expr": "Bezanson et\u00a0al", "word_idx": 62489, "sentence_idx": 940, "label": "unlabeled"}, {"type": "text", "expr": " (2012)", "word_idx": 62503, "sentence_idx": 941, "label": "unlabeled"}, {"type": "text", "expr": "Jeff Bezanson, Stefan\nKarpinski, Viral\u00a0B", "word_idx": 62510, "sentence_idx": 942, "label": "unlabeled"}, {"type": "text", "expr": " Shah, and Alan\nEdelman", "word_idx": 62550, "sentence_idx": 943, "label": "unlabeled"}, {"type": "text", "expr": " 2012", "word_idx": 62573, "sentence_idx": 944, "label": "unlabeled"}, {"type": "text", "expr": "Julia: A Fast Dynamic Language for Technical\nComputing", "word_idx": 62578, "sentence_idx": 945, "label": "unlabeled"}, {"type": "text", "expr": "(2012)", "word_idx": 62632, "sentence_idx": 946, "label": "unlabeled"}, {"type": "text", "expr": "Bik and Wijshoff (1993) \nAart\u00a0JC Bik and Harry\u00a0AG\nWijshoff", "word_idx": 62638, "sentence_idx": 947, "label": "unlabeled"}, {"type": "text", "expr": " 1993", "word_idx": 62696, "sentence_idx": 948, "label": "unlabeled"}, {"type": "text", "expr": "Bik and Wijshoff (1993)", "word_idx": 62701, "sentence_idx": 949, "label": "unlabeled"}, {"type": "text", "expr": "Aart\u00a0JC Bik and Harry\u00a0AG\nWijshoff", "word_idx": 62724, "sentence_idx": 950, "label": "unlabeled"}, {"type": "text", "expr": " 1993", "word_idx": 62757, "sentence_idx": 951, "label": "unlabeled"}, {"type": "text", "expr": "Compilation techniques for sparse matrix\ncomputations", "word_idx": 62762, "sentence_idx": 952, "label": "unlabeled"}, {"type": "text", "expr": " In  Proceedings of the 7th\ninternational conference on Supercomputing ", "word_idx": 62815, "sentence_idx": 953, "label": "unlabeled"}, {"type": "text", "expr": " ACM, 416\u2013424", "word_idx": 62886, "sentence_idx": 954, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the 7th\ninternational conference on Supercomputing", "word_idx": 62899, "sentence_idx": 955, "label": "unlabeled"}, {"type": "text", "expr": "Bulu\u00e7 et\u00a0al", "word_idx": 62964, "sentence_idx": 956, "label": "unlabeled"}, {"type": "text", "expr": " (2009) \nAydin Bulu\u00e7,\nJeremy\u00a0T Fineman, Matteo Frigo,\nJohn\u00a0R Gilbert, and Charles\u00a0E\nLeiserson", "word_idx": 62975, "sentence_idx": 957, "label": "unlabeled"}, {"type": "text", "expr": " 2009", "word_idx": 63068, "sentence_idx": 958, "label": "unlabeled"}, {"type": "text", "expr": "Bulu\u00e7 et\u00a0al", "word_idx": 63073, "sentence_idx": 959, "label": "unlabeled"}, {"type": "text", "expr": " (2009)", "word_idx": 63084, "sentence_idx": 960, "label": "unlabeled"}, {"type": "text", "expr": "Aydin Bulu\u00e7,\nJeremy\u00a0T Fineman, Matteo Frigo,\nJohn\u00a0R Gilbert, and Charles\u00a0E\nLeiserson", "word_idx": 63091, "sentence_idx": 961, "label": "unlabeled"}, {"type": "text", "expr": " 2009", "word_idx": 63175, "sentence_idx": 962, "label": "unlabeled"}, {"type": "text", "expr": "Parallel sparse matrix-vector and\nmatrix-transpose-vector multiplication using compressed sparse blocks", "word_idx": 63180, "sentence_idx": 963, "label": "unlabeled"}, {"type": "text", "expr": " In\n Proceedings of the twenty-first annual symposium on\nParallelism in algorithms and architectures ", "word_idx": 63283, "sentence_idx": 964, "label": "unlabeled"}, {"type": "text", "expr": " ACM,\n233\u2013244", "word_idx": 63384, "sentence_idx": 965, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the twenty-first annual symposium on\nParallelism in algorithms and architectures", "word_idx": 63397, "sentence_idx": 966, "label": "unlabeled"}, {"type": "text", "expr": "Cichocki (2014) \nAndrzej Cichocki", "word_idx": 63492, "sentence_idx": 967, "label": "unlabeled"}, {"type": "text", "expr": "\n2014", "word_idx": 63525, "sentence_idx": 968, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Era of big data processing: A new approach via\ntensor networks and tensor decompositions", "word_idx": 63530, "sentence_idx": 969, "label": "unlabeled"}, {"type": "text", "expr": "Cichocki (2014)", "word_idx": 63621, "sentence_idx": 970, "label": "unlabeled"}, {"type": "text", "expr": "Andrzej Cichocki", "word_idx": 63636, "sentence_idx": 971, "label": "unlabeled"}, {"type": "text", "expr": "\n2014", "word_idx": 63652, "sentence_idx": 972, "label": "unlabeled"}, {"type": "text", "expr": "Era of big data processing: A new approach via\ntensor networks and tensor decompositions", "word_idx": 63657, "sentence_idx": 973, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1403", "word_idx": 63745, "sentence_idx": 974, "label": "unlabeled"}, {"type": "text", "expr": "2048 \n(2014)", "word_idx": 63770, "sentence_idx": 975, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1403", "word_idx": 63782, "sentence_idx": 976, "label": "unlabeled"}, {"type": "text", "expr": "Davis (2006) \nTimothy\u00a0A Davis", "word_idx": 63807, "sentence_idx": 977, "label": "unlabeled"}, {"type": "text", "expr": "\n2006", "word_idx": 63836, "sentence_idx": 978, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Siam", "word_idx": 63841, "sentence_idx": 979, "label": "unlabeled"}, {"type": "text", "expr": "Davis (2006)", "word_idx": 63848, "sentence_idx": 980, "label": "unlabeled"}, {"type": "text", "expr": "Timothy\u00a0A Davis", "word_idx": 63860, "sentence_idx": 981, "label": "unlabeled"}, {"type": "text", "expr": "\n2006", "word_idx": 63875, "sentence_idx": 982, "label": "unlabeled"}, {"type": "text", "expr": "Direct methods for sparse linear systems ", "word_idx": 63880, "sentence_idx": 983, "label": "unlabeled"}, {"type": "text", "expr": "Direct methods for sparse linear systems", "word_idx": 63921, "sentence_idx": 984, "label": "unlabeled"}, {"type": "text", "expr": "Davis and Hu (2011) \nTimothy\u00a0A", "word_idx": 63961, "sentence_idx": 985, "label": "unlabeled"}, {"type": "text", "expr": " Davis and\nYifan Hu", "word_idx": 63991, "sentence_idx": 986, "label": "unlabeled"}, {"type": "text", "expr": " 2011", "word_idx": 64010, "sentence_idx": 987, "label": "unlabeled"}, {"type": "text", "expr": "\n\n The University of Florida Sparse Matrix\nCollection", "word_idx": 64015, "sentence_idx": 988, "label": "unlabeled"}, {"type": "text", "expr": "Davis and Hu (2011)", "word_idx": 64068, "sentence_idx": 989, "label": "unlabeled"}, {"type": "text", "expr": "Timothy\u00a0A", "word_idx": 64087, "sentence_idx": 990, "label": "unlabeled"}, {"type": "text", "expr": " Davis and\nYifan Hu", "word_idx": 64096, "sentence_idx": 991, "label": "unlabeled"}, {"type": "text", "expr": " 2011", "word_idx": 64115, "sentence_idx": 992, "label": "unlabeled"}, {"type": "text", "expr": "The University of Florida Sparse Matrix\nCollection", "word_idx": 64120, "sentence_idx": 993, "label": "unlabeled"}, {"type": "text", "expr": "ACM Trans", "word_idx": 64170, "sentence_idx": 994, "label": "unlabeled"}, {"type": "text", "expr": " Math", "word_idx": 64179, "sentence_idx": 995, "label": "unlabeled"}, {"type": "text", "expr": " Softw", "word_idx": 64184, "sentence_idx": 996, "label": "unlabeled"}, {"type": "text", "expr": " \n38, 1, Article 1\n(Dec", "word_idx": 64190, "sentence_idx": 997, "label": "unlabeled"}, {"type": "text", "expr": " 2011)", "word_idx": 64213, "sentence_idx": 998, "label": "unlabeled"}, {"type": "text", "expr": "ACM Trans", "word_idx": 64219, "sentence_idx": 999, "label": "unlabeled"}, {"type": "text", "expr": " Math", "word_idx": 64228, "sentence_idx": 1000, "label": "unlabeled"}, {"type": "text", "expr": " Softw", "word_idx": 64233, "sentence_idx": 1001, "label": "unlabeled"}, {"type": "text", "expr": "Ding and Shen (2017) \nYufei Ding and Xipeng\nShen", "word_idx": 64239, "sentence_idx": 1002, "label": "unlabeled"}, {"type": "text", "expr": " 2017", "word_idx": 64287, "sentence_idx": 1003, "label": "unlabeled"}, {"type": "text", "expr": "\n\n GLORE: Generalized Loop Redundancy Elimination upon\nLER-Notation", "word_idx": 64292, "sentence_idx": 1004, "label": "unlabeled"}, {"type": "text", "expr": "\n\n (2017)", "word_idx": 64359, "sentence_idx": 1005, "label": "unlabeled"}, {"type": "text", "expr": "Ding and Shen (2017)", "word_idx": 64368, "sentence_idx": 1006, "label": "unlabeled"}, {"type": "text", "expr": "Yufei Ding and Xipeng\nShen", "word_idx": 64388, "sentence_idx": 1007, "label": "unlabeled"}, {"type": "text", "expr": " 2017", "word_idx": 64414, "sentence_idx": 1008, "label": "unlabeled"}, {"type": "text", "expr": "GLORE: Generalized Loop Redundancy Elimination upon\nLER-Notation", "word_idx": 64419, "sentence_idx": 1009, "label": "unlabeled"}, {"type": "text", "expr": "(2017)", "word_idx": 64483, "sentence_idx": 1010, "label": "unlabeled"}, {"type": "text", "expr": "Einstein (1916) \nAlbert", "word_idx": 64489, "sentence_idx": 1011, "label": "unlabeled"}, {"type": "text", "expr": " Einstein", "word_idx": 64512, "sentence_idx": 1012, "label": "unlabeled"}, {"type": "text", "expr": "\n1916", "word_idx": 64521, "sentence_idx": 1013, "label": "unlabeled"}, {"type": "text", "expr": "\n\n The Foundation of the General Theory of\nRelativity", "word_idx": 64526, "sentence_idx": 1014, "label": "unlabeled"}, {"type": "text", "expr": "Einstein (1916)", "word_idx": 64579, "sentence_idx": 1015, "label": "unlabeled"}, {"type": "text", "expr": "Albert", "word_idx": 64594, "sentence_idx": 1016, "label": "unlabeled"}, {"type": "text", "expr": " Einstein", "word_idx": 64600, "sentence_idx": 1017, "label": "unlabeled"}, {"type": "text", "expr": "\n1916", "word_idx": 64609, "sentence_idx": 1018, "label": "unlabeled"}, {"type": "text", "expr": "The Foundation of the General Theory of\nRelativity", "word_idx": 64614, "sentence_idx": 1019, "label": "unlabeled"}, {"type": "text", "expr": "Annalen der Physik  354\n(1916), 769\u2013822", "word_idx": 64664, "sentence_idx": 1020, "label": "unlabeled"}, {"type": "text", "expr": "Annalen der Physik", "word_idx": 64703, "sentence_idx": 1021, "label": "unlabeled"}, {"type": "text", "expr": "Feynman\net\u00a0al", "word_idx": 64721, "sentence_idx": 1022, "label": "unlabeled"}, {"type": "text", "expr": " (1963) \nRichard Feynman, Robert\u00a0B", "word_idx": 64734, "sentence_idx": 1023, "label": "unlabeled"}, {"type": "text", "expr": "\nLeighton, and Matthew\u00a0L", "word_idx": 64768, "sentence_idx": 1024, "label": "unlabeled"}, {"type": "text", "expr": " Sands", "word_idx": 64792, "sentence_idx": 1025, "label": "unlabeled"}, {"type": "text", "expr": "\n1963", "word_idx": 64798, "sentence_idx": 1026, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Addison-Wesley", "word_idx": 64803, "sentence_idx": 1027, "label": "unlabeled"}, {"type": "text", "expr": "Feynman\net\u00a0al", "word_idx": 64820, "sentence_idx": 1028, "label": "unlabeled"}, {"type": "text", "expr": " (1963)", "word_idx": 64833, "sentence_idx": 1029, "label": "unlabeled"}, {"type": "text", "expr": "Richard Feynman, Robert\u00a0B", "word_idx": 64840, "sentence_idx": 1030, "label": "unlabeled"}, {"type": "text", "expr": "\nLeighton, and Matthew\u00a0L", "word_idx": 64865, "sentence_idx": 1031, "label": "unlabeled"}, {"type": "text", "expr": " Sands", "word_idx": 64889, "sentence_idx": 1032, "label": "unlabeled"}, {"type": "text", "expr": "\n1963", "word_idx": 64895, "sentence_idx": 1033, "label": "unlabeled"}, {"type": "text", "expr": "The Feynman Lectures on Physics ", "word_idx": 64900, "sentence_idx": 1034, "label": "unlabeled"}, {"type": "text", "expr": "The Feynman Lectures on Physics", "word_idx": 64932, "sentence_idx": 1035, "label": "unlabeled"}, {"type": "text", "expr": "Addison-Wesley", "word_idx": 64963, "sentence_idx": 1036, "label": "unlabeled"}, {"type": "text", "expr": "Gilbert\net\u00a0al", "word_idx": 64977, "sentence_idx": 1037, "label": "unlabeled"}, {"type": "text", "expr": " (1992) \nJohn\u00a0R Gilbert, Cleve\nMoler, and Robert Schreiber", "word_idx": 64990, "sentence_idx": 1038, "label": "unlabeled"}, {"type": "text", "expr": "\n1992", "word_idx": 65048, "sentence_idx": 1039, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Sparse matrices in MATLAB: Design and\nimplementation", "word_idx": 65053, "sentence_idx": 1040, "label": "unlabeled"}, {"type": "text", "expr": "Gilbert\net\u00a0al", "word_idx": 65108, "sentence_idx": 1041, "label": "unlabeled"}, {"type": "text", "expr": " (1992)", "word_idx": 65121, "sentence_idx": 1042, "label": "unlabeled"}, {"type": "text", "expr": "John\u00a0R Gilbert, Cleve\nMoler, and Robert Schreiber", "word_idx": 65128, "sentence_idx": 1043, "label": "unlabeled"}, {"type": "text", "expr": "\n1992", "word_idx": 65177, "sentence_idx": 1044, "label": "unlabeled"}, {"type": "text", "expr": "Sparse matrices in MATLAB: Design and\nimplementation", "word_idx": 65182, "sentence_idx": 1045, "label": "unlabeled"}, {"type": "text", "expr": "SIAM J", "word_idx": 65234, "sentence_idx": 1046, "label": "unlabeled"}, {"type": "text", "expr": " Matrix Anal", "word_idx": 65240, "sentence_idx": 1047, "label": "unlabeled"}, {"type": "text", "expr": " Appl", "word_idx": 65252, "sentence_idx": 1048, "label": "unlabeled"}, {"type": "text", "expr": " \n13, 1 (1992),\n333\u2013356", "word_idx": 65257, "sentence_idx": 1049, "label": "unlabeled"}, {"type": "text", "expr": "SIAM J", "word_idx": 65280, "sentence_idx": 1050, "label": "unlabeled"}, {"type": "text", "expr": " Matrix Anal", "word_idx": 65286, "sentence_idx": 1051, "label": "unlabeled"}, {"type": "text", "expr": " Appl", "word_idx": 65298, "sentence_idx": 1052, "label": "unlabeled"}, {"type": "text", "expr": "Guennebaud\net\u00a0al", "word_idx": 65303, "sentence_idx": 1053, "label": "unlabeled"}, {"type": "text", "expr": " (2010) \nGa\u00ebl Guennebaud,\nBeno\u00eet Jacob, et\u00a0al", "word_idx": 65319, "sentence_idx": 1054, "label": "unlabeled"}, {"type": "text", "expr": "\n2010", "word_idx": 65364, "sentence_idx": 1055, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Eigen v3", "word_idx": 65369, "sentence_idx": 1056, "label": "unlabeled"}, {"type": "text", "expr": "\n\n http://eigen", "word_idx": 65380, "sentence_idx": 1057, "label": "unlabeled"}, {"type": "text", "expr": "tuxfamily", "word_idx": 65395, "sentence_idx": 1058, "label": "unlabeled"}, {"type": "text", "expr": "\n(2010)", "word_idx": 65404, "sentence_idx": 1059, "label": "unlabeled"}, {"type": "text", "expr": "Guennebaud\net\u00a0al", "word_idx": 65411, "sentence_idx": 1060, "label": "unlabeled"}, {"type": "text", "expr": " (2010)", "word_idx": 65427, "sentence_idx": 1061, "label": "unlabeled"}, {"type": "text", "expr": "Ga\u00ebl Guennebaud,\nBeno\u00eet Jacob, et\u00a0al", "word_idx": 65434, "sentence_idx": 1062, "label": "unlabeled"}, {"type": "text", "expr": "\n2010", "word_idx": 65470, "sentence_idx": 1063, "label": "unlabeled"}, {"type": "text", "expr": "Eigen v3", "word_idx": 65475, "sentence_idx": 1064, "label": "unlabeled"}, {"type": "text", "expr": "http://eigen", "word_idx": 65483, "sentence_idx": 1065, "label": "unlabeled"}, {"type": "text", "expr": "tuxfamily", "word_idx": 65495, "sentence_idx": 1066, "label": "unlabeled"}, {"type": "text", "expr": "\n(2010)", "word_idx": 65504, "sentence_idx": 1067, "label": "unlabeled"}, {"type": "text", "expr": "Gustavson (1978) \nFred\u00a0G", "word_idx": 65511, "sentence_idx": 1068, "label": "unlabeled"}, {"type": "text", "expr": " Gustavson", "word_idx": 65535, "sentence_idx": 1069, "label": "unlabeled"}, {"type": "text", "expr": "\n1978", "word_idx": 65545, "sentence_idx": 1070, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Two Fast Algorithms for Sparse Matrices:\nMultiplication and Permuted Transposition", "word_idx": 65550, "sentence_idx": 1071, "label": "unlabeled"}, {"type": "text", "expr": "Gustavson (1978)", "word_idx": 65635, "sentence_idx": 1072, "label": "unlabeled"}, {"type": "text", "expr": "Fred\u00a0G", "word_idx": 65651, "sentence_idx": 1073, "label": "unlabeled"}, {"type": "text", "expr": " Gustavson", "word_idx": 65657, "sentence_idx": 1074, "label": "unlabeled"}, {"type": "text", "expr": "\n1978", "word_idx": 65667, "sentence_idx": 1075, "label": "unlabeled"}, {"type": "text", "expr": "Two Fast Algorithms for Sparse Matrices:\nMultiplication and Permuted Transposition", "word_idx": 65672, "sentence_idx": 1076, "label": "unlabeled"}, {"type": "text", "expr": "ACM Trans", "word_idx": 65754, "sentence_idx": 1077, "label": "unlabeled"}, {"type": "text", "expr": " Math", "word_idx": 65763, "sentence_idx": 1078, "label": "unlabeled"}, {"type": "text", "expr": " Softw", "word_idx": 65768, "sentence_idx": 1079, "label": "unlabeled"}, {"type": "text", "expr": " \n4, 3 (1978)", "word_idx": 65774, "sentence_idx": 1080, "label": "unlabeled"}, {"type": "text", "expr": "ACM Trans", "word_idx": 65787, "sentence_idx": 1081, "label": "unlabeled"}, {"type": "text", "expr": " Math", "word_idx": 65796, "sentence_idx": 1082, "label": "unlabeled"}, {"type": "text", "expr": " Softw", "word_idx": 65801, "sentence_idx": 1083, "label": "unlabeled"}, {"type": "text", "expr": "Hansen (1970) \nPer\u00a0Brinch Hansen", "word_idx": 65807, "sentence_idx": 1084, "label": "unlabeled"}, {"type": "text", "expr": "\n1970", "word_idx": 65839, "sentence_idx": 1085, "label": "unlabeled"}, {"type": "text", "expr": "\n\n The Nucleus of a Multiprogramming System", "word_idx": 65844, "sentence_idx": 1086, "label": "unlabeled"}, {"type": "text", "expr": "Hansen (1970)", "word_idx": 65887, "sentence_idx": 1087, "label": "unlabeled"}, {"type": "text", "expr": "Per\u00a0Brinch Hansen", "word_idx": 65900, "sentence_idx": 1088, "label": "unlabeled"}, {"type": "text", "expr": "\n1970", "word_idx": 65917, "sentence_idx": 1089, "label": "unlabeled"}, {"type": "text", "expr": "The Nucleus of a Multiprogramming System", "word_idx": 65922, "sentence_idx": 1090, "label": "unlabeled"}, {"type": "text", "expr": "Commun", "word_idx": 65962, "sentence_idx": 1091, "label": "unlabeled"}, {"type": "text", "expr": " ACM  13,\n4 (April 1970),\n238\u2013241", "word_idx": 65968, "sentence_idx": 1092, "label": "unlabeled"}, {"type": "text", "expr": "Commun", "word_idx": 66001, "sentence_idx": 1093, "label": "unlabeled"}, {"type": "text", "expr": "https://doi", "word_idx": 66007, "sentence_idx": 1094, "label": "unlabeled"}, {"type": "text", "expr": "org/10", "word_idx": 66018, "sentence_idx": 1095, "label": "unlabeled"}, {"type": "text", "expr": "1145/362258", "word_idx": 66024, "sentence_idx": 1096, "label": "unlabeled"}, {"type": "text", "expr": "362278", "word_idx": 66035, "sentence_idx": 1097, "label": "unlabeled"}, {"type": "text", "expr": "https://doi", "word_idx": 66041, "sentence_idx": 1098, "label": "unlabeled"}, {"type": "text", "expr": "org/10", "word_idx": 66052, "sentence_idx": 1099, "label": "unlabeled"}, {"type": "text", "expr": "1145/362258", "word_idx": 66058, "sentence_idx": 1100, "label": "unlabeled"}, {"type": "text", "expr": "362278", "word_idx": 66069, "sentence_idx": 1101, "label": "unlabeled"}, {"type": "text", "expr": "Heath\net\u00a0al", "word_idx": 66075, "sentence_idx": 1102, "label": "unlabeled"}, {"type": "text", "expr": " (1991) \nMichael\u00a0T Heath, Esmond\nNg, and Barry\u00a0W Peyton", "word_idx": 66086, "sentence_idx": 1103, "label": "unlabeled"}, {"type": "text", "expr": "\n1991", "word_idx": 66141, "sentence_idx": 1104, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Parallel algorithms for sparse linear systems", "word_idx": 66146, "sentence_idx": 1105, "label": "unlabeled"}, {"type": "text", "expr": "Heath\net\u00a0al", "word_idx": 66194, "sentence_idx": 1106, "label": "unlabeled"}, {"type": "text", "expr": " (1991)", "word_idx": 66205, "sentence_idx": 1107, "label": "unlabeled"}, {"type": "text", "expr": "Michael\u00a0T Heath, Esmond\nNg, and Barry\u00a0W Peyton", "word_idx": 66212, "sentence_idx": 1108, "label": "unlabeled"}, {"type": "text", "expr": "\n1991", "word_idx": 66258, "sentence_idx": 1109, "label": "unlabeled"}, {"type": "text", "expr": "Parallel algorithms for sparse linear systems", "word_idx": 66263, "sentence_idx": 1110, "label": "unlabeled"}, {"type": "text", "expr": "SIAM review  33,\n3 (1991), 420\u2013460", "word_idx": 66308, "sentence_idx": 1111, "label": "unlabeled"}, {"type": "text", "expr": "SIAM review", "word_idx": 66342, "sentence_idx": 1112, "label": "unlabeled"}, {"type": "text", "expr": "Hitchcock (1927) \nFrank\u00a0L Hitchcock", "word_idx": 66353, "sentence_idx": 1113, "label": "unlabeled"}, {"type": "text", "expr": "\n1927", "word_idx": 66388, "sentence_idx": 1114, "label": "unlabeled"}, {"type": "text", "expr": "\n\n The expression of a tensor or a polyadic as a sum\nof products", "word_idx": 66393, "sentence_idx": 1115, "label": "unlabeled"}, {"type": "text", "expr": "Hitchcock (1927)", "word_idx": 66457, "sentence_idx": 1116, "label": "unlabeled"}, {"type": "text", "expr": "Frank\u00a0L Hitchcock", "word_idx": 66473, "sentence_idx": 1117, "label": "unlabeled"}, {"type": "text", "expr": "\n1927", "word_idx": 66490, "sentence_idx": 1118, "label": "unlabeled"}, {"type": "text", "expr": "The expression of a tensor or a polyadic as a sum\nof products", "word_idx": 66495, "sentence_idx": 1119, "label": "unlabeled"}, {"type": "text", "expr": "Studies in Applied Mathematics \n6, 1-4 (1927),\n164\u2013189", "word_idx": 66556, "sentence_idx": 1120, "label": "unlabeled"}, {"type": "text", "expr": "Studies in Applied Mathematics", "word_idx": 66610, "sentence_idx": 1121, "label": "unlabeled"}, {"type": "text", "expr": "Intel (2012) \nIntel", "word_idx": 66640, "sentence_idx": 1122, "label": "unlabeled"}, {"type": "text", "expr": " 2012", "word_idx": 66659, "sentence_idx": 1123, "label": "unlabeled"}, {"type": "text", "expr": "Intel (2012)", "word_idx": 66664, "sentence_idx": 1124, "label": "unlabeled"}, {"type": "text", "expr": "Intel", "word_idx": 66676, "sentence_idx": 1125, "label": "unlabeled"}, {"type": "text", "expr": " 2012", "word_idx": 66681, "sentence_idx": 1126, "label": "unlabeled"}, {"type": "text", "expr": "Intel math kernel library reference\nmanual ", "word_idx": 66686, "sentence_idx": 1127, "label": "unlabeled"}, {"type": "text", "expr": "Intel math kernel library reference\nmanual", "word_idx": 66729, "sentence_idx": 1128, "label": "unlabeled"}, {"type": "text", "expr": "Technical Report", "word_idx": 66771, "sentence_idx": 1129, "label": "unlabeled"}, {"type": "text", "expr": "\n630813-051US, 2012", "word_idx": 66787, "sentence_idx": 1130, "label": "unlabeled"}, {"type": "text", "expr": "\n http://software", "word_idx": 66806, "sentence_idx": 1131, "label": "unlabeled"}, {"type": "text", "expr": "intel", "word_idx": 66823, "sentence_idx": 1132, "label": "unlabeled"}, {"type": "text", "expr": "com/sites/products/documentation/hpc/mkl/mklman/mklman", "word_idx": 66828, "sentence_idx": 1133, "label": "unlabeled"}, {"type": "text", "expr": "http://software", "word_idx": 66882, "sentence_idx": 1134, "label": "unlabeled"}, {"type": "text", "expr": "intel", "word_idx": 66897, "sentence_idx": 1135, "label": "unlabeled"}, {"type": "text", "expr": "com/sites/products/documentation/hpc/mkl/mklman/mklman", "word_idx": 66902, "sentence_idx": 1136, "label": "unlabeled"}, {"type": "text", "expr": "Iverson (1962) \nKenneth\u00a0E", "word_idx": 66956, "sentence_idx": 1137, "label": "unlabeled"}, {"type": "text", "expr": " Iverson", "word_idx": 66981, "sentence_idx": 1138, "label": "unlabeled"}, {"type": "text", "expr": "\n1962", "word_idx": 66989, "sentence_idx": 1139, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Wiley", "word_idx": 66994, "sentence_idx": 1140, "label": "unlabeled"}, {"type": "text", "expr": "Iverson (1962)", "word_idx": 67002, "sentence_idx": 1141, "label": "unlabeled"}, {"type": "text", "expr": "Kenneth\u00a0E", "word_idx": 67016, "sentence_idx": 1142, "label": "unlabeled"}, {"type": "text", "expr": " Iverson", "word_idx": 67025, "sentence_idx": 1143, "label": "unlabeled"}, {"type": "text", "expr": "\n1962", "word_idx": 67033, "sentence_idx": 1144, "label": "unlabeled"}, {"type": "text", "expr": "A Programming Language ", "word_idx": 67038, "sentence_idx": 1145, "label": "unlabeled"}, {"type": "text", "expr": "A Programming Language", "word_idx": 67061, "sentence_idx": 1146, "label": "unlabeled"}, {"type": "text", "expr": "Wiley", "word_idx": 67083, "sentence_idx": 1147, "label": "unlabeled"}, {"type": "text", "expr": "Kjolstad et\u00a0al", "word_idx": 67088, "sentence_idx": 1148, "label": "unlabeled"}, {"type": "text", "expr": " (2017a) \nFredrik Kjolstad, Stephen\nChou, David Lugato, Shoaib Kamil, and\nSaman Amarasinghe", "word_idx": 67102, "sentence_idx": 1149, "label": "unlabeled"}, {"type": "text", "expr": " 2017a", "word_idx": 67193, "sentence_idx": 1150, "label": "unlabeled"}, {"type": "text", "expr": "Kjolstad et\u00a0al", "word_idx": 67199, "sentence_idx": 1151, "label": "unlabeled"}, {"type": "text", "expr": " (2017a)", "word_idx": 67213, "sentence_idx": 1152, "label": "unlabeled"}, {"type": "text", "expr": "Fredrik Kjolstad, Stephen\nChou, David Lugato, Shoaib Kamil, and\nSaman Amarasinghe", "word_idx": 67221, "sentence_idx": 1153, "label": "unlabeled"}, {"type": "text", "expr": " 2017a", "word_idx": 67302, "sentence_idx": 1154, "label": "unlabeled"}, {"type": "text", "expr": "taco: a tool to generate tensor algebra kernels", "word_idx": 67308, "sentence_idx": 1155, "label": "unlabeled"}, {"type": "text", "expr": "\nIn  Proceedings of the 32nd IEEE/ACM International\nConference on Automated Software Engineering ", "word_idx": 67355, "sentence_idx": 1156, "label": "unlabeled"}, {"type": "text", "expr": " IEEE Press,\n943\u2013948", "word_idx": 67452, "sentence_idx": 1157, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the 32nd IEEE/ACM International\nConference on Automated Software Engineering", "word_idx": 67472, "sentence_idx": 1158, "label": "unlabeled"}, {"type": "text", "expr": "Kjolstad et\u00a0al", "word_idx": 67563, "sentence_idx": 1159, "label": "unlabeled"}, {"type": "text", "expr": " (2017b) \nFredrik Kjolstad, Shoaib\nKamil, Stephen Chou, David Lugato, and\nSaman Amarasinghe", "word_idx": 67577, "sentence_idx": 1160, "label": "unlabeled"}, {"type": "text", "expr": " 2017b", "word_idx": 67668, "sentence_idx": 1161, "label": "unlabeled"}, {"type": "text", "expr": "\n\n The Tensor Algebra Compiler", "word_idx": 67674, "sentence_idx": 1162, "label": "unlabeled"}, {"type": "text", "expr": "Kjolstad et\u00a0al", "word_idx": 67704, "sentence_idx": 1163, "label": "unlabeled"}, {"type": "text", "expr": " (2017b)", "word_idx": 67718, "sentence_idx": 1164, "label": "unlabeled"}, {"type": "text", "expr": "Fredrik Kjolstad, Shoaib\nKamil, Stephen Chou, David Lugato, and\nSaman Amarasinghe", "word_idx": 67726, "sentence_idx": 1165, "label": "unlabeled"}, {"type": "text", "expr": " 2017b", "word_idx": 67807, "sentence_idx": 1166, "label": "unlabeled"}, {"type": "text", "expr": "The Tensor Algebra Compiler", "word_idx": 67813, "sentence_idx": 1167, "label": "unlabeled"}, {"type": "text", "expr": " ACM Program", "word_idx": 67840, "sentence_idx": 1168, "label": "unlabeled"}, {"type": "text", "expr": " Lang", "word_idx": 67852, "sentence_idx": 1169, "label": "unlabeled"}, {"type": "text", "expr": " \n1, OOPSLA, Article\n77 (Oct", "word_idx": 67857, "sentence_idx": 1170, "label": "unlabeled"}, {"type": "text", "expr": " 2017),\n29\u00a0pages", "word_idx": 67885, "sentence_idx": 1171, "label": "unlabeled"}, {"type": "text", "expr": " ACM Program", "word_idx": 67901, "sentence_idx": 1172, "label": "unlabeled"}, {"type": "text", "expr": " Lang", "word_idx": 67913, "sentence_idx": 1173, "label": "unlabeled"}, {"type": "text", "expr": "https://doi", "word_idx": 67918, "sentence_idx": 1174, "label": "unlabeled"}, {"type": "text", "expr": "org/10", "word_idx": 67929, "sentence_idx": 1175, "label": "unlabeled"}, {"type": "text", "expr": "1145/3133901", "word_idx": 67935, "sentence_idx": 1176, "label": "unlabeled"}, {"type": "text", "expr": "https://doi", "word_idx": 67947, "sentence_idx": 1177, "label": "unlabeled"}, {"type": "text", "expr": "org/10", "word_idx": 67958, "sentence_idx": 1178, "label": "unlabeled"}, {"type": "text", "expr": "1145/3133901", "word_idx": 67964, "sentence_idx": 1179, "label": "unlabeled"}, {"type": "text", "expr": "Kjolstad\net\u00a0al", "word_idx": 67976, "sentence_idx": 1180, "label": "unlabeled"}, {"type": "text", "expr": " (2016) \nFredrik Kjolstad, Shoaib\nKamil, Jonathan Ragan-Kelley, David\u00a0IW\nLevin, Shinjiro Sueda, Desai Chen,\nEtienne Vouga, Danny\u00a0M Kaufman,\nGurtej Kanwar, Wojciech Matusik,\net\u00a0al", "word_idx": 67990, "sentence_idx": 1181, "label": "unlabeled"}, {"type": "text", "expr": " 2016", "word_idx": 68168, "sentence_idx": 1182, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Simit: A language for physical simulation", "word_idx": 68173, "sentence_idx": 1183, "label": "unlabeled"}, {"type": "text", "expr": "Kjolstad\net\u00a0al", "word_idx": 68217, "sentence_idx": 1184, "label": "unlabeled"}, {"type": "text", "expr": " (2016)", "word_idx": 68231, "sentence_idx": 1185, "label": "unlabeled"}, {"type": "text", "expr": "Fredrik Kjolstad, Shoaib\nKamil, Jonathan Ragan-Kelley, David\u00a0IW\nLevin, Shinjiro Sueda, Desai Chen,\nEtienne Vouga, Danny\u00a0M Kaufman,\nGurtej Kanwar, Wojciech Matusik,\net\u00a0al", "word_idx": 68238, "sentence_idx": 1186, "label": "unlabeled"}, {"type": "text", "expr": " 2016", "word_idx": 68407, "sentence_idx": 1187, "label": "unlabeled"}, {"type": "text", "expr": "Simit: A language for physical simulation", "word_idx": 68412, "sentence_idx": 1188, "label": "unlabeled"}, {"type": "text", "expr": "ACM Transactions on Graphics (TOG) \n35, 2 (2016),\n20", "word_idx": 68453, "sentence_idx": 1189, "label": "unlabeled"}, {"type": "text", "expr": "ACM Transactions on Graphics (TOG)", "word_idx": 68505, "sentence_idx": 1190, "label": "unlabeled"}, {"type": "text", "expr": "Knuth (1973) \nDonald\u00a0Ervin Knuth", "word_idx": 68539, "sentence_idx": 1191, "label": "unlabeled"}, {"type": "text", "expr": "\n1973", "word_idx": 68571, "sentence_idx": 1192, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Pearson Education", "word_idx": 68576, "sentence_idx": 1193, "label": "unlabeled"}, {"type": "text", "expr": "Knuth (1973)", "word_idx": 68596, "sentence_idx": 1194, "label": "unlabeled"}, {"type": "text", "expr": "Donald\u00a0Ervin Knuth", "word_idx": 68608, "sentence_idx": 1195, "label": "unlabeled"}, {"type": "text", "expr": "\n1973", "word_idx": 68626, "sentence_idx": 1196, "label": "unlabeled"}, {"type": "text", "expr": "The art of computer programming: sorting\nand searching ", "word_idx": 68631, "sentence_idx": 1197, "label": "unlabeled"}, {"type": "text", "expr": "The art of computer programming: sorting\nand searching", "word_idx": 68686, "sentence_idx": 1198, "label": "unlabeled"}, {"type": "text", "expr": "Pearson Education", "word_idx": 68740, "sentence_idx": 1199, "label": "unlabeled"}, {"type": "text", "expr": "Kolda and Bader (2009) \nTamara\u00a0G Kolda and\nBrett\u00a0W Bader", "word_idx": 68757, "sentence_idx": 1200, "label": "unlabeled"}, {"type": "text", "expr": " 2009", "word_idx": 68813, "sentence_idx": 1201, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Tensor decompositions and applications", "word_idx": 68818, "sentence_idx": 1202, "label": "unlabeled"}, {"type": "text", "expr": "Kolda and Bader (2009)", "word_idx": 68859, "sentence_idx": 1203, "label": "unlabeled"}, {"type": "text", "expr": "Tamara\u00a0G Kolda and\nBrett\u00a0W Bader", "word_idx": 68881, "sentence_idx": 1204, "label": "unlabeled"}, {"type": "text", "expr": " 2009", "word_idx": 68913, "sentence_idx": 1205, "label": "unlabeled"}, {"type": "text", "expr": "Tensor decompositions and applications", "word_idx": 68918, "sentence_idx": 1206, "label": "unlabeled"}, {"type": "text", "expr": "SIAM review  51,\n3 (2009), 455\u2013500", "word_idx": 68956, "sentence_idx": 1207, "label": "unlabeled"}, {"type": "text", "expr": "SIAM review", "word_idx": 68990, "sentence_idx": 1208, "label": "unlabeled"}, {"type": "text", "expr": "Kolecki (2002) \nJoseph\u00a0C Kolecki", "word_idx": 69001, "sentence_idx": 1209, "label": "unlabeled"}, {"type": "text", "expr": "\n2002", "word_idx": 69033, "sentence_idx": 1210, "label": "unlabeled"}, {"type": "text", "expr": "\n\n An Introduction to Tensors for Students of Physics\nand Engineering", "word_idx": 69038, "sentence_idx": 1211, "label": "unlabeled"}, {"type": "text", "expr": "Kolecki (2002)", "word_idx": 69107, "sentence_idx": 1212, "label": "unlabeled"}, {"type": "text", "expr": "Joseph\u00a0C Kolecki", "word_idx": 69121, "sentence_idx": 1213, "label": "unlabeled"}, {"type": "text", "expr": "\n2002", "word_idx": 69137, "sentence_idx": 1214, "label": "unlabeled"}, {"type": "text", "expr": "An Introduction to Tensors for Students of Physics\nand Engineering", "word_idx": 69142, "sentence_idx": 1215, "label": "unlabeled"}, {"type": "text", "expr": "Unixenguaedu  7,\nSeptember (2002), 29", "word_idx": 69208, "sentence_idx": 1216, "label": "unlabeled"}, {"type": "text", "expr": "Unixenguaedu", "word_idx": 69245, "sentence_idx": 1217, "label": "unlabeled"}, {"type": "text", "expr": "Kotlyar\net\u00a0al", "word_idx": 69257, "sentence_idx": 1218, "label": "unlabeled"}, {"type": "text", "expr": " (1997) \nVladimir Kotlyar, Keshav\nPingali, and Paul Stodghill", "word_idx": 69270, "sentence_idx": 1219, "label": "unlabeled"}, {"type": "text", "expr": "\n1997", "word_idx": 69331, "sentence_idx": 1220, "label": "unlabeled"}, {"type": "text", "expr": "\n\n A relational approach to the compilation of sparse\nmatrix programs", "word_idx": 69336, "sentence_idx": 1221, "label": "unlabeled"}, {"type": "text", "expr": "Kotlyar\net\u00a0al", "word_idx": 69405, "sentence_idx": 1222, "label": "unlabeled"}, {"type": "text", "expr": " (1997)", "word_idx": 69418, "sentence_idx": 1223, "label": "unlabeled"}, {"type": "text", "expr": "Vladimir Kotlyar, Keshav\nPingali, and Paul Stodghill", "word_idx": 69425, "sentence_idx": 1224, "label": "unlabeled"}, {"type": "text", "expr": "\n1997", "word_idx": 69477, "sentence_idx": 1225, "label": "unlabeled"}, {"type": "text", "expr": "A relational approach to the compilation of sparse\nmatrix programs", "word_idx": 69482, "sentence_idx": 1226, "label": "unlabeled"}, {"type": "text", "expr": "In  Euro-Par\u201997 Parallel Processing ", "word_idx": 69548, "sentence_idx": 1227, "label": "unlabeled"}, {"type": "text", "expr": "\nSpringer, 318\u2013327", "word_idx": 69584, "sentence_idx": 1228, "label": "unlabeled"}, {"type": "text", "expr": "Euro-Par\u201997 Parallel Processing", "word_idx": 69602, "sentence_idx": 1229, "label": "unlabeled"}, {"type": "text", "expr": "Leskovec and\nKrevl (2014) \nJure Leskovec and Andrej\nKrevl", "word_idx": 69633, "sentence_idx": 1230, "label": "unlabeled"}, {"type": "text", "expr": " 2014", "word_idx": 69690, "sentence_idx": 1231, "label": "unlabeled"}, {"type": "text", "expr": "\n\n SNAP Datasets: Stanford Large Network Dataset\nCollection", "word_idx": 69695, "sentence_idx": 1232, "label": "unlabeled"}, {"type": "text", "expr": "Leskovec and\nKrevl (2014)", "word_idx": 69754, "sentence_idx": 1233, "label": "unlabeled"}, {"type": "text", "expr": "Jure Leskovec and Andrej\nKrevl", "word_idx": 69779, "sentence_idx": 1234, "label": "unlabeled"}, {"type": "text", "expr": " 2014", "word_idx": 69809, "sentence_idx": 1235, "label": "unlabeled"}, {"type": "text", "expr": "SNAP Datasets: Stanford Large Network Dataset\nCollection", "word_idx": 69814, "sentence_idx": 1236, "label": "unlabeled"}, {"type": "text", "expr": "http://snap", "word_idx": 69870, "sentence_idx": 1237, "label": "unlabeled"}, {"type": "text", "expr": "stanford", "word_idx": 69881, "sentence_idx": 1238, "label": "unlabeled"}, {"type": "text", "expr": "edu/data ", "word_idx": 69889, "sentence_idx": 1239, "label": "unlabeled"}, {"type": "text", "expr": "\n(June 2014)", "word_idx": 69898, "sentence_idx": 1240, "label": "unlabeled"}, {"type": "text", "expr": "http://snap", "word_idx": 69910, "sentence_idx": 1241, "label": "unlabeled"}, {"type": "text", "expr": "stanford", "word_idx": 69921, "sentence_idx": 1242, "label": "unlabeled"}, {"type": "text", "expr": "edu/data", "word_idx": 69929, "sentence_idx": 1243, "label": "unlabeled"}, {"type": "text", "expr": "MATLAB (2014) \nMATLAB", "word_idx": 69937, "sentence_idx": 1244, "label": "unlabeled"}, {"type": "text", "expr": " 2014", "word_idx": 69958, "sentence_idx": 1245, "label": "unlabeled"}, {"type": "text", "expr": "\n\n The MathWorks Inc", "word_idx": 69963, "sentence_idx": 1246, "label": "unlabeled"}, {"type": "text", "expr": ", Natick,\nMassachusetts", "word_idx": 69983, "sentence_idx": 1247, "label": "unlabeled"}, {"type": "text", "expr": "MATLAB (2014)", "word_idx": 70006, "sentence_idx": 1248, "label": "unlabeled"}, {"type": "text", "expr": "MATLAB", "word_idx": 70019, "sentence_idx": 1249, "label": "unlabeled"}, {"type": "text", "expr": " 2014", "word_idx": 70025, "sentence_idx": 1250, "label": "unlabeled"}, {"type": "text", "expr": "version 8", "word_idx": 70030, "sentence_idx": 1251, "label": "unlabeled"}, {"type": "text", "expr": "0 (R2014a) ", "word_idx": 70039, "sentence_idx": 1252, "label": "unlabeled"}, {"type": "text", "expr": "version 8", "word_idx": 70050, "sentence_idx": 1253, "label": "unlabeled"}, {"type": "text", "expr": "0 (R2014a)", "word_idx": 70059, "sentence_idx": 1254, "label": "unlabeled"}, {"type": "text", "expr": "The MathWorks Inc", "word_idx": 70069, "sentence_idx": 1255, "label": "unlabeled"}, {"type": "text", "expr": ", Natick,\nMassachusetts", "word_idx": 70086, "sentence_idx": 1256, "label": "unlabeled"}, {"type": "text", "expr": "McAuley and\nLeskovec (2013) \nJulian McAuley and Jure\nLeskovec", "word_idx": 70109, "sentence_idx": 1257, "label": "unlabeled"}, {"type": "text", "expr": " 2013", "word_idx": 70170, "sentence_idx": 1258, "label": "unlabeled"}, {"type": "text", "expr": "McAuley and\nLeskovec (2013)", "word_idx": 70175, "sentence_idx": 1259, "label": "unlabeled"}, {"type": "text", "expr": "Julian McAuley and Jure\nLeskovec", "word_idx": 70202, "sentence_idx": 1260, "label": "unlabeled"}, {"type": "text", "expr": " 2013", "word_idx": 70234, "sentence_idx": 1261, "label": "unlabeled"}, {"type": "text", "expr": "Hidden factors and hidden topics: understanding\nrating dimensions with review text", "word_idx": 70239, "sentence_idx": 1262, "label": "unlabeled"}, {"type": "text", "expr": " In  Proceedings\nof the 7th ACM conference on Recommender systems ", "word_idx": 70321, "sentence_idx": 1263, "label": "unlabeled"}, {"type": "text", "expr": " ACM,\n165\u2013172", "word_idx": 70387, "sentence_idx": 1264, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings\nof the 7th ACM conference on Recommender systems", "word_idx": 70400, "sentence_idx": 1265, "label": "unlabeled"}, {"type": "text", "expr": "McKinley\net\u00a0al", "word_idx": 70460, "sentence_idx": 1266, "label": "unlabeled"}, {"type": "text", "expr": " (1996) \nKathryn\u00a0S McKinley, Steve\nCarr, and Chau-Wen Tseng", "word_idx": 70474, "sentence_idx": 1267, "label": "unlabeled"}, {"type": "text", "expr": "\n1996", "word_idx": 70533, "sentence_idx": 1268, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Improving data locality with loop transformations", "word_idx": 70538, "sentence_idx": 1269, "label": "unlabeled"}, {"type": "text", "expr": "McKinley\net\u00a0al", "word_idx": 70590, "sentence_idx": 1270, "label": "unlabeled"}, {"type": "text", "expr": " (1996)", "word_idx": 70604, "sentence_idx": 1271, "label": "unlabeled"}, {"type": "text", "expr": "Kathryn\u00a0S McKinley, Steve\nCarr, and Chau-Wen Tseng", "word_idx": 70611, "sentence_idx": 1272, "label": "unlabeled"}, {"type": "text", "expr": "\n1996", "word_idx": 70661, "sentence_idx": 1273, "label": "unlabeled"}, {"type": "text", "expr": "Improving data locality with loop transformations", "word_idx": 70666, "sentence_idx": 1274, "label": "unlabeled"}, {"type": "text", "expr": "ACM Transactions on Programming Languages and\nSystems (TOPLAS)  18, 4\n(1996), 424\u2013453", "word_idx": 70715, "sentence_idx": 1275, "label": "unlabeled"}, {"type": "text", "expr": "ACM Transactions on Programming Languages and\nSystems (TOPLAS)", "word_idx": 70800, "sentence_idx": 1276, "label": "unlabeled"}, {"type": "text", "expr": "M\u00f6cks (1988) \nJoachim M\u00f6cks", "word_idx": 70862, "sentence_idx": 1277, "label": "unlabeled"}, {"type": "text", "expr": "\n1988", "word_idx": 70889, "sentence_idx": 1278, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Topographic components model for event-related\npotentials and some biophysical considerations", "word_idx": 70894, "sentence_idx": 1279, "label": "unlabeled"}, {"type": "text", "expr": "M\u00f6cks (1988)", "word_idx": 70990, "sentence_idx": 1280, "label": "unlabeled"}, {"type": "text", "expr": "Joachim M\u00f6cks", "word_idx": 71002, "sentence_idx": 1281, "label": "unlabeled"}, {"type": "text", "expr": "\n1988", "word_idx": 71015, "sentence_idx": 1282, "label": "unlabeled"}, {"type": "text", "expr": "Topographic components model for event-related\npotentials and some biophysical considerations", "word_idx": 71020, "sentence_idx": 1283, "label": "unlabeled"}, {"type": "text", "expr": "IEEE transactions on biomedical engineering \n35, 6 (1988),\n482\u2013484", "word_idx": 71113, "sentence_idx": 1284, "label": "unlabeled"}, {"type": "text", "expr": "IEEE transactions on biomedical engineering", "word_idx": 71179, "sentence_idx": 1285, "label": "unlabeled"}, {"type": "text", "expr": "Patwary et\u00a0al", "word_idx": 71222, "sentence_idx": 1286, "label": "unlabeled"}, {"type": "text", "expr": " (2015) \nMd", "word_idx": 71235, "sentence_idx": 1287, "label": "unlabeled"}, {"type": "text", "expr": " Mostofa\u00a0Ali Patwary,\nNadathur\u00a0Rajagopalan Satish, Narayanan\nSundaram, Jongsoo Park, Michael\u00a0J\nAnderson, Satya\u00a0Gautam Vadlamudi,\nDipankar Das, Sergey\u00a0G Pudov,\nVadim\u00a0O Pirogov, and Pradeep Dubey", "word_idx": 71246, "sentence_idx": 1288, "label": "unlabeled"}, {"type": "text", "expr": "\n2015", "word_idx": 71439, "sentence_idx": 1289, "label": "unlabeled"}, {"type": "text", "expr": "Patwary et\u00a0al", "word_idx": 71444, "sentence_idx": 1290, "label": "unlabeled"}, {"type": "text", "expr": " (2015)", "word_idx": 71457, "sentence_idx": 1291, "label": "unlabeled"}, {"type": "text", "expr": " Mostofa\u00a0Ali Patwary,\nNadathur\u00a0Rajagopalan Satish, Narayanan\nSundaram, Jongsoo Park, Michael\u00a0J\nAnderson, Satya\u00a0Gautam Vadlamudi,\nDipankar Das, Sergey\u00a0G Pudov,\nVadim\u00a0O Pirogov, and Pradeep Dubey", "word_idx": 71464, "sentence_idx": 1292, "label": "unlabeled"}, {"type": "text", "expr": "\n2015", "word_idx": 71657, "sentence_idx": 1293, "label": "unlabeled"}, {"type": "text", "expr": "Parallel efficient sparse matrix-matrix\nmultiplication on multicore platforms", "word_idx": 71662, "sentence_idx": 1294, "label": "unlabeled"}, {"type": "text", "expr": " In\n International Conference on High Performance\nComputing ", "word_idx": 71739, "sentence_idx": 1295, "label": "unlabeled"}, {"type": "text", "expr": " Springer, 48\u201357", "word_idx": 71799, "sentence_idx": 1296, "label": "unlabeled"}, {"type": "text", "expr": "International Conference on High Performance\nComputing", "word_idx": 71815, "sentence_idx": 1297, "label": "unlabeled"}, {"type": "text", "expr": "Phan and Cichocki (2010) \nAnh\u00a0Huy Phan and Andrzej\nCichocki", "word_idx": 71869, "sentence_idx": 1298, "label": "unlabeled"}, {"type": "text", "expr": " 2010", "word_idx": 71928, "sentence_idx": 1299, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Tensor decompositions for feature extraction and\nclassification of high dimensional datasets", "word_idx": 71933, "sentence_idx": 1300, "label": "unlabeled"}, {"type": "text", "expr": "Phan and Cichocki (2010)", "word_idx": 72028, "sentence_idx": 1301, "label": "unlabeled"}, {"type": "text", "expr": "Anh\u00a0Huy Phan and Andrzej\nCichocki", "word_idx": 72052, "sentence_idx": 1302, "label": "unlabeled"}, {"type": "text", "expr": " 2010", "word_idx": 72085, "sentence_idx": 1303, "label": "unlabeled"}, {"type": "text", "expr": "Tensor decompositions for feature extraction and\nclassification of high dimensional datasets", "word_idx": 72090, "sentence_idx": 1304, "label": "unlabeled"}, {"type": "text", "expr": "Nonlinear theory and its applications,\nIEICE  1, 1 (2010),\n37\u201368", "word_idx": 72182, "sentence_idx": 1305, "label": "unlabeled"}, {"type": "text", "expr": "Nonlinear theory and its applications,\nIEICE", "word_idx": 72246, "sentence_idx": 1306, "label": "unlabeled"}, {"type": "text", "expr": "Pissanetzky (1984) \nSergio Pissanetzky", "word_idx": 72290, "sentence_idx": 1307, "label": "unlabeled"}, {"type": "text", "expr": "\n1984", "word_idx": 72328, "sentence_idx": 1308, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Academic Press", "word_idx": 72333, "sentence_idx": 1309, "label": "unlabeled"}, {"type": "text", "expr": "Pissanetzky (1984)", "word_idx": 72350, "sentence_idx": 1310, "label": "unlabeled"}, {"type": "text", "expr": "Sergio Pissanetzky", "word_idx": 72368, "sentence_idx": 1311, "label": "unlabeled"}, {"type": "text", "expr": "\n1984", "word_idx": 72386, "sentence_idx": 1312, "label": "unlabeled"}, {"type": "text", "expr": "Sparse Matrix Technology-electronic\nedition ", "word_idx": 72391, "sentence_idx": 1313, "label": "unlabeled"}, {"type": "text", "expr": "Sparse Matrix Technology-electronic\nedition", "word_idx": 72435, "sentence_idx": 1314, "label": "unlabeled"}, {"type": "text", "expr": "Academic Press", "word_idx": 72478, "sentence_idx": 1315, "label": "unlabeled"}, {"type": "text", "expr": "Ragan-Kelley et\u00a0al", "word_idx": 72492, "sentence_idx": 1316, "label": "unlabeled"}, {"type": "text", "expr": " (2012) \nJonathan Ragan-Kelley,\nAndrew Adams, Sylvain Paris,\nMarc Levoy, Saman Amarasinghe, and\nFr\u00e9do Durand", "word_idx": 72510, "sentence_idx": 1317, "label": "unlabeled"}, {"type": "text", "expr": " 2012", "word_idx": 72618, "sentence_idx": 1318, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Decoupling algorithms from schedules for easy\noptimization of image processing pipelines", "word_idx": 72623, "sentence_idx": 1319, "label": "unlabeled"}, {"type": "text", "expr": "\n\n (2012)", "word_idx": 72714, "sentence_idx": 1320, "label": "unlabeled"}, {"type": "text", "expr": "Ragan-Kelley et\u00a0al", "word_idx": 72723, "sentence_idx": 1321, "label": "unlabeled"}, {"type": "text", "expr": " (2012)", "word_idx": 72741, "sentence_idx": 1322, "label": "unlabeled"}, {"type": "text", "expr": "Jonathan Ragan-Kelley,\nAndrew Adams, Sylvain Paris,\nMarc Levoy, Saman Amarasinghe, and\nFr\u00e9do Durand", "word_idx": 72748, "sentence_idx": 1323, "label": "unlabeled"}, {"type": "text", "expr": " 2012", "word_idx": 72847, "sentence_idx": 1324, "label": "unlabeled"}, {"type": "text", "expr": "Decoupling algorithms from schedules for easy\noptimization of image processing pipelines", "word_idx": 72852, "sentence_idx": 1325, "label": "unlabeled"}, {"type": "text", "expr": "(2012)", "word_idx": 72940, "sentence_idx": 1326, "label": "unlabeled"}, {"type": "text", "expr": "Shashua and Levin (2001) \nAmnon Shashua and Anat\nLevin", "word_idx": 72946, "sentence_idx": 1327, "label": "unlabeled"}, {"type": "text", "expr": " 2001", "word_idx": 73000, "sentence_idx": 1328, "label": "unlabeled"}, {"type": "text", "expr": "Shashua and Levin (2001)", "word_idx": 73005, "sentence_idx": 1329, "label": "unlabeled"}, {"type": "text", "expr": "Amnon Shashua and Anat\nLevin", "word_idx": 73029, "sentence_idx": 1330, "label": "unlabeled"}, {"type": "text", "expr": " 2001", "word_idx": 73057, "sentence_idx": 1331, "label": "unlabeled"}, {"type": "text", "expr": "Linear image coding for regression and\nclassification using the tensor-rank principle", "word_idx": 73062, "sentence_idx": 1332, "label": "unlabeled"}, {"type": "text", "expr": " In\n Computer Vision and Pattern Recognition, 2001", "word_idx": 73147, "sentence_idx": 1333, "label": "unlabeled"}, {"type": "text", "expr": " CVPR\n2001", "word_idx": 73197, "sentence_idx": 1334, "label": "unlabeled"}, {"type": "text", "expr": " Proceedings of the 2001 IEEE Computer Society Conference on ,\nVol", "word_idx": 73207, "sentence_idx": 1335, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, I\u2013I", "word_idx": 73273, "sentence_idx": 1336, "label": "unlabeled"}, {"type": "text", "expr": "Computer Vision and Pattern Recognition, 2001", "word_idx": 73283, "sentence_idx": 1337, "label": "unlabeled"}, {"type": "text", "expr": " CVPR\n2001", "word_idx": 73328, "sentence_idx": 1338, "label": "unlabeled"}, {"type": "text", "expr": " Proceedings of the 2001 IEEE Computer Society Conference on", "word_idx": 73338, "sentence_idx": 1339, "label": "unlabeled"}, {"type": "text", "expr": "Smith\net\u00a0al", "word_idx": 73398, "sentence_idx": 1340, "label": "unlabeled"}, {"type": "text", "expr": " (2017a) \nShaden Smith, Alec Beri,\nand George Karypis", "word_idx": 73409, "sentence_idx": 1341, "label": "unlabeled"}, {"type": "text", "expr": " 2017a", "word_idx": 73462, "sentence_idx": 1342, "label": "unlabeled"}, {"type": "text", "expr": "Smith\net\u00a0al", "word_idx": 73468, "sentence_idx": 1343, "label": "unlabeled"}, {"type": "text", "expr": " (2017a)", "word_idx": 73479, "sentence_idx": 1344, "label": "unlabeled"}, {"type": "text", "expr": "Shaden Smith, Alec Beri,\nand George Karypis", "word_idx": 73487, "sentence_idx": 1345, "label": "unlabeled"}, {"type": "text", "expr": " 2017a", "word_idx": 73530, "sentence_idx": 1346, "label": "unlabeled"}, {"type": "text", "expr": "Constrained Tensor Factorization with Accelerated\nAO-ADMM", "word_idx": 73536, "sentence_idx": 1347, "label": "unlabeled"}, {"type": "text", "expr": " In  Parallel Processing (ICPP), 2017 46th\nInternational Conference on ", "word_idx": 73593, "sentence_idx": 1348, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 111\u2013120", "word_idx": 73664, "sentence_idx": 1349, "label": "unlabeled"}, {"type": "text", "expr": "Parallel Processing (ICPP), 2017 46th\nInternational Conference on", "word_idx": 73678, "sentence_idx": 1350, "label": "unlabeled"}, {"type": "text", "expr": "Smith et\u00a0al", "word_idx": 73743, "sentence_idx": 1351, "label": "unlabeled"}, {"type": "text", "expr": " (2017b) \nShaden Smith, Jee\u00a0W", "word_idx": 73754, "sentence_idx": 1352, "label": "unlabeled"}, {"type": "text", "expr": "\nChoi, Jiajia Li, Richard Vuduc,\nJongsoo Park, Xing Liu, and\nGeorge Karypis", "word_idx": 73783, "sentence_idx": 1353, "label": "unlabeled"}, {"type": "text", "expr": " 2017b", "word_idx": 73858, "sentence_idx": 1354, "label": "unlabeled"}, {"type": "text", "expr": "\n\n FROSTT: The Formidable Repository of Open Sparse\nTensors and Tools", "word_idx": 73864, "sentence_idx": 1355, "label": "unlabeled"}, {"type": "text", "expr": "\n\n (2017)", "word_idx": 73933, "sentence_idx": 1356, "label": "unlabeled"}, {"type": "text", "expr": "Smith et\u00a0al", "word_idx": 73942, "sentence_idx": 1357, "label": "unlabeled"}, {"type": "text", "expr": " (2017b)", "word_idx": 73953, "sentence_idx": 1358, "label": "unlabeled"}, {"type": "text", "expr": "Shaden Smith, Jee\u00a0W", "word_idx": 73961, "sentence_idx": 1359, "label": "unlabeled"}, {"type": "text", "expr": "\nChoi, Jiajia Li, Richard Vuduc,\nJongsoo Park, Xing Liu, and\nGeorge Karypis", "word_idx": 73980, "sentence_idx": 1360, "label": "unlabeled"}, {"type": "text", "expr": " 2017b", "word_idx": 74055, "sentence_idx": 1361, "label": "unlabeled"}, {"type": "text", "expr": "FROSTT: The Formidable Repository of Open Sparse\nTensors and Tools", "word_idx": 74061, "sentence_idx": 1362, "label": "unlabeled"}, {"type": "text", "expr": "(2017)", "word_idx": 74127, "sentence_idx": 1363, "label": "unlabeled"}, {"type": "text", "expr": "http://frostt", "word_idx": 74133, "sentence_idx": 1364, "label": "unlabeled"}, {"type": "text", "expr": "http://frostt", "word_idx": 74146, "sentence_idx": 1365, "label": "unlabeled"}, {"type": "text", "expr": "Smith et\u00a0al", "word_idx": 74159, "sentence_idx": 1366, "label": "unlabeled"}, {"type": "text", "expr": " (2015) \nShaden Smith, Niranjay\nRavindran, Nicholas Sidiropoulos, and\nGeorge Karypis", "word_idx": 74170, "sentence_idx": 1367, "label": "unlabeled"}, {"type": "text", "expr": " 2015", "word_idx": 74254, "sentence_idx": 1368, "label": "unlabeled"}, {"type": "text", "expr": "Smith et\u00a0al", "word_idx": 74259, "sentence_idx": 1369, "label": "unlabeled"}, {"type": "text", "expr": " (2015)", "word_idx": 74270, "sentence_idx": 1370, "label": "unlabeled"}, {"type": "text", "expr": "Shaden Smith, Niranjay\nRavindran, Nicholas Sidiropoulos, and\nGeorge Karypis", "word_idx": 74277, "sentence_idx": 1371, "label": "unlabeled"}, {"type": "text", "expr": " 2015", "word_idx": 74352, "sentence_idx": 1372, "label": "unlabeled"}, {"type": "text", "expr": "SPLATT: Efficient and Parallel Sparse Tensor-Matrix\nMultiplication", "word_idx": 74357, "sentence_idx": 1373, "label": "unlabeled"}, {"type": "text", "expr": " In  2015 IEEE International\nParallel and Distributed Processing Symposium (IPDPS) ", "word_idx": 74423, "sentence_idx": 1374, "label": "unlabeled"}, {"type": "text", "expr": "\n61\u201370", "word_idx": 74506, "sentence_idx": 1375, "label": "unlabeled"}, {"type": "text", "expr": "2015 IEEE International\nParallel and Distributed Processing Symposium (IPDPS)", "word_idx": 74512, "sentence_idx": 1376, "label": "unlabeled"}, {"type": "text", "expr": "Tinney and Walker (1967) \nWilliam\u00a0F Tinney and\nJohn\u00a0W Walker", "word_idx": 74589, "sentence_idx": 1377, "label": "unlabeled"}, {"type": "text", "expr": " 1967", "word_idx": 74649, "sentence_idx": 1378, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Direct solutions of sparse network equations by\noptimally ordered triangular factorization", "word_idx": 74654, "sentence_idx": 1379, "label": "unlabeled"}, {"type": "text", "expr": "Tinney and Walker (1967)", "word_idx": 74747, "sentence_idx": 1380, "label": "unlabeled"}, {"type": "text", "expr": "William\u00a0F Tinney and\nJohn\u00a0W Walker", "word_idx": 74771, "sentence_idx": 1381, "label": "unlabeled"}, {"type": "text", "expr": " 1967", "word_idx": 74805, "sentence_idx": 1382, "label": "unlabeled"}, {"type": "text", "expr": "Direct solutions of sparse network equations by\noptimally ordered triangular factorization", "word_idx": 74810, "sentence_idx": 1383, "label": "unlabeled"}, {"type": "text", "expr": " IEEE  55,\n11 (1967), 1801\u20131809", "word_idx": 74900, "sentence_idx": 1384, "label": "unlabeled"}, {"type": "text", "expr": " IEEE", "word_idx": 74931, "sentence_idx": 1385, "label": "unlabeled"}, {"type": "text", "expr": "Wolfe (1982) \nMichael\u00a0Joseph Wolfe", "word_idx": 74936, "sentence_idx": 1386, "label": "unlabeled"}, {"type": "text", "expr": "\n1982", "word_idx": 74970, "sentence_idx": 1387, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Ph", "word_idx": 74975, "sentence_idx": 1388, "label": "unlabeled"}, {"type": "text", "expr": " Dissertation", "word_idx": 74980, "sentence_idx": 1389, "label": "unlabeled"}, {"type": "text", "expr": " University\nof Illinois at Urbana-Champaign, Champaign, IL, USA", "word_idx": 74993, "sentence_idx": 1390, "label": "unlabeled"}, {"type": "text", "expr": "\n\n AAI8303027", "word_idx": 75056, "sentence_idx": 1391, "label": "unlabeled"}, {"type": "text", "expr": "Wolfe (1982)", "word_idx": 75069, "sentence_idx": 1392, "label": "unlabeled"}, {"type": "text", "expr": "Michael\u00a0Joseph Wolfe", "word_idx": 75081, "sentence_idx": 1393, "label": "unlabeled"}, {"type": "text", "expr": "\n1982", "word_idx": 75101, "sentence_idx": 1394, "label": "unlabeled"}, {"type": "text", "expr": "Optimizing Supercompilers for Supercomputers ", "word_idx": 75106, "sentence_idx": 1395, "label": "unlabeled"}, {"type": "text", "expr": "Optimizing Supercompilers for Supercomputers", "word_idx": 75151, "sentence_idx": 1396, "label": "unlabeled"}, {"type": "text", "expr": " Dissertation", "word_idx": 75195, "sentence_idx": 1397, "label": "unlabeled"}, {"type": "text", "expr": " University\nof Illinois at Urbana-Champaign, Champaign, IL, USA", "word_idx": 75208, "sentence_idx": 1398, "label": "unlabeled"}, {"type": "text", "expr": "AAI8303027", "word_idx": 75271, "sentence_idx": 1399, "label": "unlabeled"}, {"type": "text", "expr": "Wulf et\u00a0al", "word_idx": 75281, "sentence_idx": 1400, "label": "unlabeled"}, {"type": "text", "expr": " (1974) \nW", "word_idx": 75291, "sentence_idx": 1401, "label": "unlabeled"}, {"type": "text", "expr": " Wulf, E", "word_idx": 75301, "sentence_idx": 1402, "label": "unlabeled"}, {"type": "text", "expr": " Cohen,\nW", "word_idx": 75309, "sentence_idx": 1403, "label": "unlabeled"}, {"type": "text", "expr": " Corwin, A", "word_idx": 75318, "sentence_idx": 1404, "label": "unlabeled"}, {"type": "text", "expr": " Jones, R", "word_idx": 75328, "sentence_idx": 1405, "label": "unlabeled"}, {"type": "text", "expr": "\nLevin, C", "word_idx": 75337, "sentence_idx": 1406, "label": "unlabeled"}, {"type": "text", "expr": " Pierson, and F", "word_idx": 75346, "sentence_idx": 1407, "label": "unlabeled"}, {"type": "text", "expr": " Pollack", "word_idx": 75361, "sentence_idx": 1408, "label": "unlabeled"}, {"type": "text", "expr": "\n1974", "word_idx": 75369, "sentence_idx": 1409, "label": "unlabeled"}, {"type": "text", "expr": "\n\n HYDRA: The Kernel of a Multiprocessor Operating\nSystem", "word_idx": 75374, "sentence_idx": 1410, "label": "unlabeled"}, {"type": "text", "expr": "Wulf et\u00a0al", "word_idx": 75431, "sentence_idx": 1411, "label": "unlabeled"}, {"type": "text", "expr": " (1974)", "word_idx": 75441, "sentence_idx": 1412, "label": "unlabeled"}, {"type": "text", "expr": " Wulf, E", "word_idx": 75448, "sentence_idx": 1413, "label": "unlabeled"}, {"type": "text", "expr": " Cohen,\nW", "word_idx": 75456, "sentence_idx": 1414, "label": "unlabeled"}, {"type": "text", "expr": " Corwin, A", "word_idx": 75465, "sentence_idx": 1415, "label": "unlabeled"}, {"type": "text", "expr": " Jones, R", "word_idx": 75475, "sentence_idx": 1416, "label": "unlabeled"}, {"type": "text", "expr": "\nLevin, C", "word_idx": 75484, "sentence_idx": 1417, "label": "unlabeled"}, {"type": "text", "expr": " Pierson, and F", "word_idx": 75493, "sentence_idx": 1418, "label": "unlabeled"}, {"type": "text", "expr": " Pollack", "word_idx": 75508, "sentence_idx": 1419, "label": "unlabeled"}, {"type": "text", "expr": "\n1974", "word_idx": 75516, "sentence_idx": 1420, "label": "unlabeled"}, {"type": "text", "expr": "HYDRA: The Kernel of a Multiprocessor Operating\nSystem", "word_idx": 75521, "sentence_idx": 1421, "label": "unlabeled"}, {"type": "text", "expr": "Commun", "word_idx": 75575, "sentence_idx": 1422, "label": "unlabeled"}, {"type": "text", "expr": " ACM  17,\n6 (June 1974),\n337\u2013345", "word_idx": 75581, "sentence_idx": 1423, "label": "unlabeled"}, {"type": "text", "expr": "Commun", "word_idx": 75613, "sentence_idx": 1424, "label": "unlabeled"}, {"type": "text", "expr": "https://doi", "word_idx": 75619, "sentence_idx": 1425, "label": "unlabeled"}, {"type": "text", "expr": "org/10", "word_idx": 75630, "sentence_idx": 1426, "label": "unlabeled"}, {"type": "text", "expr": "1145/355616", "word_idx": 75636, "sentence_idx": 1427, "label": "unlabeled"}, {"type": "text", "expr": "364017", "word_idx": 75647, "sentence_idx": 1428, "label": "unlabeled"}, {"type": "text", "expr": "https://doi", "word_idx": 75653, "sentence_idx": 1429, "label": "unlabeled"}, {"type": "text", "expr": "org/10", "word_idx": 75664, "sentence_idx": 1430, "label": "unlabeled"}, {"type": "text", "expr": "1145/355616", "word_idx": 75670, "sentence_idx": 1431, "label": "unlabeled"}, {"type": "text", "expr": "364017", "word_idx": 75681, "sentence_idx": 1432, "label": "unlabeled"}, {"type": "text", "expr": "Zhao (2014) \nHuasha Zhao", "word_idx": 75687, "sentence_idx": 1433, "label": "unlabeled"}, {"type": "text", "expr": "\n2014", "word_idx": 75711, "sentence_idx": 1434, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Ph", "word_idx": 75716, "sentence_idx": 1435, "label": "unlabeled"}, {"type": "text", "expr": " Dissertation", "word_idx": 75721, "sentence_idx": 1436, "label": "unlabeled"}, {"type": "text", "expr": " EECS\nDepartment, University of California, Berkeley", "word_idx": 75734, "sentence_idx": 1437, "label": "unlabeled"}, {"type": "text", "expr": "Zhao (2014)", "word_idx": 75786, "sentence_idx": 1438, "label": "unlabeled"}, {"type": "text", "expr": "Huasha Zhao", "word_idx": 75797, "sentence_idx": 1439, "label": "unlabeled"}, {"type": "text", "expr": "\n2014", "word_idx": 75808, "sentence_idx": 1440, "label": "unlabeled"}, {"type": "text", "expr": "High Performance Machine Learning through\nCodesign and Rooflining ", "word_idx": 75813, "sentence_idx": 1441, "label": "unlabeled"}, {"type": "text", "expr": "High Performance Machine Learning through\nCodesign and Rooflining", "word_idx": 75879, "sentence_idx": 1442, "label": "unlabeled"}, {"type": "text", "expr": " Dissertation", "word_idx": 75944, "sentence_idx": 1443, "label": "unlabeled"}, {"type": "text", "expr": " EECS\nDepartment, University of California, Berkeley", "word_idx": 75957, "sentence_idx": 1444, "label": "unlabeled"}, {"type": "text", "expr": "Generated  on Tue Apr  3 11:30:38 2018 by", "word_idx": 76009, "sentence_idx": 1445, "label": "unlabeled"}, {"type": "text", "expr": "LaTeXML", "word_idx": 76050, "sentence_idx": 1446, "label": "unlabeled"}], "underlying_plans": [{"type": "text", "expr": "Discovering Underlying Plans Based on Shallow Models", "word_idx": 0, "sentence_idx": 0, "label": "unlabeled"}, {"type": "text", "expr": "Discovering Underlying Plans Based on Shallow Models", "word_idx": 52, "sentence_idx": 1, "label": "unlabeled"}, {"type": "text", "expr": "Hankz Hankui Zhuo", "word_idx": 104, "sentence_idx": 2, "label": "unlabeled"}, {"type": "text", "expr": "Hankz Hankui Zhuo", "word_idx": 121, "sentence_idx": 3, "label": "unlabeled"}, {"type": "text", "expr": "Hankz Hankui Zhuo Sun Yat-Sen University \n Yantian Zha Arizona State University \n Subbarao Kambhampati Arizona State University", "word_idx": 138, "sentence_idx": 4, "label": "unlabeled"}, {"type": "text", "expr": "2 email:  zhuohank@mail", "word_idx": 265, "sentence_idx": 5, "label": "unlabeled"}, {"type": "text", "expr": "email:", "word_idx": 288, "sentence_idx": 6, "label": "unlabeled"}, {"type": "text", "expr": "4 email:  yantian", "word_idx": 294, "sentence_idx": 7, "label": "unlabeled"}, {"type": "text", "expr": "zha@asu", "word_idx": 311, "sentence_idx": 8, "label": "unlabeled"}, {"type": "text", "expr": "email:", "word_idx": 318, "sentence_idx": 9, "label": "unlabeled"}, {"type": "text", "expr": "6 email:  rao@asu", "word_idx": 324, "sentence_idx": 10, "label": "unlabeled"}, {"type": "text", "expr": "email:", "word_idx": 341, "sentence_idx": 11, "label": "unlabeled"}, {"type": "text", "expr": "Yantian Zha", "word_idx": 347, "sentence_idx": 12, "label": "unlabeled"}, {"type": "text", "expr": "Yantian Zha", "word_idx": 358, "sentence_idx": 13, "label": "unlabeled"}, {"type": "text", "expr": "Hankz Hankui Zhuo Sun Yat-Sen University \n Yantian Zha Arizona State University \n Subbarao Kambhampati Arizona State University", "word_idx": 369, "sentence_idx": 14, "label": "unlabeled"}, {"type": "text", "expr": "2 email:  zhuohank@mail", "word_idx": 496, "sentence_idx": 15, "label": "unlabeled"}, {"type": "text", "expr": "email:", "word_idx": 519, "sentence_idx": 16, "label": "unlabeled"}, {"type": "text", "expr": "4 email:  yantian", "word_idx": 525, "sentence_idx": 17, "label": "unlabeled"}, {"type": "text", "expr": "zha@asu", "word_idx": 542, "sentence_idx": 18, "label": "unlabeled"}, {"type": "text", "expr": "email:", "word_idx": 549, "sentence_idx": 19, "label": "unlabeled"}, {"type": "text", "expr": "6 email:  rao@asu", "word_idx": 555, "sentence_idx": 20, "label": "unlabeled"}, {"type": "text", "expr": "email:", "word_idx": 572, "sentence_idx": 21, "label": "unlabeled"}, {"type": "text", "expr": "Subbarao Kambhampati", "word_idx": 578, "sentence_idx": 22, "label": "unlabeled"}, {"type": "text", "expr": "Subbarao Kambhampati", "word_idx": 598, "sentence_idx": 23, "label": "unlabeled"}, {"type": "text", "expr": "Hankz Hankui Zhuo Sun Yat-Sen University \n Yantian Zha Arizona State University \n Subbarao Kambhampati Arizona State University", "word_idx": 618, "sentence_idx": 24, "label": "unlabeled"}, {"type": "text", "expr": "2 email:  zhuohank@mail", "word_idx": 745, "sentence_idx": 25, "label": "unlabeled"}, {"type": "text", "expr": "email:", "word_idx": 768, "sentence_idx": 26, "label": "unlabeled"}, {"type": "text", "expr": "4 email:  yantian", "word_idx": 774, "sentence_idx": 27, "label": "unlabeled"}, {"type": "text", "expr": "zha@asu", "word_idx": 791, "sentence_idx": 28, "label": "unlabeled"}, {"type": "text", "expr": "email:", "word_idx": 798, "sentence_idx": 29, "label": "unlabeled"}, {"type": "text", "expr": "6 email:  rao@asu", "word_idx": 804, "sentence_idx": 30, "label": "unlabeled"}, {"type": "text", "expr": "email:", "word_idx": 821, "sentence_idx": 31, "label": "unlabeled"}, {"type": "text", "expr": "Abstract", "word_idx": 827, "sentence_idx": 32, "label": "unlabeled"}, {"type": "text", "expr": "Abstract", "word_idx": 835, "sentence_idx": 33, "label": "unlabeled"}, {"type": "text", "expr": "Plan recognition aims to discover target plans (i", "word_idx": 843, "sentence_idx": 34, "label": "unlabeled"}, {"type": "text", "expr": ", sequences of actions) behind observed actions, with history plan libraries or domain models in hand", "word_idx": 892, "sentence_idx": 35, "label": "unlabeled"}, {"type": "text", "expr": " Previous approaches either discover plans by maximally \u201cmatching\u201d observed actions to plan libraries, assuming target plans are from plan libraries, or infer plans by executing domain models to best explain the observed actions, assuming that complete domain models are available", "word_idx": 993, "sentence_idx": 36, "label": "unlabeled"}, {"type": "text", "expr": " In real world applications, however, target plans are often not from plan libraries, and complete domain models are often not available, since building complete sets of plans and complete domain models are often difficult or expensive", "word_idx": 1273, "sentence_idx": 37, "label": "unlabeled"}, {"type": "text", "expr": " In this paper we view plan libraries as corpora and learn vector representations of actions using the corpora; we then discover target plans based on the vector representations", "word_idx": 1508, "sentence_idx": 38, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, we propose two approaches,  DUP  and  RNNPlanner , to discover target plans based on vector representations of actions", "word_idx": 1685, "sentence_idx": 39, "label": "unlabeled"}, {"type": "text", "expr": "  DUP  explores the EM-style framework to capture local contexts of actions and discover target plans by optimizing the probability of target plans, while  RNNPlanner  aims to leverage long-short term contexts of actions based on RNNs (recurrent neural networks) framework to help recognize target plans", "word_idx": 1818, "sentence_idx": 40, "label": "unlabeled"}, {"type": "text", "expr": " In the experiments, we empirically show that our approaches are capable of discovering underlying plans that are not from plan libraries, without requiring domain models provided", "word_idx": 2121, "sentence_idx": 41, "label": "unlabeled"}, {"type": "text", "expr": " We demonstrate the effectiveness of our approaches by comparing its performance to traditional plan recognition approaches in three planning domains", "word_idx": 2300, "sentence_idx": 42, "label": "unlabeled"}, {"type": "text", "expr": " We also compare  DUP  and  RNNPlanner  to see their advantages and disadvantages", "word_idx": 2449, "sentence_idx": 43, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 2530, "sentence_idx": 44, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 2540, "sentence_idx": 45, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 2550, "sentence_idx": 46, "label": "unlabeled"}, {"type": "text", "expr": "Keywords: Plan Recognition Distributed Representation Shallow Model AI Planning Action Model Learning", "word_idx": 2560, "sentence_idx": 47, "label": "unlabeled"}, {"type": "text", "expr": "Keywords:", "word_idx": 2661, "sentence_idx": 48, "label": "unlabeled"}, {"type": "text", "expr": "1  Introduction", "word_idx": 2670, "sentence_idx": 49, "label": "unlabeled"}, {"type": "text", "expr": "As computer-aided cooperative work scenarios become increasingly popular, human-in-the-loop planning and decision support has become a critical planning challenge (c", "word_idx": 2685, "sentence_idx": 50, "label": "unlabeled"}, {"type": "text", "expr": " An important aspect of such a support  (15)  is recognizing what plans the human in the loop is making, and provide appropriate suggestions about their next actions  (2) ", "word_idx": 2850, "sentence_idx": 51, "label": "unlabeled"}, {"type": "text", "expr": "\nAlthough there is a lot of work on plan recognition, much of it has\ntraditionally depended on the availability of a complete domain model\n ", "word_idx": 3021, "sentence_idx": 52, "label": "unlabeled"}, {"type": "text", "expr": " As has been argued elsewhere\n (15) , such models are hard to get\nin human-in-the-loop planning scenarios", "word_idx": 3161, "sentence_idx": 53, "label": "unlabeled"}, {"type": "text", "expr": " Here, the decision support\nsystems have to make themselves useful without insisting on complete\naction models of the domain", "word_idx": 3266, "sentence_idx": 54, "label": "unlabeled"}, {"type": "text", "expr": " The situation here is akin to that faced\nby search engines and other tools for computer supported\ncooperate work, and is thus a significant departure for the \u201cplanning\nas pure inference\u201d mindset of the automated planning community", "word_idx": 3390, "sentence_idx": 55, "label": "unlabeled"}, {"type": "text", "expr": " As\nsuch, the problem\ncalls for plan recognition with \u201cshallow\u201d models of the domain\n(c", "word_idx": 3621, "sentence_idx": 56, "label": "unlabeled"}, {"type": "text", "expr": "  (13) ), that can be easily learned\nautomatically", "word_idx": 3708, "sentence_idx": 57, "label": "unlabeled"}, {"type": "text", "expr": " Compared to learning action models (\u201ccomplex\u201d models correspondingly) of the domain from limited training data, learning shallow models can avoid the overfitting issue", "word_idx": 3758, "sentence_idx": 58, "label": "unlabeled"}, {"type": "text", "expr": " One key difference between \u201cshallow\u201d and \u201ccomplex\u201d models is the size of parameters of both models is distinguish, which is comparable to learning models in machine learning community, i", "word_idx": 3926, "sentence_idx": 59, "label": "unlabeled"}, {"type": "text", "expr": ", complex models with large parameters require much more training data for learning parameter values compared to \u201cshallow\u201d models", "word_idx": 4113, "sentence_idx": 60, "label": "unlabeled"}, {"type": "text", "expr": "There has been very little work on learning such shallow models to support human-in-the-loop planning", "word_idx": 4242, "sentence_idx": 61, "label": "unlabeled"}, {"type": "text", "expr": " Some examples include the work on Woogle system  (7)  that aimed to provide support to humans in web-service composition", "word_idx": 4343, "sentence_idx": 62, "label": "unlabeled"}, {"type": "text", "expr": " That work however relied on very primitive understanding of the actions (web services in their case) that consisted merely of learning the input/output types of individual services", "word_idx": 4464, "sentence_idx": 63, "label": "unlabeled"}, {"type": "text", "expr": " In this paper, we focus on learning more informative models that can help recognize the plans under construction by the humans, and provide active support by suggesting relevant actions", "word_idx": 4645, "sentence_idx": 64, "label": "unlabeled"}, {"type": "text", "expr": " To drive this process, we propose two approaches to learning informative models, namely  DUP , standing for  D iscovering  U nderlying  P lans based on action-vector representations, and  RNNPlanner , standing for  R ecurrent  N eural  N etwork based  Planner ", "word_idx": 4831, "sentence_idx": 65, "label": "unlabeled"}, {"type": "text", "expr": " The framework of  DUP  and  RNNPlanner  is shown in Figure  1 , where we take as input a set of plans (or a  plan library ) and learn the distributed representations of actions (namely  action vectors )", "word_idx": 5092, "sentence_idx": 66, "label": "unlabeled"}, {"type": "text", "expr": " After that, our  DUP  approach exploits an EM-Style framework to discover underlying plans based on the learnt action vectors, while our  RNNPlanner  approach exploits an RNN-Style framework to generate plans to best explain observations (i", "word_idx": 5295, "sentence_idx": 67, "label": "unlabeled"}, {"type": "text", "expr": ", discover underlying plans behind the observed actions) based on the learnt action vectors", "word_idx": 5536, "sentence_idx": 68, "label": "unlabeled"}, {"type": "text", "expr": " In  DUP  we consider local contexts (with a limited window size) of actions being recognized, while in  RNNPlanner  we explore the potential influence from long and short-term actions, which can be modelled by RNN, to help recognize unknown actions", "word_idx": 5627, "sentence_idx": 69, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 5876, "sentence_idx": 70, "label": "unlabeled"}, {"type": "text", "expr": "Planner", "word_idx": 5886, "sentence_idx": 71, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 5893, "sentence_idx": 72, "label": "unlabeled"}, {"type": "text", "expr": "plan library", "word_idx": 5903, "sentence_idx": 73, "label": "unlabeled"}, {"type": "text", "expr": "action vectors", "word_idx": 5915, "sentence_idx": 74, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 5929, "sentence_idx": 75, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 5939, "sentence_idx": 76, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:  The framework of our shallow models  DUP  and  RNNPlanner", "word_idx": 5949, "sentence_idx": 77, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:", "word_idx": 6017, "sentence_idx": 78, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 6026, "sentence_idx": 79, "label": "unlabeled"}, {"type": "text", "expr": "In summary, the contributions of the paper are shown below", "word_idx": 6036, "sentence_idx": 80, "label": "unlabeled"}, {"type": "text", "expr": "In summary, the contributions of the paper are shown below", "word_idx": 6094, "sentence_idx": 81, "label": "unlabeled"}, {"type": "text", "expr": "In  (28) , we presented a version of  DUP ", "word_idx": 6152, "sentence_idx": 82, "label": "unlabeled"}, {"type": "text", "expr": " In this paper we extend  (28)  with more details to elaborate the approach", "word_idx": 6194, "sentence_idx": 83, "label": "unlabeled"}, {"type": "text", "expr": "We propose a novel model  RNNPlanner  based on RNN to explore the influence of actions from long and short-term contexts", "word_idx": 6269, "sentence_idx": 84, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 6389, "sentence_idx": 85, "label": "unlabeled"}, {"type": "text", "expr": "We compare  RNNPlanner  to  DUP  to exhibit the advantage and disadvantage of leveraging information from long and short-term contexts", "word_idx": 6399, "sentence_idx": 86, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 6533, "sentence_idx": 87, "label": "unlabeled"}, {"type": "text", "expr": "In the sequel, we first formulate our plan recognition problem, and then address the details of our approaches  DUP  and  RNNPlanner ", "word_idx": 6543, "sentence_idx": 88, "label": "unlabeled"}, {"type": "text", "expr": " After that, we empirically demonstrate that it does capture a surprising amount of structure in the observed plan sequences, leading to effective plan recognition", "word_idx": 6676, "sentence_idx": 89, "label": "unlabeled"}, {"type": "text", "expr": " We further compare its performance to traditional plan recognition techniques, including one that uses the same plan traces to learn the STRIPS-style action models, and use the learned model to support plan recognition", "word_idx": 6839, "sentence_idx": 90, "label": "unlabeled"}, {"type": "text", "expr": " We also compare  RNNPlanner  with  DUP  to see the advantage and disadvantage of leveraging long and short-term contexts of actions in different scenarios", "word_idx": 7058, "sentence_idx": 91, "label": "unlabeled"}, {"type": "text", "expr": " We finally review previous approaches related to our work and conclude our paper with further work", "word_idx": 7213, "sentence_idx": 92, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 7312, "sentence_idx": 93, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 7322, "sentence_idx": 94, "label": "unlabeled"}, {"type": "text", "expr": "2  Problem Formulation", "word_idx": 7332, "sentence_idx": 95, "label": "unlabeled"}, {"type": "text", "expr": "A plan library, denoted by  $\\mathcal{L}$ , is composed of a set of plans  $\\{p\\}$ , where  $p$  is a sequence of actions, i", "word_idx": 7354, "sentence_idx": 96, "label": "unlabeled"}, {"type": "text", "expr": ",  $p=\\langle a_{1},a_{2},\\ldots,a_{n}\\rangle$  where  $a_{i}$ ,  $1\\leq i\\leq n$ , is an action name (without any parameter) represented by a string", "word_idx": 7478, "sentence_idx": 97, "label": "unlabeled"}, {"type": "text", "expr": " For example, a string  unstack-A-B  is an action meaning that  a robot unstacks block A from block B ", "word_idx": 7627, "sentence_idx": 98, "label": "unlabeled"}, {"type": "text", "expr": " We denote the set of all possible actions by  $\\bar{\\mathcal{A}}$  which is assumed to be known beforehand", "word_idx": 7729, "sentence_idx": 99, "label": "unlabeled"}, {"type": "text", "expr": " For ease of presentation, we assume that there is an empty action,  $\\phi$ , indicating an unknown or not observed action, i", "word_idx": 7836, "sentence_idx": 100, "label": "unlabeled"}, {"type": "text", "expr": ",  $\\mathcal{A}=\\bar{\\mathcal{A}}\\cup\\{\\phi\\}$ ", "word_idx": 7961, "sentence_idx": 101, "label": "unlabeled"}, {"type": "text", "expr": " An observation of an  unknown  plan  $\\tilde{p}$  is denoted by  $\\mathcal{O}=\\langle o_{1},o_{2},\\ldots,o_{M}\\rangle$ , where  $o_{i}\\in\\mathcal{A}$ ,  $1\\leq i\\leq M$ , is either an action in  $\\bar{\\mathcal{A}}$  or an empty action  $\\phi$  indicating the corresponding action is missing or not observed", "word_idx": 8008, "sentence_idx": 102, "label": "unlabeled"}, {"type": "text", "expr": " Note that  $\\tilde{p}$  is not necessarily in the plan library  $\\mathcal{L}$ , which makes the plan recognition problem more challenging, since matching the observation to the plan library will not work any more", "word_idx": 8315, "sentence_idx": 103, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}$$", "word_idx": 8528, "sentence_idx": 104, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{p\\}$$", "word_idx": 8539, "sentence_idx": 105, "label": "unlabeled"}, {"type": "math", "expr": "$$p=\\langle a_{1},a_{2},\\ldots,a_{n}\\rangle$$", "word_idx": 8544, "sentence_idx": 106, "label": "unlabeled"}, {"type": "math", "expr": "$$a_{i}$$", "word_idx": 8585, "sentence_idx": 107, "label": "unlabeled"}, {"type": "math", "expr": "$$1\\leq i\\leq n$$", "word_idx": 8590, "sentence_idx": 108, "label": "unlabeled"}, {"type": "text", "expr": "unstack-A-B", "word_idx": 8603, "sentence_idx": 109, "label": "unlabeled"}, {"type": "text", "expr": "a robot unstacks block A from block B", "word_idx": 8614, "sentence_idx": 110, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{\\mathcal{A}}$$", "word_idx": 8651, "sentence_idx": 111, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 8668, "sentence_idx": 112, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{A}=\\bar{\\mathcal{A}}\\cup\\{\\phi\\}$$", "word_idx": 8672, "sentence_idx": 113, "label": "unlabeled"}, {"type": "text", "expr": "unknown", "word_idx": 8713, "sentence_idx": 114, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{p}$$", "word_idx": 8720, "sentence_idx": 115, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}=\\langle o_{1},o_{2},\\ldots,o_{M}\\rangle$$", "word_idx": 8729, "sentence_idx": 116, "label": "unlabeled"}, {"type": "math", "expr": "$$o_{i}\\in\\mathcal{A}$$", "word_idx": 8780, "sentence_idx": 117, "label": "unlabeled"}, {"type": "math", "expr": "$$1\\leq i\\leq M$$", "word_idx": 8799, "sentence_idx": 118, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{\\mathcal{A}}$$", "word_idx": 8812, "sentence_idx": 119, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 8829, "sentence_idx": 120, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{p}$$", "word_idx": 8833, "sentence_idx": 121, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}$$", "word_idx": 8842, "sentence_idx": 122, "label": "unlabeled"}, {"type": "text", "expr": "We assume that the human is making a plan of at most length  $M$ ", "word_idx": 8853, "sentence_idx": 123, "label": "unlabeled"}, {"type": "text", "expr": " We also\nassume that at any given point, the planner is able to observe  $M-k$  of\nthese actions", "word_idx": 8918, "sentence_idx": 124, "label": "unlabeled"}, {"type": "text", "expr": " The  $k$  unobserved actions might either be in the suffiix of the plan, or in the middle", "word_idx": 9014, "sentence_idx": 125, "label": "unlabeled"}, {"type": "text", "expr": "\nOur aim is to suggest, for each of the  $k$  unobserved actions,  $m$ \npossible choices from which the user can select the action", "word_idx": 9104, "sentence_idx": 126, "label": "unlabeled"}, {"type": "text", "expr": " (Note\nthat we would like to keep  $m$  small, ideally close to 1, so as not to\noverwhelm users)", "word_idx": 9234, "sentence_idx": 127, "label": "unlabeled"}, {"type": "text", "expr": "\nAccordingly, we will evaluate the effectiveness of the decision\nsupport in terms of whether or not the user\u2019s best/intended action is\nwithin the suggested  $m$  actions", "word_idx": 9330, "sentence_idx": 128, "label": "unlabeled"}, {"type": "math", "expr": "$$M-k$$", "word_idx": 9499, "sentence_idx": 129, "label": "unlabeled"}, {"type": "text", "expr": "Specifically, our recognition problem can be represented by a triple  $\\Re=(\\mathcal{L},\\mathcal{O},\\mathcal{A})$ ", "word_idx": 9502, "sentence_idx": 130, "label": "unlabeled"}, {"type": "text", "expr": " The solution to  $\\Re$  is to discover the unknown plan  $\\tilde{p}$ , which is a plan with unkwown observations, that best explains  $\\mathcal{O}$  given  $\\mathcal{L}$  and  $\\mathcal{A}$ ", "word_idx": 9616, "sentence_idx": 131, "label": "unlabeled"}, {"type": "text", "expr": " We have the following assumptions  A1-A3 :", "word_idx": 9807, "sentence_idx": 132, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Re=(\\mathcal{L},\\mathcal{O},\\mathcal{A})$$", "word_idx": 9850, "sentence_idx": 133, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Re$$", "word_idx": 9891, "sentence_idx": 134, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{p}$$", "word_idx": 9894, "sentence_idx": 135, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 9903, "sentence_idx": 136, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}$$", "word_idx": 9914, "sentence_idx": 137, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{A}$$", "word_idx": 9925, "sentence_idx": 138, "label": "unlabeled"}, {"type": "text", "expr": "A1-A3", "word_idx": 9936, "sentence_idx": 139, "label": "unlabeled"}, {"type": "text", "expr": "The length of the underlying plan to be discovered is known, which releases us from searching unlimited length of plans", "word_idx": 9941, "sentence_idx": 140, "label": "unlabeled"}, {"type": "text", "expr": "The length of the underlying plan to be discovered is known, which releases us from searching unlimited length of plans", "word_idx": 10060, "sentence_idx": 141, "label": "unlabeled"}, {"type": "text", "expr": "The positions of missing actions in the underlying plan is known in advance, which releases us from searching missing actions in between observed actions", "word_idx": 10179, "sentence_idx": 142, "label": "unlabeled"}, {"type": "text", "expr": "The positions of missing actions in the underlying plan is known in advance, which releases us from searching missing actions in between observed actions", "word_idx": 10332, "sentence_idx": 143, "label": "unlabeled"}, {"type": "text", "expr": "All actions observed are assumed to be correct, which indicates there is no need to criticize or rectify the observed actions", "word_idx": 10485, "sentence_idx": 144, "label": "unlabeled"}, {"type": "text", "expr": "All actions observed are assumed to be correct, which indicates there is no need to criticize or rectify the observed actions", "word_idx": 10610, "sentence_idx": 145, "label": "unlabeled"}, {"type": "text", "expr": "An example of our plan recognition problem in the  blocks  domain is shown below", "word_idx": 10735, "sentence_idx": 146, "label": "unlabeled"}, {"type": "text", "expr": "blocks", "word_idx": 10815, "sentence_idx": 147, "label": "unlabeled"}, {"type": "text", "expr": "1 http://www", "word_idx": 10821, "sentence_idx": 148, "label": "unlabeled"}, {"type": "text", "expr": "toronto", "word_idx": 10833, "sentence_idx": 149, "label": "unlabeled"}, {"type": "text", "expr": "edu/aips2000/", "word_idx": 10840, "sentence_idx": 150, "label": "unlabeled"}, {"type": "text", "expr": "Example:  A plan library  $\\mathcal{L}$  in the  blocks  domain is assumed to have four plans as shown below:", "word_idx": 10853, "sentence_idx": 151, "label": "unlabeled"}, {"type": "text", "expr": "Example:", "word_idx": 10962, "sentence_idx": 152, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}$$", "word_idx": 10970, "sentence_idx": 153, "label": "unlabeled"}, {"type": "text", "expr": "blocks", "word_idx": 10981, "sentence_idx": 154, "label": "unlabeled"}, {"type": "text", "expr": "plan 1 :  pick-up-B stack-B-A pick-up-D stack-D-C", "word_idx": 10987, "sentence_idx": 155, "label": "unlabeled"}, {"type": "text", "expr": "plan 1", "word_idx": 11036, "sentence_idx": 156, "label": "unlabeled"}, {"type": "text", "expr": "pick-up-B stack-B-A pick-up-D stack-D-C", "word_idx": 11042, "sentence_idx": 157, "label": "unlabeled"}, {"type": "text", "expr": "plan 2 :  unstack-B-A put-down-B unstack-D-C put-down-D", "word_idx": 11081, "sentence_idx": 158, "label": "unlabeled"}, {"type": "text", "expr": "plan 2", "word_idx": 11136, "sentence_idx": 159, "label": "unlabeled"}, {"type": "text", "expr": "unstack-B-A put-down-B unstack-D-C put-down-D", "word_idx": 11142, "sentence_idx": 160, "label": "unlabeled"}, {"type": "text", "expr": "plan 3 :  pick-up-B stack-B-A pick-up-C stack-C-B pick-up-D stack-D-C", "word_idx": 11187, "sentence_idx": 161, "label": "unlabeled"}, {"type": "text", "expr": "plan 3", "word_idx": 11256, "sentence_idx": 162, "label": "unlabeled"}, {"type": "text", "expr": "pick-up-B stack-B-A pick-up-C stack-C-B pick-up-D stack-D-C", "word_idx": 11262, "sentence_idx": 163, "label": "unlabeled"}, {"type": "text", "expr": "plan 4 :  unstack-D-C put-down-D unstack-C-B put-down-C unstack-B-A put-down-B", "word_idx": 11321, "sentence_idx": 164, "label": "unlabeled"}, {"type": "text", "expr": "plan 4", "word_idx": 11399, "sentence_idx": 165, "label": "unlabeled"}, {"type": "text", "expr": "unstack-D-C put-down-D unstack-C-B put-down-C unstack-B-A put-down-B", "word_idx": 11405, "sentence_idx": 166, "label": "unlabeled"}, {"type": "text", "expr": "An observation  $\\mathcal{O}$  of action sequence is shown below:", "word_idx": 11473, "sentence_idx": 167, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 11538, "sentence_idx": 168, "label": "unlabeled"}, {"type": "text", "expr": "observation:", "word_idx": 11549, "sentence_idx": 169, "label": "unlabeled"}, {"type": "text", "expr": "observation:", "word_idx": 11561, "sentence_idx": 170, "label": "unlabeled"}, {"type": "text", "expr": "pick-up-B  $\\phi$  unstack-D-C put-down-D  $\\phi$  stack-C-B  $\\phi$   $\\phi$", "word_idx": 11573, "sentence_idx": 171, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 11650, "sentence_idx": 172, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 11654, "sentence_idx": 173, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 11658, "sentence_idx": 174, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 11662, "sentence_idx": 175, "label": "unlabeled"}, {"type": "text", "expr": "Given the above input, our  DUP  algorithm outputs plans as follows:", "word_idx": 11666, "sentence_idx": 176, "label": "unlabeled"}, {"type": "text", "expr": "pick-up-B stack-B-A unstack-D-C put-down-D pick-up-C stack-C-B pick-up-D stack-D-C", "word_idx": 11734, "sentence_idx": 177, "label": "unlabeled"}, {"type": "text", "expr": "pick-up-B stack-B-A unstack-D-C put-down-D pick-up-C stack-C-B pick-up-D stack-D-C", "word_idx": 11816, "sentence_idx": 178, "label": "unlabeled"}, {"type": "text", "expr": "pick-up-B stack-B-A unstack-D-C put-down-D pick-up-C stack-C-B pick-up-D stack-D-C", "word_idx": 11898, "sentence_idx": 179, "label": "unlabeled"}, {"type": "text", "expr": "Although the \u201cplan completion\u201d problem seems to differ superficially\nfrom the traditional \u201cplan recognition\u201d problem, we point out that\nmany earlier works on plan recognition do in fact evaluate their\nrecognition algorithms in terms of completion tasks, e", "word_idx": 11980, "sentence_idx": 180, "label": "unlabeled"}, {"type": "text", "expr": " While\nthese earlier efforts use different problem settings, taking either a\nplan library or action models as input, they share one common\ncharacteristic: they all aim to look for a plan that can best explain\n(or complete) the observed actions", "word_idx": 12235, "sentence_idx": 181, "label": "unlabeled"}, {"type": "text", "expr": " This is exactly the same as our\nproblem we aim to solve", "word_idx": 12478, "sentence_idx": 182, "label": "unlabeled"}, {"type": "text", "expr": "3  Learning the distributed representations of actions", "word_idx": 12534, "sentence_idx": 183, "label": "unlabeled"}, {"type": "text", "expr": "Since actions are denoted by a name string, actions can be viewed as words, and a plan can be viewed as a sentence", "word_idx": 12588, "sentence_idx": 184, "label": "unlabeled"}, {"type": "text", "expr": " Furthermore, the plan library  $\\mathcal{L}$  can be seen as a corpus, and the set of all possible actions  $\\mathcal{A}$  is the vocabulary", "word_idx": 12702, "sentence_idx": 185, "label": "unlabeled"}, {"type": "text", "expr": " Given a plan corpus, we can exploit off-the-shelf approaches, e", "word_idx": 12843, "sentence_idx": 186, "label": "unlabeled"}, {"type": "text", "expr": ", the Skip-gram model  (20) , for learning vector representations for actions", "word_idx": 12907, "sentence_idx": 187, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}$$", "word_idx": 12984, "sentence_idx": 188, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{A}$$", "word_idx": 12995, "sentence_idx": 189, "label": "unlabeled"}, {"type": "text", "expr": "The objective of the Skip-gram model is to learn vector representations for predicting the surrounding words in a sentence or document", "word_idx": 13006, "sentence_idx": 190, "label": "unlabeled"}, {"type": "text", "expr": " Given a corpus  $\\mathcal{C}$ , composed of a sequence of training words  $\\langle w_{1},w_{2},\\ldots,w_{T}\\rangle$ , where  $T=|\\mathcal{C}|$ , the Skip-gram model maximizes the average log probability", "word_idx": 13140, "sentence_idx": 191, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{C}$$", "word_idx": 13343, "sentence_idx": 192, "label": "unlabeled"}, {"type": "math", "expr": "$$\\langle w_{1},w_{2},\\ldots,w_{T}\\rangle$$", "word_idx": 13354, "sentence_idx": 193, "label": "unlabeled"}, {"type": "math", "expr": "$$T=|\\mathcal{C}|$$", "word_idx": 13393, "sentence_idx": 194, "label": "unlabeled"}, {"type": "text", "expr": "$\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{-c\\leq j\\leq c,j\\neq 0}\\log p(w_{t+j}|w_{t})$", "word_idx": 13408, "sentence_idx": 195, "label": "unlabeled"}, {"type": "math", "expr": "$$\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{-c\\leq j\\leq c,j\\neq 0}\\log p(w_{t+j}|w_{t})$$", "word_idx": 13485, "sentence_idx": 196, "label": "unlabeled"}, {"type": "text", "expr": "where  $c$  is the size of the training window or context", "word_idx": 13560, "sentence_idx": 197, "label": "unlabeled"}, {"type": "text", "expr": "where", "word_idx": 13617, "sentence_idx": 198, "label": "unlabeled"}, {"type": "text", "expr": "The basic probability  $p(w_{t+j}|w_{t})$  is defined by the hierarchical softmax, which uses a binary tree representation of the output layer with the  $K$  words as its leaves and for each node, explicitly represents the relative probabilities of its child nodes  (20) ", "word_idx": 13622, "sentence_idx": 199, "label": "unlabeled"}, {"type": "text", "expr": " For each leaf node, there is an unique path from the root to the node, and this path is used to estimate the probability of the word represented by the leaf node", "word_idx": 13893, "sentence_idx": 200, "label": "unlabeled"}, {"type": "text", "expr": " There are no explicit output vector representations for words", "word_idx": 14055, "sentence_idx": 201, "label": "unlabeled"}, {"type": "text", "expr": " Instead, each inner node has an output vector  $v^{\\prime}_{n(w,j)}$ , and the probability of a word being the output word is defined by", "word_idx": 14117, "sentence_idx": 202, "label": "unlabeled"}, {"type": "math", "expr": "$$p(w_{t+j}|w_{t})$$", "word_idx": 14254, "sentence_idx": 203, "label": "unlabeled"}, {"type": "math", "expr": "$$v^{\\prime}_{n(w,j)}$$", "word_idx": 14270, "sentence_idx": 204, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle p(w_{t+j}|w_{t})=\\prod_{i=1}^{L(w_{t+j})-1}\\Big{\\{}\\sigma(%\n\\mathbb{I}(n(w_{t+j},i+1)=$", "word_idx": 14289, "sentence_idx": 205, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle p(w_{t+j}|w_{t})=\\prod_{i=1}^{L(w_{t+j})-1}\\Big{\\{}\\sigma(%\n\\mathbb{I}(n(w_{t+j},i+1)=$$", "word_idx": 14391, "sentence_idx": 206, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle child(n(w_{t+j},i)))\\cdot v_{n(w_{t+j},i)}\\cdot v_{w_{t}})\\Big{%\n\\}},$", "word_idx": 14491, "sentence_idx": 207, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle child(n(w_{t+j},i)))\\cdot v_{n(w_{t+j},i)}\\cdot v_{w_{t}})\\Big{%\n\\}},$$", "word_idx": 14576, "sentence_idx": 208, "label": "unlabeled"}, {"type": "text", "expr": "where", "word_idx": 14659, "sentence_idx": 209, "label": "unlabeled"}, {"type": "text", "expr": "$\\sigma(x)=1/(1+\\exp(-x))$", "word_idx": 14664, "sentence_idx": 210, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma(x)=1/(1+\\exp(-x)).$$", "word_idx": 14690, "sentence_idx": 211, "label": "unlabeled"}, {"type": "text", "expr": "$L(w)$  is the length from the root to the word  $w$  in the binary tree, e", "word_idx": 14715, "sentence_idx": 212, "label": "unlabeled"}, {"type": "text", "expr": ",  $L(w)=4$  if there are four nodes from the root to  $w$ ", "word_idx": 14790, "sentence_idx": 213, "label": "unlabeled"}, {"type": "text", "expr": "  $n(w,i)$  is the  $i$ th node from the root to  $w$ , e", "word_idx": 14849, "sentence_idx": 214, "label": "unlabeled"}, {"type": "text", "expr": ",  $n(w,1)=root$  and  $n(w,L(w))=w$ ", "word_idx": 14906, "sentence_idx": 215, "label": "unlabeled"}, {"type": "text", "expr": "  $child(n)$  is a fixed child (e", "word_idx": 14943, "sentence_idx": 216, "label": "unlabeled"}, {"type": "text", "expr": ", left child) of node  $n$ ", "word_idx": 14976, "sentence_idx": 217, "label": "unlabeled"}, {"type": "text", "expr": "  $v_{n}$  is the vector representation of the inner node  $n$ ", "word_idx": 15003, "sentence_idx": 218, "label": "unlabeled"}, {"type": "text", "expr": "  $v_{w_{t}}$  is the input vector representation of word  $w_{t}$ ", "word_idx": 15066, "sentence_idx": 219, "label": "unlabeled"}, {"type": "text", "expr": " The identity function  $\\mathbb{I}(x)$  is 1 if  $x$  is true; otherwise it is -1", "word_idx": 15133, "sentence_idx": 220, "label": "unlabeled"}, {"type": "math", "expr": "$$L(w)$$", "word_idx": 15215, "sentence_idx": 221, "label": "unlabeled"}, {"type": "math", "expr": "$$L(w)=4$$", "word_idx": 15219, "sentence_idx": 222, "label": "unlabeled"}, {"type": "math", "expr": "$$n(w,i)$$", "word_idx": 15225, "sentence_idx": 223, "label": "unlabeled"}, {"type": "math", "expr": "$$n(w,1)=root$$", "word_idx": 15231, "sentence_idx": 224, "label": "unlabeled"}, {"type": "math", "expr": "$$n(w,L(w))=w$$", "word_idx": 15242, "sentence_idx": 225, "label": "unlabeled"}, {"type": "math", "expr": "$$child(n)$$", "word_idx": 15253, "sentence_idx": 226, "label": "unlabeled"}, {"type": "math", "expr": "$$v_{n}$$", "word_idx": 15261, "sentence_idx": 227, "label": "unlabeled"}, {"type": "math", "expr": "$$v_{w_{t}}$$", "word_idx": 15266, "sentence_idx": 228, "label": "unlabeled"}, {"type": "math", "expr": "$$w_{t}$$", "word_idx": 15275, "sentence_idx": 229, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbb{I}(x)$$", "word_idx": 15280, "sentence_idx": 230, "label": "unlabeled"}, {"type": "text", "expr": "We can thus build vector representations of actions by maximizing Equation ( 1 ) with corpora or plan libraries  $\\mathcal{L}$  as input", "word_idx": 15293, "sentence_idx": 231, "label": "unlabeled"}, {"type": "text", "expr": " We will exploit the vector representations to discover the unknown plan  $\\tilde{p}$  in the next subsection", "word_idx": 15429, "sentence_idx": 232, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}$$", "word_idx": 15538, "sentence_idx": 233, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{p}$$", "word_idx": 15549, "sentence_idx": 234, "label": "unlabeled"}, {"type": "text", "expr": "4  Our  DUP  Algorithm", "word_idx": 15558, "sentence_idx": 235, "label": "unlabeled"}, {"type": "text", "expr": "Our  DUP  approach to the recognition problem  $\\Re$  functions by two phases", "word_idx": 15580, "sentence_idx": 236, "label": "unlabeled"}, {"type": "text", "expr": " We first learn vector representations of actions using the plan library  $\\mathcal{L}$ ", "word_idx": 15657, "sentence_idx": 237, "label": "unlabeled"}, {"type": "text", "expr": " We then iteratively sample actions for unobserved actions  $o_{i}$  by maximizing the probability of the unknown plan  $\\tilde{p}$  via the EM framework", "word_idx": 15745, "sentence_idx": 238, "label": "unlabeled"}, {"type": "text", "expr": " We present  DUP  in detail in the following subsections", "word_idx": 15898, "sentence_idx": 239, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Re$$", "word_idx": 15954, "sentence_idx": 240, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}$$", "word_idx": 15957, "sentence_idx": 241, "label": "unlabeled"}, {"type": "math", "expr": "$$o_{i}$$", "word_idx": 15968, "sentence_idx": 242, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{p}$$", "word_idx": 15973, "sentence_idx": 243, "label": "unlabeled"}, {"type": "text", "expr": "1  Maximizing Probability of Unknown Plans", "word_idx": 15982, "sentence_idx": 244, "label": "unlabeled"}, {"type": "text", "expr": "With the vector representations learnt in the last subsection, a straightforward way to discover the unknown plan  $\\tilde{p}$  is to explore all possible actions in  $\\bar{\\mathcal{A}}$  such that  $\\tilde{p}$  has the highest probability, which can be defined similar to Equation ( 1 ), i", "word_idx": 16024, "sentence_idx": 245, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{p}$$", "word_idx": 16314, "sentence_idx": 246, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{\\mathcal{A}}$$", "word_idx": 16323, "sentence_idx": 247, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{p}$$", "word_idx": 16340, "sentence_idx": 248, "label": "unlabeled"}, {"type": "text", "expr": "$\\mathcal{F}(\\tilde{p})=\\sum_{k=1}^{M}\\sum_{-c\\leq j\\leq c,j\\neq 0}\\log p(w_{k+%\nj}|w_{k})$", "word_idx": 16349, "sentence_idx": 249, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{F}(\\tilde{p})=\\sum_{k=1}^{M}\\sum_{-c\\leq j\\leq c,j\\neq 0}\\log p(w_{k+%\nj}|w_{k})$$", "word_idx": 16440, "sentence_idx": 250, "label": "unlabeled"}, {"type": "text", "expr": "where  $w_{k}$  denotes the  $k$ th action of  $\\tilde{p}$  and  $M$  is the length of  $\\tilde{p}$ ", "word_idx": 16529, "sentence_idx": 251, "label": "unlabeled"}, {"type": "text", "expr": " As we can see, this approach is exponentially hard with respect to the size of  $\\bar{\\mathcal{A}}$  and number of unobserved actions", "word_idx": 16629, "sentence_idx": 252, "label": "unlabeled"}, {"type": "text", "expr": " We thus design an approximate approach in the Expectation-Maximization framework to estimate an unknown plan  $\\tilde{p}$  that best explains the observation  $\\mathcal{O}$ ", "word_idx": 16763, "sentence_idx": 253, "label": "unlabeled"}, {"type": "math", "expr": "$$w_{k}$$", "word_idx": 16937, "sentence_idx": 254, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{p}$$", "word_idx": 16942, "sentence_idx": 255, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{p}$$", "word_idx": 16951, "sentence_idx": 256, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{\\mathcal{A}}$$", "word_idx": 16960, "sentence_idx": 257, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{p}$$", "word_idx": 16977, "sentence_idx": 258, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 16986, "sentence_idx": 259, "label": "unlabeled"}, {"type": "text", "expr": "To do this, we introduce new parameters to capture \u201cweights\u201d of values for each unobserved action", "word_idx": 16997, "sentence_idx": 260, "label": "unlabeled"}, {"type": "text", "expr": " Specifically speaking, assuming there are  $X$  unobserved actions in  $\\mathcal{O}$ , i", "word_idx": 17094, "sentence_idx": 261, "label": "unlabeled"}, {"type": "text", "expr": ", the number of  $\\phi$ s in  $\\mathcal{O}$  is  $X$ , we denote these unobserved actions by  $\\bar{a}_{1},,\\bar{a}_{x},,\\bar{a}_{X}$ , where the indices indicate the order they appear in  $\\mathcal{O}$ ", "word_idx": 17183, "sentence_idx": 262, "label": "unlabeled"}, {"type": "text", "expr": " Note that each  $\\bar{a}_{x}$  can be any action in  $\\bar{\\mathcal{A}}$ ", "word_idx": 17386, "sentence_idx": 263, "label": "unlabeled"}, {"type": "text", "expr": " We associate each possible value of  $\\bar{a}_{x}$  with a weight, denoted by  $\\bar{\\Gamma}_{\\bar{a}_{x},x}$ ", "word_idx": 17460, "sentence_idx": 264, "label": "unlabeled"}, {"type": "text", "expr": "  $\\bar{\\Gamma}$  is a  $|\\bar{\\mathcal{A}}|\\times X$  matrix, satisfying", "word_idx": 17571, "sentence_idx": 265, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 17644, "sentence_idx": 266, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 17655, "sentence_idx": 267, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 17659, "sentence_idx": 268, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{a}_{1},...,\\bar{a}_{x},...,\\bar{a}_{X}$$", "word_idx": 17670, "sentence_idx": 269, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 17713, "sentence_idx": 270, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{a}_{x}$$", "word_idx": 17724, "sentence_idx": 271, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{\\mathcal{A}}$$", "word_idx": 17735, "sentence_idx": 272, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{a}_{x}$$", "word_idx": 17752, "sentence_idx": 273, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{\\Gamma}_{\\bar{a}_{x},x}$$", "word_idx": 17763, "sentence_idx": 274, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{\\Gamma}$$", "word_idx": 17791, "sentence_idx": 275, "label": "unlabeled"}, {"type": "math", "expr": "$$|\\bar{\\mathcal{A}}|\\times X$$", "word_idx": 17803, "sentence_idx": 276, "label": "unlabeled"}, {"type": "text", "expr": "$\\sum_{o\\in\\bar{\\mathcal{A}}}\\bar{\\Gamma}_{o,x}=1,$", "word_idx": 17830, "sentence_idx": 277, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sum_{o\\in\\bar{\\mathcal{A}}}\\bar{\\Gamma}_{o,x}=1,$$", "word_idx": 17881, "sentence_idx": 278, "label": "unlabeled"}, {"type": "text", "expr": "where  $\\bar{\\Gamma}_{o,x}\\geq 0$  for each  $x$ ", "word_idx": 17930, "sentence_idx": 279, "label": "unlabeled"}, {"type": "text", "expr": " For the ease of specification, we extend  $\\bar{\\Gamma}$  to a bigger matrix with a size of  $|\\bar{\\mathcal{A}}|\\times M$ , denoted by  $\\Gamma$ , such that  $\\Gamma_{o,y}=\\bar{\\Gamma}_{o,x}$  if  $y$  is the index of the  $x$ th unobserved action in  $\\mathcal{O}$ , for all  $o\\in\\bar{\\mathcal{A}}$ ; otherwise,  $\\Gamma_{o,y}=1$  and  $\\Gamma_{o^{\\prime},y}=0$  for all  $o^{\\prime}\\in\\bar{\\mathcal{A}}\\wedge o^{\\prime}\\neq o$ ", "word_idx": 17979, "sentence_idx": 280, "label": "unlabeled"}, {"type": "text", "expr": " Our intuition is to estimate the unknown plan  $\\tilde{p}$  by selecting actions with the highest weights", "word_idx": 18411, "sentence_idx": 281, "label": "unlabeled"}, {"type": "text", "expr": " We thus introduce the weights to Equation ( 3 ), as shown below,", "word_idx": 18517, "sentence_idx": 282, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{\\Gamma}_{o,x}\\geq 0$$", "word_idx": 18582, "sentence_idx": 283, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{\\Gamma}$$", "word_idx": 18606, "sentence_idx": 284, "label": "unlabeled"}, {"type": "math", "expr": "$$|\\bar{\\mathcal{A}}|\\times M$$", "word_idx": 18618, "sentence_idx": 285, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma$$", "word_idx": 18645, "sentence_idx": 286, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{o,y}=\\bar{\\Gamma}_{o,x}$$", "word_idx": 18651, "sentence_idx": 287, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 18682, "sentence_idx": 288, "label": "unlabeled"}, {"type": "math", "expr": "$$o\\in\\bar{\\mathcal{A}}$$", "word_idx": 18693, "sentence_idx": 289, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{o,y}=1$$", "word_idx": 18714, "sentence_idx": 290, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{o^{\\prime},y}=0$$", "word_idx": 18728, "sentence_idx": 291, "label": "unlabeled"}, {"type": "math", "expr": "$$o^{\\prime}\\in\\bar{\\mathcal{A}}\\wedge o^{\\prime}\\neq o$$", "word_idx": 18751, "sentence_idx": 292, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{p}$$", "word_idx": 18804, "sentence_idx": 293, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle p(w_{k+j}|w_{k})=\\prod_{i=1}^{L(w_{k+j})-1}\\Big{\\{}\\sigma(%\n\\mathbb{I}(n(w_{k+j},i+1)=$", "word_idx": 18813, "sentence_idx": 294, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle p(w_{k+j}|w_{k})=\\prod_{i=1}^{L(w_{k+j})-1}\\Big{\\{}\\sigma(%\n\\mathbb{I}(n(w_{k+j},i+1)=$$", "word_idx": 18915, "sentence_idx": 295, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle child(n(w_{k+j},i)))\\cdot av_{n(w_{k+j},i)}\\cdot bv_{w_{k}})\\Big%\n{\\}},$", "word_idx": 19015, "sentence_idx": 296, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle child(n(w_{k+j},i)))\\cdot av_{n(w_{k+j},i)}\\cdot bv_{w_{k}})\\Big%\n{\\}},$$", "word_idx": 19102, "sentence_idx": 297, "label": "unlabeled"}, {"type": "text", "expr": "where  $a=\\Gamma_{w_{k+j},k+j}$  and  $b=\\Gamma_{w_{k},k}$ ", "word_idx": 19187, "sentence_idx": 298, "label": "unlabeled"}, {"type": "text", "expr": " We can see that the impact of  $w_{k+j}$  and  $w_{k}$  is penalized by weights  $a$  and  $b$  if they are unobserved actions, and stays unchanged, otherwise (since both  $a$  and  $b$  equal to 1 if they are observed actions)", "word_idx": 19246, "sentence_idx": 299, "label": "unlabeled"}, {"type": "math", "expr": "$$a=\\Gamma_{w_{k+j},k+j}$$", "word_idx": 19474, "sentence_idx": 300, "label": "unlabeled"}, {"type": "math", "expr": "$$b=\\Gamma_{w_{k},k}$$", "word_idx": 19496, "sentence_idx": 301, "label": "unlabeled"}, {"type": "math", "expr": "$$w_{k+j}$$", "word_idx": 19514, "sentence_idx": 302, "label": "unlabeled"}, {"type": "math", "expr": "$$w_{k}$$", "word_idx": 19521, "sentence_idx": 303, "label": "unlabeled"}, {"type": "text", "expr": "We assume  $X\\sim Multinomial(\\Gamma_{\\cdot,x})$ , i", "word_idx": 19526, "sentence_idx": 304, "label": "unlabeled"}, {"type": "text", "expr": ",  $p(X=o)=\\Gamma_{o,x}$ , where  $\\Gamma_{o,x}\\geq 0$  and  $\\sum_{a\\in\\bar{\\mathcal{A}}}\\eta^{a}=1$ ", "word_idx": 19578, "sentence_idx": 305, "label": "unlabeled"}, {"type": "text", "expr": "  $P(\\tilde{p}|\\Gamma)=\\prod_{x}\\Gamma_{o,x}$", "word_idx": 19680, "sentence_idx": 306, "label": "unlabeled"}, {"type": "math", "expr": "$$X\\sim Multinomial(\\Gamma_{\\cdot,x})$$", "word_idx": 19725, "sentence_idx": 307, "label": "unlabeled"}, {"type": "math", "expr": "$$p(X=o)=\\Gamma_{o,x}$$", "word_idx": 19760, "sentence_idx": 308, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{o,x}\\geq 0$$", "word_idx": 19779, "sentence_idx": 309, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sum_{a\\in\\bar{\\mathcal{A}}}\\eta^{a}=1$$", "word_idx": 19797, "sentence_idx": 310, "label": "unlabeled"}, {"type": "math", "expr": "$$P(\\tilde{p}|\\Gamma)=\\prod_{x}\\Gamma_{o,x}$$", "word_idx": 19835, "sentence_idx": 311, "label": "unlabeled"}, {"type": "text", "expr": "We redefine the objective function as shown below,", "word_idx": 19876, "sentence_idx": 312, "label": "unlabeled"}, {"type": "text", "expr": "We redefine the objective function as shown below,", "word_idx": 19926, "sentence_idx": 313, "label": "unlabeled"}, {"type": "text", "expr": "$\\mathcal{F}(\\tilde{p},\\Gamma)=\\sum_{k=1}^{M}\\sum_{-c\\leq j\\leq c,j\\neq 0}\\log p%\n(w_{k+j}|w_{k}),$", "word_idx": 19976, "sentence_idx": 314, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{F}(\\tilde{p},\\Gamma)=\\sum_{k=1}^{M}\\sum_{-c\\leq j\\leq c,j\\neq 0}\\log p%\n(w_{k+j}|w_{k}),$$", "word_idx": 20075, "sentence_idx": 315, "label": "unlabeled"}, {"type": "text", "expr": "where  $p(w_{k+j}|w_{k})$  is defined by Equation ( 4", "word_idx": 20172, "sentence_idx": 316, "label": "unlabeled"}, {"type": "text", "expr": " The gradient of Equation  5  is shown below,", "word_idx": 20225, "sentence_idx": 317, "label": "unlabeled"}, {"type": "math", "expr": "$$p(w_{k+j}|w_{k})$$", "word_idx": 20270, "sentence_idx": 318, "label": "unlabeled"}, {"type": "text", "expr": "$\\frac{\\partial\\mathcal{F}}{\\partial\\Gamma_{o,x}}=\\frac{4c(L(o)-1)}{\\Gamma_{o,x%\n}}$", "word_idx": 20286, "sentence_idx": 319, "label": "unlabeled"}, {"type": "math", "expr": "$$\\frac{\\partial\\mathcal{F}}{\\partial\\Gamma_{o,x}}=\\frac{4c(L(o)-1)}{\\Gamma_{o,x%\n}}.$$", "word_idx": 20370, "sentence_idx": 320, "label": "unlabeled"}, {"type": "text", "expr": "The only parameters needed to be updated are  $\\Gamma$ , which can be easily done by gradient descent, as shown below,", "word_idx": 20453, "sentence_idx": 321, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma$$", "word_idx": 20571, "sentence_idx": 322, "label": "unlabeled"}, {"type": "text", "expr": "$\\Gamma_{o,x}=\\Gamma_{o,x}+\\delta\\frac{\\partial\\mathcal{F}}{\\partial\\Gamma_{o,x%\n}},$", "word_idx": 20577, "sentence_idx": 323, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{o,x}=\\Gamma_{o,x}+\\delta\\frac{\\partial\\mathcal{F}}{\\partial\\Gamma_{o,x%\n}},$$", "word_idx": 20662, "sentence_idx": 324, "label": "unlabeled"}, {"type": "text", "expr": "if  $x$  is the index of unobserved action in  $\\mathcal{O}$ ; otherwise,  $\\Gamma_{o,x}$  stays unchanged, i", "word_idx": 20745, "sentence_idx": 325, "label": "unlabeled"}, {"type": "text", "expr": ",  $\\Gamma_{o,x}=1$ ", "word_idx": 20854, "sentence_idx": 326, "label": "unlabeled"}, {"type": "text", "expr": " Note that  $\\delta$  is a learning constant", "word_idx": 20874, "sentence_idx": 327, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 20918, "sentence_idx": 328, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{o,x}$$", "word_idx": 20929, "sentence_idx": 329, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{o,x}=1$$", "word_idx": 20941, "sentence_idx": 330, "label": "unlabeled"}, {"type": "math", "expr": "$$\\delta$$", "word_idx": 20955, "sentence_idx": 331, "label": "unlabeled"}, {"type": "text", "expr": "With Equation ( 7 ), we can design an EM algorithm by repeatedly sampling an unknown plan according to  $\\Gamma$  and updating  $\\Gamma$  based on Equation ( 7 ) until reaching convergence (e", "word_idx": 20961, "sentence_idx": 332, "label": "unlabeled"}, {"type": "text", "expr": ", a constant number of repetitions is reached)", "word_idx": 21152, "sentence_idx": 333, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma$$", "word_idx": 21198, "sentence_idx": 334, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma$$", "word_idx": 21204, "sentence_idx": 335, "label": "unlabeled"}, {"type": "text", "expr": "2  Overview of our  DUP  approach", "word_idx": 21210, "sentence_idx": 336, "label": "unlabeled"}, {"type": "text", "expr": "An overview of our  DUP  algorithm is shown in Algorithm  1 ", "word_idx": 21243, "sentence_idx": 337, "label": "unlabeled"}, {"type": "text", "expr": " In Step 2 of Algorithm  1 , we initialize  $\\Gamma_{o,k}=1/M$  for all  $o\\in\\bar{\\mathcal{A}}$ , if  $k$  is an index of unobserved actions in  $\\mathcal{O}$ ; and otherwise,  $\\Gamma_{o,k}=1$  and  $\\Gamma_{o^{\\prime},k}=0$  for all  $o^{\\prime}\\in\\bar{\\mathcal{A}}\\wedge o^{\\prime}\\neq o$ ", "word_idx": 21303, "sentence_idx": 338, "label": "unlabeled"}, {"type": "text", "expr": " In Step 4, we view  $\\Gamma_{\\cdot,k}$  as a probability distribution, and sample an action from  $\\bar{\\mathcal{A}}$  based on  $\\Gamma_{\\cdot,k}$  if  $k$  is an unobserved action index in  $\\mathcal{O}$ ", "word_idx": 21596, "sentence_idx": 339, "label": "unlabeled"}, {"type": "text", "expr": " In Step 5, we only update  $\\Gamma_{\\cdot,k}$  where  $k$  is an unobserved action index", "word_idx": 21803, "sentence_idx": 340, "label": "unlabeled"}, {"type": "text", "expr": " In Step 6, we linearly project all elements of the updated  $\\Gamma$  to between 0 and 1, such that we can do sampling directly based on  $\\Gamma$  in Step 4", "word_idx": 21892, "sentence_idx": 341, "label": "unlabeled"}, {"type": "text", "expr": " In Step 8, we simply select  $\\bar{a}_{x}$  based on", "word_idx": 22050, "sentence_idx": 342, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{o,k}=1/M$$", "word_idx": 22103, "sentence_idx": 343, "label": "unlabeled"}, {"type": "math", "expr": "$$o\\in\\bar{\\mathcal{A}}$$", "word_idx": 22119, "sentence_idx": 344, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 22140, "sentence_idx": 345, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{o,k}=1$$", "word_idx": 22151, "sentence_idx": 346, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{o^{\\prime},k}=0$$", "word_idx": 22165, "sentence_idx": 347, "label": "unlabeled"}, {"type": "math", "expr": "$$o^{\\prime}\\in\\bar{\\mathcal{A}}\\wedge o^{\\prime}\\neq o$$", "word_idx": 22188, "sentence_idx": 348, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{\\cdot,k}$$", "word_idx": 22241, "sentence_idx": 349, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{\\mathcal{A}}$$", "word_idx": 22257, "sentence_idx": 350, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{\\cdot,k}$$", "word_idx": 22274, "sentence_idx": 351, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 22290, "sentence_idx": 352, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{\\cdot,k}$$", "word_idx": 22301, "sentence_idx": 353, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma$$", "word_idx": 22317, "sentence_idx": 354, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma$$", "word_idx": 22323, "sentence_idx": 355, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{a}_{x}$$", "word_idx": 22329, "sentence_idx": 356, "label": "unlabeled"}, {"type": "text", "expr": "$\\bar{a}_{x}=\\arg\\max_{o\\in\\bar{\\mathcal{A}}}\\Gamma_{o,x},$", "word_idx": 22340, "sentence_idx": 357, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{a}_{x}=\\arg\\max_{o\\in\\bar{\\mathcal{A}}}\\Gamma_{o,x},$$", "word_idx": 22399, "sentence_idx": 358, "label": "unlabeled"}, {"type": "text", "expr": "for all unobserved action index  $x$ ", "word_idx": 22456, "sentence_idx": 359, "label": "unlabeled"}, {"type": "text", "expr": "Algorithm\u00a01  Framework of our  DUP  algorithm", "word_idx": 22493, "sentence_idx": 360, "label": "unlabeled"}, {"type": "text", "expr": "Algorithm\u00a01", "word_idx": 22538, "sentence_idx": 361, "label": "unlabeled"}, {"type": "text", "expr": "Input:  plan library  $\\mathcal{L}$ , observed actions  $\\mathcal{O}$ Output:  plan  $\\tilde{p}$", "word_idx": 22549, "sentence_idx": 362, "label": "unlabeled"}, {"type": "text", "expr": "Input:", "word_idx": 22645, "sentence_idx": 363, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}$$", "word_idx": 22651, "sentence_idx": 364, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 22662, "sentence_idx": 365, "label": "unlabeled"}, {"type": "text", "expr": "Output:", "word_idx": 22673, "sentence_idx": 366, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{p}$$", "word_idx": 22680, "sentence_idx": 367, "label": "unlabeled"}, {"type": "text", "expr": "1: \u00a0\u00a0learn vector representation of actions", "word_idx": 22689, "sentence_idx": 368, "label": "unlabeled"}, {"type": "text", "expr": "2: \u00a0\u00a0initialize  $\\Gamma_{o,k}$  with  $1/M$  for all  $o\\in\\bar{\\mathcal{A}}$ , when  $k$  is an unobserved action index", "word_idx": 22732, "sentence_idx": 369, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{o,k}$$", "word_idx": 22853, "sentence_idx": 370, "label": "unlabeled"}, {"type": "math", "expr": "$$1/M$$", "word_idx": 22865, "sentence_idx": 371, "label": "unlabeled"}, {"type": "math", "expr": "$$o\\in\\bar{\\mathcal{A}}$$", "word_idx": 22868, "sentence_idx": 372, "label": "unlabeled"}, {"type": "text", "expr": "3: \u00a0\u00a0 while \u00a0the maximal number of repetitions is not reached\u00a0 do", "word_idx": 22889, "sentence_idx": 373, "label": "unlabeled"}, {"type": "text", "expr": "while", "word_idx": 22954, "sentence_idx": 374, "label": "unlabeled"}, {"type": "text", "expr": "4: \u00a0\u00a0\u00a0\u00a0\u00a0sample unobserved actions in  $\\mathcal{O}$  based on  $\\Gamma$", "word_idx": 22959, "sentence_idx": 375, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 23030, "sentence_idx": 376, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma$$", "word_idx": 23041, "sentence_idx": 377, "label": "unlabeled"}, {"type": "text", "expr": "5: \u00a0\u00a0\u00a0\u00a0\u00a0update  $\\Gamma$  based on Equation ( 7 )", "word_idx": 23047, "sentence_idx": 378, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma$$", "word_idx": 23096, "sentence_idx": 379, "label": "unlabeled"}, {"type": "text", "expr": "6: \u00a0\u00a0\u00a0\u00a0\u00a0project  $\\Gamma$  to [0,1]", "word_idx": 23102, "sentence_idx": 380, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma$$", "word_idx": 23137, "sentence_idx": 381, "label": "unlabeled"}, {"type": "text", "expr": "7: \u00a0\u00a0 end \u00a0 while", "word_idx": 23143, "sentence_idx": 382, "label": "unlabeled"}, {"type": "text", "expr": "while", "word_idx": 23160, "sentence_idx": 383, "label": "unlabeled"}, {"type": "text", "expr": "8: \u00a0\u00a0select actions for unobserved actions with the largest weights in  $\\Gamma$", "word_idx": 23165, "sentence_idx": 384, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma$$", "word_idx": 23245, "sentence_idx": 385, "label": "unlabeled"}, {"type": "text", "expr": "9: \u00a0\u00a0 return  \u00a0 $\\tilde{p}$", "word_idx": 23251, "sentence_idx": 386, "label": "unlabeled"}, {"type": "text", "expr": "return", "word_idx": 23278, "sentence_idx": 387, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{p}$$", "word_idx": 23284, "sentence_idx": 388, "label": "unlabeled"}, {"type": "text", "expr": "Our  DUP  algorithm framework belongs to a family of policy gradient\nalgorithms, which have been successfully applied to complex problems,\ne", "word_idx": 23293, "sentence_idx": 389, "label": "unlabeled"}, {"type": "text", "expr": ", robot control  (21) , natural language processing\n (4) ", "word_idx": 23433, "sentence_idx": 390, "label": "unlabeled"}, {"type": "text", "expr": " Our formulation is unique in how it\nrecognizes plans, in comparison to the existing methods in the planning\ncommunity", "word_idx": 23490, "sentence_idx": 391, "label": "unlabeled"}, {"type": "text", "expr": "Note that our current study shows that even direct application of word\nvector learning methods provide competitive performance for plan\ncompletion tasks", "word_idx": 23608, "sentence_idx": 392, "label": "unlabeled"}, {"type": "text", "expr": " We believe we can further improve the performance by\nusing the planning specific structural information in the EM phase", "word_idx": 23760, "sentence_idx": 393, "label": "unlabeled"}, {"type": "text", "expr": " In\nother words, if we are provided with additional planning structural\ninformation as input, we can exploit the structural information to\nfilter candidate plans to be recognized in the EM procedure", "word_idx": 23880, "sentence_idx": 394, "label": "unlabeled"}, {"type": "text", "expr": "Note that our current study shows that even direct application of word\nvector learning methods provide competitive performance for plan\ncompletion tasks", "word_idx": 24078, "sentence_idx": 395, "label": "unlabeled"}, {"type": "text", "expr": " We believe we can further improve the performance by\nusing the planning specific structural information in the EM phase", "word_idx": 24230, "sentence_idx": 396, "label": "unlabeled"}, {"type": "text", "expr": " In\nother words, if we are provided with additional planning structural\ninformation as input, we can exploit the structural information to\nfilter candidate plans to be recognized in the EM procedure", "word_idx": 24350, "sentence_idx": 397, "label": "unlabeled"}, {"type": "text", "expr": "5  Our  RNNPlanner  approach", "word_idx": 24548, "sentence_idx": 398, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 24576, "sentence_idx": 399, "label": "unlabeled"}, {"type": "text", "expr": "Instead of using the EM-style framework, in this section we present another approach which is based on Recurrent Neural Networks (RNNs), specifically Long Short-term Memory networks (LSTMs), with the distributed representations of actions introduced in Section  3 ", "word_idx": 24586, "sentence_idx": 400, "label": "unlabeled"}, {"type": "text", "expr": " LSTM is a specific kind of RNN that works by leveraging long-short term contexts", "word_idx": 24850, "sentence_idx": 401, "label": "unlabeled"}, {"type": "text", "expr": " As we model our plan recognition problem as an action-sequence generation problem, our aim of exploring RNN-LSTM architecture is to leverage longer-horizon of action contexts to help improve the accuracy of generating new actions based on previously observed or generated actions", "word_idx": 24931, "sentence_idx": 402, "label": "unlabeled"}, {"type": "text", "expr": " We will first introduce the RNN-LSTM architecture, and then introduce our  RNNPlanner  model", "word_idx": 25211, "sentence_idx": 403, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 25304, "sentence_idx": 404, "label": "unlabeled"}, {"type": "text", "expr": "1  The RNN Model", "word_idx": 25314, "sentence_idx": 405, "label": "unlabeled"}, {"type": "text", "expr": "Specifically, the RNN architecture can be defined in the following way", "word_idx": 25330, "sentence_idx": 406, "label": "unlabeled"}, {"type": "text", "expr": " Given an input action  $x_{t}$  at the step  $t$ , RNN accepts it with weighted connections to  $N$  hidden layers that are stacked together", "word_idx": 25400, "sentence_idx": 407, "label": "unlabeled"}, {"type": "text", "expr": " And from the hidden layer stack, there is a connection to the output layer  $y_{t}$ , as well as a cyclic weighted connection going into the hidden layer stack", "word_idx": 25541, "sentence_idx": 408, "label": "unlabeled"}, {"type": "text", "expr": " And if we unroll this RNN cell along  $T$  steps, it could accept an action input sequence  $\\mathbf{x}=(x_{1},x_{T})$ , and compute a sequence of hidden states  $\\mathbf{h}=(h_{1}^{N},h_{2}^{N},,h_{T}^{N})$ ", "word_idx": 25701, "sentence_idx": 409, "label": "unlabeled"}, {"type": "text", "expr": " For each of these hidden states  $h_{t}$  ( $1\\leq t\\leq T$ ), it contributes to predicting the next step output  $y_{t+1}$ , and thus RNN computes an output vector sequence  $\\mathbf{y}=(y_{1},,y_{T})$ , by concatenating outputs from all steps together", "word_idx": 25910, "sentence_idx": 410, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t}$$", "word_idx": 26164, "sentence_idx": 411, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}$$", "word_idx": 26169, "sentence_idx": 412, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}=(x_{1}...,x_{T})$$", "word_idx": 26174, "sentence_idx": 413, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{h}=(h_{1}^{N},h_{2}^{N},...,h_{T}^{N})$$", "word_idx": 26201, "sentence_idx": 414, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{t}$$", "word_idx": 26247, "sentence_idx": 415, "label": "unlabeled"}, {"type": "math", "expr": "$$1\\leq t\\leq T$$", "word_idx": 26252, "sentence_idx": 416, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t+1}$$", "word_idx": 26265, "sentence_idx": 417, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{y}=(y_{1},...,y_{T})$$", "word_idx": 26272, "sentence_idx": 418, "label": "unlabeled"}, {"type": "text", "expr": "Given an input sequence  $\\mathbf{x}$ , an RNN model could predict an output sequence  $\\mathbf{y}$ , in which output  $y_{t}$  at each step depends on the input  $x_{t}$  at that step, and the hidden state at the previous step", "word_idx": 26300, "sentence_idx": 419, "label": "unlabeled"}, {"type": "text", "expr": " The RNN could also be utilized to directly generate, in principle, infinitely long future outputs (actions), given a single input  $x_{t}$ ", "word_idx": 26527, "sentence_idx": 420, "label": "unlabeled"}, {"type": "text", "expr": " The sequence of future actions could be generated by directly feeding the output  $y_{t}$  at a step  $t$ , to the input  $x_{t+1}$  at the next step  $t+1$ ", "word_idx": 26667, "sentence_idx": 421, "label": "unlabeled"}, {"type": "text", "expr": " This way, RNN \u201cassumes\u201d what it predicts that would happen at next step is reliable ( $y_{t}=x_{t+1}$ )", "word_idx": 26825, "sentence_idx": 422, "label": "unlabeled"}, {"type": "text", "expr": " As for training the RNN as a sequence generation model, we could utilize  $y_{t}$  to parameterize a predictive distribution  $P(x_{t+1}|y_{t})$  over all of the possible next inputs  $x_{t+1}$ , and thus we could minimize the loss:", "word_idx": 26929, "sentence_idx": 423, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}$$", "word_idx": 27162, "sentence_idx": 424, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{y}$$", "word_idx": 27172, "sentence_idx": 425, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}$$", "word_idx": 27182, "sentence_idx": 426, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t}$$", "word_idx": 27187, "sentence_idx": 427, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t}$$", "word_idx": 27192, "sentence_idx": 428, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}$$", "word_idx": 27197, "sentence_idx": 429, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t+1}$$", "word_idx": 27202, "sentence_idx": 430, "label": "unlabeled"}, {"type": "math", "expr": "$$t+1$$", "word_idx": 27209, "sentence_idx": 431, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}=x_{t+1}$$", "word_idx": 27212, "sentence_idx": 432, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}$$", "word_idx": 27225, "sentence_idx": 433, "label": "unlabeled"}, {"type": "math", "expr": "$$P(x_{t+1}|y_{t})$$", "word_idx": 27230, "sentence_idx": 434, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t+1}$$", "word_idx": 27246, "sentence_idx": 435, "label": "unlabeled"}, {"type": "text", "expr": "$\\mathcal{L}(\\mathbf{x})=-\\sum_{t=1}^{T}logP(x_{t+1}|y_{t})$", "word_idx": 27253, "sentence_idx": 436, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}(\\mathbf{x})=-\\sum_{t=1}^{T}logP(x_{t+1}|y_{t}).$$", "word_idx": 27313, "sentence_idx": 437, "label": "unlabeled"}, {"type": "text", "expr": "where  $T$  is the number of steps of an observed plan trace,  $x_{t+1}$  is the observed action at step  $t+1$ , and  $y_{t}$  is the output at step  $t$  as well as the prediction of what would happen at step  $t+1$ ", "word_idx": 27372, "sentence_idx": 438, "label": "unlabeled"}, {"type": "text", "expr": " To estimate  $y_{t}$  based on  $x_{1},\\ldots,x_{t}$ , we exploit the Long Short-term Memory (LSTM) model, which has been demonstrated effective on generating sequences  , to leverage long term information prior to  $x_{t}$  and predict  $y_{t}$  based on current input  $x_{t}$ ", "word_idx": 27590, "sentence_idx": 439, "label": "unlabeled"}, {"type": "text", "expr": " We can thus rewrite Equation ( 8 ) as:", "word_idx": 27870, "sentence_idx": 440, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t+1}$$", "word_idx": 27909, "sentence_idx": 441, "label": "unlabeled"}, {"type": "math", "expr": "$$t+1$$", "word_idx": 27916, "sentence_idx": 442, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}$$", "word_idx": 27919, "sentence_idx": 443, "label": "unlabeled"}, {"type": "math", "expr": "$$t+1$$", "word_idx": 27924, "sentence_idx": 444, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}$$", "word_idx": 27927, "sentence_idx": 445, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{1},\\ldots,x_{t}$$", "word_idx": 27932, "sentence_idx": 446, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t}$$", "word_idx": 27950, "sentence_idx": 447, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}$$", "word_idx": 27955, "sentence_idx": 448, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t}$$", "word_idx": 27960, "sentence_idx": 449, "label": "unlabeled"}, {"type": "text", "expr": "$\\mathcal{L}(\\mathbf{x})=-\\sum_{t=1}^{T}logP(x_{t+1}|y_{t})\\mathrm{LSTM}(y_{t}|%\nx_{1:t}),$", "word_idx": 27965, "sentence_idx": 450, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}(\\mathbf{x})=-\\sum_{t=1}^{T}logP(x_{t+1}|y_{t})\\mathrm{LSTM}(y_{t}|%\nx_{1:t}),$$", "word_idx": 28056, "sentence_idx": 451, "label": "unlabeled"}, {"type": "text", "expr": "where  $\\mathrm{LSTM}(y_{t}|x_{1:t})$  indicates the LSTM model estimates  $y_{t}$  based on current input  $x_{t}$  and memories of previous input prior to  $x_{t}$ ", "word_idx": 28145, "sentence_idx": 452, "label": "unlabeled"}, {"type": "text", "expr": " The framework of LSTM   is shown in Figure  2 , where  $x_{t}$  is the  $t$ th input,  $h_{t}$  is the  $t$ th hidden state", "word_idx": 28311, "sentence_idx": 453, "label": "unlabeled"}, {"type": "text", "expr": "  $i_{t}$ ,  $f_{t}$ ,  $o_{t}$  and  $c_{t}$  are the  $t$ th  input gate ,  forget gate ,  output gate ,  cell  and  cell input  activation vectors, respectively, whose dimensions are the same as the hidden vector  $h_{t}$ ", "word_idx": 28435, "sentence_idx": 454, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{LSTM}(y_{t}|x_{1:t})$$", "word_idx": 28660, "sentence_idx": 455, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{t}$$", "word_idx": 28688, "sentence_idx": 456, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t}$$", "word_idx": 28693, "sentence_idx": 457, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t}$$", "word_idx": 28698, "sentence_idx": 458, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t}$$", "word_idx": 28703, "sentence_idx": 459, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{t}$$", "word_idx": 28708, "sentence_idx": 460, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{t}$$", "word_idx": 28713, "sentence_idx": 461, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{t}$$", "word_idx": 28718, "sentence_idx": 462, "label": "unlabeled"}, {"type": "math", "expr": "$$o_{t}$$", "word_idx": 28723, "sentence_idx": 463, "label": "unlabeled"}, {"type": "math", "expr": "$$c_{t}$$", "word_idx": 28728, "sentence_idx": 464, "label": "unlabeled"}, {"type": "text", "expr": "input gate", "word_idx": 28733, "sentence_idx": 465, "label": "unlabeled"}, {"type": "text", "expr": "forget gate", "word_idx": 28743, "sentence_idx": 466, "label": "unlabeled"}, {"type": "text", "expr": "output gate", "word_idx": 28754, "sentence_idx": 467, "label": "unlabeled"}, {"type": "text", "expr": "cell input", "word_idx": 28765, "sentence_idx": 468, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{t}$$", "word_idx": 28775, "sentence_idx": 469, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:  Long Short-term Memory (LSTM) cell", "word_idx": 28780, "sentence_idx": 470, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:", "word_idx": 28825, "sentence_idx": 471, "label": "unlabeled"}, {"type": "text", "expr": "LSTM is implemented by the following functions:", "word_idx": 28834, "sentence_idx": 472, "label": "unlabeled"}, {"type": "text", "expr": "LSTM is implemented by the following functions:", "word_idx": 28881, "sentence_idx": 473, "label": "unlabeled"}, {"type": "text", "expr": "= (10)", "word_idx": 28928, "sentence_idx": 474, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle i_{t}$", "word_idx": 28934, "sentence_idx": 475, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle i_{t}$$", "word_idx": 28955, "sentence_idx": 476, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=$", "word_idx": 28974, "sentence_idx": 477, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=$$", "word_idx": 28990, "sentence_idx": 478, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\sigma(W_{xi}x_{t}+W_{hi}h_{t-1}+W_{ci}c_{t-1}+b_{i})$", "word_idx": 29004, "sentence_idx": 479, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\sigma(W_{xi}x_{t}+W_{hi}h_{t-1}+W_{ci}c_{t-1}+b_{i})$$", "word_idx": 29072, "sentence_idx": 480, "label": "unlabeled"}, {"type": "text", "expr": "= (11)", "word_idx": 29138, "sentence_idx": 481, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle f_{t}$", "word_idx": 29144, "sentence_idx": 482, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle f_{t}$$", "word_idx": 29165, "sentence_idx": 483, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=$", "word_idx": 29184, "sentence_idx": 484, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=$$", "word_idx": 29200, "sentence_idx": 485, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\sigma(W_{xf}x_{t}+W_{hf}h_{t-1}+W_{cf}c_{t-1}+b_{f})$", "word_idx": 29214, "sentence_idx": 486, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\sigma(W_{xf}x_{t}+W_{hf}h_{t-1}+W_{cf}c_{t-1}+b_{f})$$", "word_idx": 29282, "sentence_idx": 487, "label": "unlabeled"}, {"type": "text", "expr": "= (12)", "word_idx": 29348, "sentence_idx": 488, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle c_{t}$", "word_idx": 29354, "sentence_idx": 489, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle c_{t}$$", "word_idx": 29375, "sentence_idx": 490, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=$", "word_idx": 29394, "sentence_idx": 491, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=$$", "word_idx": 29410, "sentence_idx": 492, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle f_{t}\\circ c_{t-1}+i_{t}\\circ\\tanh(W_{xc}x_{t}+W_{hc}h_{t-1}+b_{%\nc})$", "word_idx": 29424, "sentence_idx": 493, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle f_{t}\\circ c_{t-1}+i_{t}\\circ\\tanh(W_{xc}x_{t}+W_{hc}h_{t-1}+b_{%\nc})$$", "word_idx": 29509, "sentence_idx": 494, "label": "unlabeled"}, {"type": "text", "expr": "= (13)", "word_idx": 29592, "sentence_idx": 495, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle o_{t}$", "word_idx": 29598, "sentence_idx": 496, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle o_{t}$$", "word_idx": 29619, "sentence_idx": 497, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=$", "word_idx": 29638, "sentence_idx": 498, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=$$", "word_idx": 29654, "sentence_idx": 499, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\sigma(W_{xo}x_{t}+W_{ho}h_{t-1}+W_{co}c_{t}+b_{o})$", "word_idx": 29668, "sentence_idx": 500, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\sigma(W_{xo}x_{t}+W_{ho}h_{t-1}+W_{co}c_{t}+b_{o})$$", "word_idx": 29734, "sentence_idx": 501, "label": "unlabeled"}, {"type": "text", "expr": "= (14)", "word_idx": 29798, "sentence_idx": 502, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle h_{t}$", "word_idx": 29804, "sentence_idx": 503, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle h_{t}$$", "word_idx": 29825, "sentence_idx": 504, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=$", "word_idx": 29844, "sentence_idx": 505, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=$$", "word_idx": 29860, "sentence_idx": 506, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle o_{t}\\circ\\tanh(c_{t})$", "word_idx": 29874, "sentence_idx": 507, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle o_{t}\\circ\\tanh(c_{t})$$", "word_idx": 29912, "sentence_idx": 508, "label": "unlabeled"}, {"type": "text", "expr": "where  $\\circ$  indicates the Hadamard product,  $\\sigma$  is the logistic sigmoid function,  $W_{xi}$  is an input-input gate matrix,  $W_{hi}$  is a hidden-input gate matrix,  $W_{ci}$  is a cell-input gate matrix,  $W_{xf}$  is an input-forget gate matrix,  $W_{hf}$  is a hidden-forget gate matrix,  $W_{cf}$  is a cell-forget gate matrix,  $W_{xc}$  is an input-cell gate matrix,  $W_{hc}$  is a hidden-cell gate matrix,  $W_{xo}$  is an input-output gate matrix,  $W_{ho}$  is a hidden-output gate matrix,  $W_{co}$  is a cell-output gate matrix", "word_idx": 29948, "sentence_idx": 509, "label": "unlabeled"}, {"type": "text", "expr": "  $b_{i}$ ,  $b_{f}$ ,  $b_{c}$ , and  $b_{o}$  are bias terms", "word_idx": 30499, "sentence_idx": 510, "label": "unlabeled"}, {"type": "text", "expr": " Note that the matrices from cell to gate vectors (i", "word_idx": 30561, "sentence_idx": 511, "label": "unlabeled"}, {"type": "text", "expr": ",  $W_{ci}$ ,  $W_{cf}$  and  $W_{co}$ ) are diagonal, such that each element  $e$  in each gate vector only receives input of element  $e$  of the cell vector", "word_idx": 30613, "sentence_idx": 512, "label": "unlabeled"}, {"type": "text", "expr": " The major innovation of LSTM is its memory cell  $c_{t}$  which essentially acts as an accumulator of the state information", "word_idx": 30772, "sentence_idx": 513, "label": "unlabeled"}, {"type": "text", "expr": "  $c_{t}$  is accessed, written and cleared by self-parameterized controlling gates, i", "word_idx": 30896, "sentence_idx": 514, "label": "unlabeled"}, {"type": "text", "expr": ",  input, forget, output  gates", "word_idx": 30982, "sentence_idx": 515, "label": "unlabeled"}, {"type": "text", "expr": " Each time a new input  $x_{t}$  comes, its information is accumulated to the memory cell if the input gate  $i_{t}$  is activated", "word_idx": 31013, "sentence_idx": 516, "label": "unlabeled"}, {"type": "text", "expr": " The past cell status  $c_{t-1}$  could be forgotten in this process if the forget gate  $f_{t}$  is activated", "word_idx": 31143, "sentence_idx": 517, "label": "unlabeled"}, {"type": "text", "expr": " Whether the latest cell output  $c_{t}$  is propagated to the final state  $h_{t}$  is further controlled by the output gate  $o_{t}$ ", "word_idx": 31253, "sentence_idx": 518, "label": "unlabeled"}, {"type": "text", "expr": " The benefit of using the memory cell and gates to control information flow is the gradient is trapped in the cell and prevented from vanishing too quickly", "word_idx": 31388, "sentence_idx": 519, "label": "unlabeled"}, {"type": "math", "expr": "$$\\circ$$", "word_idx": 31543, "sentence_idx": 520, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma$$", "word_idx": 31548, "sentence_idx": 521, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{xi}$$", "word_idx": 31554, "sentence_idx": 522, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{hi}$$", "word_idx": 31560, "sentence_idx": 523, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{ci}$$", "word_idx": 31566, "sentence_idx": 524, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{xf}$$", "word_idx": 31572, "sentence_idx": 525, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{hf}$$", "word_idx": 31578, "sentence_idx": 526, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{cf}$$", "word_idx": 31584, "sentence_idx": 527, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{xc}$$", "word_idx": 31590, "sentence_idx": 528, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{hc}$$", "word_idx": 31596, "sentence_idx": 529, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{xo}$$", "word_idx": 31602, "sentence_idx": 530, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{ho}$$", "word_idx": 31608, "sentence_idx": 531, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{co}$$", "word_idx": 31614, "sentence_idx": 532, "label": "unlabeled"}, {"type": "math", "expr": "$$b_{i}$$", "word_idx": 31620, "sentence_idx": 533, "label": "unlabeled"}, {"type": "math", "expr": "$$b_{f}$$", "word_idx": 31625, "sentence_idx": 534, "label": "unlabeled"}, {"type": "math", "expr": "$$b_{c}$$", "word_idx": 31630, "sentence_idx": 535, "label": "unlabeled"}, {"type": "math", "expr": "$$b_{o}$$", "word_idx": 31635, "sentence_idx": 536, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{ci}$$", "word_idx": 31640, "sentence_idx": 537, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{cf}$$", "word_idx": 31646, "sentence_idx": 538, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{co}$$", "word_idx": 31652, "sentence_idx": 539, "label": "unlabeled"}, {"type": "math", "expr": "$$c_{t}$$", "word_idx": 31658, "sentence_idx": 540, "label": "unlabeled"}, {"type": "math", "expr": "$$c_{t}$$", "word_idx": 31663, "sentence_idx": 541, "label": "unlabeled"}, {"type": "text", "expr": "input, forget, output", "word_idx": 31668, "sentence_idx": 542, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t}$$", "word_idx": 31689, "sentence_idx": 543, "label": "unlabeled"}, {"type": "math", "expr": "$$i_{t}$$", "word_idx": 31694, "sentence_idx": 544, "label": "unlabeled"}, {"type": "math", "expr": "$$c_{t-1}$$", "word_idx": 31699, "sentence_idx": 545, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{t}$$", "word_idx": 31706, "sentence_idx": 546, "label": "unlabeled"}, {"type": "math", "expr": "$$c_{t}$$", "word_idx": 31711, "sentence_idx": 547, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{t}$$", "word_idx": 31716, "sentence_idx": 548, "label": "unlabeled"}, {"type": "math", "expr": "$$o_{t}$$", "word_idx": 31721, "sentence_idx": 549, "label": "unlabeled"}, {"type": "text", "expr": "2  Discovering Underlying Plans with the RNN Model", "word_idx": 31726, "sentence_idx": 550, "label": "unlabeled"}, {"type": "text", "expr": "With the distributed representations of actions addressed in Section 3, we can view each plan in the plan library as a sequence of actions, and the plan library as a set of action sequences which can be utilized to train the RNN model", "word_idx": 31776, "sentence_idx": 551, "label": "unlabeled"}, {"type": "text", "expr": " The framework of RNN with sequences of actions can be seen from Figure  3 ", "word_idx": 32010, "sentence_idx": 552, "label": "unlabeled"}, {"type": "text", "expr": " The bottom row in Figure  3  is an example action sequence \u201cpick-up-B, stack-B-A, pick-up-C, stack-C-B, pick-up-D, \u2026\u201d, which corresponds to an input sequence  $\\mathbf{x}$ ", "word_idx": 32085, "sentence_idx": 553, "label": "unlabeled"}, {"type": "text", "expr": " Once an action among the bottom row is fed into the RNN, that action is assigned with an index, and an embedding layer is trained to find a vector representation based on that index", "word_idx": 32258, "sentence_idx": 554, "label": "unlabeled"}, {"type": "text", "expr": " The action vector from the embedding layer is the feature that can be used by the LSTM cell", "word_idx": 32440, "sentence_idx": 555, "label": "unlabeled"}, {"type": "text", "expr": " How the LSTM cell works has been explained in Equation  14 ", "word_idx": 32532, "sentence_idx": 556, "label": "unlabeled"}, {"type": "text", "expr": " Similar to a classic RNN cell, the LSTM cell feeds its output to both itself as a hidden state, and the softmax layer to obtain a probability distribution over all actions in the action vocabulary  $\\mathcal{A}$ ", "word_idx": 32592, "sentence_idx": 557, "label": "unlabeled"}, {"type": "text", "expr": " From the perspective of the LSTM cell at the next step, it receives a hidden state from the previous step  $h_{t-1}$ , an action vector at the current step  $x_{t}$ ", "word_idx": 32805, "sentence_idx": 558, "label": "unlabeled"}, {"type": "text", "expr": " To obtain the index of most possible action, our model samples over the action distribution output from softmax layer", "word_idx": 32971, "sentence_idx": 559, "label": "unlabeled"}, {"type": "text", "expr": " That retrieved index could be mapped to an action in the vocabulary  $\\bar{\\mathcal{A}}$ ", "word_idx": 33089, "sentence_idx": 560, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}$$", "word_idx": 33179, "sentence_idx": 561, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{A}$$", "word_idx": 33189, "sentence_idx": 562, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{t-1}$$", "word_idx": 33200, "sentence_idx": 563, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t}$$", "word_idx": 33207, "sentence_idx": 564, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{\\mathcal{A}}$$", "word_idx": 33212, "sentence_idx": 565, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:  The framework of our  RNNPlanner  approach (with one hidden layer)", "word_idx": 33229, "sentence_idx": 566, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:", "word_idx": 33306, "sentence_idx": 567, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 33315, "sentence_idx": 568, "label": "unlabeled"}, {"type": "text", "expr": "The top row in Figure  3  is the output sequence, which is denoted by \u201cOUT1, OUT2, OUT3, OUT4, OUT5, \u2026\u201d, which corresponds to the estimated sequence \u201c $y_{1}$ ,  $y_{2}$ ,  $y_{3}$ ,  $y_{4}$ ,  $y_{5}$ ,  $\\ldots$ \u201d in Equation ( 9 )", "word_idx": 33325, "sentence_idx": 569, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{1}$$", "word_idx": 33559, "sentence_idx": 570, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{2}$$", "word_idx": 33564, "sentence_idx": 571, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{3}$$", "word_idx": 33569, "sentence_idx": 572, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{4}$$", "word_idx": 33574, "sentence_idx": 573, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{5}$$", "word_idx": 33579, "sentence_idx": 574, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ldots$$", "word_idx": 33584, "sentence_idx": 575, "label": "unlabeled"}, {"type": "text", "expr": "Note that we exploit the dotted arrow to indicate two folds of meanings in Figure  3 ", "word_idx": 33590, "sentence_idx": 576, "label": "unlabeled"}, {"type": "text", "expr": " When training the RNN model, the one pointed by the head of the dotted row (the embedding of input) is used to compute the cross entropy error with the output at tail (output of LSTM cell at the previous step), and next-step observation as the input at the head, to train the model", "word_idx": 33675, "sentence_idx": 577, "label": "unlabeled"}, {"type": "text", "expr": " When using the trained RNN model to discover unknown actions, the model \u201cimagines\u201d what it predicts is the real next input, and takes it to continue its prediction", "word_idx": 33957, "sentence_idx": 578, "label": "unlabeled"}, {"type": "text", "expr": " Thus the one pointed by the head is copied and identical to the one denoted by the tail", "word_idx": 34121, "sentence_idx": 579, "label": "unlabeled"}, {"type": "text", "expr": " For example, the embedding of \u201cstack-B-A\u201d is copied from the prediction vector of \u201cOUT1\u201d if the input \u201cstack-B-A\u201d was unknown", "word_idx": 34209, "sentence_idx": 580, "label": "unlabeled"}, {"type": "text", "expr": " In addition, the arrows between each of two LSTM cells shows the unrolling of a LSTM cell", "word_idx": 34335, "sentence_idx": 581, "label": "unlabeled"}, {"type": "text", "expr": " The horizontal dashed line suggests that we obtain the action output at each step, by sampling from probability distribution, provided by the softmax layer", "word_idx": 34425, "sentence_idx": 582, "label": "unlabeled"}, {"type": "text", "expr": "With the trained RNN model, we can discover underlying actions by simply exploiting the RNN model to generate unknown actions based on observed or already discovered actions", "word_idx": 34581, "sentence_idx": 583, "label": "unlabeled"}, {"type": "text", "expr": " For example, given the observation:", "word_idx": 34754, "sentence_idx": 584, "label": "unlabeled"}, {"type": "text", "expr": "With the trained RNN model, we can discover underlying actions by simply exploiting the RNN model to generate unknown actions based on observed or already discovered actions", "word_idx": 34790, "sentence_idx": 585, "label": "unlabeled"}, {"type": "text", "expr": " For example, given the observation:", "word_idx": 34963, "sentence_idx": 586, "label": "unlabeled"}, {"type": "text", "expr": "pick-up-B  $\\phi$  unstack-D-C put-down-D  $\\phi$  stack-C-B  $\\phi$   $\\phi$", "word_idx": 34999, "sentence_idx": 587, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 35076, "sentence_idx": 588, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 35080, "sentence_idx": 589, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 35084, "sentence_idx": 590, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 35088, "sentence_idx": 591, "label": "unlabeled"}, {"type": "text", "expr": "we can generate the first  $\\phi$  based on  pick-up-B , the second  $\\phi$  based on actions from  pick-up-B  to  put-down-D , the third  $\\phi$  based on actions from  pick-up-B  to  stack-C-B , and the last  $\\phi$  based on all previously actions (including generated actions at where there was a  $\\phi$ )", "word_idx": 35092, "sentence_idx": 592, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 35402, "sentence_idx": 593, "label": "unlabeled"}, {"type": "text", "expr": "pick-up-B", "word_idx": 35406, "sentence_idx": 594, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 35415, "sentence_idx": 595, "label": "unlabeled"}, {"type": "text", "expr": "pick-up-B", "word_idx": 35419, "sentence_idx": 596, "label": "unlabeled"}, {"type": "text", "expr": "put-down-D", "word_idx": 35428, "sentence_idx": 597, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 35438, "sentence_idx": 598, "label": "unlabeled"}, {"type": "text", "expr": "pick-up-B", "word_idx": 35442, "sentence_idx": 599, "label": "unlabeled"}, {"type": "text", "expr": "stack-C-B", "word_idx": 35451, "sentence_idx": 600, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 35460, "sentence_idx": 601, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi$$", "word_idx": 35464, "sentence_idx": 602, "label": "unlabeled"}, {"type": "text", "expr": "6  Experiments", "word_idx": 35468, "sentence_idx": 603, "label": "unlabeled"}, {"type": "text", "expr": "In this section, we evaluate our  DUP  and  RNNPlanner  algorithms in three planning domains from International Planning Competition, i", "word_idx": 35482, "sentence_idx": 604, "label": "unlabeled"}, {"type": "text", "expr": ", blocks ${}^{1}$ , depots , and driverlog ${}^{2}$ ", "word_idx": 35617, "sentence_idx": 605, "label": "unlabeled"}, {"type": "text", "expr": " To generate training and testing data, we randomly created 5000 planning problems for each domain, and solved these planning problems with a planning solver, such as FF , to produce 5000 plans", "word_idx": 35669, "sentence_idx": 606, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 35862, "sentence_idx": 607, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{1}$$", "word_idx": 35872, "sentence_idx": 608, "label": "unlabeled"}, {"type": "text", "expr": "2 http://www", "word_idx": 35878, "sentence_idx": 609, "label": "unlabeled"}, {"type": "text", "expr": "edu/afs/cs/project/jair/pub/volume20/long03a-html/JAIRIPC", "word_idx": 35890, "sentence_idx": 610, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{2}$$", "word_idx": 35947, "sentence_idx": 611, "label": "unlabeled"}, {"type": "text", "expr": "3 https://fai", "word_idx": 35953, "sentence_idx": 612, "label": "unlabeled"}, {"type": "text", "expr": "uni-saarland", "word_idx": 35966, "sentence_idx": 613, "label": "unlabeled"}, {"type": "text", "expr": "de/hoffmann/ff", "word_idx": 35978, "sentence_idx": 614, "label": "unlabeled"}, {"type": "text", "expr": "We define the accuracy of our  DUP  and  RNNPlanner  algorithms as follows", "word_idx": 35992, "sentence_idx": 615, "label": "unlabeled"}, {"type": "text", "expr": " For each unobserved action  $\\bar{a}_{x}$ ,  DUP  and  RNNPlanner  suggest a set of possible actions  $S_{x}$  which have the highest value of  $\\Gamma_{\\bar{a}_{x},x}$  for all  $\\bar{a}_{x}\\in\\bar{\\mathcal{A}}$ ", "word_idx": 36066, "sentence_idx": 616, "label": "unlabeled"}, {"type": "text", "expr": " If  $S_{x}$  covers the  truth  action  $a_{truth}$ , i", "word_idx": 36280, "sentence_idx": 617, "label": "unlabeled"}, {"type": "text", "expr": ",  $a_{truth}\\in S_{x}$ , we increase the number of correct suggestions by 1", "word_idx": 36336, "sentence_idx": 618, "label": "unlabeled"}, {"type": "text", "expr": " We thus define the accuracy  $acc$  as shown below:", "word_idx": 36412, "sentence_idx": 619, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 36464, "sentence_idx": 620, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{a}_{x}$$", "word_idx": 36474, "sentence_idx": 621, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 36485, "sentence_idx": 622, "label": "unlabeled"}, {"type": "math", "expr": "$$S_{x}$$", "word_idx": 36495, "sentence_idx": 623, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Gamma_{\\bar{a}_{x},x}$$", "word_idx": 36500, "sentence_idx": 624, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{a}_{x}\\in\\bar{\\mathcal{A}}$$", "word_idx": 36522, "sentence_idx": 625, "label": "unlabeled"}, {"type": "math", "expr": "$$S_{x}$$", "word_idx": 36553, "sentence_idx": 626, "label": "unlabeled"}, {"type": "text", "expr": "truth", "word_idx": 36558, "sentence_idx": 627, "label": "unlabeled"}, {"type": "math", "expr": "$$a_{truth}$$", "word_idx": 36563, "sentence_idx": 628, "label": "unlabeled"}, {"type": "math", "expr": "$$a_{truth}\\in S_{x}$$", "word_idx": 36572, "sentence_idx": 629, "label": "unlabeled"}, {"type": "math", "expr": "$$acc$$", "word_idx": 36590, "sentence_idx": 630, "label": "unlabeled"}, {"type": "text", "expr": "$acc=\\frac{1}{T}\\sum_{i=1}^{T}\\frac{\\#\\langle correct\\textrm{-}suggestions%\n\\rangle_{i}}{K_{i}},$", "word_idx": 36593, "sentence_idx": 631, "label": "unlabeled"}, {"type": "math", "expr": "$$acc=\\frac{1}{T}\\sum_{i=1}^{T}\\frac{\\#\\langle correct\\textrm{-}suggestions%\n\\rangle_{i}}{K_{i}},$$", "word_idx": 36690, "sentence_idx": 632, "label": "unlabeled"}, {"type": "text", "expr": "where  $T$  is the size of testing set,  $\\#\\langle correct\\textrm{-}suggestions\\rangle_{i}$  is the number of correct suggestions for the  $i$ th testing plan,  $K_{i}$  is the number of unobserved actions in the  $i$ th testing plan", "word_idx": 36785, "sentence_idx": 633, "label": "unlabeled"}, {"type": "text", "expr": " We can see that the accuracy  $acc$  may be influenced by  $S_{x}$ ", "word_idx": 37019, "sentence_idx": 634, "label": "unlabeled"}, {"type": "text", "expr": " We will test different size of  $S_{x}$  in the experiment", "word_idx": 37087, "sentence_idx": 635, "label": "unlabeled"}, {"type": "math", "expr": "$$\\#\\langle correct\\textrm{-}suggestions\\rangle_{i}$$", "word_idx": 37146, "sentence_idx": 636, "label": "unlabeled"}, {"type": "math", "expr": "$$K_{i}$$", "word_idx": 37195, "sentence_idx": 637, "label": "unlabeled"}, {"type": "math", "expr": "$$acc$$", "word_idx": 37200, "sentence_idx": 638, "label": "unlabeled"}, {"type": "math", "expr": "$$S_{x}$$", "word_idx": 37203, "sentence_idx": 639, "label": "unlabeled"}, {"type": "math", "expr": "$$S_{x}$$", "word_idx": 37208, "sentence_idx": 640, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:  Features of datasets", "word_idx": 37213, "sentence_idx": 641, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:", "word_idx": 37243, "sentence_idx": 642, "label": "unlabeled"}, {"type": "text", "expr": "domain #plan #word #vocabulary", "word_idx": 37251, "sentence_idx": 643, "label": "unlabeled"}, {"type": "text", "expr": "blocks 5000 292250 1250", "word_idx": 37281, "sentence_idx": 644, "label": "unlabeled"}, {"type": "text", "expr": "292250", "word_idx": 37304, "sentence_idx": 645, "label": "unlabeled"}, {"type": "text", "expr": "depots 5000 209711 2273", "word_idx": 37310, "sentence_idx": 646, "label": "unlabeled"}, {"type": "text", "expr": "209711", "word_idx": 37333, "sentence_idx": 647, "label": "unlabeled"}, {"type": "text", "expr": "driverlog 5000 179621 1441", "word_idx": 37339, "sentence_idx": 648, "label": "unlabeled"}, {"type": "text", "expr": "179621", "word_idx": 37365, "sentence_idx": 649, "label": "unlabeled"}, {"type": "text", "expr": "State-of-the-art plan recognition approaches with plan libraries as input aim at finding a plan from plan libraries to best explain the observed actions  (10) , which we denote by  MatchPlan ", "word_idx": 37371, "sentence_idx": 650, "label": "unlabeled"}, {"type": "text", "expr": " We develop a  MatchPlan  system based on the idea of  (10)  and compare our  DUP  algorithm to  MatchPlan  with respect to different percentages of unobserved actions  $\\xi$  and different sizes of suggestion or recommendation set  $S_{x}$ ", "word_idx": 37562, "sentence_idx": 651, "label": "unlabeled"}, {"type": "text", "expr": " Another baseline is action-models based plan recognition approach  (24)  (denoted by  PRP , short for Plan Recognition as Planning)", "word_idx": 37803, "sentence_idx": 652, "label": "unlabeled"}, {"type": "text", "expr": " Since we do not have action models as input in our  DUP  algorithm, we exploited the action model learning system  ARMS   (30)  to learn action models from the plan library and feed the action models to the  PRP  approach", "word_idx": 37935, "sentence_idx": 653, "label": "unlabeled"}, {"type": "text", "expr": " We call this hybrid plan recognition approach  ARMS+PRP ", "word_idx": 38157, "sentence_idx": 654, "label": "unlabeled"}, {"type": "text", "expr": " To learn action models,  ARMS  requires state information of plans as input", "word_idx": 38214, "sentence_idx": 655, "label": "unlabeled"}, {"type": "text", "expr": " We thus added extra information, i", "word_idx": 38290, "sentence_idx": 656, "label": "unlabeled"}, {"type": "text", "expr": ", initial state and goal of each plan in the plan library, to  ARMS+PRP ", "word_idx": 38325, "sentence_idx": 657, "label": "unlabeled"}, {"type": "text", "expr": " In addition,  PRP  requires as input a set of candidate goals  $\\mathcal{G}$  for each plan to be recognized in the testing set, which was also generated and fed to  PRP  when testing", "word_idx": 38397, "sentence_idx": 658, "label": "unlabeled"}, {"type": "text", "expr": " In summary, the hybrid plan recognition approach  ARMS+PRP  has more input information, i", "word_idx": 38581, "sentence_idx": 659, "label": "unlabeled"}, {"type": "text", "expr": ", initial states and goals in plan library and candidate goals  $\\mathcal{G}$  for each testing example, than our  DUP  approach", "word_idx": 38671, "sentence_idx": 660, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 38799, "sentence_idx": 661, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 38808, "sentence_idx": 662, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 38817, "sentence_idx": 663, "label": "unlabeled"}, {"type": "math", "expr": "$$\\xi$$", "word_idx": 38826, "sentence_idx": 664, "label": "unlabeled"}, {"type": "math", "expr": "$$S_{x}$$", "word_idx": 38829, "sentence_idx": 665, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 38834, "sentence_idx": 666, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 38842, "sentence_idx": 667, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{G}$$", "word_idx": 38850, "sentence_idx": 668, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 38861, "sentence_idx": 669, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{G}$$", "word_idx": 38869, "sentence_idx": 670, "label": "unlabeled"}, {"type": "text", "expr": "To evaluate DUP, we compared it with several baselines that we elaborated above, i", "word_idx": 38880, "sentence_idx": 671, "label": "unlabeled"}, {"type": "text", "expr": ",  MatchPlan , and  ARMS+PRP ", "word_idx": 38962, "sentence_idx": 672, "label": "unlabeled"}, {"type": "text", "expr": " We randomly divided the plans into ten folds, with 500 plans in each fold", "word_idx": 38991, "sentence_idx": 673, "label": "unlabeled"}, {"type": "text", "expr": " We ran our  DUP  algorithm ten times to calculate an average of accuracies, each time with one fold for testing and the rest for training", "word_idx": 39065, "sentence_idx": 674, "label": "unlabeled"}, {"type": "text", "expr": " In the testing data, we randomly removed actions from each testing plan (i", "word_idx": 39203, "sentence_idx": 675, "label": "unlabeled"}, {"type": "text", "expr": ",  $\\mathcal{O}$ ) with a specific percentage  $\\xi$  of the plan length", "word_idx": 39278, "sentence_idx": 676, "label": "unlabeled"}, {"type": "text", "expr": " Features of datasets are shown in Table  1 , where the second column is the number of plans generated, the third column is the total number of words (or actions) of all plans, and the last column is the size of vocabulary used in all plans", "word_idx": 39350, "sentence_idx": 677, "label": "unlabeled"}, {"type": "text", "expr": "\nTo evaluate our  RNNPlanner  algorithm, we directly compared  RNNPlanner  to  DUP ", "word_idx": 39590, "sentence_idx": 678, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 39673, "sentence_idx": 679, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 39682, "sentence_idx": 680, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 39690, "sentence_idx": 681, "label": "unlabeled"}, {"type": "math", "expr": "$$\\xi$$", "word_idx": 39701, "sentence_idx": 682, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 39704, "sentence_idx": 683, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 39714, "sentence_idx": 684, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:  Accuracies of  DUP  and  ARMS+PRP  with respect to different percentage of unobserved actions", "word_idx": 39724, "sentence_idx": 685, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:", "word_idx": 39828, "sentence_idx": 686, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 39837, "sentence_idx": 687, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a05:  Accuracies of  DUP  and  ARMS+PRP  with respect to different size of recommendations", "word_idx": 39845, "sentence_idx": 688, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a05:", "word_idx": 39940, "sentence_idx": 689, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 39949, "sentence_idx": 690, "label": "unlabeled"}, {"type": "text", "expr": "1  Comparison between  DUP  and  ARMS+PRP", "word_idx": 39957, "sentence_idx": 691, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 39998, "sentence_idx": 692, "label": "unlabeled"}, {"type": "text", "expr": "We first compare our  DUP  algorithm to  ARMS+PRP  to see the advantage of  DUP ", "word_idx": 40006, "sentence_idx": 693, "label": "unlabeled"}, {"type": "text", "expr": " We varied the percentage of unobserved actions and the size of recommended actions to see the change of accuracies, respectively", "word_idx": 40086, "sentence_idx": 694, "label": "unlabeled"}, {"type": "text", "expr": " The results are shown below", "word_idx": 40215, "sentence_idx": 695, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 40243, "sentence_idx": 696, "label": "unlabeled"}, {"type": "text", "expr": "1  Varying Percentage of Unobserved Actions", "word_idx": 40251, "sentence_idx": 697, "label": "unlabeled"}, {"type": "text", "expr": "In this experiment we would like to see the change of accuracies of both our  DUP  algorithm and  ARMS+PRP  with respect to  $\\xi$  in  $\\mathcal{O}$ ", "word_idx": 40294, "sentence_idx": 698, "label": "unlabeled"}, {"type": "text", "expr": " We set the window of training context  $c$  in Equation ( 1 ) to be three, the number of iterations in Algorithm  1  to be 1500, the size of recommendations to be ten, and the learning constant  $\\delta$  in Equation ( 7 ) to be 0", "word_idx": 40444, "sentence_idx": 699, "label": "unlabeled"}, {"type": "text", "expr": " For  ARMS+PRP , we generated 20 candidate goals for each testing example including the ground-truth goal which corresponds to the ground-truth plan to be recognized", "word_idx": 40675, "sentence_idx": 700, "label": "unlabeled"}, {"type": "text", "expr": " The results are shown in Figure  4 ", "word_idx": 40840, "sentence_idx": 701, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 40876, "sentence_idx": 702, "label": "unlabeled"}, {"type": "math", "expr": "$$\\xi$$", "word_idx": 40884, "sentence_idx": 703, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 40887, "sentence_idx": 704, "label": "unlabeled"}, {"type": "math", "expr": "$$\\delta$$", "word_idx": 40898, "sentence_idx": 705, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 40904, "sentence_idx": 706, "label": "unlabeled"}, {"type": "text", "expr": "From Figure  4 , we can see that in all three domains, the accuracy of our  DUP  algorithm is generally higher  ARMS+PRP , which verifies that our  DUP  algorithm can indeed capture relations among actions better than the model-based approach  ARMS+PRP ", "word_idx": 40912, "sentence_idx": 707, "label": "unlabeled"}, {"type": "text", "expr": " The rationale is that we explore global plan information from the plan library to learn a \u201cshallow\u201d model (distributed representations of actions) and use this model with global information to best explain the observed actions", "word_idx": 41165, "sentence_idx": 708, "label": "unlabeled"}, {"type": "text", "expr": " While  ARMS+PRP  tries to leverage global plan information from the plan library to learn action models and uses the models to recognize observed actions, it enforces itself to extract \u201cexact\u201d models represented by planning models which are often with noise", "word_idx": 41392, "sentence_idx": 709, "label": "unlabeled"}, {"type": "text", "expr": " When feeding those noisy models to  PRP , since  PRP  that uses planning techniques to recognize plans is very sensitive to noise of planning models, the recognition accuracy is lower than  DUP , even though  ARMS+PRP  has more input information (i", "word_idx": 41650, "sentence_idx": 710, "label": "unlabeled"}, {"type": "text", "expr": ", initial states and candidate goals) than our  DUP  algorithm", "word_idx": 41899, "sentence_idx": 711, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 41961, "sentence_idx": 712, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 41969, "sentence_idx": 713, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 41977, "sentence_idx": 714, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 41985, "sentence_idx": 715, "label": "unlabeled"}, {"type": "text", "expr": "Looking at the changes of accuracies with respect to the percentage of\nunobserved actions, we can see that our  DUP  algorithm performs\nfairly well even when the percentage of unobserved action reaches\n25%", "word_idx": 41993, "sentence_idx": 716, "label": "unlabeled"}, {"type": "text", "expr": " In contrast,  ARMS+PRP  is sensitive to the percentage of unobserved\nactions, i", "word_idx": 42198, "sentence_idx": 717, "label": "unlabeled"}, {"type": "text", "expr": ", the accuracy goes down when more actions are\nunobserved", "word_idx": 42278, "sentence_idx": 718, "label": "unlabeled"}, {"type": "text", "expr": " This is because the noise of planning models induces more\nuncertain information, which harms the recognition accuracy, when the\npercentage of unobserved actions becomes larger", "word_idx": 42335, "sentence_idx": 719, "label": "unlabeled"}, {"type": "text", "expr": " Comparing accuracies\nof different domains, we can see that our  DUP  algorithm functions\nbetter in the  blocks  domain than the other two domains", "word_idx": 42511, "sentence_idx": 720, "label": "unlabeled"}, {"type": "text", "expr": " This is\nbecause the ratio of #word over #vocabulary in the  blocks \ndomain is much larger than the other two domains, as shown in Table\n 1 ", "word_idx": 42657, "sentence_idx": 721, "label": "unlabeled"}, {"type": "text", "expr": " We would conjecture that increasing the ratio could\nimprove the accuracy of  DUP ", "word_idx": 42797, "sentence_idx": 722, "label": "unlabeled"}, {"type": "text", "expr": " From Figure  4  (as well as Figure  6 ), we can see that it appears that the accuracy of DUP is not\naffected by increasing percentages of unobserved actions", "word_idx": 42879, "sentence_idx": 723, "label": "unlabeled"}, {"type": "text", "expr": " The rationale is (1) the percentage of unobserved actions is low,\nless than 25%, i", "word_idx": 43036, "sentence_idx": 724, "label": "unlabeled"}, {"type": "text", "expr": ",\nthere is at most one unobserved action over four continuous actions; (2) the window size of\ncontext in DUP is set to be 3, which ensures that DUP generally has \u201dstable\u201d context information to estimate\nthe unobserved action when the percentage of unobserved actions is less than 25%, resulting in the stable accuracy\nin Figure  4  (likewise for Figure  6 )", "word_idx": 43119, "sentence_idx": 725, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 43476, "sentence_idx": 726, "label": "unlabeled"}, {"type": "text", "expr": "blocks", "word_idx": 43484, "sentence_idx": 727, "label": "unlabeled"}, {"type": "text", "expr": "blocks", "word_idx": 43490, "sentence_idx": 728, "label": "unlabeled"}, {"type": "text", "expr": "2  Varying Size of Recommendation Set", "word_idx": 43496, "sentence_idx": 729, "label": "unlabeled"}, {"type": "text", "expr": "We next evaluate the performance of our  DUP  algorithm with respect to the size of recommendation set  $S_{x}$ ", "word_idx": 43533, "sentence_idx": 730, "label": "unlabeled"}, {"type": "text", "expr": " We evaluate the influence of the recommendation set by varying the size from 1 to 10", "word_idx": 43645, "sentence_idx": 731, "label": "unlabeled"}, {"type": "text", "expr": " The size of recommendation set is much smaller than the complete set", "word_idx": 43730, "sentence_idx": 732, "label": "unlabeled"}, {"type": "text", "expr": " For example, the size of complete set in the blocks domain is 1250 (shown in Table 1)", "word_idx": 43799, "sentence_idx": 733, "label": "unlabeled"}, {"type": "text", "expr": " It is less than 1% even though we recommend 10 actions for each unobserved action", "word_idx": 43885, "sentence_idx": 734, "label": "unlabeled"}, {"type": "text", "expr": " We set the context window  $c$  used in Equation ( 1 ) to be three, the percentage of unobserved actions to be 0", "word_idx": 43967, "sentence_idx": 735, "label": "unlabeled"}, {"type": "text", "expr": "25, and the learning constant  $\\delta$  in Equation ( 7 ) to be 0", "word_idx": 44080, "sentence_idx": 736, "label": "unlabeled"}, {"type": "text", "expr": " For  ARMS+PRP , the number of candidate goals for each testing example is set to 20", "word_idx": 44146, "sentence_idx": 737, "label": "unlabeled"}, {"type": "text", "expr": "  ARMS+PRP  aims to recognize plans that are optimal with respect to the cost of actions", "word_idx": 44230, "sentence_idx": 738, "label": "unlabeled"}, {"type": "text", "expr": " We relax  ARMS+PRP  to output  $|S_{x}|$  optimal plans, some of which might be suboptimal", "word_idx": 44318, "sentence_idx": 739, "label": "unlabeled"}, {"type": "text", "expr": " The results are shown in Figure  5 ", "word_idx": 44409, "sentence_idx": 740, "label": "unlabeled"}, {"type": "math", "expr": "$$S_{x}$$", "word_idx": 44445, "sentence_idx": 741, "label": "unlabeled"}, {"type": "math", "expr": "$$\\delta$$", "word_idx": 44450, "sentence_idx": 742, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 44456, "sentence_idx": 743, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 44464, "sentence_idx": 744, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 44472, "sentence_idx": 745, "label": "unlabeled"}, {"type": "math", "expr": "$$|S_{x}|$$", "word_idx": 44480, "sentence_idx": 746, "label": "unlabeled"}, {"type": "text", "expr": "From Figure  5 , we find that accuracies of the three approaches generally become larger when the size of the recommended set increases in all three domains", "word_idx": 44487, "sentence_idx": 747, "label": "unlabeled"}, {"type": "text", "expr": " This is consistent with our intuition, since the larger the recommended set is, the higher the possibility for the  truth  action to be in the recommended set", "word_idx": 44643, "sentence_idx": 748, "label": "unlabeled"}, {"type": "text", "expr": " We can also see that the accuracy of our  DUP  algorithm are generally larger than  ARMS+PRP  in all three domains, which verifies that our  DUP  algorithm can indeed better capture relations among actions and thus recognize unobserved actions better than the model-learning based approach  ARMS+PRP ", "word_idx": 44802, "sentence_idx": 749, "label": "unlabeled"}, {"type": "text", "expr": " The reason is similar to the one given for Figure  4  in the previous section", "word_idx": 45103, "sentence_idx": 750, "label": "unlabeled"}, {"type": "text", "expr": " That is, the \u201cshallow\u201d model learnt by our  DUP  algorithm is better for recognizing plans than both the \u201cexact\u201d planning model learnt by  ARMS  for recognizing plans with planning techniques", "word_idx": 45181, "sentence_idx": 751, "label": "unlabeled"}, {"type": "text", "expr": " Furthermore, the advantage of  DUP  becomes even larger when the size of recommended action set increases, which suggests our vector representation based learning approach can better capture action relations when the size of recommended action set is larger", "word_idx": 45373, "sentence_idx": 752, "label": "unlabeled"}, {"type": "text", "expr": " The possibility of actions correctly recognized by  DUP  becomes much larger than  ARMS+PRP  when the size of recommendations increases", "word_idx": 45631, "sentence_idx": 753, "label": "unlabeled"}, {"type": "text", "expr": "truth", "word_idx": 45767, "sentence_idx": 754, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 45772, "sentence_idx": 755, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 45780, "sentence_idx": 756, "label": "unlabeled"}, {"type": "text", "expr": "ARMS+PRP", "word_idx": 45788, "sentence_idx": 757, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a06:  Accuracies of  DUP  and  MatchPlan  with respect to different percentage of unobserved actions", "word_idx": 45796, "sentence_idx": 758, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a06:", "word_idx": 45901, "sentence_idx": 759, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 45910, "sentence_idx": 760, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a07:  Accuracies of  DUP  and  MatchPlan  with respect to different size of recommendations", "word_idx": 45919, "sentence_idx": 761, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a07:", "word_idx": 46015, "sentence_idx": 762, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 46024, "sentence_idx": 763, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a08:  Accuracies of  DUP  and  MatchPlan  with respect to different size of training set", "word_idx": 46033, "sentence_idx": 764, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a08:", "word_idx": 46126, "sentence_idx": 765, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 46135, "sentence_idx": 766, "label": "unlabeled"}, {"type": "text", "expr": "2  Comparison between  DUP  and  MatchPlan", "word_idx": 46144, "sentence_idx": 767, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 46186, "sentence_idx": 768, "label": "unlabeled"}, {"type": "text", "expr": "In this experiment we compare  DUP  to  MatchPlan  which is built based on the idea of  (10) ", "word_idx": 46195, "sentence_idx": 769, "label": "unlabeled"}, {"type": "text", "expr": " Likewise we varied the percentage of unobserved actions and the size of recommended actions to see the change of accuracies of both algorithms", "word_idx": 46288, "sentence_idx": 770, "label": "unlabeled"}, {"type": "text", "expr": " The results are exhibited below", "word_idx": 46431, "sentence_idx": 771, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 46463, "sentence_idx": 772, "label": "unlabeled"}, {"type": "text", "expr": "1  Varying Percentage of Unobserved Actions", "word_idx": 46472, "sentence_idx": 773, "label": "unlabeled"}, {"type": "text", "expr": "To compare our  DUP  algorithm with  MatchPlan  with respect to different percentage of unobserved actions, we set the window of training context  $c$  in Equation ( 1 ) of  DUP  to be three, the number of iterations in Algorithm  1  to be 1500, the size of recommendations to be ten, and the learning constant  $\\delta$  in Equation ( 7 ) to be 0", "word_idx": 46515, "sentence_idx": 774, "label": "unlabeled"}, {"type": "text", "expr": "\nTo make fair the comparison (with  MatchPlan ), we set the matching window of  MatchPlan  to be three, the same as the training context  $c$  of  DUP , when searching plans from plan libraries  $\\mathcal{L}$ ", "word_idx": 46862, "sentence_idx": 775, "label": "unlabeled"}, {"type": "text", "expr": " In other words, to estimate an unobserved action  $\\bar{a}_{x}$  in  $\\mathcal{O}$ ,  MatchPlan  matches previous three actions and subsequent three actions of  $\\bar{a}_{x}$  to plans in  $\\mathcal{L}$ , and recommends ten actions with the maximal number of matched actions, considering observed actions in the context of  $\\bar{a}_{x}$  and actions in  $\\mathcal{L}$  as a successful matching", "word_idx": 47071, "sentence_idx": 776, "label": "unlabeled"}, {"type": "text", "expr": " The results are shown in Figure  6 ", "word_idx": 47466, "sentence_idx": 777, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 47502, "sentence_idx": 778, "label": "unlabeled"}, {"type": "math", "expr": "$$\\delta$$", "word_idx": 47511, "sentence_idx": 779, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 47517, "sentence_idx": 780, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 47526, "sentence_idx": 781, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}$$", "word_idx": 47535, "sentence_idx": 782, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{a}_{x}$$", "word_idx": 47546, "sentence_idx": 783, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{O}$$", "word_idx": 47557, "sentence_idx": 784, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 47568, "sentence_idx": 785, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{a}_{x}$$", "word_idx": 47577, "sentence_idx": 786, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}$$", "word_idx": 47588, "sentence_idx": 787, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bar{a}_{x}$$", "word_idx": 47599, "sentence_idx": 788, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}$$", "word_idx": 47610, "sentence_idx": 789, "label": "unlabeled"}, {"type": "text", "expr": "From Figure  6 , we find that the accuracy of  DUP  is much better than  MatchPlan , which indicates that our  DUP  algorithm can better learn knowledge from plan libraries than the local matching approach  MatchPlan ", "word_idx": 47621, "sentence_idx": 790, "label": "unlabeled"}, {"type": "text", "expr": " This is because we take advantage of global plan information of the plan library when learning the \u201cshallow\u201d model, i", "word_idx": 47838, "sentence_idx": 791, "label": "unlabeled"}, {"type": "text", "expr": ", distributed representations of actions, and the model with global information can best explain the observed actions", "word_idx": 47956, "sentence_idx": 792, "label": "unlabeled"}, {"type": "text", "expr": "\nIn contrast,  MatchPlan  just utilizes local plan information when matching the observed actions to the plan library, which results in lower accuracies", "word_idx": 48073, "sentence_idx": 793, "label": "unlabeled"}, {"type": "text", "expr": " Looking at all three different domains, we can see that both algorithms perform the best in the  blocks  domain", "word_idx": 48225, "sentence_idx": 794, "label": "unlabeled"}, {"type": "text", "expr": " The reason is similar to the one provided in the last subsection (for Figure  4 ), i", "word_idx": 48337, "sentence_idx": 795, "label": "unlabeled"}, {"type": "text", "expr": ", the number of words over the number of vocabulary in the  blocks  domain is relatively larger than the other two domains, which gives us the hint that it is possible to improve accuracies by increasing the ratio of the number of words over the number of vocabularies", "word_idx": 48422, "sentence_idx": 796, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 48690, "sentence_idx": 797, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 48699, "sentence_idx": 798, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 48708, "sentence_idx": 799, "label": "unlabeled"}, {"type": "text", "expr": "blocks", "word_idx": 48717, "sentence_idx": 800, "label": "unlabeled"}, {"type": "text", "expr": "blocks", "word_idx": 48723, "sentence_idx": 801, "label": "unlabeled"}, {"type": "text", "expr": "2  Varying Size of Recommendation Set", "word_idx": 48729, "sentence_idx": 802, "label": "unlabeled"}, {"type": "text", "expr": "Likewise, we also would like to evaluate the change of accuracies when increasing the size of recommended actions", "word_idx": 48766, "sentence_idx": 803, "label": "unlabeled"}, {"type": "text", "expr": " We used the same experimental setting as done by previous subsection", "word_idx": 48879, "sentence_idx": 804, "label": "unlabeled"}, {"type": "text", "expr": " That is, we set the window of training context  $c$  of  DUP  to be three, the learning constant  $\\delta$  to be 0", "word_idx": 48948, "sentence_idx": 805, "label": "unlabeled"}, {"type": "text", "expr": "1, the number of iterations in Algorithm  1  to be 1500, the matching window of  MatchPlan  to be three", "word_idx": 49064, "sentence_idx": 806, "label": "unlabeled"}, {"type": "text", "expr": " In addition, we fix the percentage of unobserved actions to be 0", "word_idx": 49167, "sentence_idx": 807, "label": "unlabeled"}, {"type": "text", "expr": " The results are shown in Figure  7 ", "word_idx": 49232, "sentence_idx": 808, "label": "unlabeled"}, {"type": "math", "expr": "$$\\delta$$", "word_idx": 49268, "sentence_idx": 809, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 49274, "sentence_idx": 810, "label": "unlabeled"}, {"type": "text", "expr": "We can observe that the accuracy of our  DUP  algorithm are generally larger than  MatchPlan  in all three domains in Figure  7 , which suggests that our  DUP  algorithm can indeed better capture relations among actions and thus recognize unobserved actions better than the matching approach  MatchPlan ", "word_idx": 49283, "sentence_idx": 811, "label": "unlabeled"}, {"type": "text", "expr": " The reason behind this is similar to previous experiments, i", "word_idx": 49586, "sentence_idx": 812, "label": "unlabeled"}, {"type": "text", "expr": ", the global information captured from plan libraries by  DUP  can indeed better improve accuracies than local information exploited by  MatchPlan ", "word_idx": 49647, "sentence_idx": 813, "label": "unlabeled"}, {"type": "text", "expr": " In addition, looking at the trends of the curves of both  DUP  and  MatchPlan , we can see the performance of  DUP  becomes much better than  MatchPlan  when the size of recommendations increases", "word_idx": 49794, "sentence_idx": 814, "label": "unlabeled"}, {"type": "text", "expr": " This indicates the influence of global information becomes much larger when the size of recommendations increasing", "word_idx": 49990, "sentence_idx": 815, "label": "unlabeled"}, {"type": "text", "expr": " In other words, larger size of recommendations provides better chance for \u201cshallow\u201d models learnt by  DUP  to perform better", "word_idx": 50105, "sentence_idx": 816, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 50230, "sentence_idx": 817, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 50239, "sentence_idx": 818, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 50248, "sentence_idx": 819, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 50257, "sentence_idx": 820, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 50266, "sentence_idx": 821, "label": "unlabeled"}, {"type": "text", "expr": "3  Varying Size of Training Set", "word_idx": 50275, "sentence_idx": 822, "label": "unlabeled"}, {"type": "text", "expr": "To see the effect of size of training set, we ran both  DUP  and  MatchPlan  with different size of training set", "word_idx": 50306, "sentence_idx": 823, "label": "unlabeled"}, {"type": "text", "expr": " We used the same setting as done by last subsection except fixing the size of recommendations to be 10, when running both algorithms", "word_idx": 50418, "sentence_idx": 824, "label": "unlabeled"}, {"type": "text", "expr": " We varied the size of training set from 2500 to 4500", "word_idx": 50551, "sentence_idx": 825, "label": "unlabeled"}, {"type": "text", "expr": " The results are shown in Figure  8 ", "word_idx": 50604, "sentence_idx": 826, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 50640, "sentence_idx": 827, "label": "unlabeled"}, {"type": "text", "expr": "We observed that accuracies of both  DUP  and  MatchPlan  generally become higher when the size of training set increases", "word_idx": 50649, "sentence_idx": 828, "label": "unlabeled"}, {"type": "text", "expr": " This is consistent with our intuition, since the larger the size of training set is, the richer the information is available for improving the accuracies", "word_idx": 50770, "sentence_idx": 829, "label": "unlabeled"}, {"type": "text", "expr": " Comparing the curves of  DUP  and  MatchPlan , we can see that  DUP  performs much better than  MatchPlan ", "word_idx": 50924, "sentence_idx": 830, "label": "unlabeled"}, {"type": "text", "expr": " This further verifies the benefit of exploiting global information of plan libraries when learning the shallow models as done by  DUP ", "word_idx": 51031, "sentence_idx": 831, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 51166, "sentence_idx": 832, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 51175, "sentence_idx": 833, "label": "unlabeled"}, {"type": "text", "expr": "MatchPlan", "word_idx": 51184, "sentence_idx": 834, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a09:  Accuracy with respect to different number of iterations in the blocks domain", "word_idx": 51193, "sentence_idx": 835, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a09:", "word_idx": 51280, "sentence_idx": 836, "label": "unlabeled"}, {"type": "text", "expr": "3  Accuracy w", "word_idx": 51289, "sentence_idx": 837, "label": "unlabeled"}, {"type": "text", "expr": " Iterations", "word_idx": 51302, "sentence_idx": 838, "label": "unlabeled"}, {"type": "text", "expr": "In the previous experiments, we set the number of iterations in Algorithm  1  to be 1500", "word_idx": 51313, "sentence_idx": 839, "label": "unlabeled"}, {"type": "text", "expr": " In this experiment, we would like to see the influence of iterations of our  DUP  algorithm when running the EM-style procedure", "word_idx": 51401, "sentence_idx": 840, "label": "unlabeled"}, {"type": "text", "expr": " We changed the number of iterations from 300 to 3000 to see the trend of accuracy", "word_idx": 51529, "sentence_idx": 841, "label": "unlabeled"}, {"type": "text", "expr": " We exhibit the experimental results in the  blocks  domain (the results of the other two domains are similar) in Figure  9 ", "word_idx": 51611, "sentence_idx": 842, "label": "unlabeled"}, {"type": "text", "expr": "blocks", "word_idx": 51735, "sentence_idx": 843, "label": "unlabeled"}, {"type": "text", "expr": "From Figure  9 , we can see the accuracy becomes higher at the beginning and stays flat when reaching the size of 1500", "word_idx": 51741, "sentence_idx": 844, "label": "unlabeled"}, {"type": "text", "expr": " This exhibits that the EM procedure converges and has stable accuracies after the iteration reaches 1500", "word_idx": 51859, "sentence_idx": 845, "label": "unlabeled"}, {"type": "text", "expr": " Similar results can also be found in the other two domains", "word_idx": 51964, "sentence_idx": 846, "label": "unlabeled"}, {"type": "text", "expr": "4  Comparison between  RNNPlanner  and  DUP", "word_idx": 52023, "sentence_idx": 847, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 52066, "sentence_idx": 848, "label": "unlabeled"}, {"type": "text", "expr": "In this section we compare  RNNPlanner  with  DUP  to see the change of performance with respect to different distributions of missing actions in the underlying plans to be discovered", "word_idx": 52076, "sentence_idx": 849, "label": "unlabeled"}, {"type": "text", "expr": " In this experiment, we are interested in evaluating the performance on consecutive missing actions in the underlying plans since these scenarios often exist in many applications such as surveillance  (1) ", "word_idx": 52259, "sentence_idx": 850, "label": "unlabeled"}, {"type": "text", "expr": " We first test the performance of both  RNNPlanner  and  DUP  in discovering underlying plans with only consecutive missing actions in the \u201cmiddle\u201d of the plans, i", "word_idx": 52464, "sentence_idx": 851, "label": "unlabeled"}, {"type": "text", "expr": ", actions are not missing at the end or in the front, which indicates missing actions can be inferred from both previously and subsequently observed actions", "word_idx": 52627, "sentence_idx": 852, "label": "unlabeled"}, {"type": "text", "expr": " Then we evaluate both  RNNPlanner  and  DUP  in discovering underlying plans with only consecutive missing actions at the end of the plans, which indicates missing actions can only be inferred from previously observed actions", "word_idx": 52783, "sentence_idx": 853, "label": "unlabeled"}, {"type": "text", "expr": " After that, we also evaluate the performance of our  RNNPlanner  and  DUP  approaches with respect to the size of recommendation set", "word_idx": 53009, "sentence_idx": 854, "label": "unlabeled"}, {"type": "text", "expr": " In the following subsections, we present the experimental results regarding those three aspects", "word_idx": 53142, "sentence_idx": 855, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 53238, "sentence_idx": 856, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 53248, "sentence_idx": 857, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 53258, "sentence_idx": 858, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 53268, "sentence_idx": 859, "label": "unlabeled"}, {"type": "text", "expr": "1  Performance with missing actions in the middle", "word_idx": 53278, "sentence_idx": 860, "label": "unlabeled"}, {"type": "text", "expr": "To see the performance of  RNNPlanner  and  DUP  in cases when actions are missing in the middle of the underlying plan to be discovered, we vary the number of consecutive missing actions from 1 to 10, to see the change of accuracies of both  RNNPlanner  and  DUP ", "word_idx": 53327, "sentence_idx": 861, "label": "unlabeled"}, {"type": "text", "expr": " We set the window size to be 1 and the recommendation size to be 10", "word_idx": 53591, "sentence_idx": 862, "label": "unlabeled"}, {"type": "text", "expr": " The results are shown in Figure  10 ", "word_idx": 53659, "sentence_idx": 863, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 53696, "sentence_idx": 864, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 53706, "sentence_idx": 865, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a010:  Accuracy with respect to missing actions in the middle", "word_idx": 53716, "sentence_idx": 866, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a010:", "word_idx": 53782, "sentence_idx": 867, "label": "unlabeled"}, {"type": "text", "expr": "From Figure  10 , we can see that the accuracies of both  RNNPlanner  and  DUP  generally become lower when the number of consecutive unobserved actions increasing", "word_idx": 53792, "sentence_idx": 868, "label": "unlabeled"}, {"type": "text", "expr": " This is consistent with our intuition since the more actions are missing, the less information can be used to help infer the unobserved actions, which results in low accuracies", "word_idx": 53955, "sentence_idx": 869, "label": "unlabeled"}, {"type": "text", "expr": " Comparing the curves of  RNNPlanner  and  DUP , we can see that the accuracy of  DUP  is higher than  RNNPlanner  at the beginning", "word_idx": 54132, "sentence_idx": 870, "label": "unlabeled"}, {"type": "text", "expr": " This is because  DUP  exploits information of both observed actions before and after missing actions to infer the missing actions, while  RNNPlanner  just exploits observed actions before missing actions", "word_idx": 54263, "sentence_idx": 871, "label": "unlabeled"}, {"type": "text", "expr": " When the number of missing actions is larger than 3, the accuracies of  DUP  and  RNNPlanner  are both low (i", "word_idx": 54467, "sentence_idx": 872, "label": "unlabeled"}, {"type": "text", "expr": ", lower than 0", "word_idx": 54577, "sentence_idx": 873, "label": "unlabeled"}, {"type": "text", "expr": " This is because the window size of  DUP  and  RNNPlanner  is set to be 1, which indicates we exploit one action before the missing actions and one action after the missing actions to estimate the missing actions", "word_idx": 54591, "sentence_idx": 874, "label": "unlabeled"}, {"type": "text", "expr": " When the consecutive missing actions are more than 1, there may not be sufficient context information for inferring the missing actions, resulting in low accuracies", "word_idx": 54803, "sentence_idx": 875, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 54968, "sentence_idx": 876, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 54978, "sentence_idx": 877, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 54988, "sentence_idx": 878, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 54998, "sentence_idx": 879, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 55008, "sentence_idx": 880, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 55018, "sentence_idx": 881, "label": "unlabeled"}, {"type": "text", "expr": "2  Performance with missing actions at the end", "word_idx": 55028, "sentence_idx": 882, "label": "unlabeled"}, {"type": "text", "expr": "We also would like to see the performance of  RNNPlanner  and  DUP  in discovering missing actions at the end, which is prevalent in application domains that aim at discovering/predicting future actions", "word_idx": 55074, "sentence_idx": 883, "label": "unlabeled"}, {"type": "text", "expr": " Similar to previous experiments, we vary the number of consecutive unobserved or missing actions to see the change of accuracies of both  RNNPlanner  and  DUP ", "word_idx": 55276, "sentence_idx": 884, "label": "unlabeled"}, {"type": "text", "expr": " We set the window size to be 1 and the recommendation size to be 10 as well", "word_idx": 55436, "sentence_idx": 885, "label": "unlabeled"}, {"type": "text", "expr": " The experimental results are shown in Figure  11 ", "word_idx": 55512, "sentence_idx": 886, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 55562, "sentence_idx": 887, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 55572, "sentence_idx": 888, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a011:  Accuracy with respect to missing actions in the end", "word_idx": 55582, "sentence_idx": 889, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a011:", "word_idx": 55645, "sentence_idx": 890, "label": "unlabeled"}, {"type": "text", "expr": "From Figure  11 , we can observe that the accuracies of both  RNNPlanner  and  DUP  generally get decreasing when the number of consecutive missing actions increases", "word_idx": 55655, "sentence_idx": 891, "label": "unlabeled"}, {"type": "text", "expr": " This is similar to previous experimental results", "word_idx": 55820, "sentence_idx": 892, "label": "unlabeled"}, {"type": "text", "expr": " That is, the more actions are missing, the less information is available for estimating the missing actions, which results in lower accuracy", "word_idx": 55869, "sentence_idx": 893, "label": "unlabeled"}, {"type": "text", "expr": " In addition, we can also see that  RNNPlanner  generally performs better than  DUP , which indicates that the RNNs-based approach, i", "word_idx": 56010, "sentence_idx": 894, "label": "unlabeled"}, {"type": "text", "expr": ",  RNNPlanner , can indeed better exploit observed actions to predict future missing actions, since RNNs are capable of flexibly leveraging long or short-term information to help predict missing actions", "word_idx": 56143, "sentence_idx": 895, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 56345, "sentence_idx": 896, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 56355, "sentence_idx": 897, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 56365, "sentence_idx": 898, "label": "unlabeled"}, {"type": "text", "expr": "3  Performance with respect to different recommendation size", "word_idx": 56375, "sentence_idx": 899, "label": "unlabeled"}, {"type": "text", "expr": "To see the change with respect to different recommendation size, we vary the size of recommendation sets from 1 to 10 and calculate their corresponding accuracies", "word_idx": 56435, "sentence_idx": 900, "label": "unlabeled"}, {"type": "text", "expr": " We test our approaches with four cases: A", "word_idx": 56597, "sentence_idx": 901, "label": "unlabeled"}, {"type": "text", "expr": " there are five actions missing at the end; B", "word_idx": 56639, "sentence_idx": 902, "label": "unlabeled"}, {"type": "text", "expr": " there are five actions missing in the middle; C", "word_idx": 56684, "sentence_idx": 903, "label": "unlabeled"}, {"type": "text", "expr": " there is one action missing at the end; D", "word_idx": 56732, "sentence_idx": 904, "label": "unlabeled"}, {"type": "text", "expr": " there is one action missing in the middle", "word_idx": 56774, "sentence_idx": 905, "label": "unlabeled"}, {"type": "text", "expr": " The results are shown in Figures  12 - 15  corresponding to cases A-D, respectively", "word_idx": 56816, "sentence_idx": 906, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a012:  Case A: accuracy with respect to different size of recommendations", "word_idx": 56900, "sentence_idx": 907, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a012:", "word_idx": 56978, "sentence_idx": 908, "label": "unlabeled"}, {"type": "text", "expr": "Case A:", "word_idx": 56988, "sentence_idx": 909, "label": "unlabeled"}, {"type": "text", "expr": "As shown in Figure  12 ,  RNNPlanner  performs better than  DUP  mostly, except for when the recommendation set size is larger/equal to eight in blocks domain", "word_idx": 56995, "sentence_idx": 910, "label": "unlabeled"}, {"type": "text", "expr": " This is because  RNNPlanner , which contains LSTM cells, is able to actively remember or forget past observations (inputs) and computations (hidden states)", "word_idx": 57153, "sentence_idx": 911, "label": "unlabeled"}, {"type": "text", "expr": " For example, if in a set of sequences, a pattern  $A^{**}$  follows  $A^{*}$  after three words (i", "word_idx": 57309, "sentence_idx": 912, "label": "unlabeled"}, {"type": "text", "expr": ",  $,A^{*},w_{i},w_{i+1},w_{i+2},A^{**},$ ), where  $w_{i},w_{i+1},w_{i+2}$  could be any word from the vocabulary except for  $A^{*}$  and  $A^{**}$ ", "word_idx": 57408, "sentence_idx": 913, "label": "unlabeled"}, {"type": "text", "expr": " And if the window size of  DUP  is smaller than three, then  DUP  is not able to utilize this pattern to predict  $A^{**}$  mainly based on  $A^{*}$ ", "word_idx": 57558, "sentence_idx": 914, "label": "unlabeled"}, {"type": "text", "expr": " When predicting the  $A^{**}$ , the  DUP  with context size one, works by searching for the most similar word to the  $w_{i+2}$ ", "word_idx": 57708, "sentence_idx": 915, "label": "unlabeled"}, {"type": "text", "expr": " One would yet argue that we can set a larger window size for  DUP ", "word_idx": 57837, "sentence_idx": 916, "label": "unlabeled"}, {"type": "text", "expr": " Larger window size does not necessarily lead to higher accuracy, since using a larger window size also add more noise in the training of  DUP ", "word_idx": 57904, "sentence_idx": 917, "label": "unlabeled"}, {"type": "text", "expr": " Remember that the word2vec model treats equally all possible word pair samples within its context window", "word_idx": 58047, "sentence_idx": 918, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 58152, "sentence_idx": 919, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 58162, "sentence_idx": 920, "label": "unlabeled"}, {"type": "math", "expr": "$$A^{**}$$", "word_idx": 58172, "sentence_idx": 921, "label": "unlabeled"}, {"type": "math", "expr": "$$A^{*}$$", "word_idx": 58178, "sentence_idx": 922, "label": "unlabeled"}, {"type": "math", "expr": "$$...,A^{*},w_{i},w_{i+1},w_{i+2},A^{**},...$$", "word_idx": 58183, "sentence_idx": 923, "label": "unlabeled"}, {"type": "math", "expr": "$$w_{i},w_{i+1},w_{i+2}$$", "word_idx": 58225, "sentence_idx": 924, "label": "unlabeled"}, {"type": "math", "expr": "$$A^{*}$$", "word_idx": 58246, "sentence_idx": 925, "label": "unlabeled"}, {"type": "math", "expr": "$$A^{**}$$", "word_idx": 58251, "sentence_idx": 926, "label": "unlabeled"}, {"type": "math", "expr": "$$A^{**}$$", "word_idx": 58257, "sentence_idx": 927, "label": "unlabeled"}, {"type": "math", "expr": "$$A^{*}$$", "word_idx": 58263, "sentence_idx": 928, "label": "unlabeled"}, {"type": "math", "expr": "$$A^{**}$$", "word_idx": 58268, "sentence_idx": 929, "label": "unlabeled"}, {"type": "math", "expr": "$$w_{i+2}$$", "word_idx": 58274, "sentence_idx": 930, "label": "unlabeled"}, {"type": "text", "expr": "In addition, observing the accuracies (in terms of the size of recommendation  $S_{x}$ ) of all three domains , we can see only in the blocks domain that  DUP  outperforms  RNNPlanner , when  $S_{x}$  is larger than eight", "word_idx": 58281, "sentence_idx": 931, "label": "unlabeled"}, {"type": "text", "expr": " Also in the  blocks  domain,  DUP  has the best performance, comparing to how  DUP  functions in other two domains", "word_idx": 58502, "sentence_idx": 932, "label": "unlabeled"}, {"type": "text", "expr": " This is because plans from the  blocks  domain has an overall higher ratio of #word to #vocabulary, which increases the possibility that the word pattern outside a context window, would reappear inside the window, and consequently help  DUP  recognize actions in the missing positions", "word_idx": 58617, "sentence_idx": 933, "label": "unlabeled"}, {"type": "text", "expr": " Coming back to the example when we have a plan like  $,A^{*},w_{i},w_{i+1},w_{i+2},A^{**},$ , in  blocks  domain, it\u2019s more possible the word  $A^{*}$  happens again in one of  $w_{i}$ ,  $w_{i+1}$ , and  $w_{i+2}$ ", "word_idx": 58902, "sentence_idx": 934, "label": "unlabeled"}, {"type": "math", "expr": "$$S_{x}$$", "word_idx": 59118, "sentence_idx": 935, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 59123, "sentence_idx": 936, "label": "unlabeled"}, {"type": "math", "expr": "$$S_{x}$$", "word_idx": 59133, "sentence_idx": 937, "label": "unlabeled"}, {"type": "text", "expr": "blocks", "word_idx": 59138, "sentence_idx": 938, "label": "unlabeled"}, {"type": "text", "expr": "blocks", "word_idx": 59144, "sentence_idx": 939, "label": "unlabeled"}, {"type": "math", "expr": "$$...,A^{*},w_{i},w_{i+1},w_{i+2},A^{**},...$$", "word_idx": 59150, "sentence_idx": 940, "label": "unlabeled"}, {"type": "text", "expr": "blocks", "word_idx": 59192, "sentence_idx": 941, "label": "unlabeled"}, {"type": "math", "expr": "$$A^{*}$$", "word_idx": 59198, "sentence_idx": 942, "label": "unlabeled"}, {"type": "math", "expr": "$$w_{i}$$", "word_idx": 59203, "sentence_idx": 943, "label": "unlabeled"}, {"type": "math", "expr": "$$w_{i+1}$$", "word_idx": 59208, "sentence_idx": 944, "label": "unlabeled"}, {"type": "math", "expr": "$$w_{i+2}$$", "word_idx": 59215, "sentence_idx": 945, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a013:  Case B: accuracy with respect to different size of recommendations", "word_idx": 59222, "sentence_idx": 946, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a013:", "word_idx": 59300, "sentence_idx": 947, "label": "unlabeled"}, {"type": "text", "expr": "Case B:", "word_idx": 59310, "sentence_idx": 948, "label": "unlabeled"}, {"type": "text", "expr": "What we can observe here from Figure  13 , is similar to our observations in  case A ", "word_idx": 59317, "sentence_idx": 949, "label": "unlabeled"}, {"type": "text", "expr": "  RNNPlanner  generally performs better than  DUP , except for when the size of recommendation  $S_{x}$  is larger or equal to nine in  blocks  domain", "word_idx": 59402, "sentence_idx": 950, "label": "unlabeled"}, {"type": "text", "expr": " It could also be observed that both  RNNPlanner  and  DUP  have the best accuracy performance in the  blocks  domain", "word_idx": 59552, "sentence_idx": 951, "label": "unlabeled"}, {"type": "text", "expr": "case A", "word_idx": 59669, "sentence_idx": 952, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 59675, "sentence_idx": 953, "label": "unlabeled"}, {"type": "math", "expr": "$$S_{x}$$", "word_idx": 59685, "sentence_idx": 954, "label": "unlabeled"}, {"type": "text", "expr": "blocks", "word_idx": 59690, "sentence_idx": 955, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 59696, "sentence_idx": 956, "label": "unlabeled"}, {"type": "text", "expr": "blocks", "word_idx": 59706, "sentence_idx": 957, "label": "unlabeled"}, {"type": "text", "expr": "And by comparing the Figure  13  in  case B  (five removed actions in the middle) with Figure  12  in  case A  (five removed actions at the end), we can see that, the accuracy difference between  DUP  and  RNNPlanner  at each size of recommendation along the x-axis, is smaller in  case B ", "word_idx": 59712, "sentence_idx": 958, "label": "unlabeled"}, {"type": "text", "expr": " This is because,  RNNPlanner  only leverages the observed actions before a missing position, whereas  DUP  has the advantage of additionally using the observation after a missing position", "word_idx": 60001, "sentence_idx": 959, "label": "unlabeled"}, {"type": "text", "expr": "case B", "word_idx": 60189, "sentence_idx": 960, "label": "unlabeled"}, {"type": "text", "expr": "case A", "word_idx": 60195, "sentence_idx": 961, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 60201, "sentence_idx": 962, "label": "unlabeled"}, {"type": "text", "expr": "case B", "word_idx": 60211, "sentence_idx": 963, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 60217, "sentence_idx": 964, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a014:  Case C: accuracy with respect to different size of recommendations", "word_idx": 60227, "sentence_idx": 965, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a014:", "word_idx": 60305, "sentence_idx": 966, "label": "unlabeled"}, {"type": "text", "expr": "Case C:", "word_idx": 60315, "sentence_idx": 967, "label": "unlabeled"}, {"type": "text", "expr": "From Figure  14 , we can see that both  RNNPlanner  and  DUP  could outperform each other in certain domains and recommendation set sizes ( $S_{x}$ )", "word_idx": 60322, "sentence_idx": 968, "label": "unlabeled"}, {"type": "text", "expr": " In  blocks  domain,  DUP  is better when  $S_{x}$  is larger than five", "word_idx": 60471, "sentence_idx": 969, "label": "unlabeled"}, {"type": "text", "expr": " In  depots  domain,  RNNPlanner  is overwhelmingly better than  DUP ", "word_idx": 60542, "sentence_idx": 970, "label": "unlabeled"}, {"type": "text", "expr": " In  driverlog  domain,  DUP  performs overall better except that, when there is only one recommendation,  DUP  is as good as  RNNPlanner ", "word_idx": 60611, "sentence_idx": 971, "label": "unlabeled"}, {"type": "text", "expr": " To explain this observation, if the number of consecutively removed action is less or equal to context window size (e", "word_idx": 60749, "sentence_idx": 972, "label": "unlabeled"}, {"type": "text", "expr": ", window size is one, and number of missing actions is one, in our  case C ), then the fixed, and short context window of  DUP , is competitive enough", "word_idx": 60867, "sentence_idx": 973, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 61017, "sentence_idx": 974, "label": "unlabeled"}, {"type": "math", "expr": "$$S_{x}$$", "word_idx": 61027, "sentence_idx": 975, "label": "unlabeled"}, {"type": "text", "expr": "blocks", "word_idx": 61032, "sentence_idx": 976, "label": "unlabeled"}, {"type": "math", "expr": "$$S_{x}$$", "word_idx": 61038, "sentence_idx": 977, "label": "unlabeled"}, {"type": "text", "expr": "depots", "word_idx": 61043, "sentence_idx": 978, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 61049, "sentence_idx": 979, "label": "unlabeled"}, {"type": "text", "expr": "driverlog", "word_idx": 61059, "sentence_idx": 980, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 61068, "sentence_idx": 981, "label": "unlabeled"}, {"type": "text", "expr": "case C", "word_idx": 61078, "sentence_idx": 982, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a015:  Case D: accuracy with respect to different size of recommendations", "word_idx": 61084, "sentence_idx": 983, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a015:", "word_idx": 61162, "sentence_idx": 984, "label": "unlabeled"}, {"type": "text", "expr": "Case D:", "word_idx": 61172, "sentence_idx": 985, "label": "unlabeled"}, {"type": "text", "expr": "From the results in Figure  15 , we can see that  DUP  functions better than  RNNPlanner  over all three domains, whereas  DUP  is worse in  case A  and  case B , and could occasionally be better than  RNNPlanner  in  case C ", "word_idx": 61179, "sentence_idx": 986, "label": "unlabeled"}, {"type": "text", "expr": " It makes sense in that, on the one hand, within the fixed and short context window, if there is very less positions with removed actions,  DUP  would have an improved performance", "word_idx": 61404, "sentence_idx": 987, "label": "unlabeled"}, {"type": "text", "expr": " On the other hand,  RNNPlanner  is not able to leverage the information from both sides of a position with a missing action", "word_idx": 61583, "sentence_idx": 988, "label": "unlabeled"}, {"type": "text", "expr": " Therefore, in  case D ,  DUP  gains the benefit from both assumptions that there is only one missing action, and the position of that action is randomly chosen in the middle of a plan", "word_idx": 61707, "sentence_idx": 989, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 61891, "sentence_idx": 990, "label": "unlabeled"}, {"type": "text", "expr": "case A", "word_idx": 61901, "sentence_idx": 991, "label": "unlabeled"}, {"type": "text", "expr": "case B", "word_idx": 61907, "sentence_idx": 992, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 61913, "sentence_idx": 993, "label": "unlabeled"}, {"type": "text", "expr": "case C", "word_idx": 61923, "sentence_idx": 994, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 61929, "sentence_idx": 995, "label": "unlabeled"}, {"type": "text", "expr": "case D", "word_idx": 61939, "sentence_idx": 996, "label": "unlabeled"}, {"type": "text", "expr": "7  Related work", "word_idx": 61945, "sentence_idx": 997, "label": "unlabeled"}, {"type": "text", "expr": "Our work is related to planning with incomplete domain models (or model-lite planning  )", "word_idx": 61960, "sentence_idx": 998, "label": "unlabeled"}, {"type": "text", "expr": "\nFigure  16  shows the schematic view of incomplete models and their relationships in the spectrum of incompleteness", "word_idx": 62048, "sentence_idx": 999, "label": "unlabeled"}, {"type": "text", "expr": " In a full model, we know exactly the dynamics of the model (i", "word_idx": 62164, "sentence_idx": 1000, "label": "unlabeled"}, {"type": "text", "expr": ", state transitions)", "word_idx": 62226, "sentence_idx": 1001, "label": "unlabeled"}, {"type": "text", "expr": " Approximate models are the closest to full\nmodels and their representations are similar\nexcept that there can be incomplete knowledge of action descriptions", "word_idx": 62246, "sentence_idx": 1002, "label": "unlabeled"}, {"type": "text", "expr": " To enable approximate planners to\nperform more (e", "word_idx": 62403, "sentence_idx": 1003, "label": "unlabeled"}, {"type": "text", "expr": ", providing robust plans),\nplanners are assumed to have access\nto additional knowledge circumscribing the\nincompleteness  (29) ", "word_idx": 62453, "sentence_idx": 1004, "label": "unlabeled"}, {"type": "text", "expr": " Partial models are one level further down the line in terms of the degree of incompleteness", "word_idx": 62580, "sentence_idx": 1005, "label": "unlabeled"}, {"type": "text", "expr": " While approximate models can encode incompleteness in the precondition/effect descriptions of the individual actions, partial models can completely abstract portions of a plan without providing details for them", "word_idx": 62672, "sentence_idx": 1006, "label": "unlabeled"}, {"type": "text", "expr": " In such cases, even though providing complete plans is infeasible, partial models can provide \u201cplanning guidance\u201d for agents  (31) ", "word_idx": 62883, "sentence_idx": 1007, "label": "unlabeled"}, {"type": "text", "expr": " Shallow models are essentially just a step above having no planning model", "word_idx": 63015, "sentence_idx": 1008, "label": "unlabeled"}, {"type": "text", "expr": " They provide interesting contrasts to the standard precondition and effect based action models used in automated planning community", "word_idx": 63089, "sentence_idx": 1009, "label": "unlabeled"}, {"type": "text", "expr": " Our work in this paper belongs to the class of shallow models", "word_idx": 63221, "sentence_idx": 1010, "label": "unlabeled"}, {"type": "text", "expr": " In developing shallow models, we are interested in planning technology that helps humans develop plans, even in the absence of any structured models or plan traces", "word_idx": 63283, "sentence_idx": 1011, "label": "unlabeled"}, {"type": "text", "expr": " In such cases, the best that we can hope for is to learn local structures of the planning model to provide planning support, similar to providing spell-check in writing", "word_idx": 63447, "sentence_idx": 1012, "label": "unlabeled"}, {"type": "text", "expr": " While some work in web-service composition (c", "word_idx": 63616, "sentence_idx": 1013, "label": "unlabeled"}, {"type": "text", "expr": "  (8) ) did focus on this type of planning support, they were hobbled by being limited to simple input/output type comparison", "word_idx": 63662, "sentence_idx": 1014, "label": "unlabeled"}, {"type": "text", "expr": " In contrast, we expect shallow models to be useful in \u201ccritiquing\u201d the plans being generated by the humans (e", "word_idx": 63787, "sentence_idx": 1015, "label": "unlabeled"}, {"type": "text", "expr": " detecting that an action introduced by the human is not consistent with the model), and \u201cexplaining/justifying\u201d the suggestions generated by humans", "word_idx": 63897, "sentence_idx": 1016, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a016:  Schematic view of incomplete models and their relationships in the spectrum of incompleteness", "word_idx": 64045, "sentence_idx": 1017, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a016:", "word_idx": 64150, "sentence_idx": 1018, "label": "unlabeled"}, {"type": "text", "expr": "Our work is also related to plan recognition", "word_idx": 64160, "sentence_idx": 1019, "label": "unlabeled"}, {"type": "text", "expr": " Kautz and Allen proposed an approach to recognizing plans based on parsing observed actions as sequences of subactions and essentially model this knowledge as a context-free rule in an \u201caction grammar\u201d  (16) ", "word_idx": 64204, "sentence_idx": 1020, "label": "unlabeled"}, {"type": "text", "expr": " All actions, plans are uniformly referred to as goals, and a recognizer\u2019s knowledge is represented by a set of first-order statements called event hierarchy encoded in first-order logic, which defines abstraction, decomposition and functional relationships between types of events", "word_idx": 64413, "sentence_idx": 1021, "label": "unlabeled"}, {"type": "text", "expr": " Lesh and Etzioni further presented methods in scaling up activity recognition to scale up his work computationally  (18) ", "word_idx": 64694, "sentence_idx": 1022, "label": "unlabeled"}, {"type": "text", "expr": " They automatically constructed plan-library from domain primitives, which was different from  (16)  where the plan library was explicitly represented", "word_idx": 64816, "sentence_idx": 1023, "label": "unlabeled"}, {"type": "text", "expr": " In these approaches, the problem of combinatorial explosion of plan execution models impedes its application to real-world domains", "word_idx": 64966, "sentence_idx": 1024, "label": "unlabeled"}, {"type": "text", "expr": " Kabanza and Filion  (12)  proposed an anytime plan recognition algorithm to reduce the number of generated plan execution models based on weighted model counting", "word_idx": 65097, "sentence_idx": 1025, "label": "unlabeled"}, {"type": "text", "expr": " These approaches are, however, difficult to represent uncertainty", "word_idx": 65259, "sentence_idx": 1026, "label": "unlabeled"}, {"type": "text", "expr": " They offer no mechanism for preferring one consistent approach to another and incapable of deciding whether one particular plan is more likely than another, as long as both of them can be consistent enough to explain the actions observed", "word_idx": 65325, "sentence_idx": 1027, "label": "unlabeled"}, {"type": "text", "expr": " Although we exploit a library of plans in our  DUP  and  RNNPlanner  approaches, we aim to learning shallow models and utilize the shallow models to recognize plans that are not necessarily in the plan library, which is different from previous approaches that assume the plans to be recognized are from the plan library", "word_idx": 65563, "sentence_idx": 1028, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 65883, "sentence_idx": 1029, "label": "unlabeled"}, {"type": "text", "expr": "Instead of using a library of plans, Ramirez and Geffner  (24)  proposed an approach to solving the plan recognition problem using slightly modified planning algorithms, assuming the action models were given as input (note that action models can be created by experts or learnt by previous systems  )", "word_idx": 65893, "sentence_idx": 1030, "label": "unlabeled"}, {"type": "text", "expr": " Except previous work   on the plan recognition problem presented in the introduction section,  Saria and Mahadevan presented a hierarchical multi-agent markov processes as a framework for hierarchical probabilistic plan recognition in cooperative multi-agent systems  (26) ", "word_idx": 66193, "sentence_idx": 1031, "label": "unlabeled"}, {"type": "text", "expr": " Amir and Gal addressed a plan recognition approach to recognizing student behaviors using virtual science laboratories  (3) ", "word_idx": 66467, "sentence_idx": 1032, "label": "unlabeled"}, {"type": "text", "expr": " Ramirez and Geffner exploited off-the-shelf classical planners to recognize probabilistic plans  (25) ", "word_idx": 66592, "sentence_idx": 1033, "label": "unlabeled"}, {"type": "text", "expr": " Different from those approaches, we do not require any domain model knowledge provided as input", "word_idx": 66695, "sentence_idx": 1034, "label": "unlabeled"}, {"type": "text", "expr": " Instead, we automatically learn shallow domain models from previous plan cases for recognizing unknown plans that may not be identical to previous cases", "word_idx": 66791, "sentence_idx": 1035, "label": "unlabeled"}, {"type": "text", "expr": "8  Conclusion and Discussion", "word_idx": 66944, "sentence_idx": 1036, "label": "unlabeled"}, {"type": "text", "expr": "In this paper we present two novel plan recognition approaches,  DUP  and  RNNPlanner ,\nbased on vector representation of actions", "word_idx": 66972, "sentence_idx": 1037, "label": "unlabeled"}, {"type": "text", "expr": " For  DUP , we first learn the vector\nrepresentations of actions from plan libraries using the Skip-gram\nmodel which has been demonstrated to be effective", "word_idx": 67101, "sentence_idx": 1038, "label": "unlabeled"}, {"type": "text", "expr": " We then discover\nunobserved actions with the vector representations by repeatedly\nsampling actions and optimizing the probability of potential plans to\nbe recognized", "word_idx": 67255, "sentence_idx": 1039, "label": "unlabeled"}, {"type": "text", "expr": " For  RNNPlanner , we let the neural network itself to learn the word embedding, which would then be utilized by higher LSTM layers", "word_idx": 67421, "sentence_idx": 1040, "label": "unlabeled"}, {"type": "text", "expr": " We also empirically exhibit the effectiveness of our\napproaches", "word_idx": 67552, "sentence_idx": 1041, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 67616, "sentence_idx": 1042, "label": "unlabeled"}, {"type": "text", "expr": "RNNPlanner", "word_idx": 67626, "sentence_idx": 1043, "label": "unlabeled"}, {"type": "text", "expr": "While we focused on a one-shot recognition task in this paper, in\npractice, human-in-the-loop planning will consist of multiple\niterations, with DUP recognizing the plan and suggesting action\naddition alternatives; the human making a selection and revising the\nplan", "word_idx": 67636, "sentence_idx": 1044, "label": "unlabeled"}, {"type": "text", "expr": " The aim is to provide a form of flexible plan completion tool,\nakin to auto-completers for search engine queries", "word_idx": 67901, "sentence_idx": 1045, "label": "unlabeled"}, {"type": "text", "expr": " To do this\nefficiently, we need to make the DUP recognition algorithm \u201cincremental", "word_idx": 68014, "sentence_idx": 1046, "label": "unlabeled"}, {"type": "text", "expr": "While we focused on a one-shot recognition task in this paper, in\npractice, human-in-the-loop planning will consist of multiple\niterations, with DUP recognizing the plan and suggesting action\naddition alternatives; the human making a selection and revising the\nplan", "word_idx": 68097, "sentence_idx": 1047, "label": "unlabeled"}, {"type": "text", "expr": " The aim is to provide a form of flexible plan completion tool,\nakin to auto-completers for search engine queries", "word_idx": 68362, "sentence_idx": 1048, "label": "unlabeled"}, {"type": "text", "expr": " To do this\nefficiently, we need to make the DUP recognition algorithm \u201cincremental", "word_idx": 68475, "sentence_idx": 1049, "label": "unlabeled"}, {"type": "text", "expr": "The word-vector based domain model we developed in this paper provides\ninteresting contrasts to the standard precondition and effect based\naction models used in automated planning community", "word_idx": 68558, "sentence_idx": 1050, "label": "unlabeled"}, {"type": "text", "expr": " One of our future\naims is to provide a more systematic comparison of the tradeoffs\noffered by these models", "word_idx": 68747, "sentence_idx": 1051, "label": "unlabeled"}, {"type": "text", "expr": " Although we have\nfocused on the \u201cplan recognition\u201d aspects of this model until now,\nand assumed that \u201cplanning support\u201d will be limited to suggesting\npotential actions to the humans", "word_idx": 68854, "sentence_idx": 1052, "label": "unlabeled"}, {"type": "text", "expr": " In future, we will also consider\n\u201ccritiquing\u201d the plans being generated by the humans (e", "word_idx": 69036, "sentence_idx": 1053, "label": "unlabeled"}, {"type": "text", "expr": " detecting\nthat an action introduced by the human is not consistent with the\nmodel learned by DUP), and \u201cexplaining/justifying\u201d the suggestions\ngenerated by humans", "word_idx": 69125, "sentence_idx": 1054, "label": "unlabeled"}, {"type": "text", "expr": " Here, we cannot expect causal explanations of the\nsorts that can be generated with the help of complete action models\n(e", "word_idx": 69288, "sentence_idx": 1055, "label": "unlabeled"}, {"type": "text", "expr": "  (22) ), and will have to develop justifications\nanalogous to those used in recommendation systems", "word_idx": 69409, "sentence_idx": 1056, "label": "unlabeled"}, {"type": "text", "expr": "Another potential application for this type of distributed action representations proposed in this paper is social media analysis", "word_idx": 69508, "sentence_idx": 1057, "label": "unlabeled"}, {"type": "text", "expr": " In particular, work such as  (17)  shows that identification of action-outcome relationships can significantly improve the analysis of social media threads", "word_idx": 69637, "sentence_idx": 1058, "label": "unlabeled"}, {"type": "text", "expr": " The challenge of course is that such action-outcome models have to be learned from raw and noisy social media text containing mere fragments of plans", "word_idx": 69793, "sentence_idx": 1059, "label": "unlabeled"}, {"type": "text", "expr": " We believe that action vector models of the type we proposed in this paper provide a promising way of handling this challenge", "word_idx": 69943, "sentence_idx": 1060, "label": "unlabeled"}, {"type": "text", "expr": "Acknowledgements", "word_idx": 70069, "sentence_idx": 1061, "label": "unlabeled"}, {"type": "text", "expr": " \nZhuo thanks the support of the National Key Research and Development Program of China (2016YFB0201900), National Natural Science Foundation of China (U1611262), Guangdong Natural Science Funds for Distinguished Young Scholar (2017A030306028), Pearl River Science and Technology New Star of Guangzhou, and Guangdong Province Key Laboratory of Big Data Analysis and Processing for the support of this research", "word_idx": 70085, "sentence_idx": 1062, "label": "unlabeled"}, {"type": "text", "expr": " Kambhampati\u2019s research is supported in part by the ARO grant\nW911NF-13-1-0023, the ONR grants N00014-13-1-0176, N00014-09-1-0017\nand N00014-07-1-1049, and the NSF grant IIS201330813", "word_idx": 70494, "sentence_idx": 1063, "label": "unlabeled"}, {"type": "text", "expr": "Acknowledgements", "word_idx": 70676, "sentence_idx": 1064, "label": "unlabeled"}, {"type": "text", "expr": "References", "word_idx": 70692, "sentence_idx": 1065, "label": "unlabeled"}, {"type": "text", "expr": "(1) \nAbidi, B", "word_idx": 70702, "sentence_idx": 1066, "label": "unlabeled"}, {"type": "text", "expr": ", Aragam, N", "word_idx": 70715, "sentence_idx": 1067, "label": "unlabeled"}, {"type": "text", "expr": ", Yao, Y", "word_idx": 70726, "sentence_idx": 1068, "label": "unlabeled"}, {"type": "text", "expr": ", Abidi, M", "word_idx": 70734, "sentence_idx": 1069, "label": "unlabeled"}, {"type": "text", "expr": ": Survey and analysis of\nmultimodal sensor planning and integration for wide area surveillance", "word_idx": 70744, "sentence_idx": 1070, "label": "unlabeled"}, {"type": "text", "expr": "Abidi, B", "word_idx": 70838, "sentence_idx": 1071, "label": "unlabeled"}, {"type": "text", "expr": ", Aragam, N", "word_idx": 70846, "sentence_idx": 1072, "label": "unlabeled"}, {"type": "text", "expr": ", Yao, Y", "word_idx": 70857, "sentence_idx": 1073, "label": "unlabeled"}, {"type": "text", "expr": ", Abidi, M", "word_idx": 70865, "sentence_idx": 1074, "label": "unlabeled"}, {"type": "text", "expr": ": Survey and analysis of\nmultimodal sensor planning and integration for wide area surveillance", "word_idx": 70875, "sentence_idx": 1075, "label": "unlabeled"}, {"type": "text", "expr": "ACM Comput", "word_idx": 70969, "sentence_idx": 1076, "label": "unlabeled"}, {"type": "text", "expr": " Surv", "word_idx": 70979, "sentence_idx": 1077, "label": "unlabeled"}, {"type": "text", "expr": "  41 (1), 7:1\u20137:36 (2009)", "word_idx": 70984, "sentence_idx": 1078, "label": "unlabeled"}, {"type": "text", "expr": "(2) \nAlbrecht, S", "word_idx": 71009, "sentence_idx": 1079, "label": "unlabeled"}, {"type": "text", "expr": ", Ramamoorthy, S", "word_idx": 71025, "sentence_idx": 1080, "label": "unlabeled"}, {"type": "text", "expr": ": Are you doing what I think you are doing?\ncriticising uncertain agent models", "word_idx": 71041, "sentence_idx": 1081, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of the Thirty-First Conference on Uncertainty in\nArtificial Intelligence, UAI 2015, July 12-16, 2015, Amsterdam, The\nNetherlands, pp", "word_idx": 71119, "sentence_idx": 1082, "label": "unlabeled"}, {"type": "text", "expr": " 52\u201361 (2015)", "word_idx": 71270, "sentence_idx": 1083, "label": "unlabeled"}, {"type": "text", "expr": "Albrecht, S", "word_idx": 71283, "sentence_idx": 1084, "label": "unlabeled"}, {"type": "text", "expr": ", Ramamoorthy, S", "word_idx": 71294, "sentence_idx": 1085, "label": "unlabeled"}, {"type": "text", "expr": ": Are you doing what I think you are doing?\ncriticising uncertain agent models", "word_idx": 71310, "sentence_idx": 1086, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of the Thirty-First Conference on Uncertainty in\nArtificial Intelligence, UAI 2015, July 12-16, 2015, Amsterdam, The\nNetherlands, pp", "word_idx": 71388, "sentence_idx": 1087, "label": "unlabeled"}, {"type": "text", "expr": " 52\u201361 (2015)", "word_idx": 71536, "sentence_idx": 1088, "label": "unlabeled"}, {"type": "text", "expr": "(3) \nAmir, O", "word_idx": 71549, "sentence_idx": 1089, "label": "unlabeled"}, {"type": "text", "expr": ", Gal, Y", "word_idx": 71561, "sentence_idx": 1090, "label": "unlabeled"}, {"type": "text", "expr": ": Plan recognition in virtual laboratories", "word_idx": 71569, "sentence_idx": 1091, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of IJCAI, pp", "word_idx": 71611, "sentence_idx": 1092, "label": "unlabeled"}, {"type": "text", "expr": " 2392\u20132397 (2011)", "word_idx": 71642, "sentence_idx": 1093, "label": "unlabeled"}, {"type": "text", "expr": "Amir, O", "word_idx": 71659, "sentence_idx": 1094, "label": "unlabeled"}, {"type": "text", "expr": ", Gal, Y", "word_idx": 71666, "sentence_idx": 1095, "label": "unlabeled"}, {"type": "text", "expr": ": Plan recognition in virtual laboratories", "word_idx": 71674, "sentence_idx": 1096, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of IJCAI, pp", "word_idx": 71716, "sentence_idx": 1097, "label": "unlabeled"}, {"type": "text", "expr": " 2392\u20132397 (2011)", "word_idx": 71744, "sentence_idx": 1098, "label": "unlabeled"}, {"type": "text", "expr": "(4) \nBranavan, S", "word_idx": 71761, "sentence_idx": 1099, "label": "unlabeled"}, {"type": "text", "expr": ", Kushman, N", "word_idx": 71777, "sentence_idx": 1100, "label": "unlabeled"}, {"type": "text", "expr": ", Lei, T", "word_idx": 71789, "sentence_idx": 1101, "label": "unlabeled"}, {"type": "text", "expr": ", Barzilay, R", "word_idx": 71797, "sentence_idx": 1102, "label": "unlabeled"}, {"type": "text", "expr": ": Learning high-level planning\nfrom text", "word_idx": 71810, "sentence_idx": 1103, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of ACL-12 (2012)", "word_idx": 71850, "sentence_idx": 1104, "label": "unlabeled"}, {"type": "text", "expr": "Branavan, S", "word_idx": 71885, "sentence_idx": 1105, "label": "unlabeled"}, {"type": "text", "expr": ", Kushman, N", "word_idx": 71896, "sentence_idx": 1106, "label": "unlabeled"}, {"type": "text", "expr": ", Lei, T", "word_idx": 71908, "sentence_idx": 1107, "label": "unlabeled"}, {"type": "text", "expr": ", Barzilay, R", "word_idx": 71916, "sentence_idx": 1108, "label": "unlabeled"}, {"type": "text", "expr": ": Learning high-level planning\nfrom text", "word_idx": 71929, "sentence_idx": 1109, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of ACL-12 (2012)", "word_idx": 71969, "sentence_idx": 1110, "label": "unlabeled"}, {"type": "text", "expr": "(5) \nBui, H", "word_idx": 72001, "sentence_idx": 1111, "label": "unlabeled"}, {"type": "text", "expr": ": A general model for online probabilistic plan recognition", "word_idx": 72012, "sentence_idx": 1112, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of IJCAI, pp", "word_idx": 72071, "sentence_idx": 1113, "label": "unlabeled"}, {"type": "text", "expr": " 1309\u20131318 (2003)", "word_idx": 72102, "sentence_idx": 1114, "label": "unlabeled"}, {"type": "text", "expr": "Bui, H", "word_idx": 72119, "sentence_idx": 1115, "label": "unlabeled"}, {"type": "text", "expr": ": A general model for online probabilistic plan recognition", "word_idx": 72125, "sentence_idx": 1116, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of IJCAI, pp", "word_idx": 72184, "sentence_idx": 1117, "label": "unlabeled"}, {"type": "text", "expr": " 1309\u20131318 (2003)", "word_idx": 72212, "sentence_idx": 1118, "label": "unlabeled"}, {"type": "text", "expr": "(6) \nCohen, P", "word_idx": 72229, "sentence_idx": 1119, "label": "unlabeled"}, {"type": "text", "expr": ", Kaiser, E", "word_idx": 72242, "sentence_idx": 1120, "label": "unlabeled"}, {"type": "text", "expr": ", Buchanan, M", "word_idx": 72253, "sentence_idx": 1121, "label": "unlabeled"}, {"type": "text", "expr": ", Lind, S", "word_idx": 72266, "sentence_idx": 1122, "label": "unlabeled"}, {"type": "text", "expr": ", Corrigan, M", "word_idx": 72275, "sentence_idx": 1123, "label": "unlabeled"}, {"type": "text", "expr": ", Wesson,\nR", "word_idx": 72288, "sentence_idx": 1124, "label": "unlabeled"}, {"type": "text", "expr": ": Sketch-thru-plan: a multimodal interface for command and control", "word_idx": 72299, "sentence_idx": 1125, "label": "unlabeled"}, {"type": "text", "expr": "Cohen, P", "word_idx": 72365, "sentence_idx": 1126, "label": "unlabeled"}, {"type": "text", "expr": ", Kaiser, E", "word_idx": 72373, "sentence_idx": 1127, "label": "unlabeled"}, {"type": "text", "expr": ", Buchanan, M", "word_idx": 72384, "sentence_idx": 1128, "label": "unlabeled"}, {"type": "text", "expr": ", Lind, S", "word_idx": 72397, "sentence_idx": 1129, "label": "unlabeled"}, {"type": "text", "expr": ", Corrigan, M", "word_idx": 72406, "sentence_idx": 1130, "label": "unlabeled"}, {"type": "text", "expr": ", Wesson,\nR", "word_idx": 72419, "sentence_idx": 1131, "label": "unlabeled"}, {"type": "text", "expr": ": Sketch-thru-plan: a multimodal interface for command and control", "word_idx": 72430, "sentence_idx": 1132, "label": "unlabeled"}, {"type": "text", "expr": "Commun", "word_idx": 72496, "sentence_idx": 1133, "label": "unlabeled"}, {"type": "text", "expr": " ACM  58 (4), 56\u201365 (2015)", "word_idx": 72502, "sentence_idx": 1134, "label": "unlabeled"}, {"type": "text", "expr": "(7) \nDong, X", "word_idx": 72528, "sentence_idx": 1135, "label": "unlabeled"}, {"type": "text", "expr": ", Halevy, A", "word_idx": 72540, "sentence_idx": 1136, "label": "unlabeled"}, {"type": "text", "expr": ", Madhavan, J", "word_idx": 72551, "sentence_idx": 1137, "label": "unlabeled"}, {"type": "text", "expr": ", Nemes, E", "word_idx": 72564, "sentence_idx": 1138, "label": "unlabeled"}, {"type": "text", "expr": ", Zhang, J", "word_idx": 72574, "sentence_idx": 1139, "label": "unlabeled"}, {"type": "text", "expr": ": Simlarity search\nfor web services", "word_idx": 72584, "sentence_idx": 1140, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: (e)Proceedings of the Thirtieth International Conference on Very\nLarge Data Bases, pp", "word_idx": 72619, "sentence_idx": 1141, "label": "unlabeled"}, {"type": "text", "expr": " 372\u2013383 (2004)", "word_idx": 72711, "sentence_idx": 1142, "label": "unlabeled"}, {"type": "text", "expr": "Dong, X", "word_idx": 72726, "sentence_idx": 1143, "label": "unlabeled"}, {"type": "text", "expr": ", Halevy, A", "word_idx": 72733, "sentence_idx": 1144, "label": "unlabeled"}, {"type": "text", "expr": ", Madhavan, J", "word_idx": 72744, "sentence_idx": 1145, "label": "unlabeled"}, {"type": "text", "expr": ", Nemes, E", "word_idx": 72757, "sentence_idx": 1146, "label": "unlabeled"}, {"type": "text", "expr": ", Zhang, J", "word_idx": 72767, "sentence_idx": 1147, "label": "unlabeled"}, {"type": "text", "expr": ": Simlarity search\nfor web services", "word_idx": 72777, "sentence_idx": 1148, "label": "unlabeled"}, {"type": "text", "expr": "In: (e)Proceedings of the Thirtieth International Conference on Very\nLarge Data Bases, pp", "word_idx": 72812, "sentence_idx": 1149, "label": "unlabeled"}, {"type": "text", "expr": " 372\u2013383 (2004)", "word_idx": 72901, "sentence_idx": 1150, "label": "unlabeled"}, {"type": "text", "expr": "(8) \nDong, X", "word_idx": 72916, "sentence_idx": 1151, "label": "unlabeled"}, {"type": "text", "expr": ", Halevy, A", "word_idx": 72928, "sentence_idx": 1152, "label": "unlabeled"}, {"type": "text", "expr": ", Madhavan, J", "word_idx": 72939, "sentence_idx": 1153, "label": "unlabeled"}, {"type": "text", "expr": ", Nemes, E", "word_idx": 72952, "sentence_idx": 1154, "label": "unlabeled"}, {"type": "text", "expr": ", Zhang, J", "word_idx": 72962, "sentence_idx": 1155, "label": "unlabeled"}, {"type": "text", "expr": ": Simlarity search\nfor web services", "word_idx": 72972, "sentence_idx": 1156, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of VLDB, pp", "word_idx": 73007, "sentence_idx": 1157, "label": "unlabeled"}, {"type": "text", "expr": " 372\u2013383 (2004)", "word_idx": 73037, "sentence_idx": 1158, "label": "unlabeled"}, {"type": "text", "expr": "Dong, X", "word_idx": 73052, "sentence_idx": 1159, "label": "unlabeled"}, {"type": "text", "expr": ", Halevy, A", "word_idx": 73059, "sentence_idx": 1160, "label": "unlabeled"}, {"type": "text", "expr": ", Madhavan, J", "word_idx": 73070, "sentence_idx": 1161, "label": "unlabeled"}, {"type": "text", "expr": ", Nemes, E", "word_idx": 73083, "sentence_idx": 1162, "label": "unlabeled"}, {"type": "text", "expr": ", Zhang, J", "word_idx": 73093, "sentence_idx": 1163, "label": "unlabeled"}, {"type": "text", "expr": ": Simlarity search\nfor web services", "word_idx": 73103, "sentence_idx": 1164, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of VLDB, pp", "word_idx": 73138, "sentence_idx": 1165, "label": "unlabeled"}, {"type": "text", "expr": " 372\u2013383 (2004)", "word_idx": 73165, "sentence_idx": 1166, "label": "unlabeled"}, {"type": "text", "expr": "(9) \nGeib, C", "word_idx": 73180, "sentence_idx": 1167, "label": "unlabeled"}, {"type": "text", "expr": ", Goldman, R", "word_idx": 73192, "sentence_idx": 1168, "label": "unlabeled"}, {"type": "text", "expr": ": A probabilistic plan recognition algorithm based on\nplan tree grammars", "word_idx": 73204, "sentence_idx": 1169, "label": "unlabeled"}, {"type": "text", "expr": "Geib, C", "word_idx": 73276, "sentence_idx": 1170, "label": "unlabeled"}, {"type": "text", "expr": ", Goldman, R", "word_idx": 73283, "sentence_idx": 1171, "label": "unlabeled"}, {"type": "text", "expr": ": A probabilistic plan recognition algorithm based on\nplan tree grammars", "word_idx": 73295, "sentence_idx": 1172, "label": "unlabeled"}, {"type": "text", "expr": "Artificial Intelligence  173 (11), 1101\u20131132 (2009)", "word_idx": 73367, "sentence_idx": 1173, "label": "unlabeled"}, {"type": "text", "expr": "(10) \nGeib, C", "word_idx": 73418, "sentence_idx": 1174, "label": "unlabeled"}, {"type": "text", "expr": ", Steedman, M", "word_idx": 73431, "sentence_idx": 1175, "label": "unlabeled"}, {"type": "text", "expr": ": On natural language processing and plan recognition", "word_idx": 73444, "sentence_idx": 1176, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: IJCAI 2007, Proceedings of the 20th International Joint\nConference on Artificial Intelligence, Hyderabad, India, January 6-12, 2007,\npp", "word_idx": 73497, "sentence_idx": 1177, "label": "unlabeled"}, {"type": "text", "expr": " 1612\u20131617 (2007)", "word_idx": 73639, "sentence_idx": 1178, "label": "unlabeled"}, {"type": "text", "expr": "Geib, C", "word_idx": 73656, "sentence_idx": 1179, "label": "unlabeled"}, {"type": "text", "expr": ", Steedman, M", "word_idx": 73663, "sentence_idx": 1180, "label": "unlabeled"}, {"type": "text", "expr": ": On natural language processing and plan recognition", "word_idx": 73676, "sentence_idx": 1181, "label": "unlabeled"}, {"type": "text", "expr": "In: IJCAI 2007, Proceedings of the 20th International Joint\nConference on Artificial Intelligence, Hyderabad, India, January 6-12, 2007,\npp", "word_idx": 73729, "sentence_idx": 1182, "label": "unlabeled"}, {"type": "text", "expr": " 1612\u20131617 (2007)", "word_idx": 73868, "sentence_idx": 1183, "label": "unlabeled"}, {"type": "text", "expr": "(11) \nGraves, A", "word_idx": 73885, "sentence_idx": 1184, "label": "unlabeled"}, {"type": "text", "expr": ": Generating sequences with recurrent neural networks", "word_idx": 73900, "sentence_idx": 1185, "label": "unlabeled"}, {"type": "text", "expr": "\n\n URL http://arxiv", "word_idx": 73953, "sentence_idx": 1186, "label": "unlabeled"}, {"type": "text", "expr": "org/abs/1308", "word_idx": 73972, "sentence_idx": 1187, "label": "unlabeled"}, {"type": "text", "expr": "Graves, A", "word_idx": 73984, "sentence_idx": 1188, "label": "unlabeled"}, {"type": "text", "expr": ": Generating sequences with recurrent neural networks", "word_idx": 73993, "sentence_idx": 1189, "label": "unlabeled"}, {"type": "text", "expr": "CoRR  abs/1308", "word_idx": 74046, "sentence_idx": 1190, "label": "unlabeled"}, {"type": "text", "expr": "0850  (2013)", "word_idx": 74060, "sentence_idx": 1191, "label": "unlabeled"}, {"type": "text", "expr": "abs/1308", "word_idx": 74072, "sentence_idx": 1192, "label": "unlabeled"}, {"type": "text", "expr": "URL http://arxiv", "word_idx": 74080, "sentence_idx": 1193, "label": "unlabeled"}, {"type": "text", "expr": "org/abs/1308", "word_idx": 74096, "sentence_idx": 1194, "label": "unlabeled"}, {"type": "text", "expr": "(12) \nKabanza, F", "word_idx": 74108, "sentence_idx": 1195, "label": "unlabeled"}, {"type": "text", "expr": ", Filion, J", "word_idx": 74124, "sentence_idx": 1196, "label": "unlabeled"}, {"type": "text", "expr": ", Benaskeur, A", "word_idx": 74135, "sentence_idx": 1197, "label": "unlabeled"}, {"type": "text", "expr": ", Irandoust, H", "word_idx": 74149, "sentence_idx": 1198, "label": "unlabeled"}, {"type": "text", "expr": ": Controlling the\nhypothesis space in probabilistic plan recognition", "word_idx": 74163, "sentence_idx": 1199, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: IJCAI (2013)", "word_idx": 74231, "sentence_idx": 1200, "label": "unlabeled"}, {"type": "text", "expr": "Kabanza, F", "word_idx": 74250, "sentence_idx": 1201, "label": "unlabeled"}, {"type": "text", "expr": ", Filion, J", "word_idx": 74260, "sentence_idx": 1202, "label": "unlabeled"}, {"type": "text", "expr": ", Benaskeur, A", "word_idx": 74271, "sentence_idx": 1203, "label": "unlabeled"}, {"type": "text", "expr": ", Irandoust, H", "word_idx": 74285, "sentence_idx": 1204, "label": "unlabeled"}, {"type": "text", "expr": ": Controlling the\nhypothesis space in probabilistic plan recognition", "word_idx": 74299, "sentence_idx": 1205, "label": "unlabeled"}, {"type": "text", "expr": "In: IJCAI (2013)", "word_idx": 74367, "sentence_idx": 1206, "label": "unlabeled"}, {"type": "text", "expr": "(13) \nKambhampati, S", "word_idx": 74383, "sentence_idx": 1207, "label": "unlabeled"}, {"type": "text", "expr": ": Model-lite planning for the web age masses: The challenges of\nplanning with incomplete and evolving domain models", "word_idx": 74403, "sentence_idx": 1208, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of the Twenty-Second AAAI Conference on Artificial\nIntelligence, pp", "word_idx": 74518, "sentence_idx": 1209, "label": "unlabeled"}, {"type": "text", "expr": " 1601\u20131605 (2007)", "word_idx": 74604, "sentence_idx": 1210, "label": "unlabeled"}, {"type": "text", "expr": "Kambhampati, S", "word_idx": 74621, "sentence_idx": 1211, "label": "unlabeled"}, {"type": "text", "expr": ": Model-lite planning for the web age masses: The challenges of\nplanning with incomplete and evolving domain models", "word_idx": 74635, "sentence_idx": 1212, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of the Twenty-Second AAAI Conference on Artificial\nIntelligence, pp", "word_idx": 74750, "sentence_idx": 1213, "label": "unlabeled"}, {"type": "text", "expr": " 1601\u20131605 (2007)", "word_idx": 74833, "sentence_idx": 1214, "label": "unlabeled"}, {"type": "text", "expr": "(14) \nKambhampati, S", "word_idx": 74850, "sentence_idx": 1215, "label": "unlabeled"}, {"type": "text", "expr": ": Model-lite planning for the web age masses: The challenges of\nplanning with incomplete and evolving domain theories", "word_idx": 74870, "sentence_idx": 1216, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of AAAI, pp", "word_idx": 74987, "sentence_idx": 1217, "label": "unlabeled"}, {"type": "text", "expr": " 1601\u20131605 (2007)", "word_idx": 75017, "sentence_idx": 1218, "label": "unlabeled"}, {"type": "text", "expr": "Kambhampati, S", "word_idx": 75034, "sentence_idx": 1219, "label": "unlabeled"}, {"type": "text", "expr": ": Model-lite planning for the web age masses: The challenges of\nplanning with incomplete and evolving domain theories", "word_idx": 75048, "sentence_idx": 1220, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of AAAI, pp", "word_idx": 75165, "sentence_idx": 1221, "label": "unlabeled"}, {"type": "text", "expr": " 1601\u20131605 (2007)", "word_idx": 75192, "sentence_idx": 1222, "label": "unlabeled"}, {"type": "text", "expr": "(15) \nKambhampati, S", "word_idx": 75209, "sentence_idx": 1223, "label": "unlabeled"}, {"type": "text", "expr": ", Talamadupula, K", "word_idx": 75229, "sentence_idx": 1224, "label": "unlabeled"}, {"type": "text", "expr": ": Human-in-the-loop planning and decision\nsupport (2015)", "word_idx": 75246, "sentence_idx": 1225, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Rakaposhi", "word_idx": 75302, "sentence_idx": 1226, "label": "unlabeled"}, {"type": "text", "expr": "edu/hilp-tutorial", "word_idx": 75314, "sentence_idx": 1227, "label": "unlabeled"}, {"type": "text", "expr": "Kambhampati, S", "word_idx": 75331, "sentence_idx": 1228, "label": "unlabeled"}, {"type": "text", "expr": ", Talamadupula, K", "word_idx": 75345, "sentence_idx": 1229, "label": "unlabeled"}, {"type": "text", "expr": ": Human-in-the-loop planning and decision\nsupport (2015)", "word_idx": 75362, "sentence_idx": 1230, "label": "unlabeled"}, {"type": "text", "expr": "Rakaposhi", "word_idx": 75418, "sentence_idx": 1231, "label": "unlabeled"}, {"type": "text", "expr": "edu/hilp-tutorial", "word_idx": 75427, "sentence_idx": 1232, "label": "unlabeled"}, {"type": "text", "expr": "(16) \nKautz, H", "word_idx": 75444, "sentence_idx": 1233, "label": "unlabeled"}, {"type": "text", "expr": ", Allen, J", "word_idx": 75458, "sentence_idx": 1234, "label": "unlabeled"}, {"type": "text", "expr": ": Generalized plan recognition", "word_idx": 75468, "sentence_idx": 1235, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of AAAI, pp", "word_idx": 75498, "sentence_idx": 1236, "label": "unlabeled"}, {"type": "text", "expr": " 32\u201337 (1986)", "word_idx": 75528, "sentence_idx": 1237, "label": "unlabeled"}, {"type": "text", "expr": "Kautz, H", "word_idx": 75541, "sentence_idx": 1238, "label": "unlabeled"}, {"type": "text", "expr": ", Allen, J", "word_idx": 75549, "sentence_idx": 1239, "label": "unlabeled"}, {"type": "text", "expr": ": Generalized plan recognition", "word_idx": 75559, "sentence_idx": 1240, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of AAAI, pp", "word_idx": 75589, "sentence_idx": 1241, "label": "unlabeled"}, {"type": "text", "expr": " 32\u201337 (1986)", "word_idx": 75616, "sentence_idx": 1242, "label": "unlabeled"}, {"type": "text", "expr": "(17) \nK\u0131c\u0131man, E", "word_idx": 75629, "sentence_idx": 1243, "label": "unlabeled"}, {"type": "text", "expr": ", Richardson, M", "word_idx": 75645, "sentence_idx": 1244, "label": "unlabeled"}, {"type": "text", "expr": ": Towards decision support and goal\nachievement: Identifying action-outcome relationships from social media", "word_idx": 75660, "sentence_idx": 1245, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of the 21th ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, pp", "word_idx": 75767, "sentence_idx": 1246, "label": "unlabeled"}, {"type": "text", "expr": " 547\u2013556", "word_idx": 75876, "sentence_idx": 1247, "label": "unlabeled"}, {"type": "text", "expr": " ACM (2015)", "word_idx": 75884, "sentence_idx": 1248, "label": "unlabeled"}, {"type": "text", "expr": "K\u0131c\u0131man, E", "word_idx": 75895, "sentence_idx": 1249, "label": "unlabeled"}, {"type": "text", "expr": ", Richardson, M", "word_idx": 75905, "sentence_idx": 1250, "label": "unlabeled"}, {"type": "text", "expr": ": Towards decision support and goal\nachievement: Identifying action-outcome relationships from social media", "word_idx": 75920, "sentence_idx": 1251, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of the 21th ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, pp", "word_idx": 76027, "sentence_idx": 1252, "label": "unlabeled"}, {"type": "text", "expr": " 547\u2013556", "word_idx": 76133, "sentence_idx": 1253, "label": "unlabeled"}, {"type": "text", "expr": " ACM (2015)", "word_idx": 76141, "sentence_idx": 1254, "label": "unlabeled"}, {"type": "text", "expr": "(18) \nLesh, N", "word_idx": 76152, "sentence_idx": 1255, "label": "unlabeled"}, {"type": "text", "expr": ", Etzioni, O", "word_idx": 76165, "sentence_idx": 1256, "label": "unlabeled"}, {"type": "text", "expr": ": A sound and fast goal recognizer", "word_idx": 76177, "sentence_idx": 1257, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: IJCAI, pp", "word_idx": 76211, "sentence_idx": 1258, "label": "unlabeled"}, {"type": "text", "expr": " 1704\u20131710 (1995)", "word_idx": 76227, "sentence_idx": 1259, "label": "unlabeled"}, {"type": "text", "expr": "Lesh, N", "word_idx": 76244, "sentence_idx": 1260, "label": "unlabeled"}, {"type": "text", "expr": ", Etzioni, O", "word_idx": 76251, "sentence_idx": 1261, "label": "unlabeled"}, {"type": "text", "expr": ": A sound and fast goal recognizer", "word_idx": 76263, "sentence_idx": 1262, "label": "unlabeled"}, {"type": "text", "expr": "In: IJCAI, pp", "word_idx": 76297, "sentence_idx": 1263, "label": "unlabeled"}, {"type": "text", "expr": " 1704\u20131710 (1995)", "word_idx": 76310, "sentence_idx": 1264, "label": "unlabeled"}, {"type": "text", "expr": "(19) \nManikonda, L", "word_idx": 76327, "sentence_idx": 1265, "label": "unlabeled"}, {"type": "text", "expr": ", Chakraborti, T", "word_idx": 76345, "sentence_idx": 1266, "label": "unlabeled"}, {"type": "text", "expr": ", De, S", "word_idx": 76361, "sentence_idx": 1267, "label": "unlabeled"}, {"type": "text", "expr": ", Talamadupula, K", "word_idx": 76368, "sentence_idx": 1268, "label": "unlabeled"}, {"type": "text", "expr": ", Kambhampati, S", "word_idx": 76385, "sentence_idx": 1269, "label": "unlabeled"}, {"type": "text", "expr": ":\nAI-MIX: using automated planning to steer human workers towards better\ncrowdsourced plans", "word_idx": 76401, "sentence_idx": 1270, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of the Twenty-Eighth AAAI Conference on Artificial\nIntelligence, pp", "word_idx": 76492, "sentence_idx": 1271, "label": "unlabeled"}, {"type": "text", "expr": " 3004\u20133009 (2014)", "word_idx": 76578, "sentence_idx": 1272, "label": "unlabeled"}, {"type": "text", "expr": "Manikonda, L", "word_idx": 76595, "sentence_idx": 1273, "label": "unlabeled"}, {"type": "text", "expr": ", Chakraborti, T", "word_idx": 76607, "sentence_idx": 1274, "label": "unlabeled"}, {"type": "text", "expr": ", De, S", "word_idx": 76623, "sentence_idx": 1275, "label": "unlabeled"}, {"type": "text", "expr": ", Talamadupula, K", "word_idx": 76630, "sentence_idx": 1276, "label": "unlabeled"}, {"type": "text", "expr": ", Kambhampati, S", "word_idx": 76647, "sentence_idx": 1277, "label": "unlabeled"}, {"type": "text", "expr": ":\nAI-MIX: using automated planning to steer human workers towards better\ncrowdsourced plans", "word_idx": 76663, "sentence_idx": 1278, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of the Twenty-Eighth AAAI Conference on Artificial\nIntelligence, pp", "word_idx": 76754, "sentence_idx": 1279, "label": "unlabeled"}, {"type": "text", "expr": " 3004\u20133009 (2014)", "word_idx": 76837, "sentence_idx": 1280, "label": "unlabeled"}, {"type": "text", "expr": "(20) \nMikolov, T", "word_idx": 76854, "sentence_idx": 1281, "label": "unlabeled"}, {"type": "text", "expr": ", Sutskever, I", "word_idx": 76870, "sentence_idx": 1282, "label": "unlabeled"}, {"type": "text", "expr": ", Chen, K", "word_idx": 76884, "sentence_idx": 1283, "label": "unlabeled"}, {"type": "text", "expr": ", Corrado, G", "word_idx": 76893, "sentence_idx": 1284, "label": "unlabeled"}, {"type": "text", "expr": ", Dean, J", "word_idx": 76905, "sentence_idx": 1285, "label": "unlabeled"}, {"type": "text", "expr": ": Distributed\nrepresentations of words and phrases and their compositionality", "word_idx": 76914, "sentence_idx": 1286, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: NIPS, pp", "word_idx": 76991, "sentence_idx": 1287, "label": "unlabeled"}, {"type": "text", "expr": " 3111\u20133119 (2013)", "word_idx": 77006, "sentence_idx": 1288, "label": "unlabeled"}, {"type": "text", "expr": "Mikolov, T", "word_idx": 77023, "sentence_idx": 1289, "label": "unlabeled"}, {"type": "text", "expr": ", Sutskever, I", "word_idx": 77033, "sentence_idx": 1290, "label": "unlabeled"}, {"type": "text", "expr": ", Chen, K", "word_idx": 77047, "sentence_idx": 1291, "label": "unlabeled"}, {"type": "text", "expr": ", Corrado, G", "word_idx": 77056, "sentence_idx": 1292, "label": "unlabeled"}, {"type": "text", "expr": ", Dean, J", "word_idx": 77068, "sentence_idx": 1293, "label": "unlabeled"}, {"type": "text", "expr": ": Distributed\nrepresentations of words and phrases and their compositionality", "word_idx": 77077, "sentence_idx": 1294, "label": "unlabeled"}, {"type": "text", "expr": "In: NIPS, pp", "word_idx": 77154, "sentence_idx": 1295, "label": "unlabeled"}, {"type": "text", "expr": " 3111\u20133119 (2013)", "word_idx": 77166, "sentence_idx": 1296, "label": "unlabeled"}, {"type": "text", "expr": "(21) \nNg, A", "word_idx": 77183, "sentence_idx": 1297, "label": "unlabeled"}, {"type": "text", "expr": ", Kim, H", "word_idx": 77194, "sentence_idx": 1298, "label": "unlabeled"}, {"type": "text", "expr": ", Jordan, M", "word_idx": 77202, "sentence_idx": 1299, "label": "unlabeled"}, {"type": "text", "expr": ", Sastry, S", "word_idx": 77213, "sentence_idx": 1300, "label": "unlabeled"}, {"type": "text", "expr": ": Autonomous helicopter flight via\nreinforcement learning", "word_idx": 77224, "sentence_idx": 1301, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of NIPS-03 (2003)", "word_idx": 77281, "sentence_idx": 1302, "label": "unlabeled"}, {"type": "text", "expr": "Ng, A", "word_idx": 77317, "sentence_idx": 1303, "label": "unlabeled"}, {"type": "text", "expr": ", Kim, H", "word_idx": 77322, "sentence_idx": 1304, "label": "unlabeled"}, {"type": "text", "expr": ", Jordan, M", "word_idx": 77330, "sentence_idx": 1305, "label": "unlabeled"}, {"type": "text", "expr": ", Sastry, S", "word_idx": 77341, "sentence_idx": 1306, "label": "unlabeled"}, {"type": "text", "expr": ": Autonomous helicopter flight via\nreinforcement learning", "word_idx": 77352, "sentence_idx": 1307, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of NIPS-03 (2003)", "word_idx": 77409, "sentence_idx": 1308, "label": "unlabeled"}, {"type": "text", "expr": "(22) \nPetrie, C", "word_idx": 77442, "sentence_idx": 1309, "label": "unlabeled"}, {"type": "text", "expr": ": Constrained decision revision", "word_idx": 77457, "sentence_idx": 1310, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of the 10th National Conference on Artificial\nIntelligence", "word_idx": 77488, "sentence_idx": 1311, "label": "unlabeled"}, {"type": "text", "expr": " San Jose, CA, July 12-16, 1992", "word_idx": 77565, "sentence_idx": 1312, "label": "unlabeled"}, {"type": "text", "expr": " 393\u2013400 (1992)", "word_idx": 77596, "sentence_idx": 1313, "label": "unlabeled"}, {"type": "text", "expr": "Petrie, C", "word_idx": 77611, "sentence_idx": 1314, "label": "unlabeled"}, {"type": "text", "expr": ": Constrained decision revision", "word_idx": 77620, "sentence_idx": 1315, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of the 10th National Conference on Artificial\nIntelligence", "word_idx": 77651, "sentence_idx": 1316, "label": "unlabeled"}, {"type": "text", "expr": " San Jose, CA, July 12-16, 1992", "word_idx": 77725, "sentence_idx": 1317, "label": "unlabeled"}, {"type": "text", "expr": " 393\u2013400 (1992)", "word_idx": 77756, "sentence_idx": 1318, "label": "unlabeled"}, {"type": "text", "expr": "(23) \nRam\u00edrez, M", "word_idx": 77771, "sentence_idx": 1319, "label": "unlabeled"}, {"type": "text", "expr": ", Geffner, H", "word_idx": 77787, "sentence_idx": 1320, "label": "unlabeled"}, {"type": "text", "expr": ": Plan recognition as planning", "word_idx": 77799, "sentence_idx": 1321, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: IJCAI 2009, Proceedings of the 21st International Joint\nConference on Artificial Intelligence, Pasadena, California, USA, July 11-17,\n2009, pp", "word_idx": 77829, "sentence_idx": 1322, "label": "unlabeled"}, {"type": "text", "expr": " 1778\u20131783 (2009)", "word_idx": 77978, "sentence_idx": 1323, "label": "unlabeled"}, {"type": "text", "expr": "Ram\u00edrez, M", "word_idx": 77995, "sentence_idx": 1324, "label": "unlabeled"}, {"type": "text", "expr": ", Geffner, H", "word_idx": 78005, "sentence_idx": 1325, "label": "unlabeled"}, {"type": "text", "expr": ": Plan recognition as planning", "word_idx": 78017, "sentence_idx": 1326, "label": "unlabeled"}, {"type": "text", "expr": "In: IJCAI 2009, Proceedings of the 21st International Joint\nConference on Artificial Intelligence, Pasadena, California, USA, July 11-17,\n2009, pp", "word_idx": 78047, "sentence_idx": 1327, "label": "unlabeled"}, {"type": "text", "expr": " 1778\u20131783 (2009)", "word_idx": 78193, "sentence_idx": 1328, "label": "unlabeled"}, {"type": "text", "expr": "(24) \nRamirez, M", "word_idx": 78210, "sentence_idx": 1329, "label": "unlabeled"}, {"type": "text", "expr": ", Geffner, H", "word_idx": 78226, "sentence_idx": 1330, "label": "unlabeled"}, {"type": "text", "expr": ": Plan recognition as planning", "word_idx": 78238, "sentence_idx": 1331, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of IJCAI, pp", "word_idx": 78268, "sentence_idx": 1332, "label": "unlabeled"}, {"type": "text", "expr": " 1778\u20131783 (2009)", "word_idx": 78299, "sentence_idx": 1333, "label": "unlabeled"}, {"type": "text", "expr": "Ramirez, M", "word_idx": 78316, "sentence_idx": 1334, "label": "unlabeled"}, {"type": "text", "expr": ", Geffner, H", "word_idx": 78326, "sentence_idx": 1335, "label": "unlabeled"}, {"type": "text", "expr": ": Plan recognition as planning", "word_idx": 78338, "sentence_idx": 1336, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of IJCAI, pp", "word_idx": 78368, "sentence_idx": 1337, "label": "unlabeled"}, {"type": "text", "expr": " 1778\u20131783 (2009)", "word_idx": 78396, "sentence_idx": 1338, "label": "unlabeled"}, {"type": "text", "expr": "(25) \nRamirez, M", "word_idx": 78413, "sentence_idx": 1339, "label": "unlabeled"}, {"type": "text", "expr": ", Geffner, H", "word_idx": 78429, "sentence_idx": 1340, "label": "unlabeled"}, {"type": "text", "expr": ": Probabilistic plan recognition using off-the-shelf\nclassical planners", "word_idx": 78441, "sentence_idx": 1341, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of AAAI, pp", "word_idx": 78512, "sentence_idx": 1342, "label": "unlabeled"}, {"type": "text", "expr": " 1121\u20131126 (2010)", "word_idx": 78542, "sentence_idx": 1343, "label": "unlabeled"}, {"type": "text", "expr": "Ramirez, M", "word_idx": 78559, "sentence_idx": 1344, "label": "unlabeled"}, {"type": "text", "expr": ", Geffner, H", "word_idx": 78569, "sentence_idx": 1345, "label": "unlabeled"}, {"type": "text", "expr": ": Probabilistic plan recognition using off-the-shelf\nclassical planners", "word_idx": 78581, "sentence_idx": 1346, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of AAAI, pp", "word_idx": 78652, "sentence_idx": 1347, "label": "unlabeled"}, {"type": "text", "expr": " 1121\u20131126 (2010)", "word_idx": 78679, "sentence_idx": 1348, "label": "unlabeled"}, {"type": "text", "expr": "(26) \nSaria, S", "word_idx": 78696, "sentence_idx": 1349, "label": "unlabeled"}, {"type": "text", "expr": ", Mahadevan, S", "word_idx": 78710, "sentence_idx": 1350, "label": "unlabeled"}, {"type": "text", "expr": ": Probabilistic plan recognitionin multiagent systems", "word_idx": 78724, "sentence_idx": 1351, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of AAAI (2004)", "word_idx": 78777, "sentence_idx": 1352, "label": "unlabeled"}, {"type": "text", "expr": "Saria, S", "word_idx": 78810, "sentence_idx": 1353, "label": "unlabeled"}, {"type": "text", "expr": ", Mahadevan, S", "word_idx": 78818, "sentence_idx": 1354, "label": "unlabeled"}, {"type": "text", "expr": ": Probabilistic plan recognitionin multiagent systems", "word_idx": 78832, "sentence_idx": 1355, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of AAAI (2004)", "word_idx": 78885, "sentence_idx": 1356, "label": "unlabeled"}, {"type": "text", "expr": "(27) \nShi, X", "word_idx": 78915, "sentence_idx": 1357, "label": "unlabeled"}, {"type": "text", "expr": ", Chen, Z", "word_idx": 78927, "sentence_idx": 1358, "label": "unlabeled"}, {"type": "text", "expr": ", Wang, H", "word_idx": 78936, "sentence_idx": 1359, "label": "unlabeled"}, {"type": "text", "expr": ", Yeung, D", "word_idx": 78945, "sentence_idx": 1360, "label": "unlabeled"}, {"type": "text", "expr": ", Wong, W", "word_idx": 78955, "sentence_idx": 1361, "label": "unlabeled"}, {"type": "text", "expr": ", Woo, W", "word_idx": 78964, "sentence_idx": 1362, "label": "unlabeled"}, {"type": "text", "expr": ": Convolutional LSTM\nnetwork: A machine learning approach for precipitation nowcasting", "word_idx": 78972, "sentence_idx": 1363, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Advances in Neural Information Processing Systems 28: Annual\nConference on Neural Information Processing Systems 2015, December 7-12,\n2015, Montreal, Quebec, Canada, pp", "word_idx": 79058, "sentence_idx": 1364, "label": "unlabeled"}, {"type": "text", "expr": " 802\u2013810 (2015)", "word_idx": 79233, "sentence_idx": 1365, "label": "unlabeled"}, {"type": "text", "expr": "Shi, X", "word_idx": 79248, "sentence_idx": 1366, "label": "unlabeled"}, {"type": "text", "expr": ", Chen, Z", "word_idx": 79254, "sentence_idx": 1367, "label": "unlabeled"}, {"type": "text", "expr": ", Wang, H", "word_idx": 79263, "sentence_idx": 1368, "label": "unlabeled"}, {"type": "text", "expr": ", Yeung, D", "word_idx": 79272, "sentence_idx": 1369, "label": "unlabeled"}, {"type": "text", "expr": ", Wong, W", "word_idx": 79282, "sentence_idx": 1370, "label": "unlabeled"}, {"type": "text", "expr": ", Woo, W", "word_idx": 79291, "sentence_idx": 1371, "label": "unlabeled"}, {"type": "text", "expr": ": Convolutional LSTM\nnetwork: A machine learning approach for precipitation nowcasting", "word_idx": 79299, "sentence_idx": 1372, "label": "unlabeled"}, {"type": "text", "expr": "In: Advances in Neural Information Processing Systems 28: Annual\nConference on Neural Information Processing Systems 2015, December 7-12,\n2015, Montreal, Quebec, Canada, pp", "word_idx": 79385, "sentence_idx": 1373, "label": "unlabeled"}, {"type": "text", "expr": " 802\u2013810 (2015)", "word_idx": 79557, "sentence_idx": 1374, "label": "unlabeled"}, {"type": "text", "expr": "(28) \nTian, X", "word_idx": 79572, "sentence_idx": 1375, "label": "unlabeled"}, {"type": "text", "expr": ", Zhuo, H", "word_idx": 79585, "sentence_idx": 1376, "label": "unlabeled"}, {"type": "text", "expr": ", Kambhampati, S", "word_idx": 79594, "sentence_idx": 1377, "label": "unlabeled"}, {"type": "text", "expr": ": Discovering underlying plans based on\ndistributed representations of actions", "word_idx": 79610, "sentence_idx": 1378, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of the 2016 International Conference on Autonomous\nAgents & Multiagent Systems, Singapore, May 9-13, 2016, pp", "word_idx": 79688, "sentence_idx": 1379, "label": "unlabeled"}, {"type": "text", "expr": " 1135\u20131143\n(2016)", "word_idx": 79816, "sentence_idx": 1380, "label": "unlabeled"}, {"type": "text", "expr": "Tian, X", "word_idx": 79833, "sentence_idx": 1381, "label": "unlabeled"}, {"type": "text", "expr": ", Zhuo, H", "word_idx": 79840, "sentence_idx": 1382, "label": "unlabeled"}, {"type": "text", "expr": ", Kambhampati, S", "word_idx": 79849, "sentence_idx": 1383, "label": "unlabeled"}, {"type": "text", "expr": ": Discovering underlying plans based on\ndistributed representations of actions", "word_idx": 79865, "sentence_idx": 1384, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of the 2016 International Conference on Autonomous\nAgents & Multiagent Systems, Singapore, May 9-13, 2016, pp", "word_idx": 79943, "sentence_idx": 1385, "label": "unlabeled"}, {"type": "text", "expr": " 1135\u20131143\n(2016)", "word_idx": 80068, "sentence_idx": 1386, "label": "unlabeled"}, {"type": "text", "expr": "(29) \nTuan\u00a0Nguyen Subbarao\u00a0Kambhampati, M", "word_idx": 80085, "sentence_idx": 1387, "label": "unlabeled"}, {"type": "text", "expr": ": Synthesizing robust plans under\nincomplete domain models", "word_idx": 80126, "sentence_idx": 1388, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proc", "word_idx": 80184, "sentence_idx": 1389, "label": "unlabeled"}, {"type": "text", "expr": " AAAI Workshop on Generalized Planning (2011)", "word_idx": 80195, "sentence_idx": 1390, "label": "unlabeled"}, {"type": "text", "expr": "Tuan\u00a0Nguyen Subbarao\u00a0Kambhampati, M", "word_idx": 80240, "sentence_idx": 1391, "label": "unlabeled"}, {"type": "text", "expr": ": Synthesizing robust plans under\nincomplete domain models", "word_idx": 80275, "sentence_idx": 1392, "label": "unlabeled"}, {"type": "text", "expr": "In: Proc", "word_idx": 80333, "sentence_idx": 1393, "label": "unlabeled"}, {"type": "text", "expr": " AAAI Workshop on Generalized Planning (2011)", "word_idx": 80341, "sentence_idx": 1394, "label": "unlabeled"}, {"type": "text", "expr": "(30) \nYang, Q", "word_idx": 80386, "sentence_idx": 1395, "label": "unlabeled"}, {"type": "text", "expr": ", Wu, K", "word_idx": 80399, "sentence_idx": 1396, "label": "unlabeled"}, {"type": "text", "expr": ", Jiang, Y", "word_idx": 80406, "sentence_idx": 1397, "label": "unlabeled"}, {"type": "text", "expr": ": Learning action models from plan examples using\nweighted MAX-SAT", "word_idx": 80416, "sentence_idx": 1398, "label": "unlabeled"}, {"type": "text", "expr": "Yang, Q", "word_idx": 80482, "sentence_idx": 1399, "label": "unlabeled"}, {"type": "text", "expr": ", Wu, K", "word_idx": 80489, "sentence_idx": 1400, "label": "unlabeled"}, {"type": "text", "expr": ", Jiang, Y", "word_idx": 80496, "sentence_idx": 1401, "label": "unlabeled"}, {"type": "text", "expr": ": Learning action models from plan examples using\nweighted MAX-SAT", "word_idx": 80506, "sentence_idx": 1402, "label": "unlabeled"}, {"type": "text", "expr": "Artificial Intelligence Journal  171 , 107\u2013143 (2007)", "word_idx": 80572, "sentence_idx": 1403, "label": "unlabeled"}, {"type": "text", "expr": "(31) \nZhang, Y", "word_idx": 80625, "sentence_idx": 1404, "label": "unlabeled"}, {"type": "text", "expr": ", Sreedharan, S", "word_idx": 80639, "sentence_idx": 1405, "label": "unlabeled"}, {"type": "text", "expr": ", Kambhampati, S", "word_idx": 80654, "sentence_idx": 1406, "label": "unlabeled"}, {"type": "text", "expr": ": Capability models and their\napplications in planning", "word_idx": 80670, "sentence_idx": 1407, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of AAMAS, pp", "word_idx": 80724, "sentence_idx": 1408, "label": "unlabeled"}, {"type": "text", "expr": " 1151\u20131159 (2015)", "word_idx": 80755, "sentence_idx": 1409, "label": "unlabeled"}, {"type": "text", "expr": "Zhang, Y", "word_idx": 80772, "sentence_idx": 1410, "label": "unlabeled"}, {"type": "text", "expr": ", Sreedharan, S", "word_idx": 80780, "sentence_idx": 1411, "label": "unlabeled"}, {"type": "text", "expr": ", Kambhampati, S", "word_idx": 80795, "sentence_idx": 1412, "label": "unlabeled"}, {"type": "text", "expr": ": Capability models and their\napplications in planning", "word_idx": 80811, "sentence_idx": 1413, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of AAMAS, pp", "word_idx": 80865, "sentence_idx": 1414, "label": "unlabeled"}, {"type": "text", "expr": " 1151\u20131159 (2015)", "word_idx": 80893, "sentence_idx": 1415, "label": "unlabeled"}, {"type": "text", "expr": "(32) \nZhuo, H", "word_idx": 80910, "sentence_idx": 1416, "label": "unlabeled"}, {"type": "text", "expr": ", Kambhampati, S", "word_idx": 80923, "sentence_idx": 1417, "label": "unlabeled"}, {"type": "text", "expr": ": Model-lite planning: Case-based vs", "word_idx": 80939, "sentence_idx": 1418, "label": "unlabeled"}, {"type": "text", "expr": " model-based\napproaches", "word_idx": 80975, "sentence_idx": 1419, "label": "unlabeled"}, {"type": "text", "expr": "Zhuo, H", "word_idx": 80998, "sentence_idx": 1420, "label": "unlabeled"}, {"type": "text", "expr": ", Kambhampati, S", "word_idx": 81005, "sentence_idx": 1421, "label": "unlabeled"}, {"type": "text", "expr": ": Model-lite planning: Case-based vs", "word_idx": 81021, "sentence_idx": 1422, "label": "unlabeled"}, {"type": "text", "expr": " model-based\napproaches", "word_idx": 81057, "sentence_idx": 1423, "label": "unlabeled"}, {"type": "text", "expr": "Artif", "word_idx": 81080, "sentence_idx": 1424, "label": "unlabeled"}, {"type": "text", "expr": " Intell", "word_idx": 81085, "sentence_idx": 1425, "label": "unlabeled"}, {"type": "text", "expr": "  246 , 1\u201321 (2017)", "word_idx": 81092, "sentence_idx": 1426, "label": "unlabeled"}, {"type": "text", "expr": "(33) \nZhuo, H", "word_idx": 81111, "sentence_idx": 1427, "label": "unlabeled"}, {"type": "text", "expr": ", Li, L", "word_idx": 81124, "sentence_idx": 1428, "label": "unlabeled"}, {"type": "text", "expr": ": Multi-agent plan recognition with partial team traces and\nplan libraries", "word_idx": 81131, "sentence_idx": 1429, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of IJCAI, pp", "word_idx": 81205, "sentence_idx": 1430, "label": "unlabeled"}, {"type": "text", "expr": " 484\u2013489 (2011)", "word_idx": 81236, "sentence_idx": 1431, "label": "unlabeled"}, {"type": "text", "expr": "Zhuo, H", "word_idx": 81251, "sentence_idx": 1432, "label": "unlabeled"}, {"type": "text", "expr": ", Li, L", "word_idx": 81258, "sentence_idx": 1433, "label": "unlabeled"}, {"type": "text", "expr": ": Multi-agent plan recognition with partial team traces and\nplan libraries", "word_idx": 81265, "sentence_idx": 1434, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of IJCAI, pp", "word_idx": 81339, "sentence_idx": 1435, "label": "unlabeled"}, {"type": "text", "expr": " 484\u2013489 (2011)", "word_idx": 81367, "sentence_idx": 1436, "label": "unlabeled"}, {"type": "text", "expr": "(34) \nZhuo, H", "word_idx": 81382, "sentence_idx": 1437, "label": "unlabeled"}, {"type": "text", "expr": ", Nguyen, T", "word_idx": 81395, "sentence_idx": 1438, "label": "unlabeled"}, {"type": "text", "expr": ", Kambhampati, S", "word_idx": 81406, "sentence_idx": 1439, "label": "unlabeled"}, {"type": "text", "expr": ": Model-lite case-based planning", "word_idx": 81422, "sentence_idx": 1440, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: AAAI (2013)", "word_idx": 81454, "sentence_idx": 1441, "label": "unlabeled"}, {"type": "text", "expr": "Zhuo, H", "word_idx": 81472, "sentence_idx": 1442, "label": "unlabeled"}, {"type": "text", "expr": ", Nguyen, T", "word_idx": 81479, "sentence_idx": 1443, "label": "unlabeled"}, {"type": "text", "expr": ", Kambhampati, S", "word_idx": 81490, "sentence_idx": 1444, "label": "unlabeled"}, {"type": "text", "expr": ": Model-lite case-based planning", "word_idx": 81506, "sentence_idx": 1445, "label": "unlabeled"}, {"type": "text", "expr": "In: AAAI (2013)", "word_idx": 81538, "sentence_idx": 1446, "label": "unlabeled"}, {"type": "text", "expr": "(35) \nZhuo, H", "word_idx": 81553, "sentence_idx": 1447, "label": "unlabeled"}, {"type": "text", "expr": ", Yang, Q", "word_idx": 81566, "sentence_idx": 1448, "label": "unlabeled"}, {"type": "text", "expr": ", Hu, D", "word_idx": 81575, "sentence_idx": 1449, "label": "unlabeled"}, {"type": "text", "expr": ", Li, L", "word_idx": 81582, "sentence_idx": 1450, "label": "unlabeled"}, {"type": "text", "expr": ": Learning complex action models with\nquantifiers and implications", "word_idx": 81589, "sentence_idx": 1451, "label": "unlabeled"}, {"type": "text", "expr": "Zhuo, H", "word_idx": 81655, "sentence_idx": 1452, "label": "unlabeled"}, {"type": "text", "expr": ", Yang, Q", "word_idx": 81662, "sentence_idx": 1453, "label": "unlabeled"}, {"type": "text", "expr": ", Hu, D", "word_idx": 81671, "sentence_idx": 1454, "label": "unlabeled"}, {"type": "text", "expr": ", Li, L", "word_idx": 81678, "sentence_idx": 1455, "label": "unlabeled"}, {"type": "text", "expr": ": Learning complex action models with\nquantifiers and implications", "word_idx": 81685, "sentence_idx": 1456, "label": "unlabeled"}, {"type": "text", "expr": "Artificial Intelligence  174 (18), 1540\u20131569 (2010)", "word_idx": 81751, "sentence_idx": 1457, "label": "unlabeled"}, {"type": "text", "expr": "(36) \nZhuo, H", "word_idx": 81802, "sentence_idx": 1458, "label": "unlabeled"}, {"type": "text", "expr": ", Yang, Q", "word_idx": 81815, "sentence_idx": 1459, "label": "unlabeled"}, {"type": "text", "expr": ", Kambhampati, S", "word_idx": 81824, "sentence_idx": 1460, "label": "unlabeled"}, {"type": "text", "expr": ": Action-model based multi-agent plan\nrecognition", "word_idx": 81840, "sentence_idx": 1461, "label": "unlabeled"}, {"type": "text", "expr": "\n\n In: Proceedings of NIPS, pp", "word_idx": 81889, "sentence_idx": 1462, "label": "unlabeled"}, {"type": "text", "expr": " 377\u2013385 (2012)", "word_idx": 81919, "sentence_idx": 1463, "label": "unlabeled"}, {"type": "text", "expr": "Zhuo, H", "word_idx": 81934, "sentence_idx": 1464, "label": "unlabeled"}, {"type": "text", "expr": ", Yang, Q", "word_idx": 81941, "sentence_idx": 1465, "label": "unlabeled"}, {"type": "text", "expr": ", Kambhampati, S", "word_idx": 81950, "sentence_idx": 1466, "label": "unlabeled"}, {"type": "text", "expr": ": Action-model based multi-agent plan\nrecognition", "word_idx": 81966, "sentence_idx": 1467, "label": "unlabeled"}, {"type": "text", "expr": "In: Proceedings of NIPS, pp", "word_idx": 82015, "sentence_idx": 1468, "label": "unlabeled"}, {"type": "text", "expr": " 377\u2013385 (2012)", "word_idx": 82042, "sentence_idx": 1469, "label": "unlabeled"}, {"type": "text", "expr": "Generated  on Tue Apr  3 11:31:28 2018 by", "word_idx": 82057, "sentence_idx": 1470, "label": "unlabeled"}, {"type": "text", "expr": "LaTeXML", "word_idx": 82098, "sentence_idx": 1471, "label": "unlabeled"}], "WaveNet_A_Generative_Model_for_Raw_Audio": [{"type": "text", "expr": "WaveNet: A Generative Model for Raw Audio", "word_idx": 0, "sentence_idx": 0, "label": "unlabeled"}, {"type": "text", "expr": "WaveNet: A Generative Model for Raw Audio", "word_idx": 41, "sentence_idx": 1, "label": "unlabeled"}, {"type": "text", "expr": "A\u00e4ron\u00a0van\u00a0den\u00a0Oord &Sander\u00a0Dieleman &Heiga\u00a0Zen ${}^{\\dagger}$   \\AND Karen\u00a0Simonyan &Oriol\u00a0Vinyals &Alex\u00a0Graves  \\AND Nal\u00a0Kalchbrenner &\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Andrew\u00a0Senior &Koray\u00a0Kavukcuoglu  \\AND {avdnoord, sedielem, heigazen, simonyan, vinyals, gravesa, nalk, andrewsenior, korayk}@google", "word_idx": 82, "sentence_idx": 2, "label": "unlabeled"}, {"type": "text", "expr": "com\n Google DeepMind, London, UK \n ${}^{\\dagger}$ \u2009Google, London, UK", "word_idx": 359, "sentence_idx": 3, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\dagger}$$", "word_idx": 428, "sentence_idx": 4, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\dagger}$$", "word_idx": 440, "sentence_idx": 5, "label": "unlabeled"}, {"type": "text", "expr": "Abstract This paper introduces WaveNet, a deep neural network for generating raw audio waveforms", "word_idx": 452, "sentence_idx": 6, "label": "unlabeled"}, {"type": "text", "expr": " The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio", "word_idx": 548, "sentence_idx": 7, "label": "unlabeled"}, {"type": "text", "expr": "\nWhen applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin", "word_idx": 807, "sentence_idx": 8, "label": "unlabeled"}, {"type": "text", "expr": " A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity", "word_idx": 1030, "sentence_idx": 9, "label": "unlabeled"}, {"type": "text", "expr": " When trained to model music, we find that it generates novel and often highly realistic musical fragments", "word_idx": 1195, "sentence_idx": 10, "label": "unlabeled"}, {"type": "text", "expr": " We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition", "word_idx": 1301, "sentence_idx": 11, "label": "unlabeled"}, {"type": "text", "expr": "Abstract", "word_idx": 1417, "sentence_idx": 12, "label": "unlabeled"}, {"type": "text", "expr": "This paper introduces WaveNet, a deep neural network for generating raw audio waveforms", "word_idx": 1425, "sentence_idx": 13, "label": "unlabeled"}, {"type": "text", "expr": " The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio", "word_idx": 1512, "sentence_idx": 14, "label": "unlabeled"}, {"type": "text", "expr": "\nWhen applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin", "word_idx": 1771, "sentence_idx": 15, "label": "unlabeled"}, {"type": "text", "expr": " A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity", "word_idx": 1994, "sentence_idx": 16, "label": "unlabeled"}, {"type": "text", "expr": " When trained to model music, we find that it generates novel and often highly realistic musical fragments", "word_idx": 2159, "sentence_idx": 17, "label": "unlabeled"}, {"type": "text", "expr": " We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition", "word_idx": 2265, "sentence_idx": 18, "label": "unlabeled"}, {"type": "text", "expr": "\\definecolor", "word_idx": 2381, "sentence_idx": 19, "label": "unlabeled"}, {"type": "text", "expr": "GoogleRedrgb0", "word_idx": 2393, "sentence_idx": 20, "label": "unlabeled"}, {"type": "text", "expr": "97265625,0", "word_idx": 2406, "sentence_idx": 21, "label": "unlabeled"}, {"type": "text", "expr": "00390625,0", "word_idx": 2416, "sentence_idx": 22, "label": "unlabeled"}, {"type": "text", "expr": "00390625 \\definecolor GoogleBluergb0", "word_idx": 2426, "sentence_idx": 23, "label": "unlabeled"}, {"type": "text", "expr": "0078125,0", "word_idx": 2462, "sentence_idx": 24, "label": "unlabeled"}, {"type": "text", "expr": "3984375,0", "word_idx": 2471, "sentence_idx": 25, "label": "unlabeled"}, {"type": "text", "expr": "78125 \\definecolor GoogleYellowrgb0", "word_idx": 2480, "sentence_idx": 26, "label": "unlabeled"}, {"type": "text", "expr": "9453125,0", "word_idx": 2515, "sentence_idx": 27, "label": "unlabeled"}, {"type": "text", "expr": "70703125,0", "word_idx": 2524, "sentence_idx": 28, "label": "unlabeled"}, {"type": "text", "expr": "05859375 \\definecolor GoogleGreenrgb0", "word_idx": 2534, "sentence_idx": 29, "label": "unlabeled"}, {"type": "text", "expr": "57421875,0", "word_idx": 2571, "sentence_idx": 30, "label": "unlabeled"}, {"type": "text", "expr": "23046875", "word_idx": 2581, "sentence_idx": 31, "label": "unlabeled"}, {"type": "text", "expr": "\\definecolor", "word_idx": 2589, "sentence_idx": 32, "label": "unlabeled"}, {"type": "text", "expr": "\\definecolor", "word_idx": 2601, "sentence_idx": 33, "label": "unlabeled"}, {"type": "text", "expr": "\\definecolor", "word_idx": 2613, "sentence_idx": 34, "label": "unlabeled"}, {"type": "text", "expr": "1  Introduction", "word_idx": 2625, "sentence_idx": 35, "label": "unlabeled"}, {"type": "text", "expr": "This work explores raw audio generation techniques, inspired by recent advances in neural autoregressive generative models that model complex distributions such as images   and text  ", "word_idx": 2640, "sentence_idx": 36, "label": "unlabeled"}, {"type": "text", "expr": " Modeling joint probabilities over pixels or words using neural architectures as products of conditional distributions yields state-of-the-art generation", "word_idx": 2823, "sentence_idx": 37, "label": "unlabeled"}, {"type": "text", "expr": "2016a", "word_idx": 2976, "sentence_idx": 38, "label": "unlabeled"}, {"type": "text", "expr": "Remarkably, these architectures are able to model distributions over thousands of random variables (e", "word_idx": 2981, "sentence_idx": 39, "label": "unlabeled"}, {"type": "text", "expr": " 64 $\\times$ 64 pixels as in PixelRNN  )", "word_idx": 3082, "sentence_idx": 40, "label": "unlabeled"}, {"type": "text", "expr": " The question this paper addresses is whether similar approaches can succeed in generating wideband raw audio waveforms, which are signals with very high temporal resolution, at least 16,000 samples per second (see Fig", "word_idx": 3122, "sentence_idx": 41, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 1 )", "word_idx": 3340, "sentence_idx": 42, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 3345, "sentence_idx": 43, "label": "unlabeled"}, {"type": "text", "expr": "2016a", "word_idx": 3351, "sentence_idx": 44, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:  A second of generated speech", "word_idx": 3356, "sentence_idx": 45, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:", "word_idx": 3395, "sentence_idx": 46, "label": "unlabeled"}, {"type": "text", "expr": "This paper introduces  WaveNet , an audio generative model based on the PixelCNN   architecture", "word_idx": 3404, "sentence_idx": 47, "label": "unlabeled"}, {"type": "text", "expr": " The main contributions of this work are as follows:", "word_idx": 3499, "sentence_idx": 48, "label": "unlabeled"}, {"type": "text", "expr": "WaveNet", "word_idx": 3551, "sentence_idx": 49, "label": "unlabeled"}, {"type": "text", "expr": "2016a", "word_idx": 3558, "sentence_idx": 50, "label": "unlabeled"}, {"type": "text", "expr": "We show that WaveNets can generate raw speech signals with subjective naturalness never before reported in the field of text-to-speech (TTS), as assessed by human raters", "word_idx": 3563, "sentence_idx": 51, "label": "unlabeled"}, {"type": "text", "expr": "We show that WaveNets can generate raw speech signals with subjective naturalness never before reported in the field of text-to-speech (TTS), as assessed by human raters", "word_idx": 3732, "sentence_idx": 52, "label": "unlabeled"}, {"type": "text", "expr": "In order to deal with long-range temporal dependencies needed for raw audio generation, we develop new architectures based on dilated causal convolutions, which exhibit very large receptive fields", "word_idx": 3901, "sentence_idx": 53, "label": "unlabeled"}, {"type": "text", "expr": "In order to deal with long-range temporal dependencies needed for raw audio generation, we develop new architectures based on dilated causal convolutions, which exhibit very large receptive fields", "word_idx": 4097, "sentence_idx": 54, "label": "unlabeled"}, {"type": "text", "expr": "We show that when conditioned on a speaker identity, a single model can be used to generate different voices", "word_idx": 4293, "sentence_idx": 55, "label": "unlabeled"}, {"type": "text", "expr": "We show that when conditioned on a speaker identity, a single model can be used to generate different voices", "word_idx": 4401, "sentence_idx": 56, "label": "unlabeled"}, {"type": "text", "expr": "The same architecture shows strong results when tested on a small speech recognition dataset, and is promising when used to generate other audio modalities such as music", "word_idx": 4509, "sentence_idx": 57, "label": "unlabeled"}, {"type": "text", "expr": "The same architecture shows strong results when tested on a small speech recognition dataset, and is promising when used to generate other audio modalities such as music", "word_idx": 4678, "sentence_idx": 58, "label": "unlabeled"}, {"type": "text", "expr": "We believe that WaveNets provide a generic and flexible framework for tackling many applications that rely on audio generation (e", "word_idx": 4847, "sentence_idx": 59, "label": "unlabeled"}, {"type": "text", "expr": " TTS, music, speech enhancement, voice conversion, source separation)", "word_idx": 4976, "sentence_idx": 60, "label": "unlabeled"}, {"type": "text", "expr": "We believe that WaveNets provide a generic and flexible framework for tackling many applications that rely on audio generation (e", "word_idx": 5045, "sentence_idx": 61, "label": "unlabeled"}, {"type": "text", "expr": " TTS, music, speech enhancement, voice conversion, source separation)", "word_idx": 5174, "sentence_idx": 62, "label": "unlabeled"}, {"type": "text", "expr": "2  WaveNet", "word_idx": 5243, "sentence_idx": 63, "label": "unlabeled"}, {"type": "text", "expr": "In this paper we introduce a new generative model operating directly on the raw audio waveform", "word_idx": 5253, "sentence_idx": 64, "label": "unlabeled"}, {"type": "text", "expr": "\nThe joint probability of a waveform  $\\mathbf{x}=\\{x_{1},\\dots,x_{T}\\}$  is factorised as a product of conditional probabilities as follows:", "word_idx": 5347, "sentence_idx": 65, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}=\\{x_{1},\\dots,x_{T}\\}$$", "word_idx": 5488, "sentence_idx": 66, "label": "unlabeled"}, {"type": "text", "expr": "$p\\left(\\mathbf{x}\\right)=\\prod_{t=1}^{T}p\\left(x_{t}\\mid x_{1},\\dots,x_{t-1}\\right)$", "word_idx": 5520, "sentence_idx": 67, "label": "unlabeled"}, {"type": "math", "expr": "$$p\\left(\\mathbf{x}\\right)=\\prod_{t=1}^{T}p\\left(x_{t}\\mid x_{1},\\dots,x_{t-1}\\right)$$", "word_idx": 5605, "sentence_idx": 68, "label": "unlabeled"}, {"type": "text", "expr": "Each audio sample  $x_{t}$  is therefore conditioned on the samples at all previous timesteps", "word_idx": 5688, "sentence_idx": 69, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t}$$", "word_idx": 5781, "sentence_idx": 70, "label": "unlabeled"}, {"type": "text", "expr": "Similarly to PixelCNNs  , the conditional probability distribution is modelled by a stack of convolutional layers", "word_idx": 5786, "sentence_idx": 71, "label": "unlabeled"}, {"type": "text", "expr": " There are no pooling layers in the network, and the output of the model has the same time dimensionality as the input", "word_idx": 5899, "sentence_idx": 72, "label": "unlabeled"}, {"type": "text", "expr": " The model outputs a categorical distribution over the next value  $x_{t}$  with a softmax layer and it is optimized to maximize the log-likelihood of the data w", "word_idx": 6017, "sentence_idx": 73, "label": "unlabeled"}, {"type": "text", "expr": " the parameters", "word_idx": 6178, "sentence_idx": 74, "label": "unlabeled"}, {"type": "text", "expr": " Because log-likelihoods are tractable, we tune hyper-parameters on a validation set and can easily measure if the model is overfitting or underfitting", "word_idx": 6193, "sentence_idx": 75, "label": "unlabeled"}, {"type": "text", "expr": "2016a", "word_idx": 6344, "sentence_idx": 76, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t}$$", "word_idx": 6349, "sentence_idx": 77, "label": "unlabeled"}, {"type": "text", "expr": "1  Dilated Causal Convolutions", "word_idx": 6354, "sentence_idx": 78, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:  Visualization of a stack of causal convolutional layers", "word_idx": 6384, "sentence_idx": 79, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:", "word_idx": 6450, "sentence_idx": 80, "label": "unlabeled"}, {"type": "text", "expr": "The main ingredient of WaveNet are causal convolutions", "word_idx": 6459, "sentence_idx": 81, "label": "unlabeled"}, {"type": "text", "expr": " By using causal convolutions, we make sure the model cannot violate the ordering in which we model the data: the prediction  $p\\left(x_{t+1}\\mid x_{1},,x_{t}\\right)$  emitted by the model at timestep  $t$  cannot depend on any of the future timesteps  $x_{t+1},x_{t+2},\\dots,x_{T}$  as shown in Fig", "word_idx": 6513, "sentence_idx": 82, "label": "unlabeled"}, {"type": "text", "expr": " For images, the equivalent of a causal convolution is a masked convolution   which can be implemented by constructing a mask tensor and doing an elementwise multiplication of this mask with the convolution kernel before applying it", "word_idx": 6812, "sentence_idx": 83, "label": "unlabeled"}, {"type": "text", "expr": " For 1-D data such as audio one can more easily implement this by shifting the output of a normal convolution by a few timesteps", "word_idx": 7044, "sentence_idx": 84, "label": "unlabeled"}, {"type": "math", "expr": "$$p\\left(x_{t+1}\\mid x_{1},...,x_{t}\\right)$$", "word_idx": 7172, "sentence_idx": 85, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{t+1},x_{t+2},\\dots,x_{T}$$", "word_idx": 7213, "sentence_idx": 86, "label": "unlabeled"}, {"type": "text", "expr": "2016a", "word_idx": 7240, "sentence_idx": 87, "label": "unlabeled"}, {"type": "text", "expr": "At training time, the conditional predictions for all timesteps can be made in parallel because all timesteps of ground truth  $\\mathbf{x}$  are known", "word_idx": 7245, "sentence_idx": 88, "label": "unlabeled"}, {"type": "text", "expr": " When generating with the model, the predictions are sequential: after each sample is predicted, it is fed back into the network to predict the next sample", "word_idx": 7395, "sentence_idx": 89, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}$$", "word_idx": 7550, "sentence_idx": 90, "label": "unlabeled"}, {"type": "text", "expr": "Because models with causal convolutions do not have recurrent connections, they are typically faster to train than RNNs, especially when applied to very long sequences", "word_idx": 7560, "sentence_idx": 91, "label": "unlabeled"}, {"type": "text", "expr": " One of the problems of causal convolutions is that they require many layers, or large filters to increase the receptive field", "word_idx": 7727, "sentence_idx": 92, "label": "unlabeled"}, {"type": "text", "expr": " For example, in Fig", "word_idx": 7853, "sentence_idx": 93, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 2  the receptive field is only 5 (= #layers + filter length - 1)", "word_idx": 7873, "sentence_idx": 94, "label": "unlabeled"}, {"type": "text", "expr": " In this paper we use dilated convolutions to increase the receptive field by orders of magnitude, without greatly increasing computational cost", "word_idx": 7939, "sentence_idx": 95, "label": "unlabeled"}, {"type": "text", "expr": "A dilated convolution (also called  \u00e0 trous , or convolution with holes) is a convolution where the filter is applied over an area larger than its length by skipping input values with a certain step", "word_idx": 8083, "sentence_idx": 96, "label": "unlabeled"}, {"type": "text", "expr": " It is equivalent to a convolution with a larger filter derived from the original filter by dilating it with zeros, but is significantly more efficient", "word_idx": 8281, "sentence_idx": 97, "label": "unlabeled"}, {"type": "text", "expr": " A dilated convolution effectively allows the network to operate on a coarser scale than with a normal convolution", "word_idx": 8432, "sentence_idx": 98, "label": "unlabeled"}, {"type": "text", "expr": " This is similar to pooling or strided convolutions, but here the output has the same size as the input", "word_idx": 8546, "sentence_idx": 99, "label": "unlabeled"}, {"type": "text", "expr": " As a special case, dilated convolution with dilation  $1$  yields the standard convolution", "word_idx": 8649, "sentence_idx": 100, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 3  depicts dilated causal convolutions for dilations  $1$ ,  $2$ ,  $4$ , and  $8$ ", "word_idx": 8740, "sentence_idx": 101, "label": "unlabeled"}, {"type": "text", "expr": " Dilated convolutions have previously been used in various contexts, e", "word_idx": 8825, "sentence_idx": 102, "label": "unlabeled"}, {"type": "text", "expr": " signal processing  , and image segmentation  ", "word_idx": 8895, "sentence_idx": 103, "label": "unlabeled"}, {"type": "text", "expr": "\u00e0 trous", "word_idx": 8941, "sentence_idx": 104, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:  Visualization of a stack of  dilated  causal convolutional layers", "word_idx": 8948, "sentence_idx": 105, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:", "word_idx": 9024, "sentence_idx": 106, "label": "unlabeled"}, {"type": "text", "expr": "dilated", "word_idx": 9033, "sentence_idx": 107, "label": "unlabeled"}, {"type": "text", "expr": "Stacked dilated convolutions enable networks to have very large receptive fields with just a few layers, while preserving the input resolution throughout the network as well as computational efficiency", "word_idx": 9040, "sentence_idx": 108, "label": "unlabeled"}, {"type": "text", "expr": " In this paper, the dilation is doubled for every layer up to a limit and then repeated: e", "word_idx": 9241, "sentence_idx": 109, "label": "unlabeled"}, {"type": "text", "expr": "Stacked dilated convolutions enable networks to have very large receptive fields with just a few layers, while preserving the input resolution throughout the network as well as computational efficiency", "word_idx": 9331, "sentence_idx": 110, "label": "unlabeled"}, {"type": "text", "expr": " In this paper, the dilation is doubled for every layer up to a limit and then repeated: e", "word_idx": 9532, "sentence_idx": 111, "label": "unlabeled"}, {"type": "text", "expr": "$1,2,4,\\dots,512,1,2,4,\\dots,512,1,2,4,\\dots,512$", "word_idx": 9622, "sentence_idx": 112, "label": "unlabeled"}, {"type": "math", "expr": "$$1,2,4,\\dots,512,1,2,4,\\dots,512,1,2,4,\\dots,512.$$", "word_idx": 9671, "sentence_idx": 113, "label": "unlabeled"}, {"type": "text", "expr": "The intuition behind this configuration is two-fold", "word_idx": 9719, "sentence_idx": 114, "label": "unlabeled"}, {"type": "text", "expr": " First, exponentially increasing the dilation factor results in exponential receptive field growth with depth\u00a0 ", "word_idx": 9770, "sentence_idx": 115, "label": "unlabeled"}, {"type": "text", "expr": " For example each  $1,2,4,\\dots,512$  block has receptive field of size  $1024$ , and can be seen as a more efficient and discriminative (non-linear) counterpart of a  $1\\times 1024$  convolution", "word_idx": 9881, "sentence_idx": 116, "label": "unlabeled"}, {"type": "text", "expr": " Second, stacking these blocks further increases the model capacity and the receptive field size", "word_idx": 10076, "sentence_idx": 117, "label": "unlabeled"}, {"type": "math", "expr": "$$1,2,4,\\dots,512$$", "word_idx": 10172, "sentence_idx": 118, "label": "unlabeled"}, {"type": "math", "expr": "$$1024$$", "word_idx": 10187, "sentence_idx": 119, "label": "unlabeled"}, {"type": "math", "expr": "$$1\\times 1024$$", "word_idx": 10191, "sentence_idx": 120, "label": "unlabeled"}, {"type": "text", "expr": "2  Softmax distributions", "word_idx": 10203, "sentence_idx": 121, "label": "unlabeled"}, {"type": "text", "expr": "One approach to modeling the conditional distributions  $p\\left(x_{t}\\mid x_{1},\\dots,x_{t-1}\\right)$  over the individual audio samples would be to use a mixture model such as a mixture density network   or mixture of conditional Gaussian scale mixtures (MCGSM)  ", "word_idx": 10227, "sentence_idx": 122, "label": "unlabeled"}, {"type": "text", "expr": "\nHowever,   showed that a softmax distribution tends to work better, even when the data is implicitly continuous (as is the case for image pixel intensities or audio sample values)", "word_idx": 10491, "sentence_idx": 123, "label": "unlabeled"}, {"type": "text", "expr": " One of the reasons is that a categorical distribution is more flexible and can more easily model arbitrary distributions because it makes no assumptions about their shape", "word_idx": 10671, "sentence_idx": 124, "label": "unlabeled"}, {"type": "math", "expr": "$$p\\left(x_{t}\\mid x_{1},\\dots,x_{t-1}\\right)$$", "word_idx": 10842, "sentence_idx": 125, "label": "unlabeled"}, {"type": "text", "expr": "2016a", "word_idx": 10885, "sentence_idx": 126, "label": "unlabeled"}, {"type": "text", "expr": "Because raw audio is typically stored as a sequence of 16-bit integer values (one per timestep), a softmax layer would need to output 65,536 probabilities per timestep to model all possible values", "word_idx": 10890, "sentence_idx": 127, "label": "unlabeled"}, {"type": "text", "expr": " To make this more tractable, we first apply a  $\\mu$ -law companding transformation   to the data, and then quantize it to 256 possible values:", "word_idx": 11086, "sentence_idx": 128, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mu$$", "word_idx": 11230, "sentence_idx": 129, "label": "unlabeled"}, {"type": "text", "expr": "$f\\left(x_{t}\\right)=\\operatorname{sign}(x_{t})\\frac{\\ln\\left(1+\\mu\\left|x_{t}%\n\\right|\\right)}{\\ln\\left(1+\\mu\\right)},$", "word_idx": 11233, "sentence_idx": 130, "label": "unlabeled"}, {"type": "math", "expr": "$$f\\left(x_{t}\\right)=\\operatorname{sign}(x_{t})\\frac{\\ln\\left(1+\\mu\\left|x_{t}%\n\\right|\\right)}{\\ln\\left(1+\\mu\\right)},$$", "word_idx": 11353, "sentence_idx": 131, "label": "unlabeled"}, {"type": "text", "expr": "where  $-1<x_{t}<1$  and  $\\mu=255$ ", "word_idx": 11471, "sentence_idx": 132, "label": "unlabeled"}, {"type": "text", "expr": "\nThis non-linear quantization produces a significantly better reconstruction than a simple linear quantization scheme", "word_idx": 11507, "sentence_idx": 133, "label": "unlabeled"}, {"type": "text", "expr": " Especially for speech, we found that the reconstructed signal after quantization sounded very similar to the original", "word_idx": 11624, "sentence_idx": 134, "label": "unlabeled"}, {"type": "math", "expr": "$$-1<x_{t}<1$$", "word_idx": 11742, "sentence_idx": 135, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mu=255$$", "word_idx": 11752, "sentence_idx": 136, "label": "unlabeled"}, {"type": "text", "expr": "3  Gated activation units", "word_idx": 11759, "sentence_idx": 137, "label": "unlabeled"}, {"type": "text", "expr": "We use the same gated activation unit as used in the gated PixelCNN  :", "word_idx": 11784, "sentence_idx": 138, "label": "unlabeled"}, {"type": "text", "expr": "2016b", "word_idx": 11854, "sentence_idx": 139, "label": "unlabeled"}, {"type": "text", "expr": "$\\mathbf{z}=\\tanh\\left(W_{f,k}\\ast\\mathbf{x}\\right)\\odot\\sigma\\left(W_{g,k}\\ast%\n\\mathbf{x}\\right),$", "word_idx": 11859, "sentence_idx": 140, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{z}=\\tanh\\left(W_{f,k}\\ast\\mathbf{x}\\right)\\odot\\sigma\\left(W_{g,k}\\ast%\n\\mathbf{x}\\right),$$", "word_idx": 11959, "sentence_idx": 141, "label": "unlabeled"}, {"type": "text", "expr": "where  $\\ast$  denotes a convolution operator,  $\\odot$  denotes an element-wise multiplication operator,  $\\sigma(\\cdot)$  is a sigmoid function,  $k$  is the layer index,  $f$  and  $g$  denote filter and gate, respectively, and  $W$  is a learnable convolution filter", "word_idx": 12057, "sentence_idx": 142, "label": "unlabeled"}, {"type": "text", "expr": "\nIn our initial experiments, we observed that this non-linearity worked significantly better than the rectified linear activation function   for modeling audio signals", "word_idx": 12327, "sentence_idx": 143, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ast$$", "word_idx": 12494, "sentence_idx": 144, "label": "unlabeled"}, {"type": "math", "expr": "$$\\odot$$", "word_idx": 12498, "sentence_idx": 145, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma(\\cdot)$$", "word_idx": 12503, "sentence_idx": 146, "label": "unlabeled"}, {"type": "text", "expr": "4  Residual and skip connections", "word_idx": 12516, "sentence_idx": 147, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:  Overview of the residual block and the entire architecture", "word_idx": 12548, "sentence_idx": 148, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:", "word_idx": 12617, "sentence_idx": 149, "label": "unlabeled"}, {"type": "text", "expr": "Both residual   and parameterised skip connections are used throughout the network, to speed up convergence and enable training of much deeper models", "word_idx": 12626, "sentence_idx": 150, "label": "unlabeled"}, {"type": "text", "expr": " In Fig", "word_idx": 12775, "sentence_idx": 151, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 4  we show a residual block of our model, which is stacked many times in the network", "word_idx": 12782, "sentence_idx": 152, "label": "unlabeled"}, {"type": "text", "expr": "5  Conditional WaveNets", "word_idx": 12868, "sentence_idx": 153, "label": "unlabeled"}, {"type": "text", "expr": "Given an additional input  $\\mathbf{h}$ , WaveNets can model the conditional distribution  $p\\left(\\mathbf{x}\\mid\\mathbf{h}\\right)$  of the audio given this input", "word_idx": 12891, "sentence_idx": 154, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0( 1 ) now becomes", "word_idx": 13053, "sentence_idx": 155, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{h}$$", "word_idx": 13071, "sentence_idx": 156, "label": "unlabeled"}, {"type": "math", "expr": "$$p\\left(\\mathbf{x}\\mid\\mathbf{h}\\right)$$", "word_idx": 13081, "sentence_idx": 157, "label": "unlabeled"}, {"type": "text", "expr": "$p\\left(\\mathbf{x}\\mid\\mathbf{h}\\right)=\\prod_{t=1}^{T}p\\left(x_{t}\\mid x_{1},%\n\\dots,x_{t-1},\\mathbf{h}\\right)$", "word_idx": 13119, "sentence_idx": 158, "label": "unlabeled"}, {"type": "math", "expr": "$$p\\left(\\mathbf{x}\\mid\\mathbf{h}\\right)=\\prod_{t=1}^{T}p\\left(x_{t}\\mid x_{1},%\n\\dots,x_{t-1},\\mathbf{h}\\right).$$", "word_idx": 13231, "sentence_idx": 159, "label": "unlabeled"}, {"type": "text", "expr": "By conditioning the model on other input variables, we can guide WaveNet\u2019s generation to produce audio with the required characteristics", "word_idx": 13342, "sentence_idx": 160, "label": "unlabeled"}, {"type": "text", "expr": " For example, in a multi-speaker setting we can choose the speaker by feeding the speaker identity to the model as an extra input", "word_idx": 13478, "sentence_idx": 161, "label": "unlabeled"}, {"type": "text", "expr": " Similarly, for TTS we need to feed information about the text as an extra input", "word_idx": 13607, "sentence_idx": 162, "label": "unlabeled"}, {"type": "text", "expr": "By conditioning the model on other input variables, we can guide WaveNet\u2019s generation to produce audio with the required characteristics", "word_idx": 13687, "sentence_idx": 163, "label": "unlabeled"}, {"type": "text", "expr": " For example, in a multi-speaker setting we can choose the speaker by feeding the speaker identity to the model as an extra input", "word_idx": 13823, "sentence_idx": 164, "label": "unlabeled"}, {"type": "text", "expr": " Similarly, for TTS we need to feed information about the text as an extra input", "word_idx": 13952, "sentence_idx": 165, "label": "unlabeled"}, {"type": "text", "expr": "We condition the model on other inputs in two different ways: global conditioning and local conditioning", "word_idx": 14032, "sentence_idx": 166, "label": "unlabeled"}, {"type": "text", "expr": " Global conditioning is characterised by a single latent representation  $\\mathbf{h}$  that influences the output distribution across all timesteps, e", "word_idx": 14136, "sentence_idx": 167, "label": "unlabeled"}, {"type": "text", "expr": " a speaker embedding in a TTS model", "word_idx": 14286, "sentence_idx": 168, "label": "unlabeled"}, {"type": "text", "expr": " The activation function from Eq", "word_idx": 14321, "sentence_idx": 169, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0( 2 ) now becomes:", "word_idx": 14353, "sentence_idx": 170, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{h}$$", "word_idx": 14372, "sentence_idx": 171, "label": "unlabeled"}, {"type": "text", "expr": "$\\mathbf{z}=\\tanh\\left(W_{f,k}\\ast\\mathbf{x}+V_{f,k}^{T}\\mathbf{h}\\right)\\odot%\n\\sigma\\left(W_{g,k}\\ast\\mathbf{x}+V_{g,k}^{T}\\mathbf{h}\\right)$", "word_idx": 14382, "sentence_idx": 172, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{z}=\\tanh\\left(W_{f,k}\\ast\\mathbf{x}+V_{f,k}^{T}\\mathbf{h}\\right)\\odot%\n\\sigma\\left(W_{g,k}\\ast\\mathbf{x}+V_{g,k}^{T}\\mathbf{h}\\right).$$", "word_idx": 14525, "sentence_idx": 173, "label": "unlabeled"}, {"type": "text", "expr": "where  $V_{*,k}$  is a learnable linear projection, and the vector  $V_{*,k}^{T}\\mathbf{h}$  is broadcast over the time dimension", "word_idx": 14667, "sentence_idx": 174, "label": "unlabeled"}, {"type": "math", "expr": "$$V_{*,k}$$", "word_idx": 14796, "sentence_idx": 175, "label": "unlabeled"}, {"type": "math", "expr": "$$V_{*,k}^{T}\\mathbf{h}$$", "word_idx": 14803, "sentence_idx": 176, "label": "unlabeled"}, {"type": "text", "expr": "For local conditioning we have a second timeseries  $h_{t}$ , possibly with a lower sampling frequency than the audio signal, e", "word_idx": 14824, "sentence_idx": 177, "label": "unlabeled"}, {"type": "text", "expr": " linguistic features in a TTS model", "word_idx": 14951, "sentence_idx": 178, "label": "unlabeled"}, {"type": "text", "expr": " We first transform this time series using a transposed convolutional network (learned upsampling) that maps it to a new time series  $\\mathbf{y}=f(\\mathbf{h})$  with the same resolution as the audio signal, which is then used in the activation unit as follows:", "word_idx": 14986, "sentence_idx": 179, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{t}$$", "word_idx": 15247, "sentence_idx": 180, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{y}=f(\\mathbf{h})$$", "word_idx": 15252, "sentence_idx": 181, "label": "unlabeled"}, {"type": "text", "expr": "$\\mathbf{z}=\\tanh\\left(W_{f,k}\\ast\\mathbf{x}+V_{f,k}\\ast\\mathbf{y}\\right)\\odot%\n\\sigma\\left(W_{g,k}\\ast\\mathbf{x}+V_{g,k}\\ast\\mathbf{y}\\right),$", "word_idx": 15276, "sentence_idx": 182, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{z}=\\tanh\\left(W_{f,k}\\ast\\mathbf{x}+V_{f,k}\\ast\\mathbf{y}\\right)\\odot%\n\\sigma\\left(W_{g,k}\\ast\\mathbf{x}+V_{g,k}\\ast\\mathbf{y}\\right),$$", "word_idx": 15420, "sentence_idx": 183, "label": "unlabeled"}, {"type": "text", "expr": "where  $V_{f,k}\\ast\\mathbf{y}$  is now a  $1\\times 1$  convolution", "word_idx": 15562, "sentence_idx": 184, "label": "unlabeled"}, {"type": "text", "expr": " As an alternative to the transposed convolutional network, it is also possible to use  $V_{f,k}\\ast\\mathbf{h}$  and repeat these values across time", "word_idx": 15628, "sentence_idx": 185, "label": "unlabeled"}, {"type": "text", "expr": " We saw that this worked slightly worse in our experiments", "word_idx": 15776, "sentence_idx": 186, "label": "unlabeled"}, {"type": "math", "expr": "$$V_{f,k}\\ast\\mathbf{y}$$", "word_idx": 15834, "sentence_idx": 187, "label": "unlabeled"}, {"type": "math", "expr": "$$1\\times 1$$", "word_idx": 15855, "sentence_idx": 188, "label": "unlabeled"}, {"type": "math", "expr": "$$V_{f,k}\\ast\\mathbf{h}$$", "word_idx": 15864, "sentence_idx": 189, "label": "unlabeled"}, {"type": "text", "expr": "6  Context stacks", "word_idx": 15885, "sentence_idx": 190, "label": "unlabeled"}, {"type": "text", "expr": "We have already mentioned several different ways to increase the receptive field size of a WaveNet: increasing the number of dilation stages, using more layers, larger filters, greater dilation factors, or a combination thereof", "word_idx": 15902, "sentence_idx": 191, "label": "unlabeled"}, {"type": "text", "expr": " A complementary approach is to use a separate, smaller  context  stack that processes a long part of the audio signal and locally conditions a larger WaveNet that processes only a smaller part of the audio signal (cropped at the end)", "word_idx": 16129, "sentence_idx": 192, "label": "unlabeled"}, {"type": "text", "expr": " One can use multiple context stacks with varying lengths and numbers of hidden units", "word_idx": 16363, "sentence_idx": 193, "label": "unlabeled"}, {"type": "text", "expr": " Stacks with larger receptive fields have fewer units per layer", "word_idx": 16448, "sentence_idx": 194, "label": "unlabeled"}, {"type": "text", "expr": " Context stacks can also have pooling layers to run at a lower frequency", "word_idx": 16511, "sentence_idx": 195, "label": "unlabeled"}, {"type": "text", "expr": " This keeps the computational requirements at a reasonable level and is consistent with the intuition that less capacity is required to model temporal correlations at longer timescales", "word_idx": 16583, "sentence_idx": 196, "label": "unlabeled"}, {"type": "text", "expr": "context", "word_idx": 16767, "sentence_idx": 197, "label": "unlabeled"}, {"type": "text", "expr": "3  Experiments", "word_idx": 16774, "sentence_idx": 198, "label": "unlabeled"}, {"type": "text", "expr": "To measure WaveNet\u2019s audio modelling performance, we evaluate it on three different tasks: multi-speaker speech generation (not conditioned on text), TTS, and music audio modelling", "word_idx": 16788, "sentence_idx": 199, "label": "unlabeled"}, {"type": "text", "expr": " We provide samples drawn from WaveNet for these experiments on the accompanying webpage:\n https://www", "word_idx": 16968, "sentence_idx": 200, "label": "unlabeled"}, {"type": "text", "expr": "deepmind", "word_idx": 17070, "sentence_idx": 201, "label": "unlabeled"}, {"type": "text", "expr": "com/blog/wavenet-generative-model-raw-audio/ ", "word_idx": 17078, "sentence_idx": 202, "label": "unlabeled"}, {"type": "text", "expr": "https://www", "word_idx": 17123, "sentence_idx": 203, "label": "unlabeled"}, {"type": "text", "expr": "deepmind", "word_idx": 17134, "sentence_idx": 204, "label": "unlabeled"}, {"type": "text", "expr": "com/blog/wavenet-generative-model-raw-audio/", "word_idx": 17142, "sentence_idx": 205, "label": "unlabeled"}, {"type": "text", "expr": "1  Multi-Speaker Speech Generation", "word_idx": 17186, "sentence_idx": 206, "label": "unlabeled"}, {"type": "text", "expr": "For the first experiment we looked at free-form speech generation (not conditioned on text)", "word_idx": 17220, "sentence_idx": 207, "label": "unlabeled"}, {"type": "text", "expr": " We used the English multi-speaker corpus from CSTR voice cloning toolkit (VCTK)   and conditioned WaveNet only on the speaker", "word_idx": 17311, "sentence_idx": 208, "label": "unlabeled"}, {"type": "text", "expr": " The conditioning was applied by feeding the speaker ID to the model in the form of a one-hot vector", "word_idx": 17437, "sentence_idx": 209, "label": "unlabeled"}, {"type": "text", "expr": " The dataset consisted of 44 hours of data from 109 different speakers", "word_idx": 17537, "sentence_idx": 210, "label": "unlabeled"}, {"type": "text", "expr": "Because the model is not conditioned on text, it generates non-existent but human language-like words in a smooth way with realistic sounding intonations", "word_idx": 17607, "sentence_idx": 211, "label": "unlabeled"}, {"type": "text", "expr": " This is similar to generative models of language or images, where samples look realistic at first glance, but are clearly unnatural upon closer inspection", "word_idx": 17760, "sentence_idx": 212, "label": "unlabeled"}, {"type": "text", "expr": " The lack of long range coherence is partly due to the limited size of the model\u2019s receptive field (about 300 milliseconds), which means it can only remember the last 2\u20133 phonemes it produced", "word_idx": 17915, "sentence_idx": 213, "label": "unlabeled"}, {"type": "text", "expr": "Because the model is not conditioned on text, it generates non-existent but human language-like words in a smooth way with realistic sounding intonations", "word_idx": 18106, "sentence_idx": 214, "label": "unlabeled"}, {"type": "text", "expr": " This is similar to generative models of language or images, where samples look realistic at first glance, but are clearly unnatural upon closer inspection", "word_idx": 18259, "sentence_idx": 215, "label": "unlabeled"}, {"type": "text", "expr": " The lack of long range coherence is partly due to the limited size of the model\u2019s receptive field (about 300 milliseconds), which means it can only remember the last 2\u20133 phonemes it produced", "word_idx": 18414, "sentence_idx": 216, "label": "unlabeled"}, {"type": "text", "expr": "A single WaveNet was able to model speech from any of the speakers by conditioning it on a one-hot encoding of a speaker", "word_idx": 18605, "sentence_idx": 217, "label": "unlabeled"}, {"type": "text", "expr": " This confirms that it is powerful enough to capture the characteristics of all 109 speakers from the dataset in a single model", "word_idx": 18725, "sentence_idx": 218, "label": "unlabeled"}, {"type": "text", "expr": " We observed that adding speakers resulted in better validation set performance compared to training solely on a single speaker", "word_idx": 18852, "sentence_idx": 219, "label": "unlabeled"}, {"type": "text", "expr": " This suggests that WaveNet\u2019s internal representation was shared among multiple speakers", "word_idx": 18979, "sentence_idx": 220, "label": "unlabeled"}, {"type": "text", "expr": "A single WaveNet was able to model speech from any of the speakers by conditioning it on a one-hot encoding of a speaker", "word_idx": 19067, "sentence_idx": 221, "label": "unlabeled"}, {"type": "text", "expr": " This confirms that it is powerful enough to capture the characteristics of all 109 speakers from the dataset in a single model", "word_idx": 19187, "sentence_idx": 222, "label": "unlabeled"}, {"type": "text", "expr": " We observed that adding speakers resulted in better validation set performance compared to training solely on a single speaker", "word_idx": 19314, "sentence_idx": 223, "label": "unlabeled"}, {"type": "text", "expr": " This suggests that WaveNet\u2019s internal representation was shared among multiple speakers", "word_idx": 19441, "sentence_idx": 224, "label": "unlabeled"}, {"type": "text", "expr": "Finally, we observed that the model also picked up on other characteristics in the audio apart from the voice itself", "word_idx": 19529, "sentence_idx": 225, "label": "unlabeled"}, {"type": "text", "expr": " For instance, it also mimicked the acoustics and recording quality, as well as the breathing and mouth movements of the speakers", "word_idx": 19645, "sentence_idx": 226, "label": "unlabeled"}, {"type": "text", "expr": "Finally, we observed that the model also picked up on other characteristics in the audio apart from the voice itself", "word_idx": 19774, "sentence_idx": 227, "label": "unlabeled"}, {"type": "text", "expr": " For instance, it also mimicked the acoustics and recording quality, as well as the breathing and mouth movements of the speakers", "word_idx": 19890, "sentence_idx": 228, "label": "unlabeled"}, {"type": "text", "expr": "2  Text-To-Speech", "word_idx": 20019, "sentence_idx": 229, "label": "unlabeled"}, {"type": "text", "expr": "For the second experiment we looked at TTS", "word_idx": 20036, "sentence_idx": 230, "label": "unlabeled"}, {"type": "text", "expr": "\nWe used the same single-speaker speech databases from which Google\u2019s North American English and Mandarin Chinese TTS systems are built", "word_idx": 20078, "sentence_idx": 231, "label": "unlabeled"}, {"type": "text", "expr": "\nThe North American English dataset contains 24", "word_idx": 20213, "sentence_idx": 232, "label": "unlabeled"}, {"type": "text", "expr": "6 hours of speech data, and the Mandarin Chinese dataset contains 34", "word_idx": 20260, "sentence_idx": 233, "label": "unlabeled"}, {"type": "text", "expr": "8 hours; both were spoken by professional female speakers", "word_idx": 20328, "sentence_idx": 234, "label": "unlabeled"}, {"type": "text", "expr": "For the second experiment we looked at TTS", "word_idx": 20385, "sentence_idx": 235, "label": "unlabeled"}, {"type": "text", "expr": "\nWe used the same single-speaker speech databases from which Google\u2019s North American English and Mandarin Chinese TTS systems are built", "word_idx": 20427, "sentence_idx": 236, "label": "unlabeled"}, {"type": "text", "expr": "\nThe North American English dataset contains 24", "word_idx": 20562, "sentence_idx": 237, "label": "unlabeled"}, {"type": "text", "expr": "6 hours of speech data, and the Mandarin Chinese dataset contains 34", "word_idx": 20609, "sentence_idx": 238, "label": "unlabeled"}, {"type": "text", "expr": "8 hours; both were spoken by professional female speakers", "word_idx": 20677, "sentence_idx": 239, "label": "unlabeled"}, {"type": "text", "expr": "WaveNets for the TTS task were locally conditioned on  linguistic features  which were derived from input texts", "word_idx": 20734, "sentence_idx": 240, "label": "unlabeled"}, {"type": "text", "expr": "\nWe also trained WaveNets conditioned on the logarithmic fundamental frequency ( $\\log F_{0}$ ) values in addition to the linguistic features", "word_idx": 20845, "sentence_idx": 241, "label": "unlabeled"}, {"type": "text", "expr": "\nExternal models predicting  $\\log F_{0}$  values and phone durations from linguistic features were also trained for each language", "word_idx": 20986, "sentence_idx": 242, "label": "unlabeled"}, {"type": "text", "expr": "\nThe receptive field size of the WaveNets was 240 milliseconds", "word_idx": 21116, "sentence_idx": 243, "label": "unlabeled"}, {"type": "text", "expr": "\nAs example-based and model-based speech synthesis baselines, hidden Markov model (HMM)-driven unit selection concatenative  \nand long short-term memory recurrent neural network (LSTM-RNN)-based statistical parametric   speech synthesizers were built", "word_idx": 21178, "sentence_idx": 244, "label": "unlabeled"}, {"type": "text", "expr": "\nSince the same datasets and linguistic features were used to train both the baselines and WaveNets, these speech synthesizers could be fairly compared", "word_idx": 21428, "sentence_idx": 245, "label": "unlabeled"}, {"type": "text", "expr": "linguistic features", "word_idx": 21579, "sentence_idx": 246, "label": "unlabeled"}, {"type": "math", "expr": "$$\\log F_{0}$$", "word_idx": 21598, "sentence_idx": 247, "label": "unlabeled"}, {"type": "math", "expr": "$$\\log F_{0}$$", "word_idx": 21608, "sentence_idx": 248, "label": "unlabeled"}, {"type": "text", "expr": "To evaluate the performance of WaveNets for the TTS task, subjective paired comparison tests and mean opinion score (MOS) tests were conducted", "word_idx": 21618, "sentence_idx": 249, "label": "unlabeled"}, {"type": "text", "expr": "\nIn the paired comparison tests, after listening to each pair of samples, the subjects were asked to choose which they preferred, though they could choose \u201cneutral\u201d if they did not have any preference", "word_idx": 21760, "sentence_idx": 250, "label": "unlabeled"}, {"type": "text", "expr": "\nIn the MOS tests, after listening to each stimulus, the subjects were asked to rate the naturalness of the stimulus in a five-point Likert scale score (1: Bad, 2: Poor, 3: Fair, 4: Good, 5: Excellent)", "word_idx": 21960, "sentence_idx": 251, "label": "unlabeled"}, {"type": "text", "expr": "\nPlease refer to Appendix\u00a0 B  for details", "word_idx": 22161, "sentence_idx": 252, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 5  shows a selection of the subjective paired comparison test results (see Appendix\u00a0 B  for the complete table)", "word_idx": 22202, "sentence_idx": 253, "label": "unlabeled"}, {"type": "text", "expr": "\nIt can be seen from the results that WaveNet outperformed the baseline statistical parametric and concatenative speech synthesizers in both languages", "word_idx": 22315, "sentence_idx": 254, "label": "unlabeled"}, {"type": "text", "expr": "\nWe found that WaveNet conditioned on linguistic features could synthesize speech samples with natural segmental quality but sometimes it had unnatural prosody by stressing wrong words in a sentence", "word_idx": 22465, "sentence_idx": 255, "label": "unlabeled"}, {"type": "text", "expr": "\nThis could be due to the long-term dependency of  $F_{0}$  contours: the size of the receptive field of the WaveNet, 240 milliseconds, was not long enough to capture such long-term dependency", "word_idx": 22663, "sentence_idx": 256, "label": "unlabeled"}, {"type": "text", "expr": "\nWaveNet conditioned on both linguistic features and  $F_{0}$  values did not have this problem: the external  $F_{0}$  prediction model runs at a lower frequency (200 Hz) so it can learn long-range dependencies that exist in  $F_{0}$  contours", "word_idx": 22855, "sentence_idx": 257, "label": "unlabeled"}, {"type": "math", "expr": "$$F_{0}$$", "word_idx": 23099, "sentence_idx": 258, "label": "unlabeled"}, {"type": "math", "expr": "$$F_{0}$$", "word_idx": 23104, "sentence_idx": 259, "label": "unlabeled"}, {"type": "math", "expr": "$$F_{0}$$", "word_idx": 23109, "sentence_idx": 260, "label": "unlabeled"}, {"type": "math", "expr": "$$F_{0}$$", "word_idx": 23114, "sentence_idx": 261, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a05:  Subjective preference scores (%) of speech samples between (top) two baselines, (middle) two WaveNets, and (bottom) the best baseline and WaveNet", "word_idx": 23119, "sentence_idx": 262, "label": "unlabeled"}, {"type": "text", "expr": " Note that  LSTM  and\n Concat  correspond to LSTM-RNN-based statistical parametric and HMM-driven unit selection concatenative baseline synthesizers, and  WaveNet (L)  and  WaveNet (L+F)  correspond to the WaveNet conditioned on linguistic features only and that conditioned on both linguistic features and  $\\log F_{0}$  values", "word_idx": 23275, "sentence_idx": 263, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a05:", "word_idx": 23603, "sentence_idx": 264, "label": "unlabeled"}, {"type": "text", "expr": "Concat", "word_idx": 23612, "sentence_idx": 265, "label": "unlabeled"}, {"type": "text", "expr": "WaveNet (L)", "word_idx": 23618, "sentence_idx": 266, "label": "unlabeled"}, {"type": "text", "expr": "WaveNet (L+F)", "word_idx": 23629, "sentence_idx": 267, "label": "unlabeled"}, {"type": "math", "expr": "$$\\log F_{0}$$", "word_idx": 23642, "sentence_idx": 268, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a0 1  show the MOS test results", "word_idx": 23652, "sentence_idx": 269, "label": "unlabeled"}, {"type": "text", "expr": "\nIt can be seen from the table that WaveNets achieved 5-scale MOSs in naturalness above 4", "word_idx": 23687, "sentence_idx": 270, "label": "unlabeled"}, {"type": "text", "expr": "0, which were significantly better than those from the baseline systems", "word_idx": 23776, "sentence_idx": 271, "label": "unlabeled"}, {"type": "text", "expr": "\nThey were the highest ever reported MOS values with these training datasets and test sentences", "word_idx": 23847, "sentence_idx": 272, "label": "unlabeled"}, {"type": "text", "expr": "\nThe gap in the MOSs from the best synthetic speech to the natural ones decreased from 0", "word_idx": 23942, "sentence_idx": 273, "label": "unlabeled"}, {"type": "text", "expr": "69 to 0", "word_idx": 24030, "sentence_idx": 274, "label": "unlabeled"}, {"type": "text", "expr": "34 (51%) in US English and 0", "word_idx": 24037, "sentence_idx": 275, "label": "unlabeled"}, {"type": "text", "expr": "42 to 0", "word_idx": 24065, "sentence_idx": 276, "label": "unlabeled"}, {"type": "text", "expr": "13 (69%) in Mandarin Chinese", "word_idx": 24072, "sentence_idx": 277, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash Subjective 5-scale MOS in naturalness", "word_idx": 24100, "sentence_idx": 278, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 24153, "sentence_idx": 279, "label": "unlabeled"}, {"type": "text", "expr": "Subjective 5-scale MOS in naturalness", "word_idx": 24168, "sentence_idx": 280, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 24205, "sentence_idx": 281, "label": "unlabeled"}, {"type": "text", "expr": "Speech samples", "word_idx": 24220, "sentence_idx": 282, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 24234, "sentence_idx": 283, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 24249, "sentence_idx": 284, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 24264, "sentence_idx": 285, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 3", "word_idx": 24279, "sentence_idx": 286, "label": "unlabeled"}, {"type": "text", "expr": "67  $\\pm$  0", "word_idx": 24296, "sentence_idx": 287, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 24308, "sentence_idx": 288, "label": "unlabeled"}, {"type": "math", "expr": "$$\\pm$$", "word_idx": 24323, "sentence_idx": 289, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 3", "word_idx": 24326, "sentence_idx": 290, "label": "unlabeled"}, {"type": "text", "expr": "79  $\\pm$  0", "word_idx": 24343, "sentence_idx": 291, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 24355, "sentence_idx": 292, "label": "unlabeled"}, {"type": "math", "expr": "$$\\pm$$", "word_idx": 24370, "sentence_idx": 293, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 24373, "sentence_idx": 294, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 3", "word_idx": 24388, "sentence_idx": 295, "label": "unlabeled"}, {"type": "text", "expr": "86  $\\pm$  0", "word_idx": 24405, "sentence_idx": 296, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 24417, "sentence_idx": 297, "label": "unlabeled"}, {"type": "math", "expr": "$$\\pm$$", "word_idx": 24432, "sentence_idx": 298, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 3", "word_idx": 24435, "sentence_idx": 299, "label": "unlabeled"}, {"type": "text", "expr": "47  $\\pm$  0", "word_idx": 24452, "sentence_idx": 300, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 24464, "sentence_idx": 301, "label": "unlabeled"}, {"type": "math", "expr": "$$\\pm$$", "word_idx": 24479, "sentence_idx": 302, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 24482, "sentence_idx": 303, "label": "unlabeled"}, {"type": "text", "expr": "WaveNet", "word_idx": 24497, "sentence_idx": 304, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 4", "word_idx": 24504, "sentence_idx": 305, "label": "unlabeled"}, {"type": "text", "expr": "21   $\\pm$  0", "word_idx": 24521, "sentence_idx": 306, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 24534, "sentence_idx": 307, "label": "unlabeled"}, {"type": "math", "expr": "$$\\pm$$", "word_idx": 24549, "sentence_idx": 308, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 4", "word_idx": 24552, "sentence_idx": 309, "label": "unlabeled"}, {"type": "text", "expr": "08   $\\pm$  0", "word_idx": 24569, "sentence_idx": 310, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 24582, "sentence_idx": 311, "label": "unlabeled"}, {"type": "math", "expr": "$$\\pm$$", "word_idx": 24597, "sentence_idx": 312, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 24600, "sentence_idx": 313, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mu$$", "word_idx": 24615, "sentence_idx": 314, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 4", "word_idx": 24618, "sentence_idx": 315, "label": "unlabeled"}, {"type": "text", "expr": "46  $\\pm$  0", "word_idx": 24635, "sentence_idx": 316, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 24647, "sentence_idx": 317, "label": "unlabeled"}, {"type": "math", "expr": "$$\\pm$$", "word_idx": 24662, "sentence_idx": 318, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 4", "word_idx": 24665, "sentence_idx": 319, "label": "unlabeled"}, {"type": "text", "expr": "25  $\\pm$  0", "word_idx": 24682, "sentence_idx": 320, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 24694, "sentence_idx": 321, "label": "unlabeled"}, {"type": "math", "expr": "$$\\pm$$", "word_idx": 24709, "sentence_idx": 322, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 24712, "sentence_idx": 323, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 4", "word_idx": 24727, "sentence_idx": 324, "label": "unlabeled"}, {"type": "text", "expr": "55  $\\pm$  0", "word_idx": 24744, "sentence_idx": 325, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 24756, "sentence_idx": 326, "label": "unlabeled"}, {"type": "math", "expr": "$$\\pm$$", "word_idx": 24771, "sentence_idx": 327, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 4", "word_idx": 24774, "sentence_idx": 328, "label": "unlabeled"}, {"type": "text", "expr": "21  $\\pm$  0", "word_idx": 24791, "sentence_idx": 329, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 24803, "sentence_idx": 330, "label": "unlabeled"}, {"type": "math", "expr": "$$\\pm$$", "word_idx": 24818, "sentence_idx": 331, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:  Subjective 5-scale mean opinion scores of speech samples from LSTM-RNN-based statistical parametric, HMM-driven unit selection concatenative, and proposed WaveNet-based speech synthesizers, 8-bit  $\\mu$ -law encoded natural speech, and 16-bit linear pulse-code modulation (PCM) natural speech", "word_idx": 24821, "sentence_idx": 332, "label": "unlabeled"}, {"type": "text", "expr": " WaveNet improved the previous state of the art significantly, reducing the gap between natural speech and best previous model by more than 50%", "word_idx": 25123, "sentence_idx": 333, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:", "word_idx": 25266, "sentence_idx": 334, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mu$$", "word_idx": 25274, "sentence_idx": 335, "label": "unlabeled"}, {"type": "text", "expr": "3  Music", "word_idx": 25277, "sentence_idx": 336, "label": "unlabeled"}, {"type": "text", "expr": "For out third set of experiments we trained WaveNets to model two music datasets: Although it is difficult to quantitatively evaluate these models, a subjective evaluation is possible by listening to the samples they produce", "word_idx": 25285, "sentence_idx": 337, "label": "unlabeled"}, {"type": "text", "expr": "\nWe found that enlarging the receptive field was crucial to obtain samples that sounded musical", "word_idx": 25509, "sentence_idx": 338, "label": "unlabeled"}, {"type": "text", "expr": " Even with a receptive field of several seconds, the models did not enforce long-range consistency which resulted in second-to-second variations in genre, instrumentation, volume and sound quality", "word_idx": 25604, "sentence_idx": 339, "label": "unlabeled"}, {"type": "text", "expr": " Nevertheless, the samples were often harmonic and aesthetically pleasing, even when produced by unconditional models", "word_idx": 25800, "sentence_idx": 340, "label": "unlabeled"}, {"type": "text", "expr": "For out third set of experiments we trained WaveNets to model two music datasets:", "word_idx": 25917, "sentence_idx": 341, "label": "unlabeled"}, {"type": "text", "expr": "the MagnaTagATune dataset  , which consists of about 200 hours of music audio", "word_idx": 25998, "sentence_idx": 342, "label": "unlabeled"}, {"type": "text", "expr": " Each 29-second clip is annotated with tags from a set of 188, which describe the genre, instrumentation, tempo, volume and mood of the music", "word_idx": 26075, "sentence_idx": 343, "label": "unlabeled"}, {"type": "text", "expr": "the YouTube piano dataset, which consists of about 60 hours of solo piano music obtained from YouTube videos", "word_idx": 26216, "sentence_idx": 344, "label": "unlabeled"}, {"type": "text", "expr": " Because it is constrained to a single instrument, it is considerably easier to model", "word_idx": 26324, "sentence_idx": 345, "label": "unlabeled"}, {"type": "text", "expr": "the YouTube piano dataset, which consists of about 60 hours of solo piano music obtained from YouTube videos", "word_idx": 26409, "sentence_idx": 346, "label": "unlabeled"}, {"type": "text", "expr": " Because it is constrained to a single instrument, it is considerably easier to model", "word_idx": 26517, "sentence_idx": 347, "label": "unlabeled"}, {"type": "text", "expr": "Although it is difficult to quantitatively evaluate these models, a subjective evaluation is possible by listening to the samples they produce", "word_idx": 26602, "sentence_idx": 348, "label": "unlabeled"}, {"type": "text", "expr": "\nWe found that enlarging the receptive field was crucial to obtain samples that sounded musical", "word_idx": 26744, "sentence_idx": 349, "label": "unlabeled"}, {"type": "text", "expr": " Even with a receptive field of several seconds, the models did not enforce long-range consistency which resulted in second-to-second variations in genre, instrumentation, volume and sound quality", "word_idx": 26839, "sentence_idx": 350, "label": "unlabeled"}, {"type": "text", "expr": " Nevertheless, the samples were often harmonic and aesthetically pleasing, even when produced by unconditional models", "word_idx": 27035, "sentence_idx": 351, "label": "unlabeled"}, {"type": "text", "expr": "Of particular interest are conditional music models, which can generate music given a set of tags specifying e", "word_idx": 27152, "sentence_idx": 352, "label": "unlabeled"}, {"type": "text", "expr": " genre or instruments", "word_idx": 27262, "sentence_idx": 353, "label": "unlabeled"}, {"type": "text", "expr": " Similarly to conditional speech models, we insert biases that depend on a binary vector representation of the tags associated with each training clip", "word_idx": 27283, "sentence_idx": 354, "label": "unlabeled"}, {"type": "text", "expr": " This makes it possible to control various aspects of the output of the model when sampling, by feeding in a binary vector that encodes the desired properties of the samples", "word_idx": 27433, "sentence_idx": 355, "label": "unlabeled"}, {"type": "text", "expr": " We have trained such models on the MagnaTagATune dataset; although the tag data bundled with the dataset was relatively noisy and had many omissions, after cleaning it up by merging similar tags and removing those with too few associated clips, we found this approach to work reasonably well", "word_idx": 27606, "sentence_idx": 356, "label": "unlabeled"}, {"type": "text", "expr": "Of particular interest are conditional music models, which can generate music given a set of tags specifying e", "word_idx": 27898, "sentence_idx": 357, "label": "unlabeled"}, {"type": "text", "expr": " genre or instruments", "word_idx": 28008, "sentence_idx": 358, "label": "unlabeled"}, {"type": "text", "expr": " Similarly to conditional speech models, we insert biases that depend on a binary vector representation of the tags associated with each training clip", "word_idx": 28029, "sentence_idx": 359, "label": "unlabeled"}, {"type": "text", "expr": " This makes it possible to control various aspects of the output of the model when sampling, by feeding in a binary vector that encodes the desired properties of the samples", "word_idx": 28179, "sentence_idx": 360, "label": "unlabeled"}, {"type": "text", "expr": " We have trained such models on the MagnaTagATune dataset; although the tag data bundled with the dataset was relatively noisy and had many omissions, after cleaning it up by merging similar tags and removing those with too few associated clips, we found this approach to work reasonably well", "word_idx": 28352, "sentence_idx": 361, "label": "unlabeled"}, {"type": "text", "expr": "4  Speech Recognition", "word_idx": 28644, "sentence_idx": 362, "label": "unlabeled"}, {"type": "text", "expr": "Although WaveNet was designed as a generative model, it can straightforwardly be adapted to discriminative audio tasks such as speech recognition", "word_idx": 28665, "sentence_idx": 363, "label": "unlabeled"}, {"type": "text", "expr": "Although WaveNet was designed as a generative model, it can straightforwardly be adapted to discriminative audio tasks such as speech recognition", "word_idx": 28810, "sentence_idx": 364, "label": "unlabeled"}, {"type": "text", "expr": "Traditionally, speech recognition research has largely focused on using log mel-filterbank energies or mel-frequency cepstral coefficients (MFCCs), but has been moving to raw audio recently  ", "word_idx": 28955, "sentence_idx": 365, "label": "unlabeled"}, {"type": "text", "expr": " Recurrent neural networks such as LSTM-RNNs   have been a key component in these new speech classification pipelines, because they allow for building models with long range contexts", "word_idx": 29146, "sentence_idx": 366, "label": "unlabeled"}, {"type": "text", "expr": " With WaveNets we have shown that layers of dilated convolutions allow the receptive field to grow longer in a much cheaper way than using LSTM units", "word_idx": 29328, "sentence_idx": 367, "label": "unlabeled"}, {"type": "text", "expr": "As a last experiment we looked at speech recognition with WaveNets on the TIMIT   dataset", "word_idx": 29477, "sentence_idx": 368, "label": "unlabeled"}, {"type": "text", "expr": " For this task we added a mean-pooling layer after the dilated convolutions that aggregated the activations to coarser frames spanning 10 milliseconds (160 $\\times$  downsampling)", "word_idx": 29566, "sentence_idx": 369, "label": "unlabeled"}, {"type": "text", "expr": " The pooling layer was followed by a few non-causal convolutions", "word_idx": 29745, "sentence_idx": 370, "label": "unlabeled"}, {"type": "text", "expr": " We trained WaveNet with two loss terms, one to predict the next sample and one to classify the frame, the model generalized better than with a single loss and achieved  $188$  PER on the test set, which is to our knowledge the best score obtained from a model trained directly on raw audio on TIMIT", "word_idx": 29809, "sentence_idx": 371, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 30108, "sentence_idx": 372, "label": "unlabeled"}, {"type": "math", "expr": "$$18.8$$", "word_idx": 30114, "sentence_idx": 373, "label": "unlabeled"}, {"type": "text", "expr": "4  Conclusion", "word_idx": 30118, "sentence_idx": 374, "label": "unlabeled"}, {"type": "text", "expr": "This paper has presented WaveNet, a deep generative model of audio data that operates directly at the waveform level", "word_idx": 30131, "sentence_idx": 375, "label": "unlabeled"}, {"type": "text", "expr": " WaveNets are autoregressive and combine causal filters with dilated convolutions to allow their receptive fields to grow exponentially with depth, which is important to model the long-range temporal dependencies in audio signals", "word_idx": 30247, "sentence_idx": 376, "label": "unlabeled"}, {"type": "text", "expr": " We have shown how WaveNets can be conditioned on other inputs in a global (e", "word_idx": 30476, "sentence_idx": 377, "label": "unlabeled"}, {"type": "text", "expr": " speaker identity) or local way (e", "word_idx": 30553, "sentence_idx": 378, "label": "unlabeled"}, {"type": "text", "expr": " linguistic features)", "word_idx": 30587, "sentence_idx": 379, "label": "unlabeled"}, {"type": "text", "expr": " When applied to TTS, WaveNets produced samples that outperform the current best TTS systems in subjective naturalness", "word_idx": 30608, "sentence_idx": 380, "label": "unlabeled"}, {"type": "text", "expr": " Finally, WaveNets showed very promising results when applied to music audio modeling and speech recognition", "word_idx": 30726, "sentence_idx": 381, "label": "unlabeled"}, {"type": "text", "expr": "This paper has presented WaveNet, a deep generative model of audio data that operates directly at the waveform level", "word_idx": 30834, "sentence_idx": 382, "label": "unlabeled"}, {"type": "text", "expr": " WaveNets are autoregressive and combine causal filters with dilated convolutions to allow their receptive fields to grow exponentially with depth, which is important to model the long-range temporal dependencies in audio signals", "word_idx": 30950, "sentence_idx": 383, "label": "unlabeled"}, {"type": "text", "expr": " We have shown how WaveNets can be conditioned on other inputs in a global (e", "word_idx": 31179, "sentence_idx": 384, "label": "unlabeled"}, {"type": "text", "expr": " speaker identity) or local way (e", "word_idx": 31256, "sentence_idx": 385, "label": "unlabeled"}, {"type": "text", "expr": " linguistic features)", "word_idx": 31290, "sentence_idx": 386, "label": "unlabeled"}, {"type": "text", "expr": " When applied to TTS, WaveNets produced samples that outperform the current best TTS systems in subjective naturalness", "word_idx": 31311, "sentence_idx": 387, "label": "unlabeled"}, {"type": "text", "expr": " Finally, WaveNets showed very promising results when applied to music audio modeling and speech recognition", "word_idx": 31429, "sentence_idx": 388, "label": "unlabeled"}, {"type": "text", "expr": "Acknowledgements", "word_idx": 31537, "sentence_idx": 389, "label": "unlabeled"}, {"type": "text", "expr": "The authors would like to thank Lasse Espeholt, Jeffrey De Fauw and Grzegorz Swirszcz for their inputs, Adam Cain, Max Cant and Adrian Bolton for their help with artwork, Helen King, Steven Gaffney and Steve Crossan for helping to manage the project, Faith Mackinder for help with preparing the blogpost, James Besley for legal support and Demis Hassabis for managing the project and his inputs", "word_idx": 31553, "sentence_idx": 390, "label": "unlabeled"}, {"type": "text", "expr": "The authors would like to thank Lasse Espeholt, Jeffrey De Fauw and Grzegorz Swirszcz for their inputs, Adam Cain, Max Cant and Adrian Bolton for their help with artwork, Helen King, Steven Gaffney and Steve Crossan for helping to manage the project, Faith Mackinder for help with preparing the blogpost, James Besley for legal support and Demis Hassabis for managing the project and his inputs", "word_idx": 31947, "sentence_idx": 391, "label": "unlabeled"}, {"type": "text", "expr": "References", "word_idx": 32341, "sentence_idx": 392, "label": "unlabeled"}, {"type": "text", "expr": "Agiomyrgiannakis (2015) \nAgiomyrgiannakis, Yannis", "word_idx": 32351, "sentence_idx": 393, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Vocaine the vocoder and applications is speech synthesis", "word_idx": 32400, "sentence_idx": 394, "label": "unlabeled"}, {"type": "text", "expr": "Agiomyrgiannakis (2015)", "word_idx": 32459, "sentence_idx": 395, "label": "unlabeled"}, {"type": "text", "expr": "Agiomyrgiannakis, Yannis", "word_idx": 32482, "sentence_idx": 396, "label": "unlabeled"}, {"type": "text", "expr": "Vocaine the vocoder and applications is speech synthesis", "word_idx": 32506, "sentence_idx": 397, "label": "unlabeled"}, {"type": "text", "expr": "In  ICASSP , pp", "word_idx": 32562, "sentence_idx": 398, "label": "unlabeled"}, {"type": "text", "expr": "\u00a04230\u20134234, 2015", "word_idx": 32577, "sentence_idx": 399, "label": "unlabeled"}, {"type": "text", "expr": "ICASSP", "word_idx": 32593, "sentence_idx": 400, "label": "unlabeled"}, {"type": "text", "expr": "Bishop (1994) \nBishop, Christopher\u00a0M", "word_idx": 32599, "sentence_idx": 401, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Mixture density networks", "word_idx": 32635, "sentence_idx": 402, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Technical Report NCRG/94/004, Neural Computing Research Group, Aston\nUniversity, 1994", "word_idx": 32662, "sentence_idx": 403, "label": "unlabeled"}, {"type": "text", "expr": "Bishop (1994)", "word_idx": 32750, "sentence_idx": 404, "label": "unlabeled"}, {"type": "text", "expr": "Bishop, Christopher\u00a0M", "word_idx": 32763, "sentence_idx": 405, "label": "unlabeled"}, {"type": "text", "expr": "Mixture density networks", "word_idx": 32784, "sentence_idx": 406, "label": "unlabeled"}, {"type": "text", "expr": "Technical Report NCRG/94/004, Neural Computing Research Group, Aston\nUniversity, 1994", "word_idx": 32808, "sentence_idx": 407, "label": "unlabeled"}, {"type": "text", "expr": "Chen et\u00a0al", "word_idx": 32893, "sentence_idx": 408, "label": "unlabeled"}, {"type": "text", "expr": " (2015) \nChen, Liang-Chieh, Papandreou, George, Kokkinos, Iasonas, Murphy, Kevin, and\nYuille, Alan\u00a0L", "word_idx": 32903, "sentence_idx": 409, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Semantic image segmentation with deep convolutional nets and fully\nconnected CRFs", "word_idx": 33003, "sentence_idx": 410, "label": "unlabeled"}, {"type": "text", "expr": "Chen et\u00a0al", "word_idx": 33087, "sentence_idx": 411, "label": "unlabeled"}, {"type": "text", "expr": " (2015)", "word_idx": 33097, "sentence_idx": 412, "label": "unlabeled"}, {"type": "text", "expr": "Chen, Liang-Chieh, Papandreou, George, Kokkinos, Iasonas, Murphy, Kevin, and\nYuille, Alan\u00a0L", "word_idx": 33104, "sentence_idx": 413, "label": "unlabeled"}, {"type": "text", "expr": "Semantic image segmentation with deep convolutional nets and fully\nconnected CRFs", "word_idx": 33195, "sentence_idx": 414, "label": "unlabeled"}, {"type": "text", "expr": "In  ICLR , 2015", "word_idx": 33276, "sentence_idx": 415, "label": "unlabeled"}, {"type": "text", "expr": "URL  http://arxiv", "word_idx": 33291, "sentence_idx": 416, "label": "unlabeled"}, {"type": "text", "expr": "org/abs/1412", "word_idx": 33308, "sentence_idx": 417, "label": "unlabeled"}, {"type": "text", "expr": "7062 ", "word_idx": 33320, "sentence_idx": 418, "label": "unlabeled"}, {"type": "text", "expr": "http://arxiv", "word_idx": 33325, "sentence_idx": 419, "label": "unlabeled"}, {"type": "text", "expr": "org/abs/1412", "word_idx": 33337, "sentence_idx": 420, "label": "unlabeled"}, {"type": "text", "expr": "Chiba & Kajiyama (1942) \nChiba, Tsutomu and Kajiyama, Masato", "word_idx": 33349, "sentence_idx": 421, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Tokyo-Kaiseikan, 1942", "word_idx": 33409, "sentence_idx": 422, "label": "unlabeled"}, {"type": "text", "expr": "Chiba & Kajiyama (1942)", "word_idx": 33433, "sentence_idx": 423, "label": "unlabeled"}, {"type": "text", "expr": "Chiba, Tsutomu and Kajiyama, Masato", "word_idx": 33456, "sentence_idx": 424, "label": "unlabeled"}, {"type": "text", "expr": "The Vowel: Its Nature and Structure ", "word_idx": 33491, "sentence_idx": 425, "label": "unlabeled"}, {"type": "text", "expr": "The Vowel: Its Nature and Structure", "word_idx": 33527, "sentence_idx": 426, "label": "unlabeled"}, {"type": "text", "expr": "Tokyo-Kaiseikan, 1942", "word_idx": 33562, "sentence_idx": 427, "label": "unlabeled"}, {"type": "text", "expr": "Dudley (1939) \nDudley, Homer", "word_idx": 33583, "sentence_idx": 428, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Remaking speech", "word_idx": 33611, "sentence_idx": 429, "label": "unlabeled"}, {"type": "text", "expr": "Dudley (1939)", "word_idx": 33629, "sentence_idx": 430, "label": "unlabeled"}, {"type": "text", "expr": "Dudley, Homer", "word_idx": 33642, "sentence_idx": 431, "label": "unlabeled"}, {"type": "text", "expr": "Remaking speech", "word_idx": 33655, "sentence_idx": 432, "label": "unlabeled"}, {"type": "text", "expr": "The Journal of the Acoustical Society of America , 11(2):169\u2013177, 1939", "word_idx": 33670, "sentence_idx": 433, "label": "unlabeled"}, {"type": "text", "expr": "The Journal of the Acoustical Society of America", "word_idx": 33740, "sentence_idx": 434, "label": "unlabeled"}, {"type": "text", "expr": "Dutilleux (1989) \nDutilleux, Pierre", "word_idx": 33788, "sentence_idx": 435, "label": "unlabeled"}, {"type": "text", "expr": "\n\n An implementation of the \u201calgorithme \u00e0 trous\u201d to compute the\nwavelet transform", "word_idx": 33823, "sentence_idx": 436, "label": "unlabeled"}, {"type": "text", "expr": "Dutilleux (1989)", "word_idx": 33904, "sentence_idx": 437, "label": "unlabeled"}, {"type": "text", "expr": "Dutilleux, Pierre", "word_idx": 33920, "sentence_idx": 438, "label": "unlabeled"}, {"type": "text", "expr": "An implementation of the \u201calgorithme \u00e0 trous\u201d to compute the\nwavelet transform", "word_idx": 33937, "sentence_idx": 439, "label": "unlabeled"}, {"type": "text", "expr": "In Combes, Jean-Michel, Grossmann, Alexander, and Tchamitchian,\nPhilippe (eds", "word_idx": 34015, "sentence_idx": 440, "label": "unlabeled"}, {"type": "text", "expr": "),  Wavelets: Time-Frequency Methods and Phase Space ,\npp", "word_idx": 34092, "sentence_idx": 441, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0298\u2013304", "word_idx": 34149, "sentence_idx": 442, "label": "unlabeled"}, {"type": "text", "expr": " Springer Berlin Heidelberg, 1989", "word_idx": 34157, "sentence_idx": 443, "label": "unlabeled"}, {"type": "text", "expr": "Wavelets: Time-Frequency Methods and Phase Space", "word_idx": 34190, "sentence_idx": 444, "label": "unlabeled"}, {"type": "text", "expr": "Fan et\u00a0al", "word_idx": 34238, "sentence_idx": 445, "label": "unlabeled"}, {"type": "text", "expr": " (2014) \nFan, Yuchen, Qian, Yao, and Xie, Feng-Long, Soong Frank\u00a0K", "word_idx": 34247, "sentence_idx": 446, "label": "unlabeled"}, {"type": "text", "expr": "\n\n TTS synthesis with bidirectional LSTM based recurrent neural\nnetworks", "word_idx": 34313, "sentence_idx": 447, "label": "unlabeled"}, {"type": "text", "expr": "Fan et\u00a0al", "word_idx": 34385, "sentence_idx": 448, "label": "unlabeled"}, {"type": "text", "expr": " (2014)", "word_idx": 34394, "sentence_idx": 449, "label": "unlabeled"}, {"type": "text", "expr": "Fan, Yuchen, Qian, Yao, and Xie, Feng-Long, Soong Frank\u00a0K", "word_idx": 34401, "sentence_idx": 450, "label": "unlabeled"}, {"type": "text", "expr": "TTS synthesis with bidirectional LSTM based recurrent neural\nnetworks", "word_idx": 34458, "sentence_idx": 451, "label": "unlabeled"}, {"type": "text", "expr": "In  Interspeech , pp", "word_idx": 34527, "sentence_idx": 452, "label": "unlabeled"}, {"type": "text", "expr": "\u00a01964\u20131968, 2014", "word_idx": 34547, "sentence_idx": 453, "label": "unlabeled"}, {"type": "text", "expr": "Interspeech", "word_idx": 34563, "sentence_idx": 454, "label": "unlabeled"}, {"type": "text", "expr": "Fant (1970) \nFant, Gunnar", "word_idx": 34574, "sentence_idx": 455, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Mouton De Gruyter, 1970", "word_idx": 34599, "sentence_idx": 456, "label": "unlabeled"}, {"type": "text", "expr": "Fant (1970)", "word_idx": 34625, "sentence_idx": 457, "label": "unlabeled"}, {"type": "text", "expr": "Fant, Gunnar", "word_idx": 34636, "sentence_idx": 458, "label": "unlabeled"}, {"type": "text", "expr": "Acoustic Theory of Speech Production ", "word_idx": 34648, "sentence_idx": 459, "label": "unlabeled"}, {"type": "text", "expr": "Acoustic Theory of Speech Production", "word_idx": 34685, "sentence_idx": 460, "label": "unlabeled"}, {"type": "text", "expr": "Mouton De Gruyter, 1970", "word_idx": 34721, "sentence_idx": 461, "label": "unlabeled"}, {"type": "text", "expr": "Garofolo et\u00a0al", "word_idx": 34744, "sentence_idx": 462, "label": "unlabeled"}, {"type": "text", "expr": " (1993) \nGarofolo, John\u00a0S", "word_idx": 34758, "sentence_idx": 463, "label": "unlabeled"}, {"type": "text", "expr": ", Lamel, Lori\u00a0F", "word_idx": 34783, "sentence_idx": 464, "label": "unlabeled"}, {"type": "text", "expr": ", Fisher, William\u00a0M", "word_idx": 34798, "sentence_idx": 465, "label": "unlabeled"}, {"type": "text", "expr": ", Fiscus, Jonathon\u00a0G", "word_idx": 34817, "sentence_idx": 466, "label": "unlabeled"}, {"type": "text", "expr": ", and\nPallett, David\u00a0S", "word_idx": 34837, "sentence_idx": 467, "label": "unlabeled"}, {"type": "text", "expr": "\n\n DARPA TIMIT acoustic-phonetic continuous speech corpus CD-ROM", "word_idx": 34859, "sentence_idx": 468, "label": "unlabeled"}, {"type": "text", "expr": "\nNIST speech disc 1-1", "word_idx": 34923, "sentence_idx": 469, "label": "unlabeled"}, {"type": "text", "expr": "Garofolo et\u00a0al", "word_idx": 34944, "sentence_idx": 470, "label": "unlabeled"}, {"type": "text", "expr": " (1993)", "word_idx": 34958, "sentence_idx": 471, "label": "unlabeled"}, {"type": "text", "expr": "Garofolo, John\u00a0S", "word_idx": 34965, "sentence_idx": 472, "label": "unlabeled"}, {"type": "text", "expr": ", Lamel, Lori\u00a0F", "word_idx": 34981, "sentence_idx": 473, "label": "unlabeled"}, {"type": "text", "expr": ", Fisher, William\u00a0M", "word_idx": 34996, "sentence_idx": 474, "label": "unlabeled"}, {"type": "text", "expr": ", Fiscus, Jonathon\u00a0G", "word_idx": 35015, "sentence_idx": 475, "label": "unlabeled"}, {"type": "text", "expr": ", and\nPallett, David\u00a0S", "word_idx": 35035, "sentence_idx": 476, "label": "unlabeled"}, {"type": "text", "expr": "DARPA TIMIT acoustic-phonetic continuous speech corpus CD-ROM", "word_idx": 35057, "sentence_idx": 477, "label": "unlabeled"}, {"type": "text", "expr": "\nNIST speech disc 1-1", "word_idx": 35118, "sentence_idx": 478, "label": "unlabeled"}, {"type": "text", "expr": "NASA STI/Recon technical report , 93, 1993", "word_idx": 35139, "sentence_idx": 479, "label": "unlabeled"}, {"type": "text", "expr": "NASA STI/Recon technical report", "word_idx": 35181, "sentence_idx": 480, "label": "unlabeled"}, {"type": "text", "expr": "Gonzalvo et\u00a0al", "word_idx": 35212, "sentence_idx": 481, "label": "unlabeled"}, {"type": "text", "expr": " (2016) \nGonzalvo, Xavi, Tazari, Siamak, Chan, Chun-an, Becker, Markus, Gutkin,\nAlexander, and Silen, Hanna", "word_idx": 35226, "sentence_idx": 482, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Recent advances in Google real-time HMM-driven unit selection\nsynthesizer", "word_idx": 35333, "sentence_idx": 483, "label": "unlabeled"}, {"type": "text", "expr": "Gonzalvo et\u00a0al", "word_idx": 35409, "sentence_idx": 484, "label": "unlabeled"}, {"type": "text", "expr": " (2016)", "word_idx": 35423, "sentence_idx": 485, "label": "unlabeled"}, {"type": "text", "expr": "Gonzalvo, Xavi, Tazari, Siamak, Chan, Chun-an, Becker, Markus, Gutkin,\nAlexander, and Silen, Hanna", "word_idx": 35430, "sentence_idx": 486, "label": "unlabeled"}, {"type": "text", "expr": "Recent advances in Google real-time HMM-driven unit selection\nsynthesizer", "word_idx": 35528, "sentence_idx": 487, "label": "unlabeled"}, {"type": "text", "expr": "In  Interspeech , 2016", "word_idx": 35601, "sentence_idx": 488, "label": "unlabeled"}, {"type": "text", "expr": "Interspeech", "word_idx": 35623, "sentence_idx": 489, "label": "unlabeled"}, {"type": "text", "expr": "URL  http://research", "word_idx": 35634, "sentence_idx": 490, "label": "unlabeled"}, {"type": "text", "expr": "google", "word_idx": 35654, "sentence_idx": 491, "label": "unlabeled"}, {"type": "text", "expr": "com/pubs/pub45564", "word_idx": 35660, "sentence_idx": 492, "label": "unlabeled"}, {"type": "text", "expr": "html ", "word_idx": 35677, "sentence_idx": 493, "label": "unlabeled"}, {"type": "text", "expr": "http://research", "word_idx": 35682, "sentence_idx": 494, "label": "unlabeled"}, {"type": "text", "expr": "google", "word_idx": 35697, "sentence_idx": 495, "label": "unlabeled"}, {"type": "text", "expr": "com/pubs/pub45564", "word_idx": 35703, "sentence_idx": 496, "label": "unlabeled"}, {"type": "text", "expr": "He et\u00a0al", "word_idx": 35720, "sentence_idx": 497, "label": "unlabeled"}, {"type": "text", "expr": " (2015) \nHe, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian", "word_idx": 35728, "sentence_idx": 498, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Deep residual learning for image recognition", "word_idx": 35794, "sentence_idx": 499, "label": "unlabeled"}, {"type": "text", "expr": "He et\u00a0al", "word_idx": 35841, "sentence_idx": 500, "label": "unlabeled"}, {"type": "text", "expr": " (2015)", "word_idx": 35849, "sentence_idx": 501, "label": "unlabeled"}, {"type": "text", "expr": "He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian", "word_idx": 35856, "sentence_idx": 502, "label": "unlabeled"}, {"type": "text", "expr": "Deep residual learning for image recognition", "word_idx": 35913, "sentence_idx": 503, "label": "unlabeled"}, {"type": "text", "expr": "CoRR , abs/1512", "word_idx": 35957, "sentence_idx": 504, "label": "unlabeled"}, {"type": "text", "expr": "03385, 2015", "word_idx": 35972, "sentence_idx": 505, "label": "unlabeled"}, {"type": "text", "expr": "Hochreiter & Schmidhuber (1997) \nHochreiter, S", "word_idx": 35983, "sentence_idx": 506, "label": "unlabeled"}, {"type": "text", "expr": " and Schmidhuber, J", "word_idx": 36029, "sentence_idx": 507, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Long short-term memory", "word_idx": 36048, "sentence_idx": 508, "label": "unlabeled"}, {"type": "text", "expr": "Hochreiter & Schmidhuber (1997)", "word_idx": 36073, "sentence_idx": 509, "label": "unlabeled"}, {"type": "text", "expr": "Hochreiter, S", "word_idx": 36104, "sentence_idx": 510, "label": "unlabeled"}, {"type": "text", "expr": " and Schmidhuber, J", "word_idx": 36117, "sentence_idx": 511, "label": "unlabeled"}, {"type": "text", "expr": "Long short-term memory", "word_idx": 36136, "sentence_idx": 512, "label": "unlabeled"}, {"type": "text", "expr": "Neural Comput", "word_idx": 36158, "sentence_idx": 513, "label": "unlabeled"}, {"type": "text", "expr": " , 9(8):1735\u20131780, 1997", "word_idx": 36171, "sentence_idx": 514, "label": "unlabeled"}, {"type": "text", "expr": "Neural Comput", "word_idx": 36194, "sentence_idx": 515, "label": "unlabeled"}, {"type": "text", "expr": "Holschneider et\u00a0al", "word_idx": 36207, "sentence_idx": 516, "label": "unlabeled"}, {"type": "text", "expr": " (1989) \nHolschneider, Matthias, Kronland-Martinet, Richard, Morlet, Jean, and\nTchamitchian, Philippe", "word_idx": 36225, "sentence_idx": 517, "label": "unlabeled"}, {"type": "text", "expr": "\n\n A real-time algorithm for signal analysis with the help of the\nwavelet transform", "word_idx": 36326, "sentence_idx": 518, "label": "unlabeled"}, {"type": "text", "expr": "Holschneider et\u00a0al", "word_idx": 36409, "sentence_idx": 519, "label": "unlabeled"}, {"type": "text", "expr": " (1989)", "word_idx": 36427, "sentence_idx": 520, "label": "unlabeled"}, {"type": "text", "expr": "Holschneider, Matthias, Kronland-Martinet, Richard, Morlet, Jean, and\nTchamitchian, Philippe", "word_idx": 36434, "sentence_idx": 521, "label": "unlabeled"}, {"type": "text", "expr": "A real-time algorithm for signal analysis with the help of the\nwavelet transform", "word_idx": 36526, "sentence_idx": 522, "label": "unlabeled"}, {"type": "text", "expr": "In Combes, Jean-Michel, Grossmann, Alexander, and Tchamitchian,\nPhilippe (eds", "word_idx": 36606, "sentence_idx": 523, "label": "unlabeled"}, {"type": "text", "expr": "),  Wavelets: Time-Frequency Methods and Phase Space ,\npp", "word_idx": 36683, "sentence_idx": 524, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0286\u2013297", "word_idx": 36740, "sentence_idx": 525, "label": "unlabeled"}, {"type": "text", "expr": " Springer Berlin Heidelberg, 1989", "word_idx": 36748, "sentence_idx": 526, "label": "unlabeled"}, {"type": "text", "expr": "Wavelets: Time-Frequency Methods and Phase Space", "word_idx": 36781, "sentence_idx": 527, "label": "unlabeled"}, {"type": "text", "expr": "Hoshen et\u00a0al", "word_idx": 36829, "sentence_idx": 528, "label": "unlabeled"}, {"type": "text", "expr": " (2015) \nHoshen, Yedid, Weiss, Ron\u00a0J", "word_idx": 36841, "sentence_idx": 529, "label": "unlabeled"}, {"type": "text", "expr": ", and Wilson, Kevin\u00a0W", "word_idx": 36877, "sentence_idx": 530, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Speech acoustic modeling from raw multichannel waveforms", "word_idx": 36898, "sentence_idx": 531, "label": "unlabeled"}, {"type": "text", "expr": "Hoshen et\u00a0al", "word_idx": 36957, "sentence_idx": 532, "label": "unlabeled"}, {"type": "text", "expr": " (2015)", "word_idx": 36969, "sentence_idx": 533, "label": "unlabeled"}, {"type": "text", "expr": "Hoshen, Yedid, Weiss, Ron\u00a0J", "word_idx": 36976, "sentence_idx": 534, "label": "unlabeled"}, {"type": "text", "expr": ", and Wilson, Kevin\u00a0W", "word_idx": 37003, "sentence_idx": 535, "label": "unlabeled"}, {"type": "text", "expr": "Speech acoustic modeling from raw multichannel waveforms", "word_idx": 37024, "sentence_idx": 536, "label": "unlabeled"}, {"type": "text", "expr": "In  ICASSP , pp", "word_idx": 37080, "sentence_idx": 537, "label": "unlabeled"}, {"type": "text", "expr": "\u00a04624\u20134628", "word_idx": 37095, "sentence_idx": 538, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 2015", "word_idx": 37105, "sentence_idx": 539, "label": "unlabeled"}, {"type": "text", "expr": "ICASSP", "word_idx": 37116, "sentence_idx": 540, "label": "unlabeled"}, {"type": "text", "expr": "Hunt & Black (1996) \nHunt, Andrew\u00a0J", "word_idx": 37122, "sentence_idx": 541, "label": "unlabeled"}, {"type": "text", "expr": " and Black, Alan\u00a0W", "word_idx": 37157, "sentence_idx": 542, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Unit selection in a concatenative speech synthesis system using a\nlarge speech database", "word_idx": 37175, "sentence_idx": 543, "label": "unlabeled"}, {"type": "text", "expr": "Hunt & Black (1996)", "word_idx": 37265, "sentence_idx": 544, "label": "unlabeled"}, {"type": "text", "expr": "Hunt, Andrew\u00a0J", "word_idx": 37284, "sentence_idx": 545, "label": "unlabeled"}, {"type": "text", "expr": " and Black, Alan\u00a0W", "word_idx": 37298, "sentence_idx": 546, "label": "unlabeled"}, {"type": "text", "expr": "Unit selection in a concatenative speech synthesis system using a\nlarge speech database", "word_idx": 37316, "sentence_idx": 547, "label": "unlabeled"}, {"type": "text", "expr": "In  ICASSP , pp", "word_idx": 37403, "sentence_idx": 548, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0373\u2013376, 1996", "word_idx": 37418, "sentence_idx": 549, "label": "unlabeled"}, {"type": "text", "expr": "ICASSP", "word_idx": 37432, "sentence_idx": 550, "label": "unlabeled"}, {"type": "text", "expr": "Imai & Furuichi (1988) \nImai, Satoshi and Furuichi, Chieko", "word_idx": 37438, "sentence_idx": 551, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Unbiased estimation of log spectrum", "word_idx": 37496, "sentence_idx": 552, "label": "unlabeled"}, {"type": "text", "expr": "Imai & Furuichi (1988)", "word_idx": 37534, "sentence_idx": 553, "label": "unlabeled"}, {"type": "text", "expr": "Imai, Satoshi and Furuichi, Chieko", "word_idx": 37556, "sentence_idx": 554, "label": "unlabeled"}, {"type": "text", "expr": "Unbiased estimation of log spectrum", "word_idx": 37590, "sentence_idx": 555, "label": "unlabeled"}, {"type": "text", "expr": "In  EURASIP , pp", "word_idx": 37625, "sentence_idx": 556, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0203\u2013206, 1988", "word_idx": 37641, "sentence_idx": 557, "label": "unlabeled"}, {"type": "text", "expr": "EURASIP", "word_idx": 37655, "sentence_idx": 558, "label": "unlabeled"}, {"type": "text", "expr": "Itakura (1975) \nItakura, Fumitada", "word_idx": 37662, "sentence_idx": 559, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Line spectrum representation of linear predictor coefficients of\nspeech signals", "word_idx": 37695, "sentence_idx": 560, "label": "unlabeled"}, {"type": "text", "expr": "Itakura (1975)", "word_idx": 37777, "sentence_idx": 561, "label": "unlabeled"}, {"type": "text", "expr": "Itakura, Fumitada", "word_idx": 37791, "sentence_idx": 562, "label": "unlabeled"}, {"type": "text", "expr": "Line spectrum representation of linear predictor coefficients of\nspeech signals", "word_idx": 37808, "sentence_idx": 563, "label": "unlabeled"}, {"type": "text", "expr": "The Journal of the Acoust", "word_idx": 37887, "sentence_idx": 564, "label": "unlabeled"}, {"type": "text", "expr": " Society of America , 57(S1):S35\u2013S35, 1975", "word_idx": 37912, "sentence_idx": 565, "label": "unlabeled"}, {"type": "text", "expr": "The Journal of the Acoust", "word_idx": 37954, "sentence_idx": 566, "label": "unlabeled"}, {"type": "text", "expr": " Society of America", "word_idx": 37979, "sentence_idx": 567, "label": "unlabeled"}, {"type": "text", "expr": "Itakura & Saito (1970) \nItakura, Fumitada and Saito, Shuzo", "word_idx": 37998, "sentence_idx": 568, "label": "unlabeled"}, {"type": "text", "expr": "\n\n A statistical method for estimation of speech spectral density and\nformant frequencies", "word_idx": 38056, "sentence_idx": 569, "label": "unlabeled"}, {"type": "text", "expr": "Itakura & Saito (1970)", "word_idx": 38145, "sentence_idx": 570, "label": "unlabeled"}, {"type": "text", "expr": "Itakura, Fumitada and Saito, Shuzo", "word_idx": 38167, "sentence_idx": 571, "label": "unlabeled"}, {"type": "text", "expr": "A statistical method for estimation of speech spectral density and\nformant frequencies", "word_idx": 38201, "sentence_idx": 572, "label": "unlabeled"}, {"type": "text", "expr": "Trans", "word_idx": 38287, "sentence_idx": 573, "label": "unlabeled"}, {"type": "text", "expr": " IEICE , J53\u00e2\u0080\u0093A:35\u201342, 1970", "word_idx": 38292, "sentence_idx": 574, "label": "unlabeled"}, {"type": "text", "expr": "Trans", "word_idx": 38320, "sentence_idx": 575, "label": "unlabeled"}, {"type": "text", "expr": " IEICE", "word_idx": 38325, "sentence_idx": 576, "label": "unlabeled"}, {"type": "text", "expr": "ITU-T (1988) \nITU-T", "word_idx": 38331, "sentence_idx": 577, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Recommendation G", "word_idx": 38350, "sentence_idx": 578, "label": "unlabeled"}, {"type": "text", "expr": "ITU-T (1988)", "word_idx": 38369, "sentence_idx": 579, "label": "unlabeled"}, {"type": "text", "expr": "ITU-T", "word_idx": 38381, "sentence_idx": 580, "label": "unlabeled"}, {"type": "text", "expr": "Recommendation G", "word_idx": 38386, "sentence_idx": 581, "label": "unlabeled"}, {"type": "text", "expr": "Pulse Code Modulation (PCM) of voice frequencies , 1988", "word_idx": 38402, "sentence_idx": 582, "label": "unlabeled"}, {"type": "text", "expr": "Pulse Code Modulation (PCM) of voice frequencies", "word_idx": 38457, "sentence_idx": 583, "label": "unlabeled"}, {"type": "text", "expr": "J\u00f3zefowicz et\u00a0al", "word_idx": 38505, "sentence_idx": 584, "label": "unlabeled"}, {"type": "text", "expr": " (2016) \nJ\u00f3zefowicz, Rafal, Vinyals, Oriol, Schuster, Mike, Shazeer, Noam, and Wu,\nYonghui", "word_idx": 38521, "sentence_idx": 585, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Exploring the limits of language modeling", "word_idx": 38611, "sentence_idx": 586, "label": "unlabeled"}, {"type": "text", "expr": "J\u00f3zefowicz et\u00a0al", "word_idx": 38655, "sentence_idx": 587, "label": "unlabeled"}, {"type": "text", "expr": " (2016)", "word_idx": 38671, "sentence_idx": 588, "label": "unlabeled"}, {"type": "text", "expr": "J\u00f3zefowicz, Rafal, Vinyals, Oriol, Schuster, Mike, Shazeer, Noam, and Wu,\nYonghui", "word_idx": 38678, "sentence_idx": 589, "label": "unlabeled"}, {"type": "text", "expr": "Exploring the limits of language modeling", "word_idx": 38759, "sentence_idx": 590, "label": "unlabeled"}, {"type": "text", "expr": "CoRR , abs/1602", "word_idx": 38800, "sentence_idx": 591, "label": "unlabeled"}, {"type": "text", "expr": "02410, 2016", "word_idx": 38815, "sentence_idx": 592, "label": "unlabeled"}, {"type": "text", "expr": "URL  http://arxiv", "word_idx": 38826, "sentence_idx": 593, "label": "unlabeled"}, {"type": "text", "expr": "org/abs/1602", "word_idx": 38843, "sentence_idx": 594, "label": "unlabeled"}, {"type": "text", "expr": "02410 ", "word_idx": 38855, "sentence_idx": 595, "label": "unlabeled"}, {"type": "text", "expr": "http://arxiv", "word_idx": 38861, "sentence_idx": 596, "label": "unlabeled"}, {"type": "text", "expr": "org/abs/1602", "word_idx": 38873, "sentence_idx": 597, "label": "unlabeled"}, {"type": "text", "expr": "02410", "word_idx": 38885, "sentence_idx": 598, "label": "unlabeled"}, {"type": "text", "expr": "Juang & Rabiner (1985) \nJuang, Biing-Hwang and Rabiner, Lawrence", "word_idx": 38890, "sentence_idx": 599, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Mixture autoregressive hidden Markov models for speech signals", "word_idx": 38954, "sentence_idx": 600, "label": "unlabeled"}, {"type": "text", "expr": "Juang & Rabiner (1985)", "word_idx": 39019, "sentence_idx": 601, "label": "unlabeled"}, {"type": "text", "expr": "Juang, Biing-Hwang and Rabiner, Lawrence", "word_idx": 39041, "sentence_idx": 602, "label": "unlabeled"}, {"type": "text", "expr": "Mixture autoregressive hidden Markov models for speech signals", "word_idx": 39081, "sentence_idx": 603, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Trans", "word_idx": 39143, "sentence_idx": 604, "label": "unlabeled"}, {"type": "text", "expr": " Acoust", "word_idx": 39153, "sentence_idx": 605, "label": "unlabeled"}, {"type": "text", "expr": " Speech Signal Process", "word_idx": 39160, "sentence_idx": 606, "label": "unlabeled"}, {"type": "text", "expr": " , pp", "word_idx": 39182, "sentence_idx": 607, "label": "unlabeled"}, {"type": "text", "expr": "\u00a01404\u20131413,\n1985", "word_idx": 39187, "sentence_idx": 608, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Trans", "word_idx": 39203, "sentence_idx": 609, "label": "unlabeled"}, {"type": "text", "expr": " Acoust", "word_idx": 39213, "sentence_idx": 610, "label": "unlabeled"}, {"type": "text", "expr": " Speech Signal Process", "word_idx": 39220, "sentence_idx": 611, "label": "unlabeled"}, {"type": "text", "expr": "Kameoka et\u00a0al", "word_idx": 39242, "sentence_idx": 612, "label": "unlabeled"}, {"type": "text", "expr": " (2010) \nKameoka, Hirokazu, Ohishi, Yasunori, Mochihashi, Daichi, and Le\u00a0Roux, Jonathan", "word_idx": 39255, "sentence_idx": 613, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Speech analysis with multi-kernel linear prediction", "word_idx": 39342, "sentence_idx": 614, "label": "unlabeled"}, {"type": "text", "expr": "\n\n (in Japanese)", "word_idx": 39396, "sentence_idx": 615, "label": "unlabeled"}, {"type": "text", "expr": "Kameoka et\u00a0al", "word_idx": 39412, "sentence_idx": 616, "label": "unlabeled"}, {"type": "text", "expr": " (2010)", "word_idx": 39425, "sentence_idx": 617, "label": "unlabeled"}, {"type": "text", "expr": "Kameoka, Hirokazu, Ohishi, Yasunori, Mochihashi, Daichi, and Le\u00a0Roux, Jonathan", "word_idx": 39432, "sentence_idx": 618, "label": "unlabeled"}, {"type": "text", "expr": "Speech analysis with multi-kernel linear prediction", "word_idx": 39510, "sentence_idx": 619, "label": "unlabeled"}, {"type": "text", "expr": "In  Spring Conference of ASJ , pp", "word_idx": 39561, "sentence_idx": 620, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0499\u2013502, 2010", "word_idx": 39594, "sentence_idx": 621, "label": "unlabeled"}, {"type": "text", "expr": "Spring Conference of ASJ", "word_idx": 39608, "sentence_idx": 622, "label": "unlabeled"}, {"type": "text", "expr": "(in Japanese)", "word_idx": 39632, "sentence_idx": 623, "label": "unlabeled"}, {"type": "text", "expr": "Karaali et\u00a0al", "word_idx": 39645, "sentence_idx": 624, "label": "unlabeled"}, {"type": "text", "expr": " (1997) \nKaraali, Orhan, Corrigan, Gerald, Gerson, Ira, and Massey, Noel", "word_idx": 39658, "sentence_idx": 625, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Text-to-speech conversion with neural networks: A recurrent TDNN\napproach", "word_idx": 39730, "sentence_idx": 626, "label": "unlabeled"}, {"type": "text", "expr": "Karaali et\u00a0al", "word_idx": 39806, "sentence_idx": 627, "label": "unlabeled"}, {"type": "text", "expr": " (1997)", "word_idx": 39819, "sentence_idx": 628, "label": "unlabeled"}, {"type": "text", "expr": "Karaali, Orhan, Corrigan, Gerald, Gerson, Ira, and Massey, Noel", "word_idx": 39826, "sentence_idx": 629, "label": "unlabeled"}, {"type": "text", "expr": "Text-to-speech conversion with neural networks: A recurrent TDNN\napproach", "word_idx": 39889, "sentence_idx": 630, "label": "unlabeled"}, {"type": "text", "expr": "In  Eurospeech , pp", "word_idx": 39962, "sentence_idx": 631, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0561\u2013564, 1997", "word_idx": 39981, "sentence_idx": 632, "label": "unlabeled"}, {"type": "text", "expr": "Eurospeech", "word_idx": 39995, "sentence_idx": 633, "label": "unlabeled"}, {"type": "text", "expr": "Kawahara et\u00a0al", "word_idx": 40005, "sentence_idx": 634, "label": "unlabeled"}, {"type": "text", "expr": " (1999) \nKawahara, Hideki, Masuda-Katsuse, Ikuyo, and de\u00a0Cheveign\u00e9, Alain", "word_idx": 40019, "sentence_idx": 635, "label": "unlabeled"}, {"type": "text", "expr": "Kawahara et\u00a0al", "word_idx": 40092, "sentence_idx": 636, "label": "unlabeled"}, {"type": "text", "expr": " (1999)", "word_idx": 40106, "sentence_idx": 637, "label": "unlabeled"}, {"type": "text", "expr": "Kawahara, Hideki, Masuda-Katsuse, Ikuyo, and de\u00a0Cheveign\u00e9, Alain", "word_idx": 40113, "sentence_idx": 638, "label": "unlabeled"}, {"type": "text", "expr": "Restructuring speech representations using a pitch-adaptive\ntime-frequency smoothing and an instantaneous-frequency-based  $f_{0}$ \nextraction: possible role of a repetitive structure in sounds", "word_idx": 40177, "sentence_idx": 639, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{0}$$", "word_idx": 40370, "sentence_idx": 640, "label": "unlabeled"}, {"type": "text", "expr": "Speech Commn", "word_idx": 40375, "sentence_idx": 641, "label": "unlabeled"}, {"type": "text", "expr": " , 27:187\u2013207, 1999", "word_idx": 40387, "sentence_idx": 642, "label": "unlabeled"}, {"type": "text", "expr": "Speech Commn", "word_idx": 40406, "sentence_idx": 643, "label": "unlabeled"}, {"type": "text", "expr": "Kawahara et\u00a0al", "word_idx": 40418, "sentence_idx": 644, "label": "unlabeled"}, {"type": "text", "expr": " (2001) \nKawahara, Hideki, Estill, Jo, and Fujimura, Osamu", "word_idx": 40432, "sentence_idx": 645, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Aperiodicity extraction and control using mixed mode excitation and\ngroup delay manipulation for a high quality speech analysis, modification and\nsynthesis system STRAIGHT", "word_idx": 40490, "sentence_idx": 646, "label": "unlabeled"}, {"type": "text", "expr": "Kawahara et\u00a0al", "word_idx": 40664, "sentence_idx": 647, "label": "unlabeled"}, {"type": "text", "expr": " (2001)", "word_idx": 40678, "sentence_idx": 648, "label": "unlabeled"}, {"type": "text", "expr": "Kawahara, Hideki, Estill, Jo, and Fujimura, Osamu", "word_idx": 40685, "sentence_idx": 649, "label": "unlabeled"}, {"type": "text", "expr": "Aperiodicity extraction and control using mixed mode excitation and\ngroup delay manipulation for a high quality speech analysis, modification and\nsynthesis system STRAIGHT", "word_idx": 40734, "sentence_idx": 650, "label": "unlabeled"}, {"type": "text", "expr": "In  MAVEBA , pp", "word_idx": 40905, "sentence_idx": 651, "label": "unlabeled"}, {"type": "text", "expr": "\u00a013\u201315, 2001", "word_idx": 40920, "sentence_idx": 652, "label": "unlabeled"}, {"type": "text", "expr": "MAVEBA", "word_idx": 40932, "sentence_idx": 653, "label": "unlabeled"}, {"type": "text", "expr": "Law & Von\u00a0Ahn (2009) \nLaw, Edith and Von\u00a0Ahn, Luis", "word_idx": 40938, "sentence_idx": 654, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Input-agreement: a new mechanism for collecting data using human\ncomputation games", "word_idx": 40988, "sentence_idx": 655, "label": "unlabeled"}, {"type": "text", "expr": "Law & Von\u00a0Ahn (2009)", "word_idx": 41073, "sentence_idx": 656, "label": "unlabeled"}, {"type": "text", "expr": "Law, Edith and Von\u00a0Ahn, Luis", "word_idx": 41093, "sentence_idx": 657, "label": "unlabeled"}, {"type": "text", "expr": "Input-agreement: a new mechanism for collecting data using human\ncomputation games", "word_idx": 41121, "sentence_idx": 658, "label": "unlabeled"}, {"type": "text", "expr": "In  Proceedings of the SIGCHI Conference on Human Factors in\nComputing Systems , pp", "word_idx": 41203, "sentence_idx": 659, "label": "unlabeled"}, {"type": "text", "expr": "\u00a01197\u20131206", "word_idx": 41286, "sentence_idx": 660, "label": "unlabeled"}, {"type": "text", "expr": " ACM, 2009", "word_idx": 41296, "sentence_idx": 661, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the SIGCHI Conference on Human Factors in\nComputing Systems", "word_idx": 41306, "sentence_idx": 662, "label": "unlabeled"}, {"type": "text", "expr": "Maia et\u00a0al", "word_idx": 41380, "sentence_idx": 663, "label": "unlabeled"}, {"type": "text", "expr": " (2010) \nMaia, Ranniery, Zen, Heiga, and Gales, Mark J", "word_idx": 41390, "sentence_idx": 664, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Statistical parametric speech synthesis with joint estimation of\nacoustic and excitation model parameters", "word_idx": 41444, "sentence_idx": 665, "label": "unlabeled"}, {"type": "text", "expr": "Maia et\u00a0al", "word_idx": 41552, "sentence_idx": 666, "label": "unlabeled"}, {"type": "text", "expr": " (2010)", "word_idx": 41562, "sentence_idx": 667, "label": "unlabeled"}, {"type": "text", "expr": "Maia, Ranniery, Zen, Heiga, and Gales, Mark J", "word_idx": 41569, "sentence_idx": 668, "label": "unlabeled"}, {"type": "text", "expr": "Statistical parametric speech synthesis with joint estimation of\nacoustic and excitation model parameters", "word_idx": 41614, "sentence_idx": 669, "label": "unlabeled"}, {"type": "text", "expr": "In  ISCA SSW7 , pp", "word_idx": 41719, "sentence_idx": 670, "label": "unlabeled"}, {"type": "text", "expr": "\u00a088\u201393, 2010", "word_idx": 41737, "sentence_idx": 671, "label": "unlabeled"}, {"type": "text", "expr": "ISCA SSW7", "word_idx": 41749, "sentence_idx": 672, "label": "unlabeled"}, {"type": "text", "expr": "Morise et\u00a0al", "word_idx": 41758, "sentence_idx": 673, "label": "unlabeled"}, {"type": "text", "expr": " (2016) \nMorise, Masanori, Yokomori, Fumiya, and Ozawa, Kenji", "word_idx": 41770, "sentence_idx": 674, "label": "unlabeled"}, {"type": "text", "expr": "\n\n WORLD: A vocoder-based high-quality speech synthesis system for\nreal-time applications", "word_idx": 41831, "sentence_idx": 675, "label": "unlabeled"}, {"type": "text", "expr": "Morise et\u00a0al", "word_idx": 41920, "sentence_idx": 676, "label": "unlabeled"}, {"type": "text", "expr": " (2016)", "word_idx": 41932, "sentence_idx": 677, "label": "unlabeled"}, {"type": "text", "expr": "Morise, Masanori, Yokomori, Fumiya, and Ozawa, Kenji", "word_idx": 41939, "sentence_idx": 678, "label": "unlabeled"}, {"type": "text", "expr": "WORLD: A vocoder-based high-quality speech synthesis system for\nreal-time applications", "word_idx": 41991, "sentence_idx": 679, "label": "unlabeled"}, {"type": "text", "expr": "IEICE Trans", "word_idx": 42077, "sentence_idx": 680, "label": "unlabeled"}, {"type": "text", "expr": " Syst", "word_idx": 42088, "sentence_idx": 681, "label": "unlabeled"}, {"type": "text", "expr": " , E99-D(7):1877\u20131884, 2016", "word_idx": 42093, "sentence_idx": 682, "label": "unlabeled"}, {"type": "text", "expr": "IEICE Trans", "word_idx": 42120, "sentence_idx": 683, "label": "unlabeled"}, {"type": "text", "expr": " Syst", "word_idx": 42131, "sentence_idx": 684, "label": "unlabeled"}, {"type": "text", "expr": "Moulines & Charpentier (1990) \nMoulines, Eric and Charpentier, Francis", "word_idx": 42136, "sentence_idx": 685, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Pitch synchronous waveform processing techniques for text-to-speech\nsynthesis using diphones", "word_idx": 42206, "sentence_idx": 686, "label": "unlabeled"}, {"type": "text", "expr": "Moulines & Charpentier (1990)", "word_idx": 42301, "sentence_idx": 687, "label": "unlabeled"}, {"type": "text", "expr": "Moulines, Eric and Charpentier, Francis", "word_idx": 42330, "sentence_idx": 688, "label": "unlabeled"}, {"type": "text", "expr": "Pitch synchronous waveform processing techniques for text-to-speech\nsynthesis using diphones", "word_idx": 42369, "sentence_idx": 689, "label": "unlabeled"}, {"type": "text", "expr": "Speech Commn", "word_idx": 42461, "sentence_idx": 690, "label": "unlabeled"}, {"type": "text", "expr": " , 9:453\u2013467, 1990", "word_idx": 42473, "sentence_idx": 691, "label": "unlabeled"}, {"type": "text", "expr": "Speech Commn", "word_idx": 42491, "sentence_idx": 692, "label": "unlabeled"}, {"type": "text", "expr": "Muthukumar & Black (2014) \nMuthukumar, P", "word_idx": 42503, "sentence_idx": 693, "label": "unlabeled"}, {"type": "text", "expr": " and Black, Alan\u00a0W", "word_idx": 42543, "sentence_idx": 694, "label": "unlabeled"}, {"type": "text", "expr": "\n\n A deep learning approach to data-driven parameterizations for\nstatistical parametric speech synthesis", "word_idx": 42561, "sentence_idx": 695, "label": "unlabeled"}, {"type": "text", "expr": "Muthukumar & Black (2014)", "word_idx": 42665, "sentence_idx": 696, "label": "unlabeled"}, {"type": "text", "expr": "Muthukumar, P", "word_idx": 42690, "sentence_idx": 697, "label": "unlabeled"}, {"type": "text", "expr": " and Black, Alan\u00a0W", "word_idx": 42703, "sentence_idx": 698, "label": "unlabeled"}, {"type": "text", "expr": "A deep learning approach to data-driven parameterizations for\nstatistical parametric speech synthesis", "word_idx": 42721, "sentence_idx": 699, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1409", "word_idx": 42822, "sentence_idx": 700, "label": "unlabeled"}, {"type": "text", "expr": "8558 , 2014", "word_idx": 42832, "sentence_idx": 701, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1409", "word_idx": 42843, "sentence_idx": 702, "label": "unlabeled"}, {"type": "text", "expr": "Nair & Hinton (2010) \nNair, Vinod and Hinton, Geoffrey\u00a0E", "word_idx": 42853, "sentence_idx": 703, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Rectified linear units improve restricted Boltzmann machines", "word_idx": 42909, "sentence_idx": 704, "label": "unlabeled"}, {"type": "text", "expr": "Nair & Hinton (2010)", "word_idx": 42972, "sentence_idx": 705, "label": "unlabeled"}, {"type": "text", "expr": "Nair, Vinod and Hinton, Geoffrey\u00a0E", "word_idx": 42992, "sentence_idx": 706, "label": "unlabeled"}, {"type": "text", "expr": "Rectified linear units improve restricted Boltzmann machines", "word_idx": 43026, "sentence_idx": 707, "label": "unlabeled"}, {"type": "text", "expr": "In  ICML , pp", "word_idx": 43086, "sentence_idx": 708, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0807\u2013814, 2010", "word_idx": 43099, "sentence_idx": 709, "label": "unlabeled"}, {"type": "text", "expr": "Nakamura et\u00a0al", "word_idx": 43113, "sentence_idx": 710, "label": "unlabeled"}, {"type": "text", "expr": " (2014) \nNakamura, Kazuhiro, Hashimoto, Kei, Nankaku, Yoshihiko, and Tokuda, Keiichi", "word_idx": 43127, "sentence_idx": 711, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Integration of spectral feature extraction and modeling for\nHMM-based speech synthesis", "word_idx": 43211, "sentence_idx": 712, "label": "unlabeled"}, {"type": "text", "expr": "Nakamura et\u00a0al", "word_idx": 43300, "sentence_idx": 713, "label": "unlabeled"}, {"type": "text", "expr": " (2014)", "word_idx": 43314, "sentence_idx": 714, "label": "unlabeled"}, {"type": "text", "expr": "Nakamura, Kazuhiro, Hashimoto, Kei, Nankaku, Yoshihiko, and Tokuda, Keiichi", "word_idx": 43321, "sentence_idx": 715, "label": "unlabeled"}, {"type": "text", "expr": "Integration of spectral feature extraction and modeling for\nHMM-based speech synthesis", "word_idx": 43396, "sentence_idx": 716, "label": "unlabeled"}, {"type": "text", "expr": "IEICE Trans", "word_idx": 43482, "sentence_idx": 717, "label": "unlabeled"}, {"type": "text", "expr": " Syst", "word_idx": 43493, "sentence_idx": 718, "label": "unlabeled"}, {"type": "text", "expr": " , E97-D(6):1438\u20131448, 2014", "word_idx": 43498, "sentence_idx": 719, "label": "unlabeled"}, {"type": "text", "expr": "IEICE Trans", "word_idx": 43525, "sentence_idx": 720, "label": "unlabeled"}, {"type": "text", "expr": " Syst", "word_idx": 43536, "sentence_idx": 721, "label": "unlabeled"}, {"type": "text", "expr": "Palaz et\u00a0al", "word_idx": 43541, "sentence_idx": 722, "label": "unlabeled"}, {"type": "text", "expr": " (2013) \nPalaz, Dimitri, Collobert, Ronan, and Magimai-Doss, Mathew", "word_idx": 43552, "sentence_idx": 723, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Estimating phoneme class conditional probabilities from raw speech\nsignal using convolutional neural networks", "word_idx": 43619, "sentence_idx": 724, "label": "unlabeled"}, {"type": "text", "expr": "Palaz et\u00a0al", "word_idx": 43731, "sentence_idx": 725, "label": "unlabeled"}, {"type": "text", "expr": " (2013)", "word_idx": 43742, "sentence_idx": 726, "label": "unlabeled"}, {"type": "text", "expr": "Palaz, Dimitri, Collobert, Ronan, and Magimai-Doss, Mathew", "word_idx": 43749, "sentence_idx": 727, "label": "unlabeled"}, {"type": "text", "expr": "Estimating phoneme class conditional probabilities from raw speech\nsignal using convolutional neural networks", "word_idx": 43807, "sentence_idx": 728, "label": "unlabeled"}, {"type": "text", "expr": "In  Interspeech , pp", "word_idx": 43916, "sentence_idx": 729, "label": "unlabeled"}, {"type": "text", "expr": "\u00a01766\u20131770, 2013", "word_idx": 43936, "sentence_idx": 730, "label": "unlabeled"}, {"type": "text", "expr": "Interspeech", "word_idx": 43952, "sentence_idx": 731, "label": "unlabeled"}, {"type": "text", "expr": "Peltonen et\u00a0al", "word_idx": 43963, "sentence_idx": 732, "label": "unlabeled"}, {"type": "text", "expr": " (2001) \nPeltonen, Sari, Gabbouj, Moncef, and Astola, Jaakko", "word_idx": 43977, "sentence_idx": 733, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Nonlinear filter design: methodologies and challenges", "word_idx": 44037, "sentence_idx": 734, "label": "unlabeled"}, {"type": "text", "expr": "Peltonen et\u00a0al", "word_idx": 44093, "sentence_idx": 735, "label": "unlabeled"}, {"type": "text", "expr": " (2001)", "word_idx": 44107, "sentence_idx": 736, "label": "unlabeled"}, {"type": "text", "expr": "Peltonen, Sari, Gabbouj, Moncef, and Astola, Jaakko", "word_idx": 44114, "sentence_idx": 737, "label": "unlabeled"}, {"type": "text", "expr": "Nonlinear filter design: methodologies and challenges", "word_idx": 44165, "sentence_idx": 738, "label": "unlabeled"}, {"type": "text", "expr": "In  IEEE ISPA , pp", "word_idx": 44218, "sentence_idx": 739, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0102\u2013107, 2001", "word_idx": 44236, "sentence_idx": 740, "label": "unlabeled"}, {"type": "text", "expr": "IEEE ISPA", "word_idx": 44250, "sentence_idx": 741, "label": "unlabeled"}, {"type": "text", "expr": "Poritz (1982) \nPoritz, Alan\u00a0B", "word_idx": 44259, "sentence_idx": 742, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Linear predictive hidden Markov models and the speech signal", "word_idx": 44288, "sentence_idx": 743, "label": "unlabeled"}, {"type": "text", "expr": "Poritz (1982)", "word_idx": 44351, "sentence_idx": 744, "label": "unlabeled"}, {"type": "text", "expr": "Poritz, Alan\u00a0B", "word_idx": 44364, "sentence_idx": 745, "label": "unlabeled"}, {"type": "text", "expr": "Linear predictive hidden Markov models and the speech signal", "word_idx": 44378, "sentence_idx": 746, "label": "unlabeled"}, {"type": "text", "expr": "In  ICASSP , pp", "word_idx": 44438, "sentence_idx": 747, "label": "unlabeled"}, {"type": "text", "expr": "\u00a01291\u20131294, 1982", "word_idx": 44453, "sentence_idx": 748, "label": "unlabeled"}, {"type": "text", "expr": "ICASSP", "word_idx": 44469, "sentence_idx": 749, "label": "unlabeled"}, {"type": "text", "expr": "Rabiner & Juang (1993) \nRabiner, Lawrence and Juang, Biing-Hwang", "word_idx": 44475, "sentence_idx": 750, "label": "unlabeled"}, {"type": "text", "expr": "\n\n PrenticeHall, 1993", "word_idx": 44539, "sentence_idx": 751, "label": "unlabeled"}, {"type": "text", "expr": "Rabiner & Juang (1993)", "word_idx": 44560, "sentence_idx": 752, "label": "unlabeled"}, {"type": "text", "expr": "Rabiner, Lawrence and Juang, Biing-Hwang", "word_idx": 44582, "sentence_idx": 753, "label": "unlabeled"}, {"type": "text", "expr": "Fundamentals of Speech Recognition ", "word_idx": 44622, "sentence_idx": 754, "label": "unlabeled"}, {"type": "text", "expr": "Fundamentals of Speech Recognition", "word_idx": 44657, "sentence_idx": 755, "label": "unlabeled"}, {"type": "text", "expr": "PrenticeHall, 1993", "word_idx": 44691, "sentence_idx": 756, "label": "unlabeled"}, {"type": "text", "expr": "Sagisaka et\u00a0al", "word_idx": 44709, "sentence_idx": 757, "label": "unlabeled"}, {"type": "text", "expr": " (1992) \nSagisaka, Yoshinori, Kaiki, Nobuyoshi, Iwahashi, Naoto, and Mimura, Katsuhiko", "word_idx": 44723, "sentence_idx": 758, "label": "unlabeled"}, {"type": "text", "expr": "Sagisaka et\u00a0al", "word_idx": 44809, "sentence_idx": 759, "label": "unlabeled"}, {"type": "text", "expr": " (1992)", "word_idx": 44823, "sentence_idx": 760, "label": "unlabeled"}, {"type": "text", "expr": "Sagisaka, Yoshinori, Kaiki, Nobuyoshi, Iwahashi, Naoto, and Mimura, Katsuhiko", "word_idx": 44830, "sentence_idx": 761, "label": "unlabeled"}, {"type": "text", "expr": "ATR  $\\nu$ -talk speech synthesis system", "word_idx": 44907, "sentence_idx": 762, "label": "unlabeled"}, {"type": "math", "expr": "$$\\nu$$", "word_idx": 44947, "sentence_idx": 763, "label": "unlabeled"}, {"type": "text", "expr": "In  ICSLP , pp", "word_idx": 44950, "sentence_idx": 764, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0483\u2013486, 1992", "word_idx": 44964, "sentence_idx": 765, "label": "unlabeled"}, {"type": "text", "expr": "ICSLP", "word_idx": 44978, "sentence_idx": 766, "label": "unlabeled"}, {"type": "text", "expr": "Sainath et\u00a0al", "word_idx": 44983, "sentence_idx": 767, "label": "unlabeled"}, {"type": "text", "expr": " (2015) \nSainath, Tara\u00a0N", "word_idx": 44996, "sentence_idx": 768, "label": "unlabeled"}, {"type": "text", "expr": ", Weiss, Ron\u00a0J", "word_idx": 45020, "sentence_idx": 769, "label": "unlabeled"}, {"type": "text", "expr": ", Senior, Andrew, Wilson, Kevin\u00a0W", "word_idx": 45034, "sentence_idx": 770, "label": "unlabeled"}, {"type": "text", "expr": ", and Vinyals,\nOriol", "word_idx": 45067, "sentence_idx": 771, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Learning the speech front-end with raw waveform CLDNNs", "word_idx": 45087, "sentence_idx": 772, "label": "unlabeled"}, {"type": "text", "expr": "Sainath et\u00a0al", "word_idx": 45144, "sentence_idx": 773, "label": "unlabeled"}, {"type": "text", "expr": " (2015)", "word_idx": 45157, "sentence_idx": 774, "label": "unlabeled"}, {"type": "text", "expr": "Sainath, Tara\u00a0N", "word_idx": 45164, "sentence_idx": 775, "label": "unlabeled"}, {"type": "text", "expr": ", Weiss, Ron\u00a0J", "word_idx": 45179, "sentence_idx": 776, "label": "unlabeled"}, {"type": "text", "expr": ", Senior, Andrew, Wilson, Kevin\u00a0W", "word_idx": 45193, "sentence_idx": 777, "label": "unlabeled"}, {"type": "text", "expr": ", and Vinyals,\nOriol", "word_idx": 45226, "sentence_idx": 778, "label": "unlabeled"}, {"type": "text", "expr": "Learning the speech front-end with raw waveform CLDNNs", "word_idx": 45246, "sentence_idx": 779, "label": "unlabeled"}, {"type": "text", "expr": "In  Interspeech , pp", "word_idx": 45300, "sentence_idx": 780, "label": "unlabeled"}, {"type": "text", "expr": "\u00a01\u20135, 2015", "word_idx": 45320, "sentence_idx": 781, "label": "unlabeled"}, {"type": "text", "expr": "Interspeech", "word_idx": 45330, "sentence_idx": 782, "label": "unlabeled"}, {"type": "text", "expr": "Takaki & Yamagishi (2016) \nTakaki, Shinji and Yamagishi, Junichi", "word_idx": 45341, "sentence_idx": 783, "label": "unlabeled"}, {"type": "text", "expr": "\n\n A deep auto-encoder based low-dimensional feature extraction from\nFFT spectral envelopes for statistical parametric speech synthesis", "word_idx": 45405, "sentence_idx": 784, "label": "unlabeled"}, {"type": "text", "expr": "Takaki & Yamagishi (2016)", "word_idx": 45540, "sentence_idx": 785, "label": "unlabeled"}, {"type": "text", "expr": "Takaki, Shinji and Yamagishi, Junichi", "word_idx": 45565, "sentence_idx": 786, "label": "unlabeled"}, {"type": "text", "expr": "A deep auto-encoder based low-dimensional feature extraction from\nFFT spectral envelopes for statistical parametric speech synthesis", "word_idx": 45602, "sentence_idx": 787, "label": "unlabeled"}, {"type": "text", "expr": "In  ICASSP , pp", "word_idx": 45734, "sentence_idx": 788, "label": "unlabeled"}, {"type": "text", "expr": "\u00a05535\u20135539, 2016", "word_idx": 45749, "sentence_idx": 789, "label": "unlabeled"}, {"type": "text", "expr": "ICASSP", "word_idx": 45765, "sentence_idx": 790, "label": "unlabeled"}, {"type": "text", "expr": "Takamichi et\u00a0al", "word_idx": 45771, "sentence_idx": 791, "label": "unlabeled"}, {"type": "text", "expr": " (2016) \nTakamichi, Shinnosuke, Toda, Tomoki, Black, Alan\u00a0W", "word_idx": 45786, "sentence_idx": 792, "label": "unlabeled"}, {"type": "text", "expr": ", Neubig, Graham, Sakriani,\nSakti, and Nakamura, Satoshi", "word_idx": 45845, "sentence_idx": 793, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Postfilters to modify the modulation spectrum for statistical\nparametric speech synthesis", "word_idx": 45901, "sentence_idx": 794, "label": "unlabeled"}, {"type": "text", "expr": "Takamichi et\u00a0al", "word_idx": 45993, "sentence_idx": 795, "label": "unlabeled"}, {"type": "text", "expr": " (2016)", "word_idx": 46008, "sentence_idx": 796, "label": "unlabeled"}, {"type": "text", "expr": "Takamichi, Shinnosuke, Toda, Tomoki, Black, Alan\u00a0W", "word_idx": 46015, "sentence_idx": 797, "label": "unlabeled"}, {"type": "text", "expr": ", Neubig, Graham, Sakriani,\nSakti, and Nakamura, Satoshi", "word_idx": 46065, "sentence_idx": 798, "label": "unlabeled"}, {"type": "text", "expr": "Postfilters to modify the modulation spectrum for statistical\nparametric speech synthesis", "word_idx": 46121, "sentence_idx": 799, "label": "unlabeled"}, {"type": "text", "expr": "IEEE/ACM Trans", "word_idx": 46210, "sentence_idx": 800, "label": "unlabeled"}, {"type": "text", "expr": " Audio Speech Lang", "word_idx": 46224, "sentence_idx": 801, "label": "unlabeled"}, {"type": "text", "expr": " Process", "word_idx": 46242, "sentence_idx": 802, "label": "unlabeled"}, {"type": "text", "expr": " , 24(4):755\u2013767, 2016", "word_idx": 46250, "sentence_idx": 803, "label": "unlabeled"}, {"type": "text", "expr": "IEEE/ACM Trans", "word_idx": 46272, "sentence_idx": 804, "label": "unlabeled"}, {"type": "text", "expr": " Audio Speech Lang", "word_idx": 46286, "sentence_idx": 805, "label": "unlabeled"}, {"type": "text", "expr": " Process", "word_idx": 46304, "sentence_idx": 806, "label": "unlabeled"}, {"type": "text", "expr": "Theis & Bethge (2015) \nTheis, Lucas and Bethge, Matthias", "word_idx": 46312, "sentence_idx": 807, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Generative image modeling using spatial LSTMs", "word_idx": 46368, "sentence_idx": 808, "label": "unlabeled"}, {"type": "text", "expr": "Theis & Bethge (2015)", "word_idx": 46416, "sentence_idx": 809, "label": "unlabeled"}, {"type": "text", "expr": "Theis, Lucas and Bethge, Matthias", "word_idx": 46437, "sentence_idx": 810, "label": "unlabeled"}, {"type": "text", "expr": "Generative image modeling using spatial LSTMs", "word_idx": 46470, "sentence_idx": 811, "label": "unlabeled"}, {"type": "text", "expr": "In  NIPS , pp", "word_idx": 46515, "sentence_idx": 812, "label": "unlabeled"}, {"type": "text", "expr": "\u00a01927\u20131935, 2015", "word_idx": 46528, "sentence_idx": 813, "label": "unlabeled"}, {"type": "text", "expr": "Toda & Tokuda (2007) \nToda, Tomoki and Tokuda, Keiichi", "word_idx": 46544, "sentence_idx": 814, "label": "unlabeled"}, {"type": "text", "expr": "\n\n A speech parameter generation algorithm considering global variance\nfor HMM-based speech synthesis", "word_idx": 46598, "sentence_idx": 815, "label": "unlabeled"}, {"type": "text", "expr": "Toda & Tokuda (2007)", "word_idx": 46699, "sentence_idx": 816, "label": "unlabeled"}, {"type": "text", "expr": "Toda, Tomoki and Tokuda, Keiichi", "word_idx": 46719, "sentence_idx": 817, "label": "unlabeled"}, {"type": "text", "expr": "A speech parameter generation algorithm considering global variance\nfor HMM-based speech synthesis", "word_idx": 46751, "sentence_idx": 818, "label": "unlabeled"}, {"type": "text", "expr": "IEICE Trans", "word_idx": 46849, "sentence_idx": 819, "label": "unlabeled"}, {"type": "text", "expr": " Syst", "word_idx": 46860, "sentence_idx": 820, "label": "unlabeled"}, {"type": "text", "expr": " , E90-D(5):816\u2013824, 2007", "word_idx": 46865, "sentence_idx": 821, "label": "unlabeled"}, {"type": "text", "expr": "IEICE Trans", "word_idx": 46890, "sentence_idx": 822, "label": "unlabeled"}, {"type": "text", "expr": " Syst", "word_idx": 46901, "sentence_idx": 823, "label": "unlabeled"}, {"type": "text", "expr": "Toda & Tokuda (2008) \nToda, Tomoki and Tokuda, Keiichi", "word_idx": 46906, "sentence_idx": 824, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Statistical approach to vocal tract transfer function estimation\nbased on factor analyzed trajectory hmm", "word_idx": 46960, "sentence_idx": 825, "label": "unlabeled"}, {"type": "text", "expr": "Toda & Tokuda (2008)", "word_idx": 47067, "sentence_idx": 826, "label": "unlabeled"}, {"type": "text", "expr": "Toda, Tomoki and Tokuda, Keiichi", "word_idx": 47087, "sentence_idx": 827, "label": "unlabeled"}, {"type": "text", "expr": "Statistical approach to vocal tract transfer function estimation\nbased on factor analyzed trajectory hmm", "word_idx": 47119, "sentence_idx": 828, "label": "unlabeled"}, {"type": "text", "expr": "In  ICASSP , pp", "word_idx": 47223, "sentence_idx": 829, "label": "unlabeled"}, {"type": "text", "expr": "\u00a03925\u20133928, 2008", "word_idx": 47238, "sentence_idx": 830, "label": "unlabeled"}, {"type": "text", "expr": "ICASSP", "word_idx": 47254, "sentence_idx": 831, "label": "unlabeled"}, {"type": "text", "expr": "Tokuda (2011) \nTokuda, Keiichi", "word_idx": 47260, "sentence_idx": 832, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Speech synthesis as a statistical machine learning problem", "word_idx": 47290, "sentence_idx": 833, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Invited talk given at ASRU", "word_idx": 47351, "sentence_idx": 834, "label": "unlabeled"}, {"type": "text", "expr": "Tokuda (2011)", "word_idx": 47380, "sentence_idx": 835, "label": "unlabeled"}, {"type": "text", "expr": "Tokuda, Keiichi", "word_idx": 47393, "sentence_idx": 836, "label": "unlabeled"}, {"type": "text", "expr": "Speech synthesis as a statistical machine learning problem", "word_idx": 47408, "sentence_idx": 837, "label": "unlabeled"}, {"type": "text", "expr": "http://www", "word_idx": 47466, "sentence_idx": 838, "label": "unlabeled"}, {"type": "text", "expr": "nitech", "word_idx": 47476, "sentence_idx": 839, "label": "unlabeled"}, {"type": "text", "expr": "jp/~tokuda/tokuda_asru2011_for_pdf", "word_idx": 47482, "sentence_idx": 840, "label": "unlabeled"}, {"type": "text", "expr": "pdf ,\n2011", "word_idx": 47516, "sentence_idx": 841, "label": "unlabeled"}, {"type": "text", "expr": "http://www", "word_idx": 47526, "sentence_idx": 842, "label": "unlabeled"}, {"type": "text", "expr": "nitech", "word_idx": 47536, "sentence_idx": 843, "label": "unlabeled"}, {"type": "text", "expr": "jp/~tokuda/tokuda_asru2011_for_pdf", "word_idx": 47542, "sentence_idx": 844, "label": "unlabeled"}, {"type": "text", "expr": "Invited talk given at ASRU", "word_idx": 47576, "sentence_idx": 845, "label": "unlabeled"}, {"type": "text", "expr": "Tokuda & Zen (2015) \nTokuda, Keiichi and Zen, Heiga", "word_idx": 47602, "sentence_idx": 846, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Directly modeling speech waveforms by neural networks for statistical\nparametric speech synthesis", "word_idx": 47653, "sentence_idx": 847, "label": "unlabeled"}, {"type": "text", "expr": "Tokuda & Zen (2015)", "word_idx": 47753, "sentence_idx": 848, "label": "unlabeled"}, {"type": "text", "expr": "Tokuda, Keiichi and Zen, Heiga", "word_idx": 47772, "sentence_idx": 849, "label": "unlabeled"}, {"type": "text", "expr": "Directly modeling speech waveforms by neural networks for statistical\nparametric speech synthesis", "word_idx": 47802, "sentence_idx": 850, "label": "unlabeled"}, {"type": "text", "expr": "In  ICASSP , pp", "word_idx": 47899, "sentence_idx": 851, "label": "unlabeled"}, {"type": "text", "expr": "\u00a04215\u20134219, 2015", "word_idx": 47914, "sentence_idx": 852, "label": "unlabeled"}, {"type": "text", "expr": "ICASSP", "word_idx": 47930, "sentence_idx": 853, "label": "unlabeled"}, {"type": "text", "expr": "Tokuda & Zen (2016) \nTokuda, Keiichi and Zen, Heiga", "word_idx": 47936, "sentence_idx": 854, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Directly modeling voiced and unvoiced components in speech waveforms\nby neural networks", "word_idx": 47987, "sentence_idx": 855, "label": "unlabeled"}, {"type": "text", "expr": "Tokuda & Zen (2016)", "word_idx": 48077, "sentence_idx": 856, "label": "unlabeled"}, {"type": "text", "expr": "Tokuda, Keiichi and Zen, Heiga", "word_idx": 48096, "sentence_idx": 857, "label": "unlabeled"}, {"type": "text", "expr": "Directly modeling voiced and unvoiced components in speech waveforms\nby neural networks", "word_idx": 48126, "sentence_idx": 858, "label": "unlabeled"}, {"type": "text", "expr": "In  ICASSP , pp", "word_idx": 48213, "sentence_idx": 859, "label": "unlabeled"}, {"type": "text", "expr": "\u00a05640\u20135644, 2016", "word_idx": 48228, "sentence_idx": 860, "label": "unlabeled"}, {"type": "text", "expr": "ICASSP", "word_idx": 48244, "sentence_idx": 861, "label": "unlabeled"}, {"type": "text", "expr": "Tuerk & Robinson (1993) \nTuerk, Christine and Robinson, Tony", "word_idx": 48250, "sentence_idx": 862, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Speech synthesis using artificial neural networks trained on cepstral\ncoefficients", "word_idx": 48310, "sentence_idx": 863, "label": "unlabeled"}, {"type": "text", "expr": "Tuerk & Robinson (1993)", "word_idx": 48395, "sentence_idx": 864, "label": "unlabeled"}, {"type": "text", "expr": "Tuerk, Christine and Robinson, Tony", "word_idx": 48418, "sentence_idx": 865, "label": "unlabeled"}, {"type": "text", "expr": "Speech synthesis using artificial neural networks trained on cepstral\ncoefficients", "word_idx": 48453, "sentence_idx": 866, "label": "unlabeled"}, {"type": "text", "expr": "In  Proc", "word_idx": 48535, "sentence_idx": 867, "label": "unlabeled"}, {"type": "text", "expr": " Eurospeech , pp", "word_idx": 48543, "sentence_idx": 868, "label": "unlabeled"}, {"type": "text", "expr": "\u00a01713\u20131716, 1993", "word_idx": 48559, "sentence_idx": 869, "label": "unlabeled"}, {"type": "text", "expr": " Eurospeech", "word_idx": 48575, "sentence_idx": 870, "label": "unlabeled"}, {"type": "text", "expr": "T\u00fcske et\u00a0al", "word_idx": 48586, "sentence_idx": 871, "label": "unlabeled"}, {"type": "text", "expr": " (2014) \nT\u00fcske, Zolt\u00e1n, Golik, Pavel, Schl\u00fcter, Ralf, and Ney, Hermann", "word_idx": 48597, "sentence_idx": 872, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Acoustic modeling with deep neural networks using raw time signal for\nLVCSR", "word_idx": 48667, "sentence_idx": 873, "label": "unlabeled"}, {"type": "text", "expr": "T\u00fcske et\u00a0al", "word_idx": 48745, "sentence_idx": 874, "label": "unlabeled"}, {"type": "text", "expr": " (2014)", "word_idx": 48756, "sentence_idx": 875, "label": "unlabeled"}, {"type": "text", "expr": "T\u00fcske, Zolt\u00e1n, Golik, Pavel, Schl\u00fcter, Ralf, and Ney, Hermann", "word_idx": 48763, "sentence_idx": 876, "label": "unlabeled"}, {"type": "text", "expr": "Acoustic modeling with deep neural networks using raw time signal for\nLVCSR", "word_idx": 48824, "sentence_idx": 877, "label": "unlabeled"}, {"type": "text", "expr": "In  Interspeech , pp", "word_idx": 48899, "sentence_idx": 878, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0890\u2013894, 2014", "word_idx": 48919, "sentence_idx": 879, "label": "unlabeled"}, {"type": "text", "expr": "Interspeech", "word_idx": 48933, "sentence_idx": 880, "label": "unlabeled"}, {"type": "text", "expr": "Uria et\u00a0al", "word_idx": 48944, "sentence_idx": 881, "label": "unlabeled"}, {"type": "text", "expr": " (2015) \nUria, Benigno, Murray, Iain, Renals, Steve, Valentini-Botinhao, Cassia, and\nBridle, John", "word_idx": 48954, "sentence_idx": 882, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Modelling acoustic feature dependencies with artificial neural\nnetworks: Trajectory-RNADE", "word_idx": 49051, "sentence_idx": 883, "label": "unlabeled"}, {"type": "text", "expr": "Uria et\u00a0al", "word_idx": 49143, "sentence_idx": 884, "label": "unlabeled"}, {"type": "text", "expr": " (2015)", "word_idx": 49153, "sentence_idx": 885, "label": "unlabeled"}, {"type": "text", "expr": "Uria, Benigno, Murray, Iain, Renals, Steve, Valentini-Botinhao, Cassia, and\nBridle, John", "word_idx": 49160, "sentence_idx": 886, "label": "unlabeled"}, {"type": "text", "expr": "Modelling acoustic feature dependencies with artificial neural\nnetworks: Trajectory-RNADE", "word_idx": 49248, "sentence_idx": 887, "label": "unlabeled"}, {"type": "text", "expr": "In  ICASSP , pp", "word_idx": 49337, "sentence_idx": 888, "label": "unlabeled"}, {"type": "text", "expr": "\u00a04465\u20134469, 2015", "word_idx": 49352, "sentence_idx": 889, "label": "unlabeled"}, {"type": "text", "expr": "ICASSP", "word_idx": 49368, "sentence_idx": 890, "label": "unlabeled"}, {"type": "text", "expr": "van\u00a0den Oord et\u00a0al", "word_idx": 49374, "sentence_idx": 891, "label": "unlabeled"}, {"type": "text", "expr": " (2016a) \nvan\u00a0den Oord, A\u00e4ron, Kalchbrenner, Nal, and Kavukcuoglu, Koray", "word_idx": 49392, "sentence_idx": 892, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Pixel recurrent neural networks", "word_idx": 49464, "sentence_idx": 893, "label": "unlabeled"}, {"type": "text", "expr": "van\u00a0den Oord et\u00a0al", "word_idx": 49498, "sentence_idx": 894, "label": "unlabeled"}, {"type": "text", "expr": " (2016a)", "word_idx": 49516, "sentence_idx": 895, "label": "unlabeled"}, {"type": "text", "expr": "van\u00a0den Oord, A\u00e4ron, Kalchbrenner, Nal, and Kavukcuoglu, Koray", "word_idx": 49524, "sentence_idx": 896, "label": "unlabeled"}, {"type": "text", "expr": "Pixel recurrent neural networks", "word_idx": 49586, "sentence_idx": 897, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1601", "word_idx": 49617, "sentence_idx": 898, "label": "unlabeled"}, {"type": "text", "expr": "06759 , 2016a", "word_idx": 49642, "sentence_idx": 899, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1601", "word_idx": 49655, "sentence_idx": 900, "label": "unlabeled"}, {"type": "text", "expr": "06759", "word_idx": 49680, "sentence_idx": 901, "label": "unlabeled"}, {"type": "text", "expr": "van\u00a0den Oord et\u00a0al", "word_idx": 49685, "sentence_idx": 902, "label": "unlabeled"}, {"type": "text", "expr": " (2016b) \nvan\u00a0den Oord, A\u00e4ron, Kalchbrenner, Nal, Vinyals, Oriol, Espeholt, Lasse,\nGraves, Alex, and Kavukcuoglu, Koray", "word_idx": 49703, "sentence_idx": 903, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Conditional image generation with PixelCNN decoders", "word_idx": 49822, "sentence_idx": 904, "label": "unlabeled"}, {"type": "text", "expr": "van\u00a0den Oord et\u00a0al", "word_idx": 49876, "sentence_idx": 905, "label": "unlabeled"}, {"type": "text", "expr": " (2016b)", "word_idx": 49894, "sentence_idx": 906, "label": "unlabeled"}, {"type": "text", "expr": "van\u00a0den Oord, A\u00e4ron, Kalchbrenner, Nal, Vinyals, Oriol, Espeholt, Lasse,\nGraves, Alex, and Kavukcuoglu, Koray", "word_idx": 49902, "sentence_idx": 907, "label": "unlabeled"}, {"type": "text", "expr": "Conditional image generation with PixelCNN decoders", "word_idx": 50011, "sentence_idx": 908, "label": "unlabeled"}, {"type": "text", "expr": "CoRR , abs/1606", "word_idx": 50062, "sentence_idx": 909, "label": "unlabeled"}, {"type": "text", "expr": "05328, 2016b", "word_idx": 50077, "sentence_idx": 910, "label": "unlabeled"}, {"type": "text", "expr": "URL  http://arxiv", "word_idx": 50089, "sentence_idx": 911, "label": "unlabeled"}, {"type": "text", "expr": "org/abs/1606", "word_idx": 50106, "sentence_idx": 912, "label": "unlabeled"}, {"type": "text", "expr": "05328 ", "word_idx": 50118, "sentence_idx": 913, "label": "unlabeled"}, {"type": "text", "expr": "http://arxiv", "word_idx": 50124, "sentence_idx": 914, "label": "unlabeled"}, {"type": "text", "expr": "org/abs/1606", "word_idx": 50136, "sentence_idx": 915, "label": "unlabeled"}, {"type": "text", "expr": "05328", "word_idx": 50148, "sentence_idx": 916, "label": "unlabeled"}, {"type": "text", "expr": "Wu & Tokuda (2008) \nWu, Yi-Jian and Tokuda, Keiichi", "word_idx": 50153, "sentence_idx": 917, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Minimum generation error training with direct log spectral distortion\non LSPs for HMM-based speech synthesis", "word_idx": 50204, "sentence_idx": 918, "label": "unlabeled"}, {"type": "text", "expr": "Wu & Tokuda (2008)", "word_idx": 50315, "sentence_idx": 919, "label": "unlabeled"}, {"type": "text", "expr": "Wu, Yi-Jian and Tokuda, Keiichi", "word_idx": 50333, "sentence_idx": 920, "label": "unlabeled"}, {"type": "text", "expr": "Minimum generation error training with direct log spectral distortion\non LSPs for HMM-based speech synthesis", "word_idx": 50364, "sentence_idx": 921, "label": "unlabeled"}, {"type": "text", "expr": "In  Interspeech , pp", "word_idx": 50472, "sentence_idx": 922, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0577\u2013580, 2008", "word_idx": 50492, "sentence_idx": 923, "label": "unlabeled"}, {"type": "text", "expr": "Interspeech", "word_idx": 50506, "sentence_idx": 924, "label": "unlabeled"}, {"type": "text", "expr": "Yamagishi (2012) \nYamagishi, Junichi", "word_idx": 50517, "sentence_idx": 925, "label": "unlabeled"}, {"type": "text", "expr": "\n\n English multi-speaker corpus for CSTR voice cloning toolkit, 2012", "word_idx": 50553, "sentence_idx": 926, "label": "unlabeled"}, {"type": "text", "expr": "Yamagishi (2012)", "word_idx": 50621, "sentence_idx": 927, "label": "unlabeled"}, {"type": "text", "expr": "Yamagishi, Junichi", "word_idx": 50637, "sentence_idx": 928, "label": "unlabeled"}, {"type": "text", "expr": "English multi-speaker corpus for CSTR voice cloning toolkit, 2012", "word_idx": 50655, "sentence_idx": 929, "label": "unlabeled"}, {"type": "text", "expr": "URL\n http://homepages", "word_idx": 50720, "sentence_idx": 930, "label": "unlabeled"}, {"type": "text", "expr": "uk/jyamagis/page3/page58/page58", "word_idx": 50741, "sentence_idx": 931, "label": "unlabeled"}, {"type": "text", "expr": "html ", "word_idx": 50772, "sentence_idx": 932, "label": "unlabeled"}, {"type": "text", "expr": "http://homepages", "word_idx": 50777, "sentence_idx": 933, "label": "unlabeled"}, {"type": "text", "expr": "uk/jyamagis/page3/page58/page58", "word_idx": 50793, "sentence_idx": 934, "label": "unlabeled"}, {"type": "text", "expr": "Yoshimura (2002) \nYoshimura, Takayoshi", "word_idx": 50824, "sentence_idx": 935, "label": "unlabeled"}, {"type": "text", "expr": "\n\n PhD thesis, Nagoya Institute of Technology, 2002", "word_idx": 50862, "sentence_idx": 936, "label": "unlabeled"}, {"type": "text", "expr": "Yoshimura (2002)", "word_idx": 50913, "sentence_idx": 937, "label": "unlabeled"}, {"type": "text", "expr": "Yoshimura, Takayoshi", "word_idx": 50929, "sentence_idx": 938, "label": "unlabeled"}, {"type": "text", "expr": "Simultaneous modeling of phonetic and prosodic parameters, and\ncharacteristic conversion for HMM-based text-to-speech systems ", "word_idx": 50949, "sentence_idx": 939, "label": "unlabeled"}, {"type": "text", "expr": "Simultaneous modeling of phonetic and prosodic parameters, and\ncharacteristic conversion for HMM-based text-to-speech systems", "word_idx": 51075, "sentence_idx": 940, "label": "unlabeled"}, {"type": "text", "expr": "PhD thesis, Nagoya Institute of Technology, 2002", "word_idx": 51200, "sentence_idx": 941, "label": "unlabeled"}, {"type": "text", "expr": "Yu & Koltun (2016) \nYu, Fisher and Koltun, Vladlen", "word_idx": 51248, "sentence_idx": 942, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Multi-scale context aggregation by dilated convolutions", "word_idx": 51298, "sentence_idx": 943, "label": "unlabeled"}, {"type": "text", "expr": "Yu & Koltun (2016)", "word_idx": 51356, "sentence_idx": 944, "label": "unlabeled"}, {"type": "text", "expr": "Yu, Fisher and Koltun, Vladlen", "word_idx": 51374, "sentence_idx": 945, "label": "unlabeled"}, {"type": "text", "expr": "Multi-scale context aggregation by dilated convolutions", "word_idx": 51404, "sentence_idx": 946, "label": "unlabeled"}, {"type": "text", "expr": "In  ICLR , 2016", "word_idx": 51459, "sentence_idx": 947, "label": "unlabeled"}, {"type": "text", "expr": "URL  http://arxiv", "word_idx": 51474, "sentence_idx": 948, "label": "unlabeled"}, {"type": "text", "expr": "org/abs/1511", "word_idx": 51491, "sentence_idx": 949, "label": "unlabeled"}, {"type": "text", "expr": "07122 ", "word_idx": 51503, "sentence_idx": 950, "label": "unlabeled"}, {"type": "text", "expr": "http://arxiv", "word_idx": 51509, "sentence_idx": 951, "label": "unlabeled"}, {"type": "text", "expr": "org/abs/1511", "word_idx": 51521, "sentence_idx": 952, "label": "unlabeled"}, {"type": "text", "expr": "07122", "word_idx": 51533, "sentence_idx": 953, "label": "unlabeled"}, {"type": "text", "expr": "Zen (2006) \nZen, Heiga", "word_idx": 51538, "sentence_idx": 954, "label": "unlabeled"}, {"type": "text", "expr": "\n\n An example of context-dependent label format for HMM-based speech\nsynthesis in English, 2006", "word_idx": 51560, "sentence_idx": 955, "label": "unlabeled"}, {"type": "text", "expr": "Zen (2006)", "word_idx": 51655, "sentence_idx": 956, "label": "unlabeled"}, {"type": "text", "expr": "Zen, Heiga", "word_idx": 51665, "sentence_idx": 957, "label": "unlabeled"}, {"type": "text", "expr": "An example of context-dependent label format for HMM-based speech\nsynthesis in English, 2006", "word_idx": 51675, "sentence_idx": 958, "label": "unlabeled"}, {"type": "text", "expr": "URL  http://hts", "word_idx": 51767, "sentence_idx": 959, "label": "unlabeled"}, {"type": "text", "expr": "nitech", "word_idx": 51782, "sentence_idx": 960, "label": "unlabeled"}, {"type": "text", "expr": "jp/?Download ", "word_idx": 51788, "sentence_idx": 961, "label": "unlabeled"}, {"type": "text", "expr": "http://hts", "word_idx": 51801, "sentence_idx": 962, "label": "unlabeled"}, {"type": "text", "expr": "nitech", "word_idx": 51811, "sentence_idx": 963, "label": "unlabeled"}, {"type": "text", "expr": "jp/?Download", "word_idx": 51817, "sentence_idx": 964, "label": "unlabeled"}, {"type": "text", "expr": "Zen et\u00a0al", "word_idx": 51829, "sentence_idx": 965, "label": "unlabeled"}, {"type": "text", "expr": " (2007) \nZen, Heiga, Tokuda, Keiichi, and Kitamura, Tadashi", "word_idx": 51838, "sentence_idx": 966, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Reformulating the HMM as a trajectory model by imposing explicit\nrelationships between static and dynamic features", "word_idx": 51897, "sentence_idx": 967, "label": "unlabeled"}, {"type": "text", "expr": "Zen et\u00a0al", "word_idx": 52014, "sentence_idx": 968, "label": "unlabeled"}, {"type": "text", "expr": " (2007)", "word_idx": 52023, "sentence_idx": 969, "label": "unlabeled"}, {"type": "text", "expr": "Zen, Heiga, Tokuda, Keiichi, and Kitamura, Tadashi", "word_idx": 52030, "sentence_idx": 970, "label": "unlabeled"}, {"type": "text", "expr": "Reformulating the HMM as a trajectory model by imposing explicit\nrelationships between static and dynamic features", "word_idx": 52080, "sentence_idx": 971, "label": "unlabeled"}, {"type": "text", "expr": "Comput", "word_idx": 52194, "sentence_idx": 972, "label": "unlabeled"}, {"type": "text", "expr": " Speech Lang", "word_idx": 52200, "sentence_idx": 973, "label": "unlabeled"}, {"type": "text", "expr": " , 21(1):153\u2013173,\n2007", "word_idx": 52212, "sentence_idx": 974, "label": "unlabeled"}, {"type": "text", "expr": "Comput", "word_idx": 52234, "sentence_idx": 975, "label": "unlabeled"}, {"type": "text", "expr": " Speech Lang", "word_idx": 52240, "sentence_idx": 976, "label": "unlabeled"}, {"type": "text", "expr": "Zen et\u00a0al", "word_idx": 52252, "sentence_idx": 977, "label": "unlabeled"}, {"type": "text", "expr": " (2009) \nZen, Heiga, Tokuda, Keiichi, and Black, Alan\u00a0W", "word_idx": 52261, "sentence_idx": 978, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Statistical parametric speech synthesis", "word_idx": 52316, "sentence_idx": 979, "label": "unlabeled"}, {"type": "text", "expr": "Zen et\u00a0al", "word_idx": 52358, "sentence_idx": 980, "label": "unlabeled"}, {"type": "text", "expr": " (2009)", "word_idx": 52367, "sentence_idx": 981, "label": "unlabeled"}, {"type": "text", "expr": "Zen, Heiga, Tokuda, Keiichi, and Black, Alan\u00a0W", "word_idx": 52374, "sentence_idx": 982, "label": "unlabeled"}, {"type": "text", "expr": "Statistical parametric speech synthesis", "word_idx": 52420, "sentence_idx": 983, "label": "unlabeled"}, {"type": "text", "expr": "Speech Commn", "word_idx": 52459, "sentence_idx": 984, "label": "unlabeled"}, {"type": "text", "expr": " , 51(11):1039\u20131064, 2009", "word_idx": 52471, "sentence_idx": 985, "label": "unlabeled"}, {"type": "text", "expr": "Speech Commn", "word_idx": 52496, "sentence_idx": 986, "label": "unlabeled"}, {"type": "text", "expr": "Zen et\u00a0al", "word_idx": 52508, "sentence_idx": 987, "label": "unlabeled"}, {"type": "text", "expr": " (2013) \nZen, Heiga, Senior, Andrew, and Schuster, Mike", "word_idx": 52517, "sentence_idx": 988, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Statistical parametric speech synthesis using deep neural networks", "word_idx": 52572, "sentence_idx": 989, "label": "unlabeled"}, {"type": "text", "expr": "Zen et\u00a0al", "word_idx": 52641, "sentence_idx": 990, "label": "unlabeled"}, {"type": "text", "expr": " (2013)", "word_idx": 52650, "sentence_idx": 991, "label": "unlabeled"}, {"type": "text", "expr": "Zen, Heiga, Senior, Andrew, and Schuster, Mike", "word_idx": 52657, "sentence_idx": 992, "label": "unlabeled"}, {"type": "text", "expr": "Statistical parametric speech synthesis using deep neural networks", "word_idx": 52703, "sentence_idx": 993, "label": "unlabeled"}, {"type": "text", "expr": "In  Proc", "word_idx": 52769, "sentence_idx": 994, "label": "unlabeled"}, {"type": "text", "expr": " ICASSP , pp", "word_idx": 52777, "sentence_idx": 995, "label": "unlabeled"}, {"type": "text", "expr": "\u00a07962\u20137966, 2013", "word_idx": 52789, "sentence_idx": 996, "label": "unlabeled"}, {"type": "text", "expr": " ICASSP", "word_idx": 52805, "sentence_idx": 997, "label": "unlabeled"}, {"type": "text", "expr": "Zen et\u00a0al", "word_idx": 52812, "sentence_idx": 998, "label": "unlabeled"}, {"type": "text", "expr": " (2016) \nZen, Heiga, Agiomyrgiannakis, Yannis, Egberts, Niels, Henderson, Fergus, and\nSzczepaniak, Przemys\u0142aw", "word_idx": 52821, "sentence_idx": 999, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Fast, compact, and high quality LSTM-RNN based statistical\nparametric speech synthesizers for mobile devices", "word_idx": 52930, "sentence_idx": 1000, "label": "unlabeled"}, {"type": "text", "expr": "Zen et\u00a0al", "word_idx": 53041, "sentence_idx": 1001, "label": "unlabeled"}, {"type": "text", "expr": " (2016)", "word_idx": 53050, "sentence_idx": 1002, "label": "unlabeled"}, {"type": "text", "expr": "Zen, Heiga, Agiomyrgiannakis, Yannis, Egberts, Niels, Henderson, Fergus, and\nSzczepaniak, Przemys\u0142aw", "word_idx": 53057, "sentence_idx": 1003, "label": "unlabeled"}, {"type": "text", "expr": "Fast, compact, and high quality LSTM-RNN based statistical\nparametric speech synthesizers for mobile devices", "word_idx": 53157, "sentence_idx": 1004, "label": "unlabeled"}, {"type": "text", "expr": "In  Interspeech , 2016", "word_idx": 53265, "sentence_idx": 1005, "label": "unlabeled"}, {"type": "text", "expr": "Interspeech", "word_idx": 53287, "sentence_idx": 1006, "label": "unlabeled"}, {"type": "text", "expr": "URL  https://arxiv", "word_idx": 53298, "sentence_idx": 1007, "label": "unlabeled"}, {"type": "text", "expr": "org/abs/1606", "word_idx": 53316, "sentence_idx": 1008, "label": "unlabeled"}, {"type": "text", "expr": "06061 ", "word_idx": 53328, "sentence_idx": 1009, "label": "unlabeled"}, {"type": "text", "expr": "https://arxiv", "word_idx": 53334, "sentence_idx": 1010, "label": "unlabeled"}, {"type": "text", "expr": "org/abs/1606", "word_idx": 53347, "sentence_idx": 1011, "label": "unlabeled"}, {"type": "text", "expr": "06061", "word_idx": 53359, "sentence_idx": 1012, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0A  Text-to-Speech Background", "word_idx": 53364, "sentence_idx": 1013, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0A", "word_idx": 53401, "sentence_idx": 1014, "label": "unlabeled"}, {"type": "text", "expr": "The goal of TTS synthesis is to render naturally sounding speech signals given a text to be synthesized", "word_idx": 53411, "sentence_idx": 1015, "label": "unlabeled"}, {"type": "text", "expr": "\nHuman speech production process first translates a text (or concept) into movements of muscles associated with\narticulators and speech production-related organs", "word_idx": 53514, "sentence_idx": 1016, "label": "unlabeled"}, {"type": "text", "expr": "\nThen using air-flow from lung, vocal source excitation signals, which contain both periodic\n(by vocal cord vibration) and aperiodic (by turbulent noise) components, are generated", "word_idx": 53675, "sentence_idx": 1017, "label": "unlabeled"}, {"type": "text", "expr": "\nBy filtering the vocal source excitation signals by time-varying vocal tract transfer functions\ncontrolled by the articulators, their frequency characteristics are modulated", "word_idx": 53854, "sentence_idx": 1018, "label": "unlabeled"}, {"type": "text", "expr": "\nFinally, the generated speech signals are emitted", "word_idx": 54028, "sentence_idx": 1019, "label": "unlabeled"}, {"type": "text", "expr": "\nThe aim of TTS is to mimic this process by computers in some way", "word_idx": 54078, "sentence_idx": 1020, "label": "unlabeled"}, {"type": "text", "expr": "The goal of TTS synthesis is to render naturally sounding speech signals given a text to be synthesized", "word_idx": 54143, "sentence_idx": 1021, "label": "unlabeled"}, {"type": "text", "expr": "\nHuman speech production process first translates a text (or concept) into movements of muscles associated with\narticulators and speech production-related organs", "word_idx": 54246, "sentence_idx": 1022, "label": "unlabeled"}, {"type": "text", "expr": "\nThen using air-flow from lung, vocal source excitation signals, which contain both periodic\n(by vocal cord vibration) and aperiodic (by turbulent noise) components, are generated", "word_idx": 54407, "sentence_idx": 1023, "label": "unlabeled"}, {"type": "text", "expr": "\nBy filtering the vocal source excitation signals by time-varying vocal tract transfer functions\ncontrolled by the articulators, their frequency characteristics are modulated", "word_idx": 54586, "sentence_idx": 1024, "label": "unlabeled"}, {"type": "text", "expr": "\nFinally, the generated speech signals are emitted", "word_idx": 54760, "sentence_idx": 1025, "label": "unlabeled"}, {"type": "text", "expr": "\nThe aim of TTS is to mimic this process by computers in some way", "word_idx": 54810, "sentence_idx": 1026, "label": "unlabeled"}, {"type": "text", "expr": "TTS can be viewed as a sequence-to-sequence mapping problem;\nfrom a sequence of\ndiscrete symbols (text) to a real-valued time series (speech signals)", "word_idx": 54875, "sentence_idx": 1027, "label": "unlabeled"}, {"type": "text", "expr": "\nA typical TTS pipeline has two parts; 1) text analysis and 2) speech synthesis", "word_idx": 55024, "sentence_idx": 1028, "label": "unlabeled"}, {"type": "text", "expr": "\nThe text analysis part typically includes a number of natural language processing (NLP) steps,\nsuch as sentence segmentation, word segmentation, text normalization,\npart-of-speech (POS) tagging,\nand grapheme-to-phoneme (G2P) conversion", "word_idx": 55103, "sentence_idx": 1029, "label": "unlabeled"}, {"type": "text", "expr": "\nIt takes a word sequence as input and outputs\na phoneme sequence with a variety of linguistic contexts", "word_idx": 55339, "sentence_idx": 1030, "label": "unlabeled"}, {"type": "text", "expr": "\nThe speech synthesis part takes the context-dependent phoneme sequence as its input and outputs a synthesized speech waveform", "word_idx": 55442, "sentence_idx": 1031, "label": "unlabeled"}, {"type": "text", "expr": "\nThis part typically includes prosody prediction and speech waveform generation", "word_idx": 55568, "sentence_idx": 1032, "label": "unlabeled"}, {"type": "text", "expr": "TTS can be viewed as a sequence-to-sequence mapping problem;\nfrom a sequence of\ndiscrete symbols (text) to a real-valued time series (speech signals)", "word_idx": 55647, "sentence_idx": 1033, "label": "unlabeled"}, {"type": "text", "expr": "\nA typical TTS pipeline has two parts; 1) text analysis and 2) speech synthesis", "word_idx": 55796, "sentence_idx": 1034, "label": "unlabeled"}, {"type": "text", "expr": "\nThe text analysis part typically includes a number of natural language processing (NLP) steps,\nsuch as sentence segmentation, word segmentation, text normalization,\npart-of-speech (POS) tagging,\nand grapheme-to-phoneme (G2P) conversion", "word_idx": 55875, "sentence_idx": 1035, "label": "unlabeled"}, {"type": "text", "expr": "\nIt takes a word sequence as input and outputs\na phoneme sequence with a variety of linguistic contexts", "word_idx": 56111, "sentence_idx": 1036, "label": "unlabeled"}, {"type": "text", "expr": "\nThe speech synthesis part takes the context-dependent phoneme sequence as its input and outputs a synthesized speech waveform", "word_idx": 56214, "sentence_idx": 1037, "label": "unlabeled"}, {"type": "text", "expr": "\nThis part typically includes prosody prediction and speech waveform generation", "word_idx": 56340, "sentence_idx": 1038, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a06:  Outline of statistical parametric speech synthesis", "word_idx": 56419, "sentence_idx": 1039, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a06:", "word_idx": 56480, "sentence_idx": 1040, "label": "unlabeled"}, {"type": "text", "expr": "There are two main approaches to realize the speech synthesis part; non-parametric, example-based approach known as concatenative speech synthesis  , and parametric, model-based approach known as statistical parametric speech synthesis  ", "word_idx": 56489, "sentence_idx": 1041, "label": "unlabeled"}, {"type": "text", "expr": "\nThe concatenative approach builds up the utterance from units of recorded speech, whereas the statistical parametric approach uses a generative model to synthesize the speech", "word_idx": 56726, "sentence_idx": 1042, "label": "unlabeled"}, {"type": "text", "expr": "\nThe statistical parametric approach first extracts a sequence of vocoder parameters    $\\mathbf{o}=\\{\\mathbf{o}_{1},\\dots,\\mathbf{o}_{N}\\}$  from speech signals  $\\mathbf{x}=\\{x_{1},\\dots,x_{T}\\}$  and linguistic features  $\\mathbf{l}$  from the text  $W$ , where  $N$  and  $T$  correspond to the numbers of vocoder parameter vectors and speech signals", "word_idx": 56901, "sentence_idx": 1043, "label": "unlabeled"}, {"type": "text", "expr": "\nTypically a vocoder parameter vector  $\\mathbf{o}_{n}$  is extracted at every 5 milliseconds", "word_idx": 57255, "sentence_idx": 1044, "label": "unlabeled"}, {"type": "text", "expr": "\nIt often includes cepstra   or line spectral pairs  , which represent\nvocal tract transfer function, and fundamental\nfrequency ( $F_{0}$ ) and aperiodicity  , which represent characteristics of vocal source excitation signals", "word_idx": 57348, "sentence_idx": 1045, "label": "unlabeled"}, {"type": "text", "expr": "\nThen a set of generative models, such as hidden Markov models (HMMs)  , feed-forward neural networks  , and recurrent neural networks  , is trained from the extracted vocoder parameters and linguistic features as", "word_idx": 57574, "sentence_idx": 1046, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{o}=\\{\\mathbf{o}_{1},\\dots,\\mathbf{o}_{N}\\}$$", "word_idx": 57787, "sentence_idx": 1047, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}=\\{x_{1},\\dots,x_{T}\\}$$", "word_idx": 57837, "sentence_idx": 1048, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{l}$$", "word_idx": 57869, "sentence_idx": 1049, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{o}_{n}$$", "word_idx": 57879, "sentence_idx": 1050, "label": "unlabeled"}, {"type": "math", "expr": "$$F_{0}$$", "word_idx": 57893, "sentence_idx": 1051, "label": "unlabeled"}, {"type": "text", "expr": "$\\hat{\\Lambda}=\\operatornamewithlimits{\\mathrm{arg\\,max}}_{\\Lambda}p\\left(%\n\\mathbf{o}\\mid\\mathbf{l},\\Lambda\\right),$", "word_idx": 57898, "sentence_idx": 1052, "label": "unlabeled"}, {"type": "math", "expr": "$$\\hat{\\Lambda}=\\operatornamewithlimits{\\mathrm{arg\\,max}}_{\\Lambda}p\\left(%\n\\mathbf{o}\\mid\\mathbf{l},\\Lambda\\right),$$", "word_idx": 58015, "sentence_idx": 1053, "label": "unlabeled"}, {"type": "text", "expr": "where  $\\Lambda$  denotes the set of parameters of the generative model", "word_idx": 58130, "sentence_idx": 1054, "label": "unlabeled"}, {"type": "text", "expr": "\nAt the synthesis stage, the most probable vocoder parameters are generated given linguistic features extracted from a text to be synthesized as", "word_idx": 58201, "sentence_idx": 1055, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Lambda$$", "word_idx": 58345, "sentence_idx": 1056, "label": "unlabeled"}, {"type": "text", "expr": "$\\hat{\\mathbf{o}}=\\operatornamewithlimits{\\mathrm{arg\\,max}}_{\\mathbf{o}}p(%\n\\mathbf{o}\\mid\\mathbf{l},\\hat{\\Lambda})$", "word_idx": 58352, "sentence_idx": 1057, "label": "unlabeled"}, {"type": "math", "expr": "$$\\hat{\\mathbf{o}}=\\operatornamewithlimits{\\mathrm{arg\\,max}}_{\\mathbf{o}}p(%\n\\mathbf{o}\\mid\\mathbf{l},\\hat{\\Lambda}).$$", "word_idx": 58469, "sentence_idx": 1058, "label": "unlabeled"}, {"type": "text", "expr": "Then a speech waveform is reconstructed from  $\\hat{\\mathbf{o}}$  using a vocoder", "word_idx": 58585, "sentence_idx": 1059, "label": "unlabeled"}, {"type": "text", "expr": "\nThe statistical parametric approach offers various advantages over the\nconcatenative one such as small footprint\nand flexibility to change its voice characteristics", "word_idx": 58666, "sentence_idx": 1060, "label": "unlabeled"}, {"type": "text", "expr": "\nHowever, its subjective naturalness is often significantly worse than that of the concatenative approach; synthesized speech often sounds muffled and has artifacts", "word_idx": 58831, "sentence_idx": 1061, "label": "unlabeled"}, {"type": "text", "expr": "\n  reported three major factors that can degrade the subjective naturalness; quality of vocoders, accuracy of generative models, and effect of oversmoothing", "word_idx": 58995, "sentence_idx": 1062, "label": "unlabeled"}, {"type": "text", "expr": "\nThe first factor causes the artifacts and the second and third factors lead to the muffleness in the synthesized speech", "word_idx": 59151, "sentence_idx": 1063, "label": "unlabeled"}, {"type": "text", "expr": "\nThere have been a number of attempts to address these issues individually, such as developing high-quality vocoders  , improving the accuracy of generative models  , and compensating the oversmoothing effect  ", "word_idx": 59271, "sentence_idx": 1064, "label": "unlabeled"}, {"type": "text", "expr": "\n  showed that state-of-the-art statistical parametric speech syntheziers matched state-of-the-art concatenative ones in some languages", "word_idx": 59481, "sentence_idx": 1065, "label": "unlabeled"}, {"type": "text", "expr": "\nHowever, its vocoded sound quality is still a major issue", "word_idx": 59616, "sentence_idx": 1066, "label": "unlabeled"}, {"type": "math", "expr": "$$\\hat{\\mathbf{o}}$$", "word_idx": 59674, "sentence_idx": 1067, "label": "unlabeled"}, {"type": "text", "expr": "Extracting vocoder parameters can be viewed as estimation of a generative model parameters given speech signals  ", "word_idx": 59690, "sentence_idx": 1068, "label": "unlabeled"}, {"type": "text", "expr": "\nFor example, linear predictive analysis  , which has been used in speech coding, assumes that the generative model of speech signals is\na linear auto-regressive (AR) zero-mean Gaussian process;", "word_idx": 59803, "sentence_idx": 1069, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle x_{t}$", "word_idx": 59997, "sentence_idx": 1070, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle x_{t}$$", "word_idx": 60018, "sentence_idx": 1071, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\sum_{p=1}^{P}a_{p}x_{t-p}+\\epsilon_{t}$", "word_idx": 60037, "sentence_idx": 1072, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\sum_{p=1}^{P}a_{p}x_{t-p}+\\epsilon_{t}$$", "word_idx": 60092, "sentence_idx": 1073, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\epsilon_{t}$", "word_idx": 60145, "sentence_idx": 1074, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\epsilon_{t}$$", "word_idx": 60172, "sentence_idx": 1075, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\sim\\mathcal{N}(0,G^{2})$", "word_idx": 60197, "sentence_idx": 1076, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\sim\\mathcal{N}(0,G^{2})$$", "word_idx": 60236, "sentence_idx": 1077, "label": "unlabeled"}, {"type": "text", "expr": "where  $a_{p}$  is a  $p$ -th order linear predictive coefficient (LPC) and  $G^{2}$ \nis a variance of modeling error", "word_idx": 60273, "sentence_idx": 1078, "label": "unlabeled"}, {"type": "text", "expr": "\nThese parameters are estimated based on the maximum likelihood (ML) criterion", "word_idx": 60390, "sentence_idx": 1079, "label": "unlabeled"}, {"type": "text", "expr": "\nIn this sense, the training part of the statistical parametric approach can be viewed as a two-step optimization and sub-optimal: extract vocoder parameters by fitting a generative model of speech signals then model trajectories of the extracted vocoder parameters by a separate generative model for time series  ", "word_idx": 60468, "sentence_idx": 1080, "label": "unlabeled"}, {"type": "text", "expr": "\nThere have been attempts to integrate these two steps into a single one\n ", "word_idx": 60782, "sentence_idx": 1081, "label": "unlabeled"}, {"type": "text", "expr": "\nFor example,   integrated non-stationary, nonzero-mean Gaussian process generative model of speech signals and LSTM-RNN-based sequence generative model to a single one and jointly optimized them by back-propagation", "word_idx": 60856, "sentence_idx": 1082, "label": "unlabeled"}, {"type": "text", "expr": "\nAlthough they showed that this model could approximate natural speech signals, its segmental naturalness was significantly worse than the non-integrated model due to over-generalization and over-estimation of noise components in speech signals", "word_idx": 61071, "sentence_idx": 1083, "label": "unlabeled"}, {"type": "math", "expr": "$$a_{p}$$", "word_idx": 61315, "sentence_idx": 1084, "label": "unlabeled"}, {"type": "math", "expr": "$$G^{2}$$", "word_idx": 61320, "sentence_idx": 1085, "label": "unlabeled"}, {"type": "text", "expr": "The conventional generative models of raw audio signals have a number of assumptions which are inspired from the speech production, such as\n Although these assumptions are convenient, samples from these generative models tend to be noisy and lose important details to make these audio signals sounding natural", "word_idx": 61325, "sentence_idx": 1086, "label": "unlabeled"}, {"type": "text", "expr": "The conventional generative models of raw audio signals have a number of assumptions which are inspired from the speech production, such as", "word_idx": 61634, "sentence_idx": 1087, "label": "unlabeled"}, {"type": "text", "expr": "Use of fixed-length analysis window; They are typically based on a stationary stochastic process  ", "word_idx": 61773, "sentence_idx": 1088, "label": "unlabeled"}, {"type": "text", "expr": " To model time-varying speech signals by a stationary stochastic process, parameters of these generative models are estimated within a fixed-length, overlapping and shifting analysis window (typically its length is 20 to 30 milliseconds, and shift is 5 to 10 milliseconds)", "word_idx": 61871, "sentence_idx": 1089, "label": "unlabeled"}, {"type": "text", "expr": "\nHowever, some phones such as stops are time-limited by less than 20 milliseconds  ", "word_idx": 62143, "sentence_idx": 1090, "label": "unlabeled"}, {"type": "text", "expr": "\nTherefore, using such fixed-size analysis window has limitations", "word_idx": 62226, "sentence_idx": 1091, "label": "unlabeled"}, {"type": "text", "expr": "Linear filter; These generative models are typically realized as a linear time-invariant filter   within a windowed frame", "word_idx": 62291, "sentence_idx": 1092, "label": "unlabeled"}, {"type": "text", "expr": " However, the relationship between successive audio samples can be highly non-linear", "word_idx": 62412, "sentence_idx": 1093, "label": "unlabeled"}, {"type": "text", "expr": "Gaussian process assumption; The conventional generative models are based on Gaussian process  ", "word_idx": 62496, "sentence_idx": 1094, "label": "unlabeled"}, {"type": "text", "expr": " From the source-filter model of speech production   point of view, this is equivalent to assuming that a vocal source excitation signal is a sample from a Gaussian distribution  ", "word_idx": 62591, "sentence_idx": 1095, "label": "unlabeled"}, {"type": "text", "expr": "\nTogether with the linear assumption above, it results in assuming that speech signals are normally distributed", "word_idx": 62770, "sentence_idx": 1096, "label": "unlabeled"}, {"type": "text", "expr": "\nHowever, distributions of real speech signals can be significantly different from Gaussian", "word_idx": 62881, "sentence_idx": 1097, "label": "unlabeled"}, {"type": "text", "expr": "Although these assumptions are convenient, samples from these generative models tend to be noisy and lose important details to make these audio signals sounding natural", "word_idx": 62972, "sentence_idx": 1098, "label": "unlabeled"}, {"type": "text", "expr": "WaveNet, which was described in Section\u00a0 2 , has none of the above-mentioned assumptions", "word_idx": 63140, "sentence_idx": 1099, "label": "unlabeled"}, {"type": "text", "expr": "\nIt incorporates almost no prior knowledge about audio signals, except the choice of the receptive field and  $\\mu$ -law encoding of the signal", "word_idx": 63228, "sentence_idx": 1100, "label": "unlabeled"}, {"type": "text", "expr": "\nIt can also be viewed as a non-linear causal filter for quantized signals", "word_idx": 63371, "sentence_idx": 1101, "label": "unlabeled"}, {"type": "text", "expr": "\nAlthough such non-linear filter can represent complicated signals while preserving the details, designing such filters is usually difficult  ", "word_idx": 63445, "sentence_idx": 1102, "label": "unlabeled"}, {"type": "text", "expr": "\nWaveNets give a way to train them from data", "word_idx": 63587, "sentence_idx": 1103, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mu$$", "word_idx": 63631, "sentence_idx": 1104, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0B  Details of TTS Experiment", "word_idx": 63634, "sentence_idx": 1105, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0B", "word_idx": 63671, "sentence_idx": 1106, "label": "unlabeled"}, {"type": "text", "expr": "The HMM-driven unit selection and WaveNet TTS systems were built from speech at 16 kHz sampling", "word_idx": 63681, "sentence_idx": 1107, "label": "unlabeled"}, {"type": "text", "expr": "\nAlthough LSTM-RNNs were trained from speech at 22", "word_idx": 63776, "sentence_idx": 1108, "label": "unlabeled"}, {"type": "text", "expr": "05 kHz sampling,\nspeech at 16 kHz sampling was synthesized at runtime using a resampling functionality in the Vocaine vocoder  ", "word_idx": 63826, "sentence_idx": 1109, "label": "unlabeled"}, {"type": "text", "expr": "\nBoth the LSTM-RNN-based statistical parametric and HMM-driven unit selection speech synthesizers were built from the speech datasets in the 16-bit linear PCM, whereas the WaveNet-based ones were trained from the same speech datasets in the 8-bit  $\\mu$ -law encoding", "word_idx": 63953, "sentence_idx": 1110, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mu$$", "word_idx": 64220, "sentence_idx": 1111, "label": "unlabeled"}, {"type": "text", "expr": "The linguistic features include phone, syllable, word, phrase, and utterance-level features   (e", "word_idx": 64223, "sentence_idx": 1112, "label": "unlabeled"}, {"type": "text", "expr": " phone identities, syllable stress, the number of syllables in a word, and position of the current syllable\nin a phrase) with additional frame position and phone duration features  ", "word_idx": 64319, "sentence_idx": 1113, "label": "unlabeled"}, {"type": "text", "expr": " These features were derived and associated with speech every 5 milliseconds by phone-level forced alignment at the training stage", "word_idx": 64500, "sentence_idx": 1114, "label": "unlabeled"}, {"type": "text", "expr": "\nWe used LSTM-RNN-based phone duration and autoregressive CNN-based  $\\log F_{0}$  prediction models", "word_idx": 64630, "sentence_idx": 1115, "label": "unlabeled"}, {"type": "text", "expr": "\nThey were trained so as to minimize the mean squared errors (MSE)", "word_idx": 64730, "sentence_idx": 1116, "label": "unlabeled"}, {"type": "text", "expr": "\nIt is important to note that no post-processing was applied to the audio signals generated from the WaveNets", "word_idx": 64796, "sentence_idx": 1117, "label": "unlabeled"}, {"type": "math", "expr": "$$\\log F_{0}$$", "word_idx": 64905, "sentence_idx": 1118, "label": "unlabeled"}, {"type": "text", "expr": "The subjective listening tests were blind and crowdsourced", "word_idx": 64915, "sentence_idx": 1119, "label": "unlabeled"}, {"type": "text", "expr": "\n100 sentences not included in the training data were used for evaluation", "word_idx": 64973, "sentence_idx": 1120, "label": "unlabeled"}, {"type": "text", "expr": "\nEach subject could evaluate up to 8 and 63 stimuli for North American English and Mandarin Chinese, respectively", "word_idx": 65046, "sentence_idx": 1121, "label": "unlabeled"}, {"type": "text", "expr": "\nTest stimuli were randomly chosen and presented for each subject", "word_idx": 65159, "sentence_idx": 1122, "label": "unlabeled"}, {"type": "text", "expr": "\nIn the paired comparison test, each pair of speech samples was the same text synthesized by the different models", "word_idx": 65224, "sentence_idx": 1123, "label": "unlabeled"}, {"type": "text", "expr": "\nIn the MOS test, each stimulus was presented to subjects in isolation", "word_idx": 65337, "sentence_idx": 1124, "label": "unlabeled"}, {"type": "text", "expr": "\nEach pair was evaluated by eight subjects in the paired comparison test, and each stimulus was evaluated by eight subjects in the MOS test", "word_idx": 65407, "sentence_idx": 1125, "label": "unlabeled"}, {"type": "text", "expr": "\nThe subjects were paid and native speakers performing the task", "word_idx": 65546, "sentence_idx": 1126, "label": "unlabeled"}, {"type": "text", "expr": "\nThose ratings (about 40%) where headphones were not used were excluded when computing the preference and mean opinion scores", "word_idx": 65609, "sentence_idx": 1127, "label": "unlabeled"}, {"type": "text", "expr": "\nTable\u00a0 2  shows the full details of the paired comparison test shown in Fig", "word_idx": 65734, "sentence_idx": 1128, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash Subjective preference (%) in naturalness \\arraybackslash", "word_idx": 65810, "sentence_idx": 1129, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 65882, "sentence_idx": 1130, "label": "unlabeled"}, {"type": "text", "expr": "Subjective preference (%) in naturalness", "word_idx": 65897, "sentence_idx": 1131, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 65937, "sentence_idx": 1132, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\arraybackslash \\arraybackslash \\arraybackslash", "word_idx": 65952, "sentence_idx": 1133, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66015, "sentence_idx": 1134, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66030, "sentence_idx": 1135, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66045, "sentence_idx": 1136, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66060, "sentence_idx": 1137, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66075, "sentence_idx": 1138, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash WaveNet", "word_idx": 66090, "sentence_idx": 1139, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66113, "sentence_idx": 1140, "label": "unlabeled"}, {"type": "text", "expr": "WaveNet", "word_idx": 66128, "sentence_idx": 1141, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash WaveNet", "word_idx": 66135, "sentence_idx": 1142, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66158, "sentence_idx": 1143, "label": "unlabeled"}, {"type": "text", "expr": "WaveNet", "word_idx": 66173, "sentence_idx": 1144, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash No", "word_idx": 66180, "sentence_idx": 1145, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66198, "sentence_idx": 1146, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66213, "sentence_idx": 1147, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66228, "sentence_idx": 1148, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66243, "sentence_idx": 1149, "label": "unlabeled"}, {"type": "text", "expr": "Language", "word_idx": 66258, "sentence_idx": 1150, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash LSTM", "word_idx": 66266, "sentence_idx": 1151, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66286, "sentence_idx": 1152, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash Concat", "word_idx": 66301, "sentence_idx": 1153, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66323, "sentence_idx": 1154, "label": "unlabeled"}, {"type": "text", "expr": "Concat", "word_idx": 66338, "sentence_idx": 1155, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash (L)", "word_idx": 66344, "sentence_idx": 1156, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66363, "sentence_idx": 1157, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash (L+F)", "word_idx": 66378, "sentence_idx": 1158, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66399, "sentence_idx": 1159, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash preference", "word_idx": 66414, "sentence_idx": 1160, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66440, "sentence_idx": 1161, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash $p$  value", "word_idx": 66455, "sentence_idx": 1162, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66481, "sentence_idx": 1163, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\arraybackslash", "word_idx": 66496, "sentence_idx": 1164, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66527, "sentence_idx": 1165, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 23", "word_idx": 66542, "sentence_idx": 1166, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66560, "sentence_idx": 1167, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 63", "word_idx": 66575, "sentence_idx": 1168, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66593, "sentence_idx": 1169, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66608, "sentence_idx": 1170, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66623, "sentence_idx": 1171, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66638, "sentence_idx": 1172, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66653, "sentence_idx": 1173, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 13", "word_idx": 66668, "sentence_idx": 1174, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66686, "sentence_idx": 1175, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash $\\ll 10^{-9}$", "word_idx": 66701, "sentence_idx": 1176, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66730, "sentence_idx": 1177, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ll 10^{-9}$$", "word_idx": 66745, "sentence_idx": 1178, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\arraybackslash", "word_idx": 66756, "sentence_idx": 1179, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66787, "sentence_idx": 1180, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 18", "word_idx": 66802, "sentence_idx": 1181, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66820, "sentence_idx": 1182, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66835, "sentence_idx": 1183, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66850, "sentence_idx": 1184, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 69", "word_idx": 66865, "sentence_idx": 1185, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66883, "sentence_idx": 1186, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66898, "sentence_idx": 1187, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66913, "sentence_idx": 1188, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 12", "word_idx": 66928, "sentence_idx": 1189, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66946, "sentence_idx": 1190, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash $\\ll 10^{-9}$", "word_idx": 66961, "sentence_idx": 1191, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 66990, "sentence_idx": 1192, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ll 10^{-9}$$", "word_idx": 67005, "sentence_idx": 1193, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\arraybackslash", "word_idx": 67016, "sentence_idx": 1194, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67047, "sentence_idx": 1195, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 7", "word_idx": 67062, "sentence_idx": 1196, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67079, "sentence_idx": 1197, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67094, "sentence_idx": 1198, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67109, "sentence_idx": 1199, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67124, "sentence_idx": 1200, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67139, "sentence_idx": 1201, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 82", "word_idx": 67154, "sentence_idx": 1202, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67172, "sentence_idx": 1203, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 10", "word_idx": 67187, "sentence_idx": 1204, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67205, "sentence_idx": 1205, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash $\\ll 10^{-9}$", "word_idx": 67220, "sentence_idx": 1206, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67249, "sentence_idx": 1207, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ll 10^{-9}$$", "word_idx": 67264, "sentence_idx": 1208, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\arraybackslash \\arraybackslash", "word_idx": 67275, "sentence_idx": 1209, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67322, "sentence_idx": 1210, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67337, "sentence_idx": 1211, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67352, "sentence_idx": 1212, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 32", "word_idx": 67367, "sentence_idx": 1213, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67385, "sentence_idx": 1214, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 41", "word_idx": 67400, "sentence_idx": 1215, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67418, "sentence_idx": 1216, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67433, "sentence_idx": 1217, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67448, "sentence_idx": 1218, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 26", "word_idx": 67463, "sentence_idx": 1219, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67481, "sentence_idx": 1220, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash $0003$", "word_idx": 67496, "sentence_idx": 1221, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67518, "sentence_idx": 1222, "label": "unlabeled"}, {"type": "math", "expr": "$$0.003$$", "word_idx": 67533, "sentence_idx": 1223, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\arraybackslash \\arraybackslash", "word_idx": 67538, "sentence_idx": 1224, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67585, "sentence_idx": 1225, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67600, "sentence_idx": 1226, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67615, "sentence_idx": 1227, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 20", "word_idx": 67630, "sentence_idx": 1228, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67648, "sentence_idx": 1229, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67663, "sentence_idx": 1230, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67678, "sentence_idx": 1231, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 49", "word_idx": 67693, "sentence_idx": 1232, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67711, "sentence_idx": 1233, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 30", "word_idx": 67726, "sentence_idx": 1234, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67744, "sentence_idx": 1235, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash $\\ll 10^{-9}$", "word_idx": 67759, "sentence_idx": 1236, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67788, "sentence_idx": 1237, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ll 10^{-9}$$", "word_idx": 67803, "sentence_idx": 1238, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\arraybackslash \\arraybackslash", "word_idx": 67814, "sentence_idx": 1239, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67861, "sentence_idx": 1240, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67876, "sentence_idx": 1241, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67891, "sentence_idx": 1242, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67906, "sentence_idx": 1243, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67921, "sentence_idx": 1244, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 17", "word_idx": 67936, "sentence_idx": 1245, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67954, "sentence_idx": 1246, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 37", "word_idx": 67969, "sentence_idx": 1247, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 67987, "sentence_idx": 1248, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 44", "word_idx": 68002, "sentence_idx": 1249, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68020, "sentence_idx": 1250, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash $\\ll 10^{-9}$", "word_idx": 68035, "sentence_idx": 1251, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68064, "sentence_idx": 1252, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ll 10^{-9}$$", "word_idx": 68079, "sentence_idx": 1253, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\arraybackslash", "word_idx": 68090, "sentence_idx": 1254, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68121, "sentence_idx": 1255, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 50", "word_idx": 68136, "sentence_idx": 1256, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68154, "sentence_idx": 1257, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 15", "word_idx": 68169, "sentence_idx": 1258, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68187, "sentence_idx": 1259, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68202, "sentence_idx": 1260, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68217, "sentence_idx": 1261, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68232, "sentence_idx": 1262, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68247, "sentence_idx": 1263, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 33", "word_idx": 68262, "sentence_idx": 1264, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68280, "sentence_idx": 1265, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash $\\ll 10^{-9}$", "word_idx": 68295, "sentence_idx": 1266, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68324, "sentence_idx": 1267, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ll 10^{-9}$$", "word_idx": 68339, "sentence_idx": 1268, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\arraybackslash", "word_idx": 68350, "sentence_idx": 1269, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68381, "sentence_idx": 1270, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 25", "word_idx": 68396, "sentence_idx": 1271, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68414, "sentence_idx": 1272, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68429, "sentence_idx": 1273, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68444, "sentence_idx": 1274, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 23", "word_idx": 68459, "sentence_idx": 1275, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68477, "sentence_idx": 1276, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68492, "sentence_idx": 1277, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68507, "sentence_idx": 1278, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 51", "word_idx": 68522, "sentence_idx": 1279, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68540, "sentence_idx": 1280, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 0", "word_idx": 68555, "sentence_idx": 1281, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68572, "sentence_idx": 1282, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\arraybackslash \\arraybackslash", "word_idx": 68587, "sentence_idx": 1283, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68634, "sentence_idx": 1284, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 12", "word_idx": 68649, "sentence_idx": 1285, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68667, "sentence_idx": 1286, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68682, "sentence_idx": 1287, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68697, "sentence_idx": 1288, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68712, "sentence_idx": 1289, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68727, "sentence_idx": 1290, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 29", "word_idx": 68742, "sentence_idx": 1291, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68760, "sentence_idx": 1292, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 58", "word_idx": 68775, "sentence_idx": 1293, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68793, "sentence_idx": 1294, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash $\\ll 10^{-9}$", "word_idx": 68808, "sentence_idx": 1295, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68837, "sentence_idx": 1296, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ll 10^{-9}$$", "word_idx": 68852, "sentence_idx": 1297, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\arraybackslash \\arraybackslash", "word_idx": 68863, "sentence_idx": 1298, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68910, "sentence_idx": 1299, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68925, "sentence_idx": 1300, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68940, "sentence_idx": 1301, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 17", "word_idx": 68955, "sentence_idx": 1302, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 68973, "sentence_idx": 1303, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 43", "word_idx": 68988, "sentence_idx": 1304, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 69006, "sentence_idx": 1305, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 69021, "sentence_idx": 1306, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 69036, "sentence_idx": 1307, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 39", "word_idx": 69051, "sentence_idx": 1308, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 69069, "sentence_idx": 1309, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash $\\ll 10^{-9}$", "word_idx": 69084, "sentence_idx": 1310, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 69113, "sentence_idx": 1311, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ll 10^{-9}$$", "word_idx": 69128, "sentence_idx": 1312, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\arraybackslash \\arraybackslash", "word_idx": 69139, "sentence_idx": 1313, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 69186, "sentence_idx": 1314, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 69201, "sentence_idx": 1315, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 69216, "sentence_idx": 1316, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 7", "word_idx": 69231, "sentence_idx": 1317, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 69248, "sentence_idx": 1318, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 69263, "sentence_idx": 1319, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 69278, "sentence_idx": 1320, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 55", "word_idx": 69293, "sentence_idx": 1321, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 69311, "sentence_idx": 1322, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 36", "word_idx": 69326, "sentence_idx": 1323, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 69344, "sentence_idx": 1324, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash $\\ll 10^{-9}$", "word_idx": 69359, "sentence_idx": 1325, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 69388, "sentence_idx": 1326, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ll 10^{-9}$$", "word_idx": 69403, "sentence_idx": 1327, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash \\arraybackslash \\arraybackslash", "word_idx": 69414, "sentence_idx": 1328, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 69461, "sentence_idx": 1329, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 69476, "sentence_idx": 1330, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 69491, "sentence_idx": 1331, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 69506, "sentence_idx": 1332, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 69521, "sentence_idx": 1333, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 10", "word_idx": 69536, "sentence_idx": 1334, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 69554, "sentence_idx": 1335, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 25", "word_idx": 69569, "sentence_idx": 1336, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 69587, "sentence_idx": 1337, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash 64", "word_idx": 69602, "sentence_idx": 1338, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 69620, "sentence_idx": 1339, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash $\\ll 10^{-9}$", "word_idx": 69635, "sentence_idx": 1340, "label": "unlabeled"}, {"type": "text", "expr": "\\arraybackslash", "word_idx": 69664, "sentence_idx": 1341, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ll 10^{-9}$$", "word_idx": 69679, "sentence_idx": 1342, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:  Subjective preference scores of speech samples between LSTM-RNN-based statistical parametric ( LSTM ), HMM-driven unit selection concatenative ( Concat ), and proposed WaveNet-based speech synthesizers", "word_idx": 69690, "sentence_idx": 1343, "label": "unlabeled"}, {"type": "text", "expr": "\nEach row of the table denotes scores of a paired comparison test between two synthesizers", "word_idx": 69901, "sentence_idx": 1344, "label": "unlabeled"}, {"type": "text", "expr": "\nScores of the synthesizers which were significantly better than their competing ones at  $p<001$  level were shown in the bold type", "word_idx": 69991, "sentence_idx": 1345, "label": "unlabeled"}, {"type": "text", "expr": "\nNote that  WaveNet  (L) and  WaveNet  (L+F) correspond to WaveNet conditioned on linguistic features only and that conditioned on both linguistic features and  $F_{0}$  values", "word_idx": 70123, "sentence_idx": 1346, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:", "word_idx": 70299, "sentence_idx": 1347, "label": "unlabeled"}, {"type": "text", "expr": "Concat", "word_idx": 70307, "sentence_idx": 1348, "label": "unlabeled"}, {"type": "math", "expr": "$$p<0.01$$", "word_idx": 70313, "sentence_idx": 1349, "label": "unlabeled"}, {"type": "text", "expr": "WaveNet", "word_idx": 70319, "sentence_idx": 1350, "label": "unlabeled"}, {"type": "text", "expr": "WaveNet", "word_idx": 70326, "sentence_idx": 1351, "label": "unlabeled"}, {"type": "math", "expr": "$$F_{0}$$", "word_idx": 70333, "sentence_idx": 1352, "label": "unlabeled"}, {"type": "text", "expr": "Generated  on Tue Apr  3 11:31:39 2018 by", "word_idx": 70338, "sentence_idx": 1353, "label": "unlabeled"}, {"type": "text", "expr": "LaTeXML", "word_idx": 70379, "sentence_idx": 1354, "label": "unlabeled"}], "faster_rcnn": [{"type": "text", "expr": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "word_idx": 0, "sentence_idx": 0, "label": "unlabeled"}, {"type": "text", "expr": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "word_idx": 78, "sentence_idx": 1, "label": "unlabeled"}, {"type": "text", "expr": "Shaoqing\u00a0Ren", "word_idx": 156, "sentence_idx": 2, "label": "unlabeled"}, {"type": "text", "expr": "Shaoqing\u00a0Ren", "word_idx": 168, "sentence_idx": 3, "label": "unlabeled"}, {"type": "text", "expr": "Kaiming\u00a0He", "word_idx": 180, "sentence_idx": 4, "label": "unlabeled"}, {"type": "text", "expr": "Kaiming\u00a0He", "word_idx": 190, "sentence_idx": 5, "label": "unlabeled"}, {"type": "text", "expr": "Ross Girshick", "word_idx": 200, "sentence_idx": 6, "label": "unlabeled"}, {"type": "text", "expr": "Ross Girshick", "word_idx": 213, "sentence_idx": 7, "label": "unlabeled"}, {"type": "text", "expr": "and\u00a0Jian\u00a0Sun \\IEEEcompsocitemizethanks \\IEEEcompsocthanksitem S", "word_idx": 226, "sentence_idx": 8, "label": "unlabeled"}, {"type": "text", "expr": " Ren is with University of Science and Technology of China, Hefei, China", "word_idx": 289, "sentence_idx": 9, "label": "unlabeled"}, {"type": "text", "expr": " This work was done when S", "word_idx": 361, "sentence_idx": 10, "label": "unlabeled"}, {"type": "text", "expr": " Ren was an intern at Microsoft Research", "word_idx": 387, "sentence_idx": 11, "label": "unlabeled"}, {"type": "text", "expr": " Email: sqren@mail", "word_idx": 427, "sentence_idx": 12, "label": "unlabeled"}, {"type": "text", "expr": "cn\n \\IEEEcompsocthanksitem K", "word_idx": 445, "sentence_idx": 13, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He and J", "word_idx": 473, "sentence_idx": 14, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sun are with Visual Computing Group, Microsoft Research", "word_idx": 482, "sentence_idx": 15, "label": "unlabeled"}, {"type": "text", "expr": " E-mail: {kahe,jiansun}@microsoft", "word_idx": 538, "sentence_idx": 16, "label": "unlabeled"}, {"type": "text", "expr": "com\n \\IEEEcompsocthanksitem R", "word_idx": 571, "sentence_idx": 17, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Girshick is with Facebook AI Research", "word_idx": 600, "sentence_idx": 18, "label": "unlabeled"}, {"type": "text", "expr": " The majority of this work was done when R", "word_idx": 638, "sentence_idx": 19, "label": "unlabeled"}, {"type": "text", "expr": " Girshick was with Microsoft Research", "word_idx": 680, "sentence_idx": 20, "label": "unlabeled"}, {"type": "text", "expr": " E-mail: rbg@fb", "word_idx": 717, "sentence_idx": 21, "label": "unlabeled"}, {"type": "text", "expr": "\\IEEEcompsocitemizethanks", "word_idx": 732, "sentence_idx": 22, "label": "unlabeled"}, {"type": "text", "expr": "\\IEEEcompsocthanksitem", "word_idx": 757, "sentence_idx": 23, "label": "unlabeled"}, {"type": "text", "expr": "\\IEEEcompsocthanksitem", "word_idx": 779, "sentence_idx": 24, "label": "unlabeled"}, {"type": "text", "expr": "\\IEEEcompsocthanksitem", "word_idx": 801, "sentence_idx": 25, "label": "unlabeled"}, {"type": "text", "expr": "Abstract", "word_idx": 823, "sentence_idx": 26, "label": "unlabeled"}, {"type": "text", "expr": "Abstract", "word_idx": 831, "sentence_idx": 27, "label": "unlabeled"}, {"type": "text", "expr": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations", "word_idx": 839, "sentence_idx": 28, "label": "unlabeled"}, {"type": "text", "expr": "\nAdvances like SPPnet   and Fast R-CNN   have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck", "word_idx": 950, "sentence_idx": 29, "label": "unlabeled"}, {"type": "text", "expr": "\nIn this work, we introduce a  Region Proposal Network  (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals", "word_idx": 1102, "sentence_idx": 30, "label": "unlabeled"}, {"type": "text", "expr": " An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position", "word_idx": 1285, "sentence_idx": 31, "label": "unlabeled"}, {"type": "text", "expr": " The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection", "word_idx": 1407, "sentence_idx": 32, "label": "unlabeled"}, {"type": "text", "expr": "\nWe further merge RPN and Fast R-CNN into a single network by sharing their convolutional features\u2014using the recently popular terminology of neural networks with \u201cattention\u201d mechanisms, the RPN component tells the unified network where to look", "word_idx": 1523, "sentence_idx": 33, "label": "unlabeled"}, {"type": "text", "expr": "\nFor the very deep VGG-16 model  , our detection system has a frame rate of 5fps ( including all steps ) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image", "word_idx": 1766, "sentence_idx": 34, "label": "unlabeled"}, {"type": "text", "expr": " In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks", "word_idx": 2020, "sentence_idx": 35, "label": "unlabeled"}, {"type": "text", "expr": " Code has been made publicly available", "word_idx": 2150, "sentence_idx": 36, "label": "unlabeled"}, {"type": "text", "expr": "Region Proposal Network", "word_idx": 2188, "sentence_idx": 37, "label": "unlabeled"}, {"type": "text", "expr": "including all steps", "word_idx": 2211, "sentence_idx": 38, "label": "unlabeled"}, {"type": "text", "expr": "\\IEEEcompsoctitleabstractindextext", "word_idx": 2230, "sentence_idx": 39, "label": "unlabeled"}, {"type": "text", "expr": "bject Detection, Region Proposal, Convolutional Neural Network", "word_idx": 2264, "sentence_idx": 40, "label": "unlabeled"}, {"type": "text", "expr": "bject Detection, Region Proposal, Convolutional Neural Network", "word_idx": 2326, "sentence_idx": 41, "label": "unlabeled"}, {"type": "text", "expr": "\\IEEEpeerreviewmaketitle", "word_idx": 2388, "sentence_idx": 42, "label": "unlabeled"}, {"type": "text", "expr": "1  Introduction", "word_idx": 2412, "sentence_idx": 43, "label": "unlabeled"}, {"type": "text", "expr": "Recent advances in object detection are driven by the success of region proposal methods ( \\eg ,  ) and region-based convolutional neural networks (R-CNNs)  ", "word_idx": 2427, "sentence_idx": 44, "label": "unlabeled"}, {"type": "text", "expr": " Although region-based CNNs were computationally expensive as originally developed in  , their cost has been drastically reduced thanks to sharing convolutions across proposals  ", "word_idx": 2584, "sentence_idx": 45, "label": "unlabeled"}, {"type": "text", "expr": " The latest incarnation, Fast R-CNN  , achieves near real-time rates using very deep networks  ,  when ignoring the time spent on region proposals ", "word_idx": 2762, "sentence_idx": 46, "label": "unlabeled"}, {"type": "text", "expr": " Now, proposals are the test-time computational bottleneck in state-of-the-art detection systems", "word_idx": 2909, "sentence_idx": 47, "label": "unlabeled"}, {"type": "text", "expr": "when ignoring the time spent on region proposals", "word_idx": 3005, "sentence_idx": 48, "label": "unlabeled"}, {"type": "text", "expr": "Region proposal methods typically rely on inexpensive features and economical inference schemes", "word_idx": 3053, "sentence_idx": 49, "label": "unlabeled"}, {"type": "text", "expr": "\nSelective Search  , one of the most popular methods, greedily merges superpixels based on engineered low-level features", "word_idx": 3148, "sentence_idx": 50, "label": "unlabeled"}, {"type": "text", "expr": " Yet when compared to efficient detection networks  , Selective Search is an order of magnitude slower, at 2 seconds per image in a CPU implementation", "word_idx": 3268, "sentence_idx": 51, "label": "unlabeled"}, {"type": "text", "expr": "\nEdgeBoxes   currently provides the best tradeoff between proposal quality and speed, at 0", "word_idx": 3418, "sentence_idx": 52, "label": "unlabeled"}, {"type": "text", "expr": "2 seconds per image", "word_idx": 3508, "sentence_idx": 53, "label": "unlabeled"}, {"type": "text", "expr": " Nevertheless, the region proposal step still consumes as much running time as the detection network", "word_idx": 3527, "sentence_idx": 54, "label": "unlabeled"}, {"type": "text", "expr": "One may note that fast region-based CNNs take advantage of GPUs, while the region proposal methods used in research are implemented on the CPU, making such runtime comparisons inequitable", "word_idx": 3627, "sentence_idx": 55, "label": "unlabeled"}, {"type": "text", "expr": " An obvious way to accelerate proposal computation is to re-implement it for the GPU", "word_idx": 3814, "sentence_idx": 56, "label": "unlabeled"}, {"type": "text", "expr": " This may be an effective engineering solution, but re-implementation ignores the down-stream detection network and therefore misses important opportunities for sharing computation", "word_idx": 3898, "sentence_idx": 57, "label": "unlabeled"}, {"type": "text", "expr": "One may note that fast region-based CNNs take advantage of GPUs, while the region proposal methods used in research are implemented on the CPU, making such runtime comparisons inequitable", "word_idx": 4078, "sentence_idx": 58, "label": "unlabeled"}, {"type": "text", "expr": " An obvious way to accelerate proposal computation is to re-implement it for the GPU", "word_idx": 4265, "sentence_idx": 59, "label": "unlabeled"}, {"type": "text", "expr": " This may be an effective engineering solution, but re-implementation ignores the down-stream detection network and therefore misses important opportunities for sharing computation", "word_idx": 4349, "sentence_idx": 60, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:  Different schemes for addressing multiple scales and sizes", "word_idx": 4529, "sentence_idx": 61, "label": "unlabeled"}, {"type": "text", "expr": " (a) Pyramids of images and feature maps are built, and the classifier is run at all scales", "word_idx": 4598, "sentence_idx": 62, "label": "unlabeled"}, {"type": "text", "expr": " (b) Pyramids of filters with multiple scales/sizes are run on the feature map", "word_idx": 4689, "sentence_idx": 63, "label": "unlabeled"}, {"type": "text", "expr": " (c) We use pyramids of reference boxes in the regression functions", "word_idx": 4767, "sentence_idx": 64, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:", "word_idx": 4834, "sentence_idx": 65, "label": "unlabeled"}, {"type": "text", "expr": "In this paper, we show that an algorithmic change\u2014computing proposals with a deep convolutional neural network\u2014leads to an elegant and effective solution where proposal computation is nearly cost-free given the detection network\u2019s computation", "word_idx": 4843, "sentence_idx": 66, "label": "unlabeled"}, {"type": "text", "expr": "\nTo this end, we introduce novel  Region Proposal Networks  (RPNs) that share convolutional layers with state-of-the-art object detection networks  ", "word_idx": 5085, "sentence_idx": 67, "label": "unlabeled"}, {"type": "text", "expr": " By sharing convolutions at test-time, the marginal cost for computing proposals is small ( \\eg , 10ms per image)", "word_idx": 5233, "sentence_idx": 68, "label": "unlabeled"}, {"type": "text", "expr": "Region Proposal Networks", "word_idx": 5346, "sentence_idx": 69, "label": "unlabeled"}, {"type": "text", "expr": "Our observation is that the convolutional feature maps used by region-based detectors, like Fast R-CNN, can also be used for generating region proposals", "word_idx": 5370, "sentence_idx": 70, "label": "unlabeled"}, {"type": "text", "expr": "\nOn top of these convolutional features, we construct an RPN by adding a few additional convolutional layers that simultaneously regress region bounds and objectness scores at each location on a regular grid", "word_idx": 5522, "sentence_idx": 71, "label": "unlabeled"}, {"type": "text", "expr": "\nThe RPN is thus a kind of fully convolutional network (FCN)   and can be trained end-to-end specifically for the task for generating detection proposals", "word_idx": 5729, "sentence_idx": 72, "label": "unlabeled"}, {"type": "text", "expr": "RPNs are designed to efficiently predict region proposals with a wide range of scales and aspect ratios", "word_idx": 5882, "sentence_idx": 73, "label": "unlabeled"}, {"type": "text", "expr": " In contrast to prevalent methods   that use pyramids of images (Figure\u00a0 1 , a) or pyramids of filters (Figure\u00a0 1 , b), we introduce novel \u201canchor\u201d boxes that serve as references at multiple scales and aspect ratios", "word_idx": 5985, "sentence_idx": 74, "label": "unlabeled"}, {"type": "text", "expr": " Our scheme can be thought of as a pyramid of regression references (Figure\u00a0 1 , c), which avoids enumerating images or filters of multiple scales or aspect ratios", "word_idx": 6200, "sentence_idx": 75, "label": "unlabeled"}, {"type": "text", "expr": " This model performs well when trained and tested using single-scale images and thus benefits running speed", "word_idx": 6363, "sentence_idx": 76, "label": "unlabeled"}, {"type": "text", "expr": "To unify RPNs with Fast R-CNN   object detection networks, we propose a training scheme that alternates between fine-tuning for the region proposal task and then fine-tuning for object detection, while keeping the proposals fixed", "word_idx": 6470, "sentence_idx": 77, "label": "unlabeled"}, {"type": "text", "expr": "\nThis scheme converges quickly and produces a unified network with convolutional features that are shared between both tasks", "word_idx": 6699, "sentence_idx": 78, "label": "unlabeled"}, {"type": "text", "expr": "1 Since the publication of the conference version of this paper  , we have also found that RPNs can be trained jointly with Fast R-CNN networks leading to less training time", "word_idx": 6823, "sentence_idx": 79, "label": "unlabeled"}, {"type": "text", "expr": "We comprehensively evaluate our method on the PASCAL VOC detection benchmarks   where RPNs with Fast R-CNNs produce detection accuracy better than the strong baseline of Selective Search with Fast R-CNNs", "word_idx": 6996, "sentence_idx": 80, "label": "unlabeled"}, {"type": "text", "expr": " Meanwhile, our method waives nearly all computational burdens of Selective Search at test-time\u2014the effective running time for proposals is just 10 milliseconds", "word_idx": 7199, "sentence_idx": 81, "label": "unlabeled"}, {"type": "text", "expr": "\nUsing the expensive very deep models of  , our detection method still has a frame rate of 5fps ( including all steps ) on a GPU, and thus is a practical object detection system in terms of both speed and accuracy", "word_idx": 7359, "sentence_idx": 82, "label": "unlabeled"}, {"type": "text", "expr": "\nWe also report results on the MS COCO dataset   and investigate the improvements on PASCAL VOC using the COCO data", "word_idx": 7572, "sentence_idx": 83, "label": "unlabeled"}, {"type": "text", "expr": "\nCode has been made publicly available at  https://github", "word_idx": 7687, "sentence_idx": 84, "label": "unlabeled"}, {"type": "text", "expr": "com/shaoqingren/faster_rcnn  (in MATLAB) and  https://github", "word_idx": 7744, "sentence_idx": 85, "label": "unlabeled"}, {"type": "text", "expr": "com/rbgirshick/py-faster-rcnn  (in Python)", "word_idx": 7804, "sentence_idx": 86, "label": "unlabeled"}, {"type": "text", "expr": "including all steps", "word_idx": 7846, "sentence_idx": 87, "label": "unlabeled"}, {"type": "text", "expr": "https://github", "word_idx": 7865, "sentence_idx": 88, "label": "unlabeled"}, {"type": "text", "expr": "com/shaoqingren/faster_rcnn", "word_idx": 7879, "sentence_idx": 89, "label": "unlabeled"}, {"type": "text", "expr": "https://github", "word_idx": 7906, "sentence_idx": 90, "label": "unlabeled"}, {"type": "text", "expr": "com/rbgirshick/py-faster-rcnn", "word_idx": 7920, "sentence_idx": 91, "label": "unlabeled"}, {"type": "text", "expr": "A preliminary version of this manuscript was published previously  ", "word_idx": 7949, "sentence_idx": 92, "label": "unlabeled"}, {"type": "text", "expr": " Since then, the frameworks of RPN and Faster R-CNN have been adopted and generalized to other methods, such as 3D object detection  , part-based detection  , instance segmentation  , and image captioning  ", "word_idx": 8016, "sentence_idx": 93, "label": "unlabeled"}, {"type": "text", "expr": " Our fast and effective object detection system has also been built in commercial systems such as at Pinterests  , with user engagement improvements reported", "word_idx": 8222, "sentence_idx": 94, "label": "unlabeled"}, {"type": "text", "expr": "In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the basis of several 1st-place entries   in the tracks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation", "word_idx": 8379, "sentence_idx": 95, "label": "unlabeled"}, {"type": "text", "expr": " RPNs completely learn to propose regions from data, and thus can easily benefit from deeper and more expressive features (such as the 101-layer residual nets adopted in  )", "word_idx": 8580, "sentence_idx": 96, "label": "unlabeled"}, {"type": "text", "expr": " Faster R-CNN and RPN are also used by several other leading entries in these competitions ", "word_idx": 8752, "sentence_idx": 97, "label": "unlabeled"}, {"type": "text", "expr": " These results suggest that our method is not only a cost-efficient solution for practical usage, but also an effective way of improving object detection accuracy", "word_idx": 8843, "sentence_idx": 98, "label": "unlabeled"}, {"type": "text", "expr": "2 http://image-net", "word_idx": 9005, "sentence_idx": 99, "label": "unlabeled"}, {"type": "text", "expr": "org/challenges/LSVRC/2015/results", "word_idx": 9023, "sentence_idx": 100, "label": "unlabeled"}, {"type": "text", "expr": "http://image-net", "word_idx": 9056, "sentence_idx": 101, "label": "unlabeled"}, {"type": "text", "expr": "org/challenges/LSVRC/2015/results", "word_idx": 9072, "sentence_idx": 102, "label": "unlabeled"}, {"type": "text", "expr": "2  Related Work", "word_idx": 9105, "sentence_idx": 103, "label": "unlabeled"}, {"type": "text", "expr": "Object Proposals", "word_idx": 9120, "sentence_idx": 104, "label": "unlabeled"}, {"type": "text", "expr": "  There is a large literature on object proposal methods", "word_idx": 9136, "sentence_idx": 105, "label": "unlabeled"}, {"type": "text", "expr": " Comprehensive surveys and comparisons of object proposal methods can be found in  ", "word_idx": 9192, "sentence_idx": 106, "label": "unlabeled"}, {"type": "text", "expr": " Widely used object proposal methods include those based on grouping super-pixels ( \\eg , Selective Search  , CPMC  , MCG  ) and those based on sliding windows ( \\eg , objectness in windows  , EdgeBoxes  )", "word_idx": 9275, "sentence_idx": 107, "label": "unlabeled"}, {"type": "text", "expr": " Object proposal methods were adopted as external modules independent of the detectors ( \\eg , Selective Search   object detectors, R-CNN  , and Fast R-CNN  )", "word_idx": 9480, "sentence_idx": 108, "label": "unlabeled"}, {"type": "text", "expr": "Object Proposals", "word_idx": 9638, "sentence_idx": 109, "label": "unlabeled"}, {"type": "text", "expr": "Deep Networks for Object Detection", "word_idx": 9654, "sentence_idx": 110, "label": "unlabeled"}, {"type": "text", "expr": "  The R-CNN method   trains CNNs end-to-end to classify the proposal regions into object categories or background", "word_idx": 9688, "sentence_idx": 111, "label": "unlabeled"}, {"type": "text", "expr": " R-CNN mainly plays as a classifier, and it does not predict object bounds (except for refining by bounding box regression)", "word_idx": 9801, "sentence_idx": 112, "label": "unlabeled"}, {"type": "text", "expr": "\nIts accuracy depends on the performance of the region proposal module (see comparisons in  )", "word_idx": 9924, "sentence_idx": 113, "label": "unlabeled"}, {"type": "text", "expr": "\nSeveral papers have proposed ways of using deep networks for predicting object bounding boxes  ", "word_idx": 10017, "sentence_idx": 114, "label": "unlabeled"}, {"type": "text", "expr": "\nIn the OverFeat method  , a fully-connected layer is trained to predict the box coordinates for the localization task that assumes a single object", "word_idx": 10113, "sentence_idx": 115, "label": "unlabeled"}, {"type": "text", "expr": " The fully-connected layer is then turned into a convolutional layer for detecting multiple class-specific objects", "word_idx": 10260, "sentence_idx": 116, "label": "unlabeled"}, {"type": "text", "expr": " The MultiBox methods   generate region proposals from a network whose last fully-connected layer simultaneously predicts multiple class-agnostic boxes, generalizing the \u201csingle-box\u201d fashion of OverFeat", "word_idx": 10374, "sentence_idx": 117, "label": "unlabeled"}, {"type": "text", "expr": " These class-agnostic boxes are used as proposals for R-CNN  ", "word_idx": 10576, "sentence_idx": 118, "label": "unlabeled"}, {"type": "text", "expr": "\nThe MultiBox proposal network is applied on a single image crop or multiple large image crops ( \\eg , 224 $\\times$ 224), in contrast to our fully convolutional scheme", "word_idx": 10637, "sentence_idx": 119, "label": "unlabeled"}, {"type": "text", "expr": " MultiBox does not share features between the proposal and detection networks", "word_idx": 10804, "sentence_idx": 120, "label": "unlabeled"}, {"type": "text", "expr": "\nWe discuss OverFeat and MultiBox in more depth later in context with our method", "word_idx": 10881, "sentence_idx": 121, "label": "unlabeled"}, {"type": "text", "expr": "\nConcurrent with our work, the DeepMask method   is developed for learning segmentation proposals", "word_idx": 10961, "sentence_idx": 122, "label": "unlabeled"}, {"type": "text", "expr": "Deep Networks for Object Detection", "word_idx": 11058, "sentence_idx": 123, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 11092, "sentence_idx": 124, "label": "unlabeled"}, {"type": "text", "expr": "Shared computation of convolutions   has been attracting increasing attention for efficient, yet accurate, visual recognition", "word_idx": 11098, "sentence_idx": 125, "label": "unlabeled"}, {"type": "text", "expr": " The OverFeat paper   computes convolutional features from an image pyramid for classification, localization, and detection", "word_idx": 11223, "sentence_idx": 126, "label": "unlabeled"}, {"type": "text", "expr": " Adaptively-sized pooling (SPP)   on shared convolutional feature maps is developed for efficient region-based object detection   and semantic segmentation  ", "word_idx": 11346, "sentence_idx": 127, "label": "unlabeled"}, {"type": "text", "expr": " Fast R-CNN   enables end-to-end detector training on shared convolutional features and shows compelling accuracy and speed", "word_idx": 11503, "sentence_idx": 128, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:  Faster R-CNN is a single, unified network for object detection", "word_idx": 11626, "sentence_idx": 129, "label": "unlabeled"}, {"type": "text", "expr": " The RPN module serves as the \u2018attention\u2019 of this unified network", "word_idx": 11699, "sentence_idx": 130, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:", "word_idx": 11764, "sentence_idx": 131, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:  Left : Region Proposal Network (RPN)", "word_idx": 11773, "sentence_idx": 132, "label": "unlabeled"}, {"type": "text", "expr": "  Right : Example detections using RPN proposals on PASCAL VOC 2007 test", "word_idx": 11820, "sentence_idx": 133, "label": "unlabeled"}, {"type": "text", "expr": " Our method detects objects in a wide range of scales and aspect ratios", "word_idx": 11892, "sentence_idx": 134, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:", "word_idx": 11963, "sentence_idx": 135, "label": "unlabeled"}, {"type": "text", "expr": "Right", "word_idx": 11972, "sentence_idx": 136, "label": "unlabeled"}, {"type": "text", "expr": "3  Faster R-CNN", "word_idx": 11977, "sentence_idx": 137, "label": "unlabeled"}, {"type": "text", "expr": "Our object detection system, called Faster R-CNN, is composed of two modules", "word_idx": 11992, "sentence_idx": 138, "label": "unlabeled"}, {"type": "text", "expr": " The first module is a deep fully convolutional network that proposes regions, and the second module is the Fast R-CNN detector   that uses the proposed regions", "word_idx": 12068, "sentence_idx": 139, "label": "unlabeled"}, {"type": "text", "expr": " The entire system is a single, unified network for object detection (Figure\u00a0 2 )", "word_idx": 12228, "sentence_idx": 140, "label": "unlabeled"}, {"type": "text", "expr": "\nUsing the recently popular terminology of neural networks with \u2018attention\u2019   mechanisms, the RPN module tells the Fast R-CNN module where to look", "word_idx": 12309, "sentence_idx": 141, "label": "unlabeled"}, {"type": "text", "expr": "\nIn Section\u00a0 3", "word_idx": 12455, "sentence_idx": 142, "label": "unlabeled"}, {"type": "text", "expr": "1  we introduce the designs and properties of the network for region proposal", "word_idx": 12469, "sentence_idx": 143, "label": "unlabeled"}, {"type": "text", "expr": " In Section\u00a0 3", "word_idx": 12546, "sentence_idx": 144, "label": "unlabeled"}, {"type": "text", "expr": "2  we develop algorithms for training both modules with features shared", "word_idx": 12560, "sentence_idx": 145, "label": "unlabeled"}, {"type": "text", "expr": "1  Region Proposal Networks", "word_idx": 12631, "sentence_idx": 146, "label": "unlabeled"}, {"type": "text", "expr": "A Region Proposal Network (RPN) takes an image (of any size) as input and outputs a set of rectangular object proposals, each with an objectness score", "word_idx": 12658, "sentence_idx": 147, "label": "unlabeled"}, {"type": "text", "expr": " \nWe model this process with a fully convolutional network  , which we describe in this section", "word_idx": 12808, "sentence_idx": 148, "label": "unlabeled"}, {"type": "text", "expr": "\nBecause our ultimate goal is to share computation with a Fast R-CNN object detection network  , we assume that both nets share a common set of convolutional layers", "word_idx": 12903, "sentence_idx": 149, "label": "unlabeled"}, {"type": "text", "expr": "\nIn our experiments, we investigate the Zeiler and Fergus model   (ZF), which has 5 shareable convolutional layers and the Simonyan and Zisserman model   (VGG-16), which has 13 shareable convolutional layers", "word_idx": 13067, "sentence_idx": 150, "label": "unlabeled"}, {"type": "text", "expr": "3 \u201cRegion\u201d is a generic term and in this paper we only consider  rectangular  regions, as is common for many methods ( \\eg ,  )", "word_idx": 13274, "sentence_idx": 151, "label": "unlabeled"}, {"type": "text", "expr": " \u201cObjectness\u201d measures membership to a set of object classes  \\vs \u00a0background", "word_idx": 13401, "sentence_idx": 152, "label": "unlabeled"}, {"type": "text", "expr": "rectangular", "word_idx": 13478, "sentence_idx": 153, "label": "unlabeled"}, {"type": "text", "expr": "To generate region proposals, we slide a small network over the convolutional feature map output by the last shared convolutional layer", "word_idx": 13489, "sentence_idx": 154, "label": "unlabeled"}, {"type": "text", "expr": "\nThis small network takes as input an  $n\\times n$  spatial window of the input convolutional feature map", "word_idx": 13624, "sentence_idx": 155, "label": "unlabeled"}, {"type": "text", "expr": "\nEach sliding window is mapped to a lower-dimensional feature (256-d for ZF and 512-d for VGG, with ReLU   following)", "word_idx": 13729, "sentence_idx": 156, "label": "unlabeled"}, {"type": "text", "expr": "\nThis feature is fed into two sibling fully-connected layers\u2014a box-regression layer ( reg ) and a box-classification layer ( cls )", "word_idx": 13846, "sentence_idx": 157, "label": "unlabeled"}, {"type": "text", "expr": "\nWe use  $n=3$  in this paper, noting that the effective receptive field on the input image is large (171 and 228 pixels for ZF and VGG, respectively)", "word_idx": 13976, "sentence_idx": 158, "label": "unlabeled"}, {"type": "text", "expr": "\nThis mini-network is illustrated at a single position in Figure\u00a0 3  (left)", "word_idx": 14126, "sentence_idx": 159, "label": "unlabeled"}, {"type": "text", "expr": "\nNote that because the mini-network operates in a sliding-window fashion, the fully-connected layers are shared across all spatial locations", "word_idx": 14201, "sentence_idx": 160, "label": "unlabeled"}, {"type": "text", "expr": "\nThis architecture is naturally implemented with an  $n\\times n$  convolutional layer followed by two sibling  $1\\times 1$  convolutional layers (for  reg  and  cls , respectively)", "word_idx": 14341, "sentence_idx": 161, "label": "unlabeled"}, {"type": "math", "expr": "$$n\\times n$$", "word_idx": 14521, "sentence_idx": 162, "label": "unlabeled"}, {"type": "math", "expr": "$$n=3$$", "word_idx": 14530, "sentence_idx": 163, "label": "unlabeled"}, {"type": "math", "expr": "$$n\\times n$$", "word_idx": 14533, "sentence_idx": 164, "label": "unlabeled"}, {"type": "math", "expr": "$$1\\times 1$$", "word_idx": 14542, "sentence_idx": 165, "label": "unlabeled"}, {"type": "text", "expr": "1  Anchors", "word_idx": 14551, "sentence_idx": 166, "label": "unlabeled"}, {"type": "text", "expr": "At each sliding-window location, we simultaneously predict multiple region proposals, where the number of maximum possible proposals for each location is denoted as  $k$ ", "word_idx": 14561, "sentence_idx": 167, "label": "unlabeled"}, {"type": "text", "expr": "\nSo the  reg  layer has  $4k$  outputs encoding the coordinates of  $k$  boxes, and the  cls  layer outputs  $2k$  scores that estimate probability of object or not object for each proposal ", "word_idx": 14731, "sentence_idx": 168, "label": "unlabeled"}, {"type": "text", "expr": " The  $k$  proposals are parameterized  relative  to  $k$  reference boxes, which we call  anchors ", "word_idx": 14921, "sentence_idx": 169, "label": "unlabeled"}, {"type": "text", "expr": " An anchor is centered at the sliding window in question, and is associated with a scale and aspect ratio (Figure\u00a0 3 , left)", "word_idx": 15020, "sentence_idx": 170, "label": "unlabeled"}, {"type": "text", "expr": " By default we use 3 scales and 3 aspect ratios, yielding  $k=9$  anchors at each sliding position", "word_idx": 15144, "sentence_idx": 171, "label": "unlabeled"}, {"type": "text", "expr": " For a convolutional feature map of a size  $W\\times H$  (typically  $\\sim$ 2,400), there are  $WHk$  anchors in total", "word_idx": 15242, "sentence_idx": 172, "label": "unlabeled"}, {"type": "text", "expr": "4 For simplicity we implement the  cls  layer as a two-class softmax layer", "word_idx": 15360, "sentence_idx": 173, "label": "unlabeled"}, {"type": "text", "expr": " Alternatively, one may use logistic regression to produce  $k$  scores", "word_idx": 15434, "sentence_idx": 174, "label": "unlabeled"}, {"type": "text", "expr": "relative", "word_idx": 15505, "sentence_idx": 175, "label": "unlabeled"}, {"type": "text", "expr": "anchors", "word_idx": 15513, "sentence_idx": 176, "label": "unlabeled"}, {"type": "math", "expr": "$$k=9$$", "word_idx": 15520, "sentence_idx": 177, "label": "unlabeled"}, {"type": "math", "expr": "$$W\\times H$$", "word_idx": 15523, "sentence_idx": 178, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sim$$", "word_idx": 15532, "sentence_idx": 179, "label": "unlabeled"}, {"type": "math", "expr": "$$WHk$$", "word_idx": 15536, "sentence_idx": 180, "label": "unlabeled"}, {"type": "text", "expr": "Translation-Invariant Anchors", "word_idx": 15539, "sentence_idx": 181, "label": "unlabeled"}, {"type": "text", "expr": "Translation-Invariant Anchors", "word_idx": 15568, "sentence_idx": 182, "label": "unlabeled"}, {"type": "text", "expr": "Translation-Invariant Anchors", "word_idx": 15597, "sentence_idx": 183, "label": "unlabeled"}, {"type": "text", "expr": "An important property of our approach is that it is  translation invariant , both in terms of the anchors and the functions that compute proposals relative to the anchors", "word_idx": 15626, "sentence_idx": 184, "label": "unlabeled"}, {"type": "text", "expr": "\nIf one translates an object in an image, the proposal should translate and the same function should be able to predict the proposal in either location", "word_idx": 15796, "sentence_idx": 185, "label": "unlabeled"}, {"type": "text", "expr": " This translation-invariant property is guaranteed by our method ", "word_idx": 15947, "sentence_idx": 186, "label": "unlabeled"}, {"type": "text", "expr": "\nAs a comparison, the MultiBox method   uses k-means to generate 800 anchors, which are  not  translation invariant", "word_idx": 16012, "sentence_idx": 187, "label": "unlabeled"}, {"type": "text", "expr": " So MultiBox does not guarantee that the same proposal is generated if an object is translated", "word_idx": 16127, "sentence_idx": 188, "label": "unlabeled"}, {"type": "text", "expr": "translation invariant", "word_idx": 16221, "sentence_idx": 189, "label": "unlabeled"}, {"type": "text", "expr": "5 As is the case of FCNs  , our network is translation invariant up to the network\u2019s total stride", "word_idx": 16242, "sentence_idx": 190, "label": "unlabeled"}, {"type": "text", "expr": "The translation-invariant property also reduces the model size", "word_idx": 16339, "sentence_idx": 191, "label": "unlabeled"}, {"type": "text", "expr": "\nMultiBox has a  $(4+1)\\times 800$ -dimensional fully-connected output layer, whereas our method has a  $(4+2)\\times 9$ -dimensional convolutional output layer in the case of  $k=9$  anchors", "word_idx": 16401, "sentence_idx": 192, "label": "unlabeled"}, {"type": "text", "expr": " As a result, our output layer has  $28\\times 10^{4}$  parameters ( $512\\times(4+2)\\times 9$  for VGG-16), two orders of magnitude fewer than MultiBox\u2019s output layer that has  $61\\times 10^{6}$  parameters ( $1536\\times(4+1)\\times 800$  for GoogleNet   in MultiBox  )", "word_idx": 16591, "sentence_idx": 193, "label": "unlabeled"}, {"type": "text", "expr": "\nIf considering the feature projection layers, our proposal layers still have an order of magnitude fewer parameters than MultiBox ", "word_idx": 16858, "sentence_idx": 194, "label": "unlabeled"}, {"type": "text", "expr": " We expect our method to have less risk of overfitting on small datasets, like PASCAL VOC", "word_idx": 16989, "sentence_idx": 195, "label": "unlabeled"}, {"type": "math", "expr": "$$(4+1)\\times 800$$", "word_idx": 17078, "sentence_idx": 196, "label": "unlabeled"}, {"type": "math", "expr": "$$(4+2)\\times 9$$", "word_idx": 17093, "sentence_idx": 197, "label": "unlabeled"}, {"type": "math", "expr": "$$k=9$$", "word_idx": 17106, "sentence_idx": 198, "label": "unlabeled"}, {"type": "math", "expr": "$$2.8\\times 10^{4}$$", "word_idx": 17109, "sentence_idx": 199, "label": "unlabeled"}, {"type": "math", "expr": "$$512\\times(4+2)\\times 9$$", "word_idx": 17125, "sentence_idx": 200, "label": "unlabeled"}, {"type": "math", "expr": "$$6.1\\times 10^{6}$$", "word_idx": 17147, "sentence_idx": 201, "label": "unlabeled"}, {"type": "math", "expr": "$$1536\\times(4+1)\\times 800$$", "word_idx": 17163, "sentence_idx": 202, "label": "unlabeled"}, {"type": "text", "expr": "6 Considering the feature projection layers, our proposal layers\u2019 parameter count is  $3\\times 3\\times 512\\times 512+512\\times 6\\times 9=24\\times 10^{6}$ ; MultiBox\u2019s proposal layers\u2019 parameter count is  $7\\times 7\\times(64+96+64+64)\\times 1536+1536\\times 5\\times 800=27\\times 10^{6}$ ", "word_idx": 17188, "sentence_idx": 203, "label": "unlabeled"}, {"type": "math", "expr": "$$3\\times 3\\times 512\\times 512+512\\times 6\\times 9=2.4\\times 10^{6}$$", "word_idx": 17473, "sentence_idx": 204, "label": "unlabeled"}, {"type": "math", "expr": "$$7\\times 7\\times(64+96+64+64)\\times 1536+1536\\times 5\\times 800=27\\times 10^{6}$$", "word_idx": 17539, "sentence_idx": 205, "label": "unlabeled"}, {"type": "text", "expr": "Multi-Scale Anchors as Regression References", "word_idx": 17617, "sentence_idx": 206, "label": "unlabeled"}, {"type": "text", "expr": "Multi-Scale Anchors as Regression References", "word_idx": 17661, "sentence_idx": 207, "label": "unlabeled"}, {"type": "text", "expr": "Multi-Scale Anchors as Regression References", "word_idx": 17705, "sentence_idx": 208, "label": "unlabeled"}, {"type": "text", "expr": "Our design of anchors presents a novel scheme for addressing multiple scales (and aspect ratios)", "word_idx": 17749, "sentence_idx": 209, "label": "unlabeled"}, {"type": "text", "expr": "\nAs shown in Figure\u00a0 1 , there have been two popular ways for multi-scale predictions", "word_idx": 17845, "sentence_idx": 210, "label": "unlabeled"}, {"type": "text", "expr": " The first way is based on image/feature pyramids,  \\eg , in DPM   and CNN-based methods  ", "word_idx": 17930, "sentence_idx": 211, "label": "unlabeled"}, {"type": "text", "expr": " The images are resized at multiple scales, and feature maps (HOG   or deep convolutional features  ) are computed for each scale (Figure\u00a0 1 (a))", "word_idx": 18020, "sentence_idx": 212, "label": "unlabeled"}, {"type": "text", "expr": " This way is often useful but is time-consuming", "word_idx": 18165, "sentence_idx": 213, "label": "unlabeled"}, {"type": "text", "expr": " The second way is to use sliding windows of multiple scales (and/or aspect ratios) on the feature maps", "word_idx": 18212, "sentence_idx": 214, "label": "unlabeled"}, {"type": "text", "expr": " For example, in DPM  , models of different aspect ratios are trained separately using different filter sizes (such as 5 $\\times$ 7 and 7 $\\times$ 5)", "word_idx": 18315, "sentence_idx": 215, "label": "unlabeled"}, {"type": "text", "expr": " If this way is used to address multiple scales, it can be thought of as a \u201cpyramid of filters\u201d (Figure\u00a0 1 (b))", "word_idx": 18464, "sentence_idx": 216, "label": "unlabeled"}, {"type": "text", "expr": " The second way is usually adopted jointly with the first way  ", "word_idx": 18575, "sentence_idx": 217, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 18638, "sentence_idx": 218, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 18644, "sentence_idx": 219, "label": "unlabeled"}, {"type": "text", "expr": "As a comparison, our anchor-based method is built on  a pyramid of anchors , which is more cost-efficient", "word_idx": 18650, "sentence_idx": 220, "label": "unlabeled"}, {"type": "text", "expr": " Our method classifies and regresses bounding boxes with reference to anchor boxes of multiple scales and aspect ratios", "word_idx": 18755, "sentence_idx": 221, "label": "unlabeled"}, {"type": "text", "expr": "\nIt only relies on images and feature maps of a single scale, and uses filters (sliding windows on the feature map) of a single size", "word_idx": 18874, "sentence_idx": 222, "label": "unlabeled"}, {"type": "text", "expr": " We show by experiments the effects of this scheme for addressing multiple scales and sizes (Table\u00a0 8 )", "word_idx": 19006, "sentence_idx": 223, "label": "unlabeled"}, {"type": "text", "expr": "a pyramid of anchors", "word_idx": 19109, "sentence_idx": 224, "label": "unlabeled"}, {"type": "text", "expr": "Because of this multi-scale design based on anchors, we can simply use the convolutional features computed on a single-scale image, as is also done by the Fast R-CNN detector  ", "word_idx": 19129, "sentence_idx": 225, "label": "unlabeled"}, {"type": "text", "expr": " The design of multi-scale anchors is a key component for sharing features without extra cost for addressing scales", "word_idx": 19305, "sentence_idx": 226, "label": "unlabeled"}, {"type": "text", "expr": "2  Loss Function", "word_idx": 19420, "sentence_idx": 227, "label": "unlabeled"}, {"type": "text", "expr": "For training RPNs, we assign a binary class label (of being an object or not) to each anchor", "word_idx": 19436, "sentence_idx": 228, "label": "unlabeled"}, {"type": "text", "expr": "\nWe assign a positive label to two kinds of anchors: (i) the anchor/anchors with the highest Intersection-over-Union (IoU) overlap with a ground-truth box,  or  (ii) an anchor that has an IoU overlap higher than 0", "word_idx": 19528, "sentence_idx": 229, "label": "unlabeled"}, {"type": "text", "expr": "7 with any ground-truth box", "word_idx": 19741, "sentence_idx": 230, "label": "unlabeled"}, {"type": "text", "expr": "\nNote that a single ground-truth box may assign positive labels to multiple anchors", "word_idx": 19768, "sentence_idx": 231, "label": "unlabeled"}, {"type": "text", "expr": " Usually the second condition is sufficient to determine the positive samples; but we still adopt the first condition for the reason that in some rare cases the second condition may find no positive sample", "word_idx": 19851, "sentence_idx": 232, "label": "unlabeled"}, {"type": "text", "expr": "\nWe assign a negative label to a non-positive anchor if its IoU ratio is lower than 0", "word_idx": 20056, "sentence_idx": 233, "label": "unlabeled"}, {"type": "text", "expr": "3 for all ground-truth boxes", "word_idx": 20141, "sentence_idx": 234, "label": "unlabeled"}, {"type": "text", "expr": "\nAnchors that are neither positive nor negative do not contribute to the training objective", "word_idx": 20169, "sentence_idx": 235, "label": "unlabeled"}, {"type": "text", "expr": "With these definitions, we minimize an objective function following the multi-task loss in Fast R-CNN  ", "word_idx": 20260, "sentence_idx": 236, "label": "unlabeled"}, {"type": "text", "expr": " Our loss function for an image is defined as:", "word_idx": 20363, "sentence_idx": 237, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle L(\\{p_{i}\\},\\{t_{i}\\})=\\frac{1}{N_{\\mathit{cls}}}\\sum_{i}L_{%\n\\mathit{cls}}(p_{i},p^{*}_{i})$", "word_idx": 20409, "sentence_idx": 238, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle L(\\{p_{i}\\},\\{t_{i}\\})=\\frac{1}{N_{\\mathit{cls}}}\\sum_{i}L_{%\n\\mathit{cls}}(p_{i},p^{*}_{i})$$", "word_idx": 20517, "sentence_idx": 239, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle+\\lambda\\frac{1}{N_{\\mathit{reg}}}\\sum_{i}p^{*}_{i}L_{\\mathit{reg%\n}}(t_{i},t^{*}_{i})$", "word_idx": 20623, "sentence_idx": 240, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle+\\lambda\\frac{1}{N_{\\mathit{reg}}}\\sum_{i}p^{*}_{i}L_{\\mathit{reg%\n}}(t_{i},t^{*}_{i}).$$", "word_idx": 20724, "sentence_idx": 241, "label": "unlabeled"}, {"type": "text", "expr": "Here,  $i$  is the index of an anchor in a mini-batch and  $p_{i}$  is the predicted probability of anchor  $i$  being an object", "word_idx": 20824, "sentence_idx": 242, "label": "unlabeled"}, {"type": "text", "expr": " The ground-truth label  $p^{*}_{i}$  is 1 if the anchor is positive, and is 0 if the anchor is negative", "word_idx": 20952, "sentence_idx": 243, "label": "unlabeled"}, {"type": "text", "expr": "  $t_{i}$  is a vector representing the 4 parameterized coordinates of the predicted bounding box, and  $t^{*}_{i}$  is that of the ground-truth box associated with a positive anchor", "word_idx": 21056, "sentence_idx": 244, "label": "unlabeled"}, {"type": "text", "expr": "\nThe classification loss  $L_{\\mathit{cls}}$  is log loss over two classes (object  \\vs \u00a0not object)", "word_idx": 21238, "sentence_idx": 245, "label": "unlabeled"}, {"type": "text", "expr": "\nFor the regression loss, we use  $L_{\\mathit{reg}}(t_{i},t^{*}_{i})=R(t_{i}-t^{*}_{i})$  where  $R$  is the robust loss function (smooth L ${}_{1}$ ) defined in  ", "word_idx": 21338, "sentence_idx": 246, "label": "unlabeled"}, {"type": "text", "expr": " The term  $p^{*}_{i}L_{\\mathit{reg}}$  means the regression loss is activated only for positive anchors ( $p^{*}_{i}=1$ ) and is disabled otherwise ( $p^{*}_{i}=0$ )", "word_idx": 21501, "sentence_idx": 247, "label": "unlabeled"}, {"type": "text", "expr": " The outputs of the  cls  and  reg  layers consist of  $\\{p_{i}\\}$  and  $\\{t_{i}\\}$  respectively", "word_idx": 21667, "sentence_idx": 248, "label": "unlabeled"}, {"type": "math", "expr": "$$p_{i}$$", "word_idx": 21765, "sentence_idx": 249, "label": "unlabeled"}, {"type": "math", "expr": "$$p^{*}_{i}$$", "word_idx": 21770, "sentence_idx": 250, "label": "unlabeled"}, {"type": "math", "expr": "$$t_{i}$$", "word_idx": 21779, "sentence_idx": 251, "label": "unlabeled"}, {"type": "math", "expr": "$$t^{*}_{i}$$", "word_idx": 21784, "sentence_idx": 252, "label": "unlabeled"}, {"type": "math", "expr": "$$L_{\\mathit{cls}}$$", "word_idx": 21793, "sentence_idx": 253, "label": "unlabeled"}, {"type": "math", "expr": "$$L_{\\mathit{reg}}(t_{i},t^{*}_{i})=R(t_{i}-t^{*}_{i})$$", "word_idx": 21809, "sentence_idx": 254, "label": "unlabeled"}, {"type": "math", "expr": "$${}_{1}$$", "word_idx": 21861, "sentence_idx": 255, "label": "unlabeled"}, {"type": "math", "expr": "$$p^{*}_{i}L_{\\mathit{reg}}$$", "word_idx": 21867, "sentence_idx": 256, "label": "unlabeled"}, {"type": "math", "expr": "$$p^{*}_{i}=1$$", "word_idx": 21892, "sentence_idx": 257, "label": "unlabeled"}, {"type": "math", "expr": "$$p^{*}_{i}=0$$", "word_idx": 21903, "sentence_idx": 258, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{p_{i}\\}$$", "word_idx": 21914, "sentence_idx": 259, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{t_{i}\\}$$", "word_idx": 21923, "sentence_idx": 260, "label": "unlabeled"}, {"type": "text", "expr": "The two terms are normalized by  $N_{\\mathit{cls}}$  and  $N_{\\mathit{reg}}$  and weighted by a balancing parameter  $\\lambda$ ", "word_idx": 21932, "sentence_idx": 261, "label": "unlabeled"}, {"type": "text", "expr": " In our current implementation (as in the released code), the  $cls$  term in Eqn", "word_idx": 22059, "sentence_idx": 262, "label": "unlabeled"}, {"type": "text", "expr": "( 1 ) is normalized by the mini-batch size ( \\ie ,  $N_{\\mathit{cls}}=256$ ) and the  $reg$  term is normalized by the number of anchor locations ( \\ie ,  $N_{\\mathit{reg}}\\sim 2,400$ )", "word_idx": 22140, "sentence_idx": 263, "label": "unlabeled"}, {"type": "text", "expr": " By default we set  $\\lambda=10$ , and thus both  cls  and  reg  terms are roughly equally weighted", "word_idx": 22325, "sentence_idx": 264, "label": "unlabeled"}, {"type": "text", "expr": " We show by experiments that the results are insensitive to the values of  $\\lambda$  in a wide range (Table\u00a0 9 )", "word_idx": 22424, "sentence_idx": 265, "label": "unlabeled"}, {"type": "text", "expr": "\nWe also note that the normalization as above is not required and could be simplified", "word_idx": 22537, "sentence_idx": 266, "label": "unlabeled"}, {"type": "math", "expr": "$$N_{\\mathit{cls}}$$", "word_idx": 22622, "sentence_idx": 267, "label": "unlabeled"}, {"type": "math", "expr": "$$N_{\\mathit{reg}}$$", "word_idx": 22638, "sentence_idx": 268, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda$$", "word_idx": 22654, "sentence_idx": 269, "label": "unlabeled"}, {"type": "math", "expr": "$$cls$$", "word_idx": 22661, "sentence_idx": 270, "label": "unlabeled"}, {"type": "math", "expr": "$$N_{\\mathit{cls}}=256$$", "word_idx": 22664, "sentence_idx": 271, "label": "unlabeled"}, {"type": "math", "expr": "$$reg$$", "word_idx": 22684, "sentence_idx": 272, "label": "unlabeled"}, {"type": "math", "expr": "$$N_{\\mathit{reg}}\\sim 2,400$$", "word_idx": 22687, "sentence_idx": 273, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda=10$$", "word_idx": 22713, "sentence_idx": 274, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda$$", "word_idx": 22723, "sentence_idx": 275, "label": "unlabeled"}, {"type": "text", "expr": "For bounding box regression, we adopt the parameterizations of the 4 coordinates following  :", "word_idx": 22730, "sentence_idx": 276, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle t_{\\textrm{x}}$", "word_idx": 22823, "sentence_idx": 277, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle t_{\\textrm{x}}$$", "word_idx": 22853, "sentence_idx": 278, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=(x-x_{\\textrm{a}})/w_{\\textrm{a}},\\quad t_{\\textrm{y}}=(y-y_{%\n\\textrm{a}})/h_{\\textrm{a}},$", "word_idx": 22881, "sentence_idx": 279, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=(x-x_{\\textrm{a}})/w_{\\textrm{a}},\\quad t_{\\textrm{y}}=(y-y_{%\n\\textrm{a}})/h_{\\textrm{a}},$$", "word_idx": 22988, "sentence_idx": 280, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle t_{\\textrm{w}}$", "word_idx": 23093, "sentence_idx": 281, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle t_{\\textrm{w}}$$", "word_idx": 23123, "sentence_idx": 282, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\log(w/w_{\\textrm{a}}),\\quad t_{\\textrm{h}}=\\log(h/h_{\\textrm{a}%\n}),$", "word_idx": 23151, "sentence_idx": 283, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\log(w/w_{\\textrm{a}}),\\quad t_{\\textrm{h}}=\\log(h/h_{\\textrm{a}%\n}),$$", "word_idx": 23236, "sentence_idx": 284, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle t^{*}_{\\textrm{x}}$", "word_idx": 23319, "sentence_idx": 285, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle t^{*}_{\\textrm{x}}$$", "word_idx": 23353, "sentence_idx": 286, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=(x^{*}-x_{\\textrm{a}})/w_{\\textrm{a}},\\quad t^{*}_{\\textrm{y}}=(%\ny^{*}-y_{\\textrm{a}})/h_{\\textrm{a}},$", "word_idx": 23385, "sentence_idx": 287, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=(x^{*}-x_{\\textrm{a}})/w_{\\textrm{a}},\\quad t^{*}_{\\textrm{y}}=(%\ny^{*}-y_{\\textrm{a}})/h_{\\textrm{a}},$$", "word_idx": 23504, "sentence_idx": 288, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle t^{*}_{\\textrm{w}}$", "word_idx": 23621, "sentence_idx": 289, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle t^{*}_{\\textrm{w}}$$", "word_idx": 23655, "sentence_idx": 290, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\log(w^{*}/w_{\\textrm{a}}),\\quad t^{*}_{\\textrm{h}}=\\log(h^{*}/h%\n_{\\textrm{a}}),$", "word_idx": 23687, "sentence_idx": 291, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\log(w^{*}/w_{\\textrm{a}}),\\quad t^{*}_{\\textrm{h}}=\\log(h^{*}/h%\n_{\\textrm{a}}),$$", "word_idx": 23784, "sentence_idx": 292, "label": "unlabeled"}, {"type": "text", "expr": "where  $x$ ,  $y$ ,  $w$ , and  $h$  denote the box\u2019s center coordinates and its width and height", "word_idx": 23879, "sentence_idx": 293, "label": "unlabeled"}, {"type": "text", "expr": "\nVariables  $x$ ,  $x_{\\textrm{a}}$ , and  $x^{*}$  are for the predicted box, anchor box, and ground-truth box respectively (likewise for  $y,w,h$ )", "word_idx": 23976, "sentence_idx": 294, "label": "unlabeled"}, {"type": "text", "expr": " This can be thought of as bounding-box regression from an anchor box to a nearby ground-truth box", "word_idx": 24125, "sentence_idx": 295, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{\\textrm{a}}$$", "word_idx": 24223, "sentence_idx": 296, "label": "unlabeled"}, {"type": "math", "expr": "$$x^{*}$$", "word_idx": 24237, "sentence_idx": 297, "label": "unlabeled"}, {"type": "math", "expr": "$$y,w,h$$", "word_idx": 24242, "sentence_idx": 298, "label": "unlabeled"}, {"type": "text", "expr": "Nevertheless, our method achieves bounding-box regression by a different manner from previous RoI-based (Region of Interest) methods  ", "word_idx": 24247, "sentence_idx": 299, "label": "unlabeled"}, {"type": "text", "expr": "\nIn  , bounding-box regression is performed on features pooled from  arbitrarily  sized RoIs, and the regression weights are  shared  by all region sizes", "word_idx": 24381, "sentence_idx": 300, "label": "unlabeled"}, {"type": "text", "expr": " In our formulation, the features used for regression are of the  same  spatial size ( $3\\times 3$ ) on the feature maps", "word_idx": 24534, "sentence_idx": 301, "label": "unlabeled"}, {"type": "text", "expr": " To account for varying sizes, a set of  $k$  bounding-box regressors are learned", "word_idx": 24654, "sentence_idx": 302, "label": "unlabeled"}, {"type": "text", "expr": " Each regressor is responsible for one scale and one aspect ratio, and the  $k$  regressors do  not  share weights", "word_idx": 24735, "sentence_idx": 303, "label": "unlabeled"}, {"type": "text", "expr": " As such, it is still possible to predict boxes of various sizes even though the features are of a fixed size/scale, thanks to the design of anchors", "word_idx": 24849, "sentence_idx": 304, "label": "unlabeled"}, {"type": "text", "expr": "arbitrarily", "word_idx": 24997, "sentence_idx": 305, "label": "unlabeled"}, {"type": "text", "expr": "shared", "word_idx": 25008, "sentence_idx": 306, "label": "unlabeled"}, {"type": "math", "expr": "$$3\\times 3$$", "word_idx": 25014, "sentence_idx": 307, "label": "unlabeled"}, {"type": "text", "expr": "3  Training RPNs", "word_idx": 25023, "sentence_idx": 308, "label": "unlabeled"}, {"type": "text", "expr": "The RPN can be trained end-to-end by back-propagation and stochastic gradient descent (SGD)  ", "word_idx": 25039, "sentence_idx": 309, "label": "unlabeled"}, {"type": "text", "expr": "\nWe follow the \u201cimage-centric\u201d sampling strategy from   to train this network", "word_idx": 25132, "sentence_idx": 310, "label": "unlabeled"}, {"type": "text", "expr": " Each mini-batch arises from a single image that contains many positive and negative example anchors", "word_idx": 25209, "sentence_idx": 311, "label": "unlabeled"}, {"type": "text", "expr": " It is possible to optimize for the loss functions of all anchors, but this will bias towards negative samples as they are dominate", "word_idx": 25309, "sentence_idx": 312, "label": "unlabeled"}, {"type": "text", "expr": " Instead, we randomly sample 256 anchors in an image to compute the loss function of a mini-batch, where the sampled positive and negative anchors have a ratio of  up to  1:1", "word_idx": 25440, "sentence_idx": 313, "label": "unlabeled"}, {"type": "text", "expr": " If there are fewer than 128 positive samples in an image, we pad the mini-batch with negative ones", "word_idx": 25614, "sentence_idx": 314, "label": "unlabeled"}, {"type": "text", "expr": "up to", "word_idx": 25713, "sentence_idx": 315, "label": "unlabeled"}, {"type": "text", "expr": "We randomly initialize all new layers by drawing weights from a zero-mean Gaussian distribution with standard deviation 0", "word_idx": 25718, "sentence_idx": 316, "label": "unlabeled"}, {"type": "text", "expr": " All other layers ( \\ie , the shared convolutional layers) are initialized by pre-training a model for ImageNet classification  , as is standard practice  ", "word_idx": 25839, "sentence_idx": 317, "label": "unlabeled"}, {"type": "text", "expr": " We tune all layers of the ZF net, and conv3 $\\_1$  and up for the VGG net to conserve memory  ", "word_idx": 25994, "sentence_idx": 318, "label": "unlabeled"}, {"type": "text", "expr": "\nWe use a learning rate of 0", "word_idx": 26089, "sentence_idx": 319, "label": "unlabeled"}, {"type": "text", "expr": "001 for 60k mini-batches, and 0", "word_idx": 26117, "sentence_idx": 320, "label": "unlabeled"}, {"type": "text", "expr": "0001 for the next 20k mini-batches on the PASCAL VOC dataset", "word_idx": 26148, "sentence_idx": 321, "label": "unlabeled"}, {"type": "text", "expr": " We use a momentum of 0", "word_idx": 26208, "sentence_idx": 322, "label": "unlabeled"}, {"type": "text", "expr": "9 and a weight decay of 0", "word_idx": 26231, "sentence_idx": 323, "label": "unlabeled"}, {"type": "text", "expr": "0005  ", "word_idx": 26256, "sentence_idx": 324, "label": "unlabeled"}, {"type": "text", "expr": " Our implementation uses Caffe  ", "word_idx": 26262, "sentence_idx": 325, "label": "unlabeled"}, {"type": "math", "expr": "$$\\_1$$", "word_idx": 26294, "sentence_idx": 326, "label": "unlabeled"}, {"type": "text", "expr": "2  Sharing Features for RPN and Fast R-CNN", "word_idx": 26297, "sentence_idx": 327, "label": "unlabeled"}, {"type": "text", "expr": "Thus far we have described how to train a network for region proposal generation, without considering the region-based object detection CNN that will utilize these proposals", "word_idx": 26339, "sentence_idx": 328, "label": "unlabeled"}, {"type": "text", "expr": "\nFor the detection network, we adopt Fast R-CNN  ", "word_idx": 26512, "sentence_idx": 329, "label": "unlabeled"}, {"type": "text", "expr": "\nNext we describe algorithms that learn a unified network composed of RPN and Fast R-CNN with shared convolutional layers (Figure\u00a0 2 )", "word_idx": 26561, "sentence_idx": 330, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:  the learned average proposal size for each anchor using the ZF net (numbers for  $s=600$ )", "word_idx": 26695, "sentence_idx": 331, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:", "word_idx": 26795, "sentence_idx": 332, "label": "unlabeled"}, {"type": "math", "expr": "$$s=600$$", "word_idx": 26803, "sentence_idx": 333, "label": "unlabeled"}, {"type": "text", "expr": "anchor", "word_idx": 26808, "sentence_idx": 334, "label": "unlabeled"}, {"type": "text", "expr": "anchor", "word_idx": 26814, "sentence_idx": 335, "label": "unlabeled"}, {"type": "text", "expr": "anchor", "word_idx": 26820, "sentence_idx": 336, "label": "unlabeled"}, {"type": "text", "expr": "$128^{2}$ , 2:1", "word_idx": 26826, "sentence_idx": 337, "label": "unlabeled"}, {"type": "math", "expr": "$$128^{2}$$", "word_idx": 26841, "sentence_idx": 338, "label": "unlabeled"}, {"type": "text", "expr": ", 2:1", "word_idx": 26848, "sentence_idx": 339, "label": "unlabeled"}, {"type": "text", "expr": "$128^{2}$ , 1:1", "word_idx": 26853, "sentence_idx": 340, "label": "unlabeled"}, {"type": "math", "expr": "$$128^{2}$$", "word_idx": 26868, "sentence_idx": 341, "label": "unlabeled"}, {"type": "text", "expr": ", 1:1", "word_idx": 26875, "sentence_idx": 342, "label": "unlabeled"}, {"type": "text", "expr": "$128^{2}$ , 1:2", "word_idx": 26880, "sentence_idx": 343, "label": "unlabeled"}, {"type": "math", "expr": "$$128^{2}$$", "word_idx": 26895, "sentence_idx": 344, "label": "unlabeled"}, {"type": "text", "expr": ", 1:2", "word_idx": 26902, "sentence_idx": 345, "label": "unlabeled"}, {"type": "text", "expr": "$256^{2}$ , 2:1", "word_idx": 26907, "sentence_idx": 346, "label": "unlabeled"}, {"type": "math", "expr": "$$256^{2}$$", "word_idx": 26922, "sentence_idx": 347, "label": "unlabeled"}, {"type": "text", "expr": ", 2:1", "word_idx": 26929, "sentence_idx": 348, "label": "unlabeled"}, {"type": "text", "expr": "$256^{2}$ , 1:1", "word_idx": 26934, "sentence_idx": 349, "label": "unlabeled"}, {"type": "math", "expr": "$$256^{2}$$", "word_idx": 26949, "sentence_idx": 350, "label": "unlabeled"}, {"type": "text", "expr": ", 1:1", "word_idx": 26956, "sentence_idx": 351, "label": "unlabeled"}, {"type": "text", "expr": "$256^{2}$ , 1:2", "word_idx": 26961, "sentence_idx": 352, "label": "unlabeled"}, {"type": "math", "expr": "$$256^{2}$$", "word_idx": 26976, "sentence_idx": 353, "label": "unlabeled"}, {"type": "text", "expr": ", 1:2", "word_idx": 26983, "sentence_idx": 354, "label": "unlabeled"}, {"type": "text", "expr": "$512^{2}$ , 2:1", "word_idx": 26988, "sentence_idx": 355, "label": "unlabeled"}, {"type": "math", "expr": "$$512^{2}$$", "word_idx": 27003, "sentence_idx": 356, "label": "unlabeled"}, {"type": "text", "expr": ", 2:1", "word_idx": 27010, "sentence_idx": 357, "label": "unlabeled"}, {"type": "text", "expr": "$512^{2}$ , 1:1", "word_idx": 27015, "sentence_idx": 358, "label": "unlabeled"}, {"type": "math", "expr": "$$512^{2}$$", "word_idx": 27030, "sentence_idx": 359, "label": "unlabeled"}, {"type": "text", "expr": ", 1:1", "word_idx": 27037, "sentence_idx": 360, "label": "unlabeled"}, {"type": "text", "expr": "$512^{2}$ , 1:2", "word_idx": 27042, "sentence_idx": 361, "label": "unlabeled"}, {"type": "math", "expr": "$$512^{2}$$", "word_idx": 27057, "sentence_idx": 362, "label": "unlabeled"}, {"type": "text", "expr": ", 1:2", "word_idx": 27064, "sentence_idx": 363, "label": "unlabeled"}, {"type": "text", "expr": "proposal", "word_idx": 27069, "sentence_idx": 364, "label": "unlabeled"}, {"type": "text", "expr": "proposal", "word_idx": 27077, "sentence_idx": 365, "label": "unlabeled"}, {"type": "text", "expr": "proposal", "word_idx": 27085, "sentence_idx": 366, "label": "unlabeled"}, {"type": "text", "expr": "188 $\\times$ 111", "word_idx": 27093, "sentence_idx": 367, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 27109, "sentence_idx": 368, "label": "unlabeled"}, {"type": "text", "expr": "113 $\\times$ 114", "word_idx": 27115, "sentence_idx": 369, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 27131, "sentence_idx": 370, "label": "unlabeled"}, {"type": "text", "expr": "70 $\\times$ 92", "word_idx": 27137, "sentence_idx": 371, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 27151, "sentence_idx": 372, "label": "unlabeled"}, {"type": "text", "expr": "416 $\\times$ 229", "word_idx": 27157, "sentence_idx": 373, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 27173, "sentence_idx": 374, "label": "unlabeled"}, {"type": "text", "expr": "261 $\\times$ 284", "word_idx": 27179, "sentence_idx": 375, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 27195, "sentence_idx": 376, "label": "unlabeled"}, {"type": "text", "expr": "174 $\\times$ 332", "word_idx": 27201, "sentence_idx": 377, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 27217, "sentence_idx": 378, "label": "unlabeled"}, {"type": "text", "expr": "768 $\\times$ 437", "word_idx": 27223, "sentence_idx": 379, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 27239, "sentence_idx": 380, "label": "unlabeled"}, {"type": "text", "expr": "499 $\\times$ 501", "word_idx": 27245, "sentence_idx": 381, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 27261, "sentence_idx": 382, "label": "unlabeled"}, {"type": "text", "expr": "355 $\\times$ 715", "word_idx": 27267, "sentence_idx": 383, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 27283, "sentence_idx": 384, "label": "unlabeled"}, {"type": "text", "expr": "Both RPN and Fast R-CNN, trained independently, will modify their convolutional layers in different ways", "word_idx": 27289, "sentence_idx": 385, "label": "unlabeled"}, {"type": "text", "expr": "\nWe therefore need to develop a technique that allows for sharing convolutional layers between the two networks, rather than learning two separate networks", "word_idx": 27393, "sentence_idx": 386, "label": "unlabeled"}, {"type": "text", "expr": " We discuss three ways for training networks with features shared:", "word_idx": 27548, "sentence_idx": 387, "label": "unlabeled"}, {"type": "text", "expr": "Both RPN and Fast R-CNN, trained independently, will modify their convolutional layers in different ways", "word_idx": 27614, "sentence_idx": 388, "label": "unlabeled"}, {"type": "text", "expr": "\nWe therefore need to develop a technique that allows for sharing convolutional layers between the two networks, rather than learning two separate networks", "word_idx": 27718, "sentence_idx": 389, "label": "unlabeled"}, {"type": "text", "expr": " We discuss three ways for training networks with features shared:", "word_idx": 27873, "sentence_idx": 390, "label": "unlabeled"}, {"type": "text", "expr": "(i)  Alternating training ", "word_idx": 27939, "sentence_idx": 391, "label": "unlabeled"}, {"type": "text", "expr": " In this solution, we first train RPN, and use the proposals to train Fast R-CNN", "word_idx": 27965, "sentence_idx": 392, "label": "unlabeled"}, {"type": "text", "expr": " The network tuned by Fast R-CNN is then used to initialize RPN, and this process is iterated", "word_idx": 28045, "sentence_idx": 393, "label": "unlabeled"}, {"type": "text", "expr": " This is the solution that is used in all experiments in this paper", "word_idx": 28138, "sentence_idx": 394, "label": "unlabeled"}, {"type": "text", "expr": "Alternating training", "word_idx": 28205, "sentence_idx": 395, "label": "unlabeled"}, {"type": "text", "expr": "(ii)  Approximate joint training ", "word_idx": 28225, "sentence_idx": 396, "label": "unlabeled"}, {"type": "text", "expr": " In this solution, the RPN and Fast R-CNN networks are merged into one network during training as in Figure\u00a0 2 ", "word_idx": 28258, "sentence_idx": 397, "label": "unlabeled"}, {"type": "text", "expr": " In each SGD iteration, the forward pass generates region proposals which are treated just like fixed, pre-computed proposals when training a Fast R-CNN detector", "word_idx": 28369, "sentence_idx": 398, "label": "unlabeled"}, {"type": "text", "expr": "\nThe backward propagation takes place as usual, where for the shared layers the backward propagated signals from both the RPN loss and the Fast R-CNN loss are combined", "word_idx": 28530, "sentence_idx": 399, "label": "unlabeled"}, {"type": "text", "expr": " This solution is easy to implement", "word_idx": 28697, "sentence_idx": 400, "label": "unlabeled"}, {"type": "text", "expr": " But this solution ignores the derivative  \\wrt the proposal boxes\u2019 coordinates that are also network responses, so is approximate", "word_idx": 28732, "sentence_idx": 401, "label": "unlabeled"}, {"type": "text", "expr": "\nIn our experiments, we have empirically found this solver produces close results, yet reduces the training time by about 25-50% comparing with alternating training", "word_idx": 28862, "sentence_idx": 402, "label": "unlabeled"}, {"type": "text", "expr": " This solver is included in our released Python code", "word_idx": 29026, "sentence_idx": 403, "label": "unlabeled"}, {"type": "text", "expr": "Approximate joint training", "word_idx": 29078, "sentence_idx": 404, "label": "unlabeled"}, {"type": "text", "expr": "(iii)  Non-approximate joint training ", "word_idx": 29104, "sentence_idx": 405, "label": "unlabeled"}, {"type": "text", "expr": " As discussed above, the bounding boxes predicted by RPN are also functions of the input", "word_idx": 29142, "sentence_idx": 406, "label": "unlabeled"}, {"type": "text", "expr": " The RoI pooling layer   in Fast R-CNN accepts the convolutional features and also the predicted bounding boxes as input, so a theoretically valid backpropagation solver should also involve gradients  \\wrt the box coordinates", "word_idx": 29230, "sentence_idx": 407, "label": "unlabeled"}, {"type": "text", "expr": " These gradients are ignored in the above approximate joint training", "word_idx": 29455, "sentence_idx": 408, "label": "unlabeled"}, {"type": "text", "expr": "\nIn a non-approximate joint training solution, we need an RoI pooling layer that is differentiable  \\wrt the box coordinates", "word_idx": 29523, "sentence_idx": 409, "label": "unlabeled"}, {"type": "text", "expr": "\nThis is a nontrivial problem and a solution can be given by an \u201cRoI warping\u201d layer as developed in  , which is beyond the scope of this paper", "word_idx": 29647, "sentence_idx": 410, "label": "unlabeled"}, {"type": "text", "expr": "Non-approximate joint training", "word_idx": 29789, "sentence_idx": 411, "label": "unlabeled"}, {"type": "text", "expr": "4-Step Alternating Training ", "word_idx": 29819, "sentence_idx": 412, "label": "unlabeled"}, {"type": "text", "expr": " In this paper, we adopt a pragmatic 4-step training algorithm to learn shared features via alternating optimization", "word_idx": 29847, "sentence_idx": 413, "label": "unlabeled"}, {"type": "text", "expr": "\nIn the first step, we train the RPN as described in Section\u00a0 3", "word_idx": 29963, "sentence_idx": 414, "label": "unlabeled"}, {"type": "text", "expr": " This network is initialized with an ImageNet-pre-trained model and fine-tuned end-to-end for the region proposal task", "word_idx": 30026, "sentence_idx": 415, "label": "unlabeled"}, {"type": "text", "expr": "\nIn the second step, we train a separate detection network by Fast R-CNN using the proposals generated by the step-1 RPN", "word_idx": 30144, "sentence_idx": 416, "label": "unlabeled"}, {"type": "text", "expr": " This detection network is also initialized by the ImageNet-pre-trained model", "word_idx": 30264, "sentence_idx": 417, "label": "unlabeled"}, {"type": "text", "expr": " At this point the two networks do not share convolutional layers", "word_idx": 30341, "sentence_idx": 418, "label": "unlabeled"}, {"type": "text", "expr": " In the third step, we use the detector network to initialize RPN training, but we fix the shared convolutional layers and only fine-tune the layers unique to RPN", "word_idx": 30406, "sentence_idx": 419, "label": "unlabeled"}, {"type": "text", "expr": " Now the two networks share convolutional layers", "word_idx": 30568, "sentence_idx": 420, "label": "unlabeled"}, {"type": "text", "expr": " Finally, keeping the shared convolutional layers fixed, we fine-tune the unique layers of Fast R-CNN", "word_idx": 30616, "sentence_idx": 421, "label": "unlabeled"}, {"type": "text", "expr": " As such, both networks share the same convolutional layers and form a unified network", "word_idx": 30717, "sentence_idx": 422, "label": "unlabeled"}, {"type": "text", "expr": " A similar alternating training can be run for more iterations, but we have observed negligible improvements", "word_idx": 30803, "sentence_idx": 423, "label": "unlabeled"}, {"type": "text", "expr": "4-Step Alternating Training", "word_idx": 30911, "sentence_idx": 424, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:  Detection results on  PASCAL VOC 2007 test set  (trained on VOC 2007 trainval)", "word_idx": 30938, "sentence_idx": 425, "label": "unlabeled"}, {"type": "text", "expr": " The detectors are Fast R-CNN with ZF, but using various proposal methods for training and testing", "word_idx": 31026, "sentence_idx": 426, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:", "word_idx": 31124, "sentence_idx": 427, "label": "unlabeled"}, {"type": "text", "expr": "PASCAL VOC 2007 test set", "word_idx": 31132, "sentence_idx": 428, "label": "unlabeled"}, {"type": "text", "expr": "train-time region proposals test-time region proposals", "word_idx": 31156, "sentence_idx": 429, "label": "unlabeled"}, {"type": "text", "expr": "train-time region proposals", "word_idx": 31210, "sentence_idx": 430, "label": "unlabeled"}, {"type": "text", "expr": "test-time region proposals", "word_idx": 31237, "sentence_idx": 431, "label": "unlabeled"}, {"type": "text", "expr": "test-time region proposals", "word_idx": 31263, "sentence_idx": 432, "label": "unlabeled"}, {"type": "text", "expr": "method # boxes method # proposals mAP (%)", "word_idx": 31289, "sentence_idx": 433, "label": "unlabeled"}, {"type": "text", "expr": "method", "word_idx": 31330, "sentence_idx": 434, "label": "unlabeled"}, {"type": "text", "expr": "# boxes", "word_idx": 31336, "sentence_idx": 435, "label": "unlabeled"}, {"type": "text", "expr": "method", "word_idx": 31343, "sentence_idx": 436, "label": "unlabeled"}, {"type": "text", "expr": "method", "word_idx": 31349, "sentence_idx": 437, "label": "unlabeled"}, {"type": "text", "expr": "# proposals", "word_idx": 31355, "sentence_idx": 438, "label": "unlabeled"}, {"type": "text", "expr": "# proposals", "word_idx": 31366, "sentence_idx": 439, "label": "unlabeled"}, {"type": "text", "expr": "mAP (%)", "word_idx": 31377, "sentence_idx": 440, "label": "unlabeled"}, {"type": "text", "expr": "mAP (%)", "word_idx": 31384, "sentence_idx": 441, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 SS 2000 58", "word_idx": 31391, "sentence_idx": 442, "label": "unlabeled"}, {"type": "text", "expr": "EB 2000 EB 2000 58", "word_idx": 31409, "sentence_idx": 443, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF, shared 2000 RPN+ZF, shared 300 59", "word_idx": 31427, "sentence_idx": 444, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF, shared", "word_idx": 31468, "sentence_idx": 445, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF, shared", "word_idx": 31482, "sentence_idx": 446, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF, shared", "word_idx": 31496, "sentence_idx": 447, "label": "unlabeled"}, {"type": "text", "expr": "ablation experiments follow below", "word_idx": 31510, "sentence_idx": 448, "label": "unlabeled"}, {"type": "text", "expr": "ablation experiments follow below", "word_idx": 31543, "sentence_idx": 449, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF, unshared 2000 RPN+ZF, unshared 300 58", "word_idx": 31576, "sentence_idx": 450, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF, unshared", "word_idx": 31621, "sentence_idx": 451, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF, unshared", "word_idx": 31637, "sentence_idx": 452, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF, unshared", "word_idx": 31653, "sentence_idx": 453, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 RPN+ZF 100 55", "word_idx": 31669, "sentence_idx": 454, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF", "word_idx": 31690, "sentence_idx": 455, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF", "word_idx": 31696, "sentence_idx": 456, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 RPN+ZF 300 56", "word_idx": 31702, "sentence_idx": 457, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF", "word_idx": 31723, "sentence_idx": 458, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF", "word_idx": 31729, "sentence_idx": 459, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 RPN+ZF 1000 56", "word_idx": 31735, "sentence_idx": 460, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF", "word_idx": 31757, "sentence_idx": 461, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF", "word_idx": 31763, "sentence_idx": 462, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 RPN+ZF (no NMS) 6000 55", "word_idx": 31769, "sentence_idx": 463, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF (no NMS)", "word_idx": 31800, "sentence_idx": 464, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF (no NMS)", "word_idx": 31815, "sentence_idx": 465, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 100 44", "word_idx": 31830, "sentence_idx": 466, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF (no  cls )", "word_idx": 31844, "sentence_idx": 467, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF (no", "word_idx": 31861, "sentence_idx": 468, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 300 51", "word_idx": 31871, "sentence_idx": 469, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF (no  cls )", "word_idx": 31885, "sentence_idx": 470, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF (no", "word_idx": 31902, "sentence_idx": 471, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 1000 55", "word_idx": 31912, "sentence_idx": 472, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF (no  cls )", "word_idx": 31927, "sentence_idx": 473, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF (no", "word_idx": 31944, "sentence_idx": 474, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 300 52", "word_idx": 31954, "sentence_idx": 475, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF (no  reg )", "word_idx": 31968, "sentence_idx": 476, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF (no", "word_idx": 31985, "sentence_idx": 477, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 1000 51", "word_idx": 31995, "sentence_idx": 478, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF (no  reg )", "word_idx": 32010, "sentence_idx": 479, "label": "unlabeled"}, {"type": "text", "expr": "RPN+ZF (no", "word_idx": 32027, "sentence_idx": 480, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 RPN+VGG 300 59", "word_idx": 32037, "sentence_idx": 481, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG", "word_idx": 32059, "sentence_idx": 482, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG", "word_idx": 32066, "sentence_idx": 483, "label": "unlabeled"}, {"type": "text", "expr": "3  Implementation Details", "word_idx": 32073, "sentence_idx": 484, "label": "unlabeled"}, {"type": "text", "expr": "We train and test both region proposal and object detection networks on images of a single scale  ", "word_idx": 32098, "sentence_idx": 485, "label": "unlabeled"}, {"type": "text", "expr": " We re-scale the images such that their shorter side is  $s=600$  pixels  ", "word_idx": 32196, "sentence_idx": 486, "label": "unlabeled"}, {"type": "text", "expr": " Multi-scale feature extraction (using an image pyramid) may improve accuracy but does not exhibit a good speed-accuracy trade-off  ", "word_idx": 32270, "sentence_idx": 487, "label": "unlabeled"}, {"type": "text", "expr": " On the re-scaled images, the total stride for both ZF and VGG nets on the last convolutional layer is 16 pixels, and thus is  $\\sim$ 10 pixels on a typical PASCAL image before resizing ( $\\sim$ 500 $\\times$ 375)", "word_idx": 32402, "sentence_idx": 488, "label": "unlabeled"}, {"type": "text", "expr": " Even such a large stride provides good results, though accuracy may be further improved with a smaller stride", "word_idx": 32614, "sentence_idx": 489, "label": "unlabeled"}, {"type": "math", "expr": "$$s=600$$", "word_idx": 32724, "sentence_idx": 490, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sim$$", "word_idx": 32729, "sentence_idx": 491, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sim$$", "word_idx": 32733, "sentence_idx": 492, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 32737, "sentence_idx": 493, "label": "unlabeled"}, {"type": "text", "expr": "For anchors, we use 3 scales with box areas of  $128^{2}$ ,  $256^{2}$ , and  $512^{2}$  pixels, and 3 aspect ratios of 1:1, 1:2, and 2:1", "word_idx": 32743, "sentence_idx": 494, "label": "unlabeled"}, {"type": "text", "expr": " These hyper-parameters are  not  carefully chosen for a particular dataset, and we provide ablation experiments on their effects in the next section", "word_idx": 32880, "sentence_idx": 495, "label": "unlabeled"}, {"type": "text", "expr": "\nAs discussed, our solution does not need an image pyramid or filter pyramid to predict regions of multiple scales, saving considerable running time", "word_idx": 33029, "sentence_idx": 496, "label": "unlabeled"}, {"type": "text", "expr": "\nFigure\u00a0 3  (right) shows the capability of our method for a wide range of scales and aspect ratios", "word_idx": 33177, "sentence_idx": 497, "label": "unlabeled"}, {"type": "text", "expr": " Table\u00a0 1  shows the learned average proposal size for each anchor using the ZF net", "word_idx": 33276, "sentence_idx": 498, "label": "unlabeled"}, {"type": "text", "expr": "\nWe note that our algorithm allows predictions that are larger than the underlying receptive field", "word_idx": 33359, "sentence_idx": 499, "label": "unlabeled"}, {"type": "text", "expr": " Such predictions are not impossible\u2014one may still roughly infer the extent of an object if only the middle of the object is visible", "word_idx": 33457, "sentence_idx": 500, "label": "unlabeled"}, {"type": "math", "expr": "$$128^{2}$$", "word_idx": 33589, "sentence_idx": 501, "label": "unlabeled"}, {"type": "math", "expr": "$$256^{2}$$", "word_idx": 33596, "sentence_idx": 502, "label": "unlabeled"}, {"type": "math", "expr": "$$512^{2}$$", "word_idx": 33603, "sentence_idx": 503, "label": "unlabeled"}, {"type": "text", "expr": "The anchor boxes that cross image boundaries need to be handled with care", "word_idx": 33610, "sentence_idx": 504, "label": "unlabeled"}, {"type": "text", "expr": " During training, we ignore all cross-boundary anchors so they do not contribute to the loss", "word_idx": 33683, "sentence_idx": 505, "label": "unlabeled"}, {"type": "text", "expr": " For a typical  $1000\\times 600$  image, there will be roughly 20000 ( $\\approx 60\\times 40\\times 9$ ) anchors in total", "word_idx": 33775, "sentence_idx": 506, "label": "unlabeled"}, {"type": "text", "expr": " With the cross-boundary anchors ignored, there are about 6000 anchors per image for training", "word_idx": 33894, "sentence_idx": 507, "label": "unlabeled"}, {"type": "text", "expr": " If the boundary-crossing outliers are not ignored in training, they introduce large, difficult to correct error terms in the objective, and training does not converge", "word_idx": 33987, "sentence_idx": 508, "label": "unlabeled"}, {"type": "text", "expr": " During testing, however, we still apply the fully convolutional RPN to the entire image", "word_idx": 34154, "sentence_idx": 509, "label": "unlabeled"}, {"type": "text", "expr": " This may generate cross-boundary proposal boxes, which we clip to the image boundary", "word_idx": 34242, "sentence_idx": 510, "label": "unlabeled"}, {"type": "math", "expr": "$$1000\\times 600$$", "word_idx": 34327, "sentence_idx": 511, "label": "unlabeled"}, {"type": "math", "expr": "$$\\approx 60\\times 40\\times 9$$", "word_idx": 34341, "sentence_idx": 512, "label": "unlabeled"}, {"type": "text", "expr": "Some RPN proposals highly overlap with each other", "word_idx": 34368, "sentence_idx": 513, "label": "unlabeled"}, {"type": "text", "expr": " To reduce redundancy, we adopt non-maximum suppression (NMS) on the proposal regions based on their  cls  scores", "word_idx": 34417, "sentence_idx": 514, "label": "unlabeled"}, {"type": "text", "expr": " We fix the IoU threshold for NMS at 0", "word_idx": 34530, "sentence_idx": 515, "label": "unlabeled"}, {"type": "text", "expr": "7, which leaves us about 2000 proposal regions per image", "word_idx": 34568, "sentence_idx": 516, "label": "unlabeled"}, {"type": "text", "expr": " As we will show, NMS does not harm the ultimate detection accuracy, but substantially reduces the number of proposals", "word_idx": 34624, "sentence_idx": 517, "label": "unlabeled"}, {"type": "text", "expr": "\nAfter NMS, we use the top- $N$  ranked proposal regions for detection", "word_idx": 34742, "sentence_idx": 518, "label": "unlabeled"}, {"type": "text", "expr": " In the following, we train Fast R-CNN using 2000 RPN proposals, but evaluate different numbers of proposals at test-time", "word_idx": 34812, "sentence_idx": 519, "label": "unlabeled"}, {"type": "text", "expr": "4  Experiments", "word_idx": 34933, "sentence_idx": 520, "label": "unlabeled"}, {"type": "text", "expr": "1  Experiments on PASCAL VOC", "word_idx": 34947, "sentence_idx": 521, "label": "unlabeled"}, {"type": "text", "expr": "We comprehensively evaluate our method on the PASCAL VOC 2007 detection benchmark  ", "word_idx": 34975, "sentence_idx": 522, "label": "unlabeled"}, {"type": "text", "expr": " This dataset consists of about 5k trainval images and 5k test images over 20 object categories", "word_idx": 35058, "sentence_idx": 523, "label": "unlabeled"}, {"type": "text", "expr": "\nWe also provide results on the PASCAL VOC 2012 benchmark for a few models", "word_idx": 35153, "sentence_idx": 524, "label": "unlabeled"}, {"type": "text", "expr": "\nFor the ImageNet pre-trained network, we use the \u201cfast\u201d version of ZF net   that has 5 convolutional layers and 3 fully-connected layers, and the public VGG-16 model    that has 13 convolutional layers and 3 fully-connected layers", "word_idx": 35227, "sentence_idx": 525, "label": "unlabeled"}, {"type": "text", "expr": "\nWe primarily evaluate detection mean Average Precision (mAP), because this is the actual metric for object detection (rather than focusing on object proposal proxy metrics)", "word_idx": 35458, "sentence_idx": 526, "label": "unlabeled"}, {"type": "text", "expr": "7 www", "word_idx": 35631, "sentence_idx": 527, "label": "unlabeled"}, {"type": "text", "expr": "robots", "word_idx": 35636, "sentence_idx": 528, "label": "unlabeled"}, {"type": "text", "expr": "uk/~vgg/research/very_deep/", "word_idx": 35642, "sentence_idx": 529, "label": "unlabeled"}, {"type": "text", "expr": "robots", "word_idx": 35669, "sentence_idx": 530, "label": "unlabeled"}, {"type": "text", "expr": "uk/~vgg/research/very_deep/", "word_idx": 35675, "sentence_idx": 531, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a0 2  (top) shows Fast R-CNN results when trained and tested using various region proposal methods", "word_idx": 35702, "sentence_idx": 532, "label": "unlabeled"}, {"type": "text", "expr": " These results use the ZF net", "word_idx": 35804, "sentence_idx": 533, "label": "unlabeled"}, {"type": "text", "expr": "\nFor Selective Search (SS)  , we generate about 2000 proposals by the \u201cfast\u201d mode", "word_idx": 35833, "sentence_idx": 534, "label": "unlabeled"}, {"type": "text", "expr": " For EdgeBoxes (EB)  , we generate the proposals by the default EB setting tuned for 0", "word_idx": 35914, "sentence_idx": 535, "label": "unlabeled"}, {"type": "text", "expr": "7 IoU", "word_idx": 36000, "sentence_idx": 536, "label": "unlabeled"}, {"type": "text", "expr": "\nSS has an mAP of 58", "word_idx": 36005, "sentence_idx": 537, "label": "unlabeled"}, {"type": "text", "expr": "7% and EB has an mAP of 58", "word_idx": 36025, "sentence_idx": 538, "label": "unlabeled"}, {"type": "text", "expr": "6% under the Fast R-CNN framework", "word_idx": 36051, "sentence_idx": 539, "label": "unlabeled"}, {"type": "text", "expr": "\nRPN with Fast R-CNN achieves competitive results, with an mAP of 59", "word_idx": 36084, "sentence_idx": 540, "label": "unlabeled"}, {"type": "text", "expr": "9% while using  up to  300 proposals ", "word_idx": 36152, "sentence_idx": 541, "label": "unlabeled"}, {"type": "text", "expr": "\nUsing RPN yields a much faster detection system than using either SS or EB because of shared convolutional computations; the fewer proposals also reduce the region-wise fully-connected layers\u2019 cost (Table\u00a0 5 )", "word_idx": 36189, "sentence_idx": 542, "label": "unlabeled"}, {"type": "text", "expr": "up to", "word_idx": 36399, "sentence_idx": 543, "label": "unlabeled"}, {"type": "text", "expr": "8 For RPN, the number of proposals ( \\eg , 300) is the maximum number for an image", "word_idx": 36404, "sentence_idx": 544, "label": "unlabeled"}, {"type": "text", "expr": " RPN may produce fewer proposals after NMS, and thus the average number of proposals is smaller", "word_idx": 36486, "sentence_idx": 545, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a03:  Detection results on  PASCAL VOC 2007 test set ", "word_idx": 36581, "sentence_idx": 546, "label": "unlabeled"}, {"type": "text", "expr": " The detector is Fast R-CNN and VGG-16", "word_idx": 36638, "sentence_idx": 547, "label": "unlabeled"}, {"type": "text", "expr": " Training data: \u201c07\u201d: VOC 2007 trainval, \u201c07+12\u201d: union set of VOC 2007 trainval and VOC 2012 trainval", "word_idx": 36676, "sentence_idx": 548, "label": "unlabeled"}, {"type": "text", "expr": " For RPN, the train-time proposals for Fast R-CNN are 2000", "word_idx": 36778, "sentence_idx": 549, "label": "unlabeled"}, {"type": "text", "expr": "  ${}^{\\dagger}$ : this number was reported in  ; using the repository provided by this paper, this result is higher (68", "word_idx": 36836, "sentence_idx": 550, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a03:", "word_idx": 36956, "sentence_idx": 551, "label": "unlabeled"}, {"type": "text", "expr": "PASCAL VOC 2007 test set", "word_idx": 36964, "sentence_idx": 552, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\dagger}$$", "word_idx": 36988, "sentence_idx": 553, "label": "unlabeled"}, {"type": "text", "expr": "method # proposals data mAP (%)", "word_idx": 37000, "sentence_idx": 554, "label": "unlabeled"}, {"type": "text", "expr": "method", "word_idx": 37031, "sentence_idx": 555, "label": "unlabeled"}, {"type": "text", "expr": "# proposals", "word_idx": 37037, "sentence_idx": 556, "label": "unlabeled"}, {"type": "text", "expr": "mAP (%)", "word_idx": 37048, "sentence_idx": 557, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 07", "word_idx": 37055, "sentence_idx": 558, "label": "unlabeled"}, {"type": "text", "expr": "9 ${}^{\\dagger}$", "word_idx": 37065, "sentence_idx": 559, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\dagger}$$", "word_idx": 37081, "sentence_idx": 560, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 07+12 70", "word_idx": 37093, "sentence_idx": 561, "label": "unlabeled"}, {"type": "text", "expr": "07+12", "word_idx": 37109, "sentence_idx": 562, "label": "unlabeled"}, {"type": "text", "expr": "07+12", "word_idx": 37114, "sentence_idx": 563, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG, unshared 300 07 68", "word_idx": 37119, "sentence_idx": 564, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG, unshared", "word_idx": 37146, "sentence_idx": 565, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG, shared 300 07 69", "word_idx": 37163, "sentence_idx": 566, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG, shared", "word_idx": 37188, "sentence_idx": 567, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG, shared 300 07+12 73", "word_idx": 37203, "sentence_idx": 568, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG, shared", "word_idx": 37231, "sentence_idx": 569, "label": "unlabeled"}, {"type": "text", "expr": "07+12", "word_idx": 37246, "sentence_idx": 570, "label": "unlabeled"}, {"type": "text", "expr": "07+12", "word_idx": 37251, "sentence_idx": 571, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG, shared 300 COCO+07+12 78", "word_idx": 37256, "sentence_idx": 572, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG, shared", "word_idx": 37289, "sentence_idx": 573, "label": "unlabeled"}, {"type": "text", "expr": "COCO+07+12", "word_idx": 37304, "sentence_idx": 574, "label": "unlabeled"}, {"type": "text", "expr": "COCO+07+12", "word_idx": 37314, "sentence_idx": 575, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a04:  Detection results on  PASCAL VOC 2012 test set ", "word_idx": 37324, "sentence_idx": 576, "label": "unlabeled"}, {"type": "text", "expr": " The detector is Fast R-CNN and VGG-16", "word_idx": 37381, "sentence_idx": 577, "label": "unlabeled"}, {"type": "text", "expr": " Training data: \u201c07\u201d: VOC 2007 trainval, \u201c07++12\u201d: union set of VOC 2007 trainval+test and VOC 2012 trainval", "word_idx": 37419, "sentence_idx": 578, "label": "unlabeled"}, {"type": "text", "expr": " For RPN, the train-time proposals for Fast R-CNN are 2000", "word_idx": 37527, "sentence_idx": 579, "label": "unlabeled"}, {"type": "text", "expr": "  ${}^{\\dagger}$ :  http://host", "word_idx": 37585, "sentence_idx": 580, "label": "unlabeled"}, {"type": "text", "expr": "robots", "word_idx": 37616, "sentence_idx": 581, "label": "unlabeled"}, {"type": "text", "expr": "uk:8080/anonymous/HZJTQA", "word_idx": 37622, "sentence_idx": 582, "label": "unlabeled"}, {"type": "text", "expr": "html ", "word_idx": 37646, "sentence_idx": 583, "label": "unlabeled"}, {"type": "text", "expr": "  ${}^{\\ddagger}$ :  http://host", "word_idx": 37651, "sentence_idx": 584, "label": "unlabeled"}, {"type": "text", "expr": "robots", "word_idx": 37683, "sentence_idx": 585, "label": "unlabeled"}, {"type": "text", "expr": "uk:8080/anonymous/YNPLXB", "word_idx": 37689, "sentence_idx": 586, "label": "unlabeled"}, {"type": "text", "expr": "html ", "word_idx": 37713, "sentence_idx": 587, "label": "unlabeled"}, {"type": "text", "expr": "  ${}^{\\S}$ :  http://host", "word_idx": 37718, "sentence_idx": 588, "label": "unlabeled"}, {"type": "text", "expr": "robots", "word_idx": 37744, "sentence_idx": 589, "label": "unlabeled"}, {"type": "text", "expr": "uk:8080/anonymous/XEDH10", "word_idx": 37750, "sentence_idx": 590, "label": "unlabeled"}, {"type": "text", "expr": "html ", "word_idx": 37774, "sentence_idx": 591, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a04:", "word_idx": 37779, "sentence_idx": 592, "label": "unlabeled"}, {"type": "text", "expr": "PASCAL VOC 2012 test set", "word_idx": 37787, "sentence_idx": 593, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\dagger}$$", "word_idx": 37811, "sentence_idx": 594, "label": "unlabeled"}, {"type": "text", "expr": "http://host", "word_idx": 37823, "sentence_idx": 595, "label": "unlabeled"}, {"type": "text", "expr": "robots", "word_idx": 37834, "sentence_idx": 596, "label": "unlabeled"}, {"type": "text", "expr": "uk:8080/anonymous/HZJTQA", "word_idx": 37840, "sentence_idx": 597, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\ddagger}$$", "word_idx": 37864, "sentence_idx": 598, "label": "unlabeled"}, {"type": "text", "expr": "http://host", "word_idx": 37877, "sentence_idx": 599, "label": "unlabeled"}, {"type": "text", "expr": "robots", "word_idx": 37888, "sentence_idx": 600, "label": "unlabeled"}, {"type": "text", "expr": "uk:8080/anonymous/YNPLXB", "word_idx": 37894, "sentence_idx": 601, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\S}$$", "word_idx": 37918, "sentence_idx": 602, "label": "unlabeled"}, {"type": "text", "expr": "http://host", "word_idx": 37925, "sentence_idx": 603, "label": "unlabeled"}, {"type": "text", "expr": "robots", "word_idx": 37936, "sentence_idx": 604, "label": "unlabeled"}, {"type": "text", "expr": "uk:8080/anonymous/XEDH10", "word_idx": 37942, "sentence_idx": 605, "label": "unlabeled"}, {"type": "text", "expr": "method # proposals data mAP (%)", "word_idx": 37966, "sentence_idx": 606, "label": "unlabeled"}, {"type": "text", "expr": "method", "word_idx": 37997, "sentence_idx": 607, "label": "unlabeled"}, {"type": "text", "expr": "# proposals", "word_idx": 38003, "sentence_idx": 608, "label": "unlabeled"}, {"type": "text", "expr": "mAP (%)", "word_idx": 38014, "sentence_idx": 609, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 12 65", "word_idx": 38021, "sentence_idx": 610, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 07++12 68", "word_idx": 38034, "sentence_idx": 611, "label": "unlabeled"}, {"type": "text", "expr": "07++12", "word_idx": 38051, "sentence_idx": 612, "label": "unlabeled"}, {"type": "text", "expr": "07++12", "word_idx": 38057, "sentence_idx": 613, "label": "unlabeled"}, {"type": "text", "expr": "300 12 67", "word_idx": 38063, "sentence_idx": 614, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG, shared ${}^{\\dagger}$", "word_idx": 38072, "sentence_idx": 615, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG, shared", "word_idx": 38102, "sentence_idx": 616, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\dagger}$$", "word_idx": 38117, "sentence_idx": 617, "label": "unlabeled"}, {"type": "text", "expr": "300 07++12 70", "word_idx": 38129, "sentence_idx": 618, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG, shared ${}^{\\ddagger}$", "word_idx": 38142, "sentence_idx": 619, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG, shared", "word_idx": 38173, "sentence_idx": 620, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\ddagger}$$", "word_idx": 38188, "sentence_idx": 621, "label": "unlabeled"}, {"type": "text", "expr": "07++12", "word_idx": 38201, "sentence_idx": 622, "label": "unlabeled"}, {"type": "text", "expr": "07++12", "word_idx": 38207, "sentence_idx": 623, "label": "unlabeled"}, {"type": "text", "expr": "300 COCO+07++12 75", "word_idx": 38213, "sentence_idx": 624, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG, shared ${}^{\\S}$", "word_idx": 38231, "sentence_idx": 625, "label": "unlabeled"}, {"type": "text", "expr": "RPN+VGG, shared", "word_idx": 38256, "sentence_idx": 626, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{\\S}$$", "word_idx": 38271, "sentence_idx": 627, "label": "unlabeled"}, {"type": "text", "expr": "COCO+07++12", "word_idx": 38278, "sentence_idx": 628, "label": "unlabeled"}, {"type": "text", "expr": "COCO+07++12", "word_idx": 38289, "sentence_idx": 629, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a05:  Timing  (ms) on a K40 GPU, except SS proposal is evaluated in a CPU", "word_idx": 38300, "sentence_idx": 630, "label": "unlabeled"}, {"type": "text", "expr": " \u201cRegion-wise\u201d includes NMS, pooling, fully-connected, and softmax layers", "word_idx": 38377, "sentence_idx": 631, "label": "unlabeled"}, {"type": "text", "expr": " See our released code for the profiling of running time", "word_idx": 38450, "sentence_idx": 632, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a05:", "word_idx": 38506, "sentence_idx": 633, "label": "unlabeled"}, {"type": "text", "expr": "Timing", "word_idx": 38514, "sentence_idx": 634, "label": "unlabeled"}, {"type": "text", "expr": "model system conv proposal region-wise total rate", "word_idx": 38520, "sentence_idx": 635, "label": "unlabeled"}, {"type": "text", "expr": "model", "word_idx": 38569, "sentence_idx": 636, "label": "unlabeled"}, {"type": "text", "expr": "system", "word_idx": 38574, "sentence_idx": 637, "label": "unlabeled"}, {"type": "text", "expr": "proposal", "word_idx": 38580, "sentence_idx": 638, "label": "unlabeled"}, {"type": "text", "expr": "region-wise", "word_idx": 38588, "sentence_idx": 639, "label": "unlabeled"}, {"type": "text", "expr": "total", "word_idx": 38599, "sentence_idx": 640, "label": "unlabeled"}, {"type": "text", "expr": "VGG SS + Fast R-CNN 146 1510 174 1830 0", "word_idx": 38604, "sentence_idx": 641, "label": "unlabeled"}, {"type": "text", "expr": "5 fps", "word_idx": 38643, "sentence_idx": 642, "label": "unlabeled"}, {"type": "text", "expr": "SS + Fast R-CNN", "word_idx": 38648, "sentence_idx": 643, "label": "unlabeled"}, {"type": "text", "expr": "SS + Fast R-CNN", "word_idx": 38663, "sentence_idx": 644, "label": "unlabeled"}, {"type": "text", "expr": "5 fps", "word_idx": 38678, "sentence_idx": 645, "label": "unlabeled"}, {"type": "text", "expr": "5 fps", "word_idx": 38683, "sentence_idx": 646, "label": "unlabeled"}, {"type": "text", "expr": "VGG RPN + Fast R-CNN 141 10 47 198 5 fps", "word_idx": 38688, "sentence_idx": 647, "label": "unlabeled"}, {"type": "text", "expr": "RPN + Fast R-CNN", "word_idx": 38728, "sentence_idx": 648, "label": "unlabeled"}, {"type": "text", "expr": "RPN + Fast R-CNN", "word_idx": 38744, "sentence_idx": 649, "label": "unlabeled"}, {"type": "text", "expr": "5 fps", "word_idx": 38760, "sentence_idx": 650, "label": "unlabeled"}, {"type": "text", "expr": "5 fps", "word_idx": 38765, "sentence_idx": 651, "label": "unlabeled"}, {"type": "text", "expr": "ZF RPN + Fast R-CNN 31 3 25 59 17 fps", "word_idx": 38770, "sentence_idx": 652, "label": "unlabeled"}, {"type": "text", "expr": "RPN + Fast R-CNN", "word_idx": 38807, "sentence_idx": 653, "label": "unlabeled"}, {"type": "text", "expr": "RPN + Fast R-CNN", "word_idx": 38823, "sentence_idx": 654, "label": "unlabeled"}, {"type": "text", "expr": "17 fps", "word_idx": 38839, "sentence_idx": 655, "label": "unlabeled"}, {"type": "text", "expr": "17 fps", "word_idx": 38845, "sentence_idx": 656, "label": "unlabeled"}, {"type": "text", "expr": "Ablation Experiments on RPN", "word_idx": 38851, "sentence_idx": 657, "label": "unlabeled"}, {"type": "text", "expr": " \nTo investigate the behavior of RPNs as a proposal method, we conducted several ablation studies", "word_idx": 38878, "sentence_idx": 658, "label": "unlabeled"}, {"type": "text", "expr": "\nFirst, we show the effect of sharing convolutional layers between the RPN and Fast R-CNN detection network", "word_idx": 38975, "sentence_idx": 659, "label": "unlabeled"}, {"type": "text", "expr": "\nTo do this, we stop after the second step in the 4-step training process", "word_idx": 39082, "sentence_idx": 660, "label": "unlabeled"}, {"type": "text", "expr": "\nUsing separate networks reduces the result slightly to 58", "word_idx": 39155, "sentence_idx": 661, "label": "unlabeled"}, {"type": "text", "expr": "7% (RPN+ZF, unshared, Table\u00a0 2 )", "word_idx": 39213, "sentence_idx": 662, "label": "unlabeled"}, {"type": "text", "expr": "\nWe observe that this is because in the third step when the detector-tuned features are used to fine-tune the RPN, the proposal quality is improved", "word_idx": 39245, "sentence_idx": 663, "label": "unlabeled"}, {"type": "text", "expr": "Ablation Experiments on RPN", "word_idx": 39392, "sentence_idx": 664, "label": "unlabeled"}, {"type": "text", "expr": "Next, we disentangle the RPN\u2019s influence on training the Fast R-CNN detection network", "word_idx": 39419, "sentence_idx": 665, "label": "unlabeled"}, {"type": "text", "expr": "\nFor this purpose, we train a Fast R-CNN model by using the 2000 SS proposals and ZF net", "word_idx": 39504, "sentence_idx": 666, "label": "unlabeled"}, {"type": "text", "expr": "\nWe fix this detector and evaluate the detection mAP by changing the proposal regions used at test-time", "word_idx": 39592, "sentence_idx": 667, "label": "unlabeled"}, {"type": "text", "expr": "\nIn these ablation experiments, the RPN does not share features with the detector", "word_idx": 39695, "sentence_idx": 668, "label": "unlabeled"}, {"type": "text", "expr": "Next, we disentangle the RPN\u2019s influence on training the Fast R-CNN detection network", "word_idx": 39776, "sentence_idx": 669, "label": "unlabeled"}, {"type": "text", "expr": "\nFor this purpose, we train a Fast R-CNN model by using the 2000 SS proposals and ZF net", "word_idx": 39861, "sentence_idx": 670, "label": "unlabeled"}, {"type": "text", "expr": "\nWe fix this detector and evaluate the detection mAP by changing the proposal regions used at test-time", "word_idx": 39949, "sentence_idx": 671, "label": "unlabeled"}, {"type": "text", "expr": "\nIn these ablation experiments, the RPN does not share features with the detector", "word_idx": 40052, "sentence_idx": 672, "label": "unlabeled"}, {"type": "text", "expr": "Replacing SS with 300 RPN proposals at test-time leads to an mAP of 56", "word_idx": 40133, "sentence_idx": 673, "label": "unlabeled"}, {"type": "text", "expr": " The loss in mAP is because of the inconsistency between the training/testing proposals", "word_idx": 40203, "sentence_idx": 674, "label": "unlabeled"}, {"type": "text", "expr": " This result serves as the baseline for the following comparisons", "word_idx": 40290, "sentence_idx": 675, "label": "unlabeled"}, {"type": "text", "expr": "Replacing SS with 300 RPN proposals at test-time leads to an mAP of 56", "word_idx": 40355, "sentence_idx": 676, "label": "unlabeled"}, {"type": "text", "expr": " The loss in mAP is because of the inconsistency between the training/testing proposals", "word_idx": 40425, "sentence_idx": 677, "label": "unlabeled"}, {"type": "text", "expr": " This result serves as the baseline for the following comparisons", "word_idx": 40512, "sentence_idx": 678, "label": "unlabeled"}, {"type": "text", "expr": "Somewhat surprisingly, the RPN still leads to a competitive result (55", "word_idx": 40577, "sentence_idx": 679, "label": "unlabeled"}, {"type": "text", "expr": "1%) when using the top-ranked 100 proposals at test-time, indicating that the top-ranked RPN proposals are accurate", "word_idx": 40647, "sentence_idx": 680, "label": "unlabeled"}, {"type": "text", "expr": " On the other extreme, using the top-ranked 6000 RPN proposals (without NMS) has a comparable mAP (55", "word_idx": 40762, "sentence_idx": 681, "label": "unlabeled"}, {"type": "text", "expr": "2%), suggesting NMS does not harm the detection mAP and may reduce false alarms", "word_idx": 40863, "sentence_idx": 682, "label": "unlabeled"}, {"type": "text", "expr": "Somewhat surprisingly, the RPN still leads to a competitive result (55", "word_idx": 40942, "sentence_idx": 683, "label": "unlabeled"}, {"type": "text", "expr": "1%) when using the top-ranked 100 proposals at test-time, indicating that the top-ranked RPN proposals are accurate", "word_idx": 41012, "sentence_idx": 684, "label": "unlabeled"}, {"type": "text", "expr": " On the other extreme, using the top-ranked 6000 RPN proposals (without NMS) has a comparable mAP (55", "word_idx": 41127, "sentence_idx": 685, "label": "unlabeled"}, {"type": "text", "expr": "2%), suggesting NMS does not harm the detection mAP and may reduce false alarms", "word_idx": 41228, "sentence_idx": 686, "label": "unlabeled"}, {"type": "text", "expr": "Next, we separately investigate the roles of RPN\u2019s  cls  and  reg  outputs by turning off either of them at test-time", "word_idx": 41307, "sentence_idx": 687, "label": "unlabeled"}, {"type": "text", "expr": "\nWhen the  cls  layer is removed at test-time (thus no NMS/ranking is used), we randomly sample  $N$  proposals from the unscored regions", "word_idx": 41424, "sentence_idx": 688, "label": "unlabeled"}, {"type": "text", "expr": " The mAP is nearly unchanged with  $N=1000$  (55", "word_idx": 41561, "sentence_idx": 689, "label": "unlabeled"}, {"type": "text", "expr": "8%), but degrades considerably to 44", "word_idx": 41609, "sentence_idx": 690, "label": "unlabeled"}, {"type": "text", "expr": "6% when  $N=100$ ", "word_idx": 41645, "sentence_idx": 691, "label": "unlabeled"}, {"type": "text", "expr": " This shows that the  cls  scores account for the accuracy of the highest ranked proposals", "word_idx": 41662, "sentence_idx": 692, "label": "unlabeled"}, {"type": "math", "expr": "$$N=1000$$", "word_idx": 41752, "sentence_idx": 693, "label": "unlabeled"}, {"type": "math", "expr": "$$N=100$$", "word_idx": 41758, "sentence_idx": 694, "label": "unlabeled"}, {"type": "text", "expr": "On the other hand, when the  reg  layer is removed at test-time (so the proposals become anchor boxes), the mAP drops to 52", "word_idx": 41763, "sentence_idx": 695, "label": "unlabeled"}, {"type": "text", "expr": " This suggests that the high-quality proposals are mainly due to the regressed box bounds", "word_idx": 41886, "sentence_idx": 696, "label": "unlabeled"}, {"type": "text", "expr": " The anchor boxes, though having multiple scales and aspect ratios, are not sufficient for accurate detection", "word_idx": 41975, "sentence_idx": 697, "label": "unlabeled"}, {"type": "text", "expr": "We also evaluate the effects of more powerful networks on the proposal quality of RPN alone", "word_idx": 42084, "sentence_idx": 698, "label": "unlabeled"}, {"type": "text", "expr": " We use VGG-16 to train the RPN, and still use the above detector of SS+ZF", "word_idx": 42175, "sentence_idx": 699, "label": "unlabeled"}, {"type": "text", "expr": " The mAP improves from 56", "word_idx": 42249, "sentence_idx": 700, "label": "unlabeled"}, {"type": "text", "expr": "8% (using RPN+ZF) to 59", "word_idx": 42274, "sentence_idx": 701, "label": "unlabeled"}, {"type": "text", "expr": "2% (using RPN+VGG)", "word_idx": 42297, "sentence_idx": 702, "label": "unlabeled"}, {"type": "text", "expr": "\nThis is a promising result, because it suggests that the proposal quality of RPN+VGG is better than that of RPN+ZF", "word_idx": 42315, "sentence_idx": 703, "label": "unlabeled"}, {"type": "text", "expr": " Because proposals of RPN+ZF are competitive with SS (both are 58", "word_idx": 42430, "sentence_idx": 704, "label": "unlabeled"}, {"type": "text", "expr": "7% when consistently used for training and testing), we may expect RPN+VGG to be better than SS", "word_idx": 42495, "sentence_idx": 705, "label": "unlabeled"}, {"type": "text", "expr": " The following experiments justify this hypothesis", "word_idx": 42590, "sentence_idx": 706, "label": "unlabeled"}, {"type": "text", "expr": "We also evaluate the effects of more powerful networks on the proposal quality of RPN alone", "word_idx": 42640, "sentence_idx": 707, "label": "unlabeled"}, {"type": "text", "expr": " We use VGG-16 to train the RPN, and still use the above detector of SS+ZF", "word_idx": 42731, "sentence_idx": 708, "label": "unlabeled"}, {"type": "text", "expr": " The mAP improves from 56", "word_idx": 42805, "sentence_idx": 709, "label": "unlabeled"}, {"type": "text", "expr": "8% (using RPN+ZF) to 59", "word_idx": 42830, "sentence_idx": 710, "label": "unlabeled"}, {"type": "text", "expr": "2% (using RPN+VGG)", "word_idx": 42853, "sentence_idx": 711, "label": "unlabeled"}, {"type": "text", "expr": "\nThis is a promising result, because it suggests that the proposal quality of RPN+VGG is better than that of RPN+ZF", "word_idx": 42871, "sentence_idx": 712, "label": "unlabeled"}, {"type": "text", "expr": " Because proposals of RPN+ZF are competitive with SS (both are 58", "word_idx": 42986, "sentence_idx": 713, "label": "unlabeled"}, {"type": "text", "expr": "7% when consistently used for training and testing), we may expect RPN+VGG to be better than SS", "word_idx": 43051, "sentence_idx": 714, "label": "unlabeled"}, {"type": "text", "expr": " The following experiments justify this hypothesis", "word_idx": 43146, "sentence_idx": 715, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a06:  Results on PASCAL VOC 2007 test set with Fast R-CNN detectors and VGG-16", "word_idx": 43196, "sentence_idx": 716, "label": "unlabeled"}, {"type": "text", "expr": "\nFor RPN, the train-time proposals for Fast R-CNN are 2000", "word_idx": 43278, "sentence_idx": 717, "label": "unlabeled"}, {"type": "text", "expr": " RPN ${}^{*}$  denotes the unsharing feature version", "word_idx": 43336, "sentence_idx": 718, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a06:", "word_idx": 43388, "sentence_idx": 719, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{*}$$", "word_idx": 43396, "sentence_idx": 720, "label": "unlabeled"}, {"type": "text", "expr": "method # box data mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv", "word_idx": 43402, "sentence_idx": 721, "label": "unlabeled"}, {"type": "text", "expr": "method", "word_idx": 43527, "sentence_idx": 722, "label": "unlabeled"}, {"type": "text", "expr": "method", "word_idx": 43533, "sentence_idx": 723, "label": "unlabeled"}, {"type": "text", "expr": "# box", "word_idx": 43539, "sentence_idx": 724, "label": "unlabeled"}, {"type": "text", "expr": "# box", "word_idx": 43544, "sentence_idx": 725, "label": "unlabeled"}, {"type": "text", "expr": "bottle", "word_idx": 43549, "sentence_idx": 726, "label": "unlabeled"}, {"type": "text", "expr": "bottle", "word_idx": 43555, "sentence_idx": 727, "label": "unlabeled"}, {"type": "text", "expr": "chair", "word_idx": 43561, "sentence_idx": 728, "label": "unlabeled"}, {"type": "text", "expr": "chair", "word_idx": 43566, "sentence_idx": 729, "label": "unlabeled"}, {"type": "text", "expr": "table", "word_idx": 43571, "sentence_idx": 730, "label": "unlabeled"}, {"type": "text", "expr": "table", "word_idx": 43576, "sentence_idx": 731, "label": "unlabeled"}, {"type": "text", "expr": "horse", "word_idx": 43581, "sentence_idx": 732, "label": "unlabeled"}, {"type": "text", "expr": "horse", "word_idx": 43586, "sentence_idx": 733, "label": "unlabeled"}, {"type": "text", "expr": "mbike", "word_idx": 43591, "sentence_idx": 734, "label": "unlabeled"}, {"type": "text", "expr": "mbike", "word_idx": 43596, "sentence_idx": 735, "label": "unlabeled"}, {"type": "text", "expr": "person", "word_idx": 43601, "sentence_idx": 736, "label": "unlabeled"}, {"type": "text", "expr": "person", "word_idx": 43607, "sentence_idx": 737, "label": "unlabeled"}, {"type": "text", "expr": "plant", "word_idx": 43613, "sentence_idx": 738, "label": "unlabeled"}, {"type": "text", "expr": "plant", "word_idx": 43618, "sentence_idx": 739, "label": "unlabeled"}, {"type": "text", "expr": "sheep", "word_idx": 43623, "sentence_idx": 740, "label": "unlabeled"}, {"type": "text", "expr": "sheep", "word_idx": 43628, "sentence_idx": 741, "label": "unlabeled"}, {"type": "text", "expr": "train", "word_idx": 43633, "sentence_idx": 742, "label": "unlabeled"}, {"type": "text", "expr": "train", "word_idx": 43638, "sentence_idx": 743, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 07 66", "word_idx": 43643, "sentence_idx": 744, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 07+12 70", "word_idx": 43656, "sentence_idx": 745, "label": "unlabeled"}, {"type": "text", "expr": "07+12", "word_idx": 43672, "sentence_idx": 746, "label": "unlabeled"}, {"type": "text", "expr": "07+12", "word_idx": 43677, "sentence_idx": 747, "label": "unlabeled"}, {"type": "text", "expr": "300 07 68", "word_idx": 43682, "sentence_idx": 748, "label": "unlabeled"}, {"type": "text", "expr": "RPN ${}^{*}$", "word_idx": 43691, "sentence_idx": 749, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{*}$$", "word_idx": 43703, "sentence_idx": 750, "label": "unlabeled"}, {"type": "text", "expr": "RPN 300 07 69", "word_idx": 43709, "sentence_idx": 751, "label": "unlabeled"}, {"type": "text", "expr": "RPN 300 07+12 73", "word_idx": 43722, "sentence_idx": 752, "label": "unlabeled"}, {"type": "text", "expr": "07+12", "word_idx": 43738, "sentence_idx": 753, "label": "unlabeled"}, {"type": "text", "expr": "07+12", "word_idx": 43743, "sentence_idx": 754, "label": "unlabeled"}, {"type": "text", "expr": "RPN 300 COCO+07+12 78", "word_idx": 43748, "sentence_idx": 755, "label": "unlabeled"}, {"type": "text", "expr": "COCO+07+12", "word_idx": 43769, "sentence_idx": 756, "label": "unlabeled"}, {"type": "text", "expr": "COCO+07+12", "word_idx": 43779, "sentence_idx": 757, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a07:  Results on PASCAL VOC 2012 test set with Fast R-CNN detectors and VGG-16", "word_idx": 43789, "sentence_idx": 758, "label": "unlabeled"}, {"type": "text", "expr": "\nFor RPN, the train-time proposals for Fast R-CNN are 2000", "word_idx": 43871, "sentence_idx": 759, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a07:", "word_idx": 43929, "sentence_idx": 760, "label": "unlabeled"}, {"type": "text", "expr": "method # box data mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv", "word_idx": 43937, "sentence_idx": 761, "label": "unlabeled"}, {"type": "text", "expr": "method", "word_idx": 44062, "sentence_idx": 762, "label": "unlabeled"}, {"type": "text", "expr": "method", "word_idx": 44068, "sentence_idx": 763, "label": "unlabeled"}, {"type": "text", "expr": "# box", "word_idx": 44074, "sentence_idx": 764, "label": "unlabeled"}, {"type": "text", "expr": "# box", "word_idx": 44079, "sentence_idx": 765, "label": "unlabeled"}, {"type": "text", "expr": "bottle", "word_idx": 44084, "sentence_idx": 766, "label": "unlabeled"}, {"type": "text", "expr": "bottle", "word_idx": 44090, "sentence_idx": 767, "label": "unlabeled"}, {"type": "text", "expr": "chair", "word_idx": 44096, "sentence_idx": 768, "label": "unlabeled"}, {"type": "text", "expr": "chair", "word_idx": 44101, "sentence_idx": 769, "label": "unlabeled"}, {"type": "text", "expr": "table", "word_idx": 44106, "sentence_idx": 770, "label": "unlabeled"}, {"type": "text", "expr": "table", "word_idx": 44111, "sentence_idx": 771, "label": "unlabeled"}, {"type": "text", "expr": "horse", "word_idx": 44116, "sentence_idx": 772, "label": "unlabeled"}, {"type": "text", "expr": "horse", "word_idx": 44121, "sentence_idx": 773, "label": "unlabeled"}, {"type": "text", "expr": "mbike", "word_idx": 44126, "sentence_idx": 774, "label": "unlabeled"}, {"type": "text", "expr": "mbike", "word_idx": 44131, "sentence_idx": 775, "label": "unlabeled"}, {"type": "text", "expr": "person", "word_idx": 44136, "sentence_idx": 776, "label": "unlabeled"}, {"type": "text", "expr": "person", "word_idx": 44142, "sentence_idx": 777, "label": "unlabeled"}, {"type": "text", "expr": "plant", "word_idx": 44148, "sentence_idx": 778, "label": "unlabeled"}, {"type": "text", "expr": "plant", "word_idx": 44153, "sentence_idx": 779, "label": "unlabeled"}, {"type": "text", "expr": "sheep", "word_idx": 44158, "sentence_idx": 780, "label": "unlabeled"}, {"type": "text", "expr": "sheep", "word_idx": 44163, "sentence_idx": 781, "label": "unlabeled"}, {"type": "text", "expr": "train", "word_idx": 44168, "sentence_idx": 782, "label": "unlabeled"}, {"type": "text", "expr": "train", "word_idx": 44173, "sentence_idx": 783, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 12 65", "word_idx": 44178, "sentence_idx": 784, "label": "unlabeled"}, {"type": "text", "expr": "SS 2000 07++12 68", "word_idx": 44191, "sentence_idx": 785, "label": "unlabeled"}, {"type": "text", "expr": "07++12", "word_idx": 44208, "sentence_idx": 786, "label": "unlabeled"}, {"type": "text", "expr": "07++12", "word_idx": 44214, "sentence_idx": 787, "label": "unlabeled"}, {"type": "text", "expr": "RPN 300 12 67", "word_idx": 44220, "sentence_idx": 788, "label": "unlabeled"}, {"type": "text", "expr": "RPN 300 07++12 70", "word_idx": 44233, "sentence_idx": 789, "label": "unlabeled"}, {"type": "text", "expr": "07++12", "word_idx": 44250, "sentence_idx": 790, "label": "unlabeled"}, {"type": "text", "expr": "07++12", "word_idx": 44256, "sentence_idx": 791, "label": "unlabeled"}, {"type": "text", "expr": "RPN 300 COCO+07++12 75", "word_idx": 44262, "sentence_idx": 792, "label": "unlabeled"}, {"type": "text", "expr": "COCO+07++12", "word_idx": 44284, "sentence_idx": 793, "label": "unlabeled"}, {"type": "text", "expr": "COCO+07++12", "word_idx": 44295, "sentence_idx": 794, "label": "unlabeled"}, {"type": "text", "expr": "Performance of VGG-16", "word_idx": 44306, "sentence_idx": 795, "label": "unlabeled"}, {"type": "text", "expr": " \nTable\u00a0 3  shows the results of VGG-16 for both proposal and detection", "word_idx": 44327, "sentence_idx": 796, "label": "unlabeled"}, {"type": "text", "expr": " Using RPN+VGG, the result is 68", "word_idx": 44398, "sentence_idx": 797, "label": "unlabeled"}, {"type": "text", "expr": "5% for  unshared  features, slightly higher than the SS baseline", "word_idx": 44430, "sentence_idx": 798, "label": "unlabeled"}, {"type": "text", "expr": " As shown above, this is because the proposals generated by RPN+VGG are more accurate than SS", "word_idx": 44494, "sentence_idx": 799, "label": "unlabeled"}, {"type": "text", "expr": " Unlike SS that is pre-defined, the RPN is actively trained and benefits from better networks", "word_idx": 44587, "sentence_idx": 800, "label": "unlabeled"}, {"type": "text", "expr": " For the feature- shared  variant, the result is 69", "word_idx": 44680, "sentence_idx": 801, "label": "unlabeled"}, {"type": "text", "expr": "9%\u2014better than the strong SS baseline, yet with nearly cost-free proposals", "word_idx": 44731, "sentence_idx": 802, "label": "unlabeled"}, {"type": "text", "expr": " We further train the RPN and detection network on the union set of PASCAL VOC 2007 trainval and 2012 trainval", "word_idx": 44805, "sentence_idx": 803, "label": "unlabeled"}, {"type": "text", "expr": " The mAP is  73", "word_idx": 44915, "sentence_idx": 804, "label": "unlabeled"}, {"type": "text", "expr": "\nFigure\u00a0 5  shows some results on the PASCAL VOC 2007 test set", "word_idx": 44930, "sentence_idx": 805, "label": "unlabeled"}, {"type": "text", "expr": "\nOn the PASCAL VOC 2012 test set (Table\u00a0 4 ), our method has an mAP of  70", "word_idx": 44992, "sentence_idx": 806, "label": "unlabeled"}, {"type": "text", "expr": "4%  trained on the union set of VOC 2007 trainval+test and VOC 2012 trainval", "word_idx": 45066, "sentence_idx": 807, "label": "unlabeled"}, {"type": "text", "expr": " Table\u00a0 6  and Table\u00a0 7  show the detailed numbers", "word_idx": 45142, "sentence_idx": 808, "label": "unlabeled"}, {"type": "text", "expr": "Performance of VGG-16", "word_idx": 45192, "sentence_idx": 809, "label": "unlabeled"}, {"type": "text", "expr": "unshared", "word_idx": 45213, "sentence_idx": 810, "label": "unlabeled"}, {"type": "text", "expr": "shared", "word_idx": 45221, "sentence_idx": 811, "label": "unlabeled"}, {"type": "text", "expr": "In Table\u00a0 5  we summarize the running time of the entire object detection system", "word_idx": 45227, "sentence_idx": 812, "label": "unlabeled"}, {"type": "text", "expr": " SS takes 1-2 seconds depending on content (on average about 1", "word_idx": 45307, "sentence_idx": 813, "label": "unlabeled"}, {"type": "text", "expr": "5s), and Fast R-CNN with VGG-16 takes 320ms on 2000 SS proposals (or 223ms if using SVD on fully-connected layers  )", "word_idx": 45369, "sentence_idx": 814, "label": "unlabeled"}, {"type": "text", "expr": " Our system with VGG-16 takes in total  198ms  for both proposal and detection", "word_idx": 45485, "sentence_idx": 815, "label": "unlabeled"}, {"type": "text", "expr": " With the convolutional features shared, the RPN alone only takes 10ms computing the additional layers", "word_idx": 45563, "sentence_idx": 816, "label": "unlabeled"}, {"type": "text", "expr": "\nOur region-wise computation is also lower, thanks to fewer proposals (300 per image)", "word_idx": 45665, "sentence_idx": 817, "label": "unlabeled"}, {"type": "text", "expr": " Our system has a frame-rate of 17 fps with the ZF net", "word_idx": 45750, "sentence_idx": 818, "label": "unlabeled"}, {"type": "text", "expr": "198ms", "word_idx": 45804, "sentence_idx": 819, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a08:  Detection results of Faster R-CNN on PASCAL VOC 2007 test set using  different settings of anchors ", "word_idx": 45809, "sentence_idx": 820, "label": "unlabeled"}, {"type": "text", "expr": " The network is VGG-16", "word_idx": 45918, "sentence_idx": 821, "label": "unlabeled"}, {"type": "text", "expr": " The training data is VOC 2007 trainval", "word_idx": 45940, "sentence_idx": 822, "label": "unlabeled"}, {"type": "text", "expr": " The default setting of using 3 scales and 3 aspect ratios (69", "word_idx": 45979, "sentence_idx": 823, "label": "unlabeled"}, {"type": "text", "expr": "9%) is the same as that in Table\u00a0 3 ", "word_idx": 46041, "sentence_idx": 824, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a08:", "word_idx": 46077, "sentence_idx": 825, "label": "unlabeled"}, {"type": "text", "expr": "different settings of anchors", "word_idx": 46085, "sentence_idx": 826, "label": "unlabeled"}, {"type": "text", "expr": "settings anchor scales aspect ratios mAP (%)", "word_idx": 46114, "sentence_idx": 827, "label": "unlabeled"}, {"type": "text", "expr": "settings", "word_idx": 46158, "sentence_idx": 828, "label": "unlabeled"}, {"type": "text", "expr": "anchor scales", "word_idx": 46166, "sentence_idx": 829, "label": "unlabeled"}, {"type": "text", "expr": "aspect ratios", "word_idx": 46179, "sentence_idx": 830, "label": "unlabeled"}, {"type": "text", "expr": "mAP (%)", "word_idx": 46192, "sentence_idx": 831, "label": "unlabeled"}, {"type": "text", "expr": "1 scale, 1 ratio 1:1 65", "word_idx": 46199, "sentence_idx": 832, "label": "unlabeled"}, {"type": "text", "expr": "1 scale, 1 ratio", "word_idx": 46222, "sentence_idx": 833, "label": "unlabeled"}, {"type": "math", "expr": "$$128^{2}$$", "word_idx": 46238, "sentence_idx": 834, "label": "unlabeled"}, {"type": "text", "expr": "1:1 66", "word_idx": 46245, "sentence_idx": 835, "label": "unlabeled"}, {"type": "math", "expr": "$$256^{2}$$", "word_idx": 46251, "sentence_idx": 836, "label": "unlabeled"}, {"type": "text", "expr": "1 scale, 3 ratios {2:1, 1:1, 1:2} 68", "word_idx": 46258, "sentence_idx": 837, "label": "unlabeled"}, {"type": "text", "expr": "1 scale, 3 ratios", "word_idx": 46294, "sentence_idx": 838, "label": "unlabeled"}, {"type": "math", "expr": "$$128^{2}$$", "word_idx": 46311, "sentence_idx": 839, "label": "unlabeled"}, {"type": "text", "expr": "{2:1, 1:1, 1:2}", "word_idx": 46318, "sentence_idx": 840, "label": "unlabeled"}, {"type": "text", "expr": "{2:1, 1:1, 1:2}", "word_idx": 46333, "sentence_idx": 841, "label": "unlabeled"}, {"type": "text", "expr": "{2:1, 1:1, 1:2} 67", "word_idx": 46348, "sentence_idx": 842, "label": "unlabeled"}, {"type": "math", "expr": "$$256^{2}$$", "word_idx": 46366, "sentence_idx": 843, "label": "unlabeled"}, {"type": "text", "expr": "{2:1, 1:1, 1:2}", "word_idx": 46373, "sentence_idx": 844, "label": "unlabeled"}, {"type": "text", "expr": "{2:1, 1:1, 1:2}", "word_idx": 46388, "sentence_idx": 845, "label": "unlabeled"}, {"type": "text", "expr": "3 scales, 1 ratio 1:1 69", "word_idx": 46403, "sentence_idx": 846, "label": "unlabeled"}, {"type": "text", "expr": "3 scales, 1 ratio", "word_idx": 46427, "sentence_idx": 847, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{128^{2},256^{2},512^{2}\\}$$", "word_idx": 46444, "sentence_idx": 848, "label": "unlabeled"}, {"type": "text", "expr": "3 scales, 3 ratios {2:1, 1:1, 1:2} 69", "word_idx": 46471, "sentence_idx": 849, "label": "unlabeled"}, {"type": "text", "expr": "3 scales, 3 ratios", "word_idx": 46508, "sentence_idx": 850, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{128^{2},256^{2},512^{2}\\}$$", "word_idx": 46526, "sentence_idx": 851, "label": "unlabeled"}, {"type": "text", "expr": "{2:1, 1:1, 1:2}", "word_idx": 46553, "sentence_idx": 852, "label": "unlabeled"}, {"type": "text", "expr": "{2:1, 1:1, 1:2}", "word_idx": 46568, "sentence_idx": 853, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a09:  Detection results of Faster R-CNN on PASCAL VOC 2007 test set using   in Equation\u00a0( 1 )", "word_idx": 46583, "sentence_idx": 854, "label": "unlabeled"}, {"type": "text", "expr": " The network is VGG-16", "word_idx": 46680, "sentence_idx": 855, "label": "unlabeled"}, {"type": "text", "expr": " The training data is VOC 2007 trainval", "word_idx": 46702, "sentence_idx": 856, "label": "unlabeled"}, {"type": "text", "expr": " The default setting of using  $\\lambda=10$  (69", "word_idx": 46741, "sentence_idx": 857, "label": "unlabeled"}, {"type": "text", "expr": "9%) is the same as that in Table\u00a0 3 ", "word_idx": 46789, "sentence_idx": 858, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a09:", "word_idx": 46825, "sentence_idx": 859, "label": "unlabeled"}, {"type": "text", "expr": "different values of  $\\lambda$", "word_idx": 46833, "sentence_idx": 860, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda$$", "word_idx": 46863, "sentence_idx": 861, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda=10$$", "word_idx": 46870, "sentence_idx": 862, "label": "unlabeled"}, {"type": "text", "expr": "1 1 10 100", "word_idx": 46880, "sentence_idx": 863, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda$$", "word_idx": 46890, "sentence_idx": 864, "label": "unlabeled"}, {"type": "text", "expr": "mAP (%) 67", "word_idx": 46897, "sentence_idx": 865, "label": "unlabeled"}, {"type": "text", "expr": "mAP (%)", "word_idx": 46907, "sentence_idx": 866, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:  Recall  \\vs \u00a0IoU overlap ratio on the PASCAL VOC 2007 test set", "word_idx": 46914, "sentence_idx": 867, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:", "word_idx": 46987, "sentence_idx": 868, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a010:  ", "word_idx": 46996, "sentence_idx": 869, "label": "unlabeled"}, {"type": "text", "expr": " Detection results are on the PASCAL VOC 2007 test set using the ZF model and Fast R-CNN", "word_idx": 47007, "sentence_idx": 870, "label": "unlabeled"}, {"type": "text", "expr": " RPN uses unshared features", "word_idx": 47095, "sentence_idx": 871, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a010:", "word_idx": 47122, "sentence_idx": 872, "label": "unlabeled"}, {"type": "text", "expr": "One-Stage Detection  \\vs \u00a0Two-Stage Proposal + Detection", "word_idx": 47131, "sentence_idx": 873, "label": "unlabeled"}, {"type": "text", "expr": "proposals detector mAP (%)", "word_idx": 47187, "sentence_idx": 874, "label": "unlabeled"}, {"type": "text", "expr": "proposals", "word_idx": 47213, "sentence_idx": 875, "label": "unlabeled"}, {"type": "text", "expr": "detector", "word_idx": 47222, "sentence_idx": 876, "label": "unlabeled"}, {"type": "text", "expr": "mAP (%)", "word_idx": 47230, "sentence_idx": 877, "label": "unlabeled"}, {"type": "text", "expr": "Two-Stage RPN + ZF, unshared 300 Fast R-CNN + ZF, 1 scale 58", "word_idx": 47237, "sentence_idx": 878, "label": "unlabeled"}, {"type": "text", "expr": "Two-Stage", "word_idx": 47297, "sentence_idx": 879, "label": "unlabeled"}, {"type": "text", "expr": "Two-Stage", "word_idx": 47306, "sentence_idx": 880, "label": "unlabeled"}, {"type": "text", "expr": "RPN + ZF, unshared", "word_idx": 47315, "sentence_idx": 881, "label": "unlabeled"}, {"type": "text", "expr": "RPN + ZF, unshared", "word_idx": 47333, "sentence_idx": 882, "label": "unlabeled"}, {"type": "text", "expr": "Fast R-CNN + ZF, 1 scale", "word_idx": 47351, "sentence_idx": 883, "label": "unlabeled"}, {"type": "text", "expr": "Fast R-CNN + ZF, 1 scale", "word_idx": 47375, "sentence_idx": 884, "label": "unlabeled"}, {"type": "text", "expr": "One-Stage dense, 3 scales, 3 aspect ratios 20000 Fast R-CNN + ZF, 1 scale 53", "word_idx": 47399, "sentence_idx": 885, "label": "unlabeled"}, {"type": "text", "expr": "One-Stage", "word_idx": 47475, "sentence_idx": 886, "label": "unlabeled"}, {"type": "text", "expr": "dense, 3 scales, 3 aspect ratios", "word_idx": 47484, "sentence_idx": 887, "label": "unlabeled"}, {"type": "text", "expr": "20000", "word_idx": 47516, "sentence_idx": 888, "label": "unlabeled"}, {"type": "text", "expr": "Fast R-CNN + ZF, 1 scale", "word_idx": 47521, "sentence_idx": 889, "label": "unlabeled"}, {"type": "text", "expr": "One-Stage dense, 3 scales, 3 aspect ratios 20000 Fast R-CNN + ZF, 5 scales 53", "word_idx": 47545, "sentence_idx": 890, "label": "unlabeled"}, {"type": "text", "expr": "One-Stage", "word_idx": 47622, "sentence_idx": 891, "label": "unlabeled"}, {"type": "text", "expr": "One-Stage", "word_idx": 47631, "sentence_idx": 892, "label": "unlabeled"}, {"type": "text", "expr": "dense, 3 scales, 3 aspect ratios", "word_idx": 47640, "sentence_idx": 893, "label": "unlabeled"}, {"type": "text", "expr": "dense, 3 scales, 3 aspect ratios", "word_idx": 47672, "sentence_idx": 894, "label": "unlabeled"}, {"type": "text", "expr": "20000", "word_idx": 47704, "sentence_idx": 895, "label": "unlabeled"}, {"type": "text", "expr": "20000", "word_idx": 47709, "sentence_idx": 896, "label": "unlabeled"}, {"type": "text", "expr": "Fast R-CNN + ZF, 5 scales", "word_idx": 47714, "sentence_idx": 897, "label": "unlabeled"}, {"type": "text", "expr": "Fast R-CNN + ZF, 5 scales", "word_idx": 47739, "sentence_idx": 898, "label": "unlabeled"}, {"type": "text", "expr": "Sensitivities to Hyper-parameters", "word_idx": 47764, "sentence_idx": 899, "label": "unlabeled"}, {"type": "text", "expr": "  In Table\u00a0 8  we investigate the settings of anchors", "word_idx": 47797, "sentence_idx": 900, "label": "unlabeled"}, {"type": "text", "expr": " By default we use 3 scales and 3 aspect ratios (69", "word_idx": 47850, "sentence_idx": 901, "label": "unlabeled"}, {"type": "text", "expr": "9% mAP in Table\u00a0 8 )", "word_idx": 47901, "sentence_idx": 902, "label": "unlabeled"}, {"type": "text", "expr": "\nIf using just one anchor at each position, the mAP drops by a considerable margin of 3-4%", "word_idx": 47921, "sentence_idx": 903, "label": "unlabeled"}, {"type": "text", "expr": " The mAP is higher if using 3 scales (with 1 aspect ratio) or 3 aspect ratios (with 1 scale), demonstrating that using anchors of multiple sizes as the regression references is an effective solution", "word_idx": 48011, "sentence_idx": 904, "label": "unlabeled"}, {"type": "text", "expr": " Using just 3 scales with 1 aspect ratio (69", "word_idx": 48209, "sentence_idx": 905, "label": "unlabeled"}, {"type": "text", "expr": "8%) is as good as using 3 scales with 3 aspect ratios on this dataset, suggesting that scales and aspect ratios are not disentangled dimensions for the detection accuracy", "word_idx": 48253, "sentence_idx": 906, "label": "unlabeled"}, {"type": "text", "expr": " But we still adopt these two dimensions in our designs to keep our system flexible", "word_idx": 48423, "sentence_idx": 907, "label": "unlabeled"}, {"type": "text", "expr": "Sensitivities to Hyper-parameters", "word_idx": 48506, "sentence_idx": 908, "label": "unlabeled"}, {"type": "text", "expr": "In Table\u00a0 9  we compare different values of  $\\lambda$  in Equation\u00a0( 1 )", "word_idx": 48539, "sentence_idx": 909, "label": "unlabeled"}, {"type": "text", "expr": " By default we use  $\\lambda=10$  which makes the two terms in Equation\u00a0( 1 ) roughly equally weighted after normalization", "word_idx": 48612, "sentence_idx": 910, "label": "unlabeled"}, {"type": "text", "expr": " Table\u00a0 9  shows that our result is impacted just marginally (by  $\\sim 1\\%$ ) when  $\\lambda$  is within a scale of about two orders of magnitude (1 to 100)", "word_idx": 48734, "sentence_idx": 911, "label": "unlabeled"}, {"type": "text", "expr": " This demonstrates that the result is insensitive to  $\\lambda$  in a wide range", "word_idx": 48891, "sentence_idx": 912, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda$$", "word_idx": 48971, "sentence_idx": 913, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda=10$$", "word_idx": 48978, "sentence_idx": 914, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sim 1\\%$$", "word_idx": 48988, "sentence_idx": 915, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda$$", "word_idx": 48996, "sentence_idx": 916, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda$$", "word_idx": 49003, "sentence_idx": 917, "label": "unlabeled"}, {"type": "text", "expr": "Analysis of Recall-to-IoU", "word_idx": 49010, "sentence_idx": 918, "label": "unlabeled"}, {"type": "text", "expr": " \nNext we compute the recall of proposals at different IoU ratios with ground-truth boxes", "word_idx": 49035, "sentence_idx": 919, "label": "unlabeled"}, {"type": "text", "expr": " It is noteworthy that the Recall-to-IoU metric is just  loosely    related to the ultimate detection accuracy", "word_idx": 49124, "sentence_idx": 920, "label": "unlabeled"}, {"type": "text", "expr": " It is more appropriate to use this metric to  diagnose  the proposal method than to evaluate it", "word_idx": 49234, "sentence_idx": 921, "label": "unlabeled"}, {"type": "text", "expr": "Analysis of Recall-to-IoU", "word_idx": 49330, "sentence_idx": 922, "label": "unlabeled"}, {"type": "text", "expr": "loosely", "word_idx": 49355, "sentence_idx": 923, "label": "unlabeled"}, {"type": "text", "expr": "diagnose", "word_idx": 49362, "sentence_idx": 924, "label": "unlabeled"}, {"type": "text", "expr": "In Figure\u00a0 4 , we show the results of using 300, 1000, and 2000 proposals", "word_idx": 49370, "sentence_idx": 925, "label": "unlabeled"}, {"type": "text", "expr": " We compare with SS and EB, and the  $N$  proposals are the top- $N$  ranked ones based on the confidence generated by these methods", "word_idx": 49443, "sentence_idx": 926, "label": "unlabeled"}, {"type": "text", "expr": "\nThe plots show that the RPN method behaves gracefully when the number of proposals drops from 2000 to 300", "word_idx": 49575, "sentence_idx": 927, "label": "unlabeled"}, {"type": "text", "expr": " This explains why the RPN has a good ultimate detection mAP when using as few as 300 proposals", "word_idx": 49681, "sentence_idx": 928, "label": "unlabeled"}, {"type": "text", "expr": " As we analyzed before, this property is mainly attributed to the  cls  term of the RPN", "word_idx": 49776, "sentence_idx": 929, "label": "unlabeled"}, {"type": "text", "expr": " The recall of SS and EB drops more quickly than RPN when the proposals are fewer", "word_idx": 49863, "sentence_idx": 930, "label": "unlabeled"}, {"type": "text", "expr": "The OverFeat paper   proposes a detection method that uses regressors and classifiers on sliding windows over convolutional feature maps", "word_idx": 49944, "sentence_idx": 931, "label": "unlabeled"}, {"type": "text", "expr": " OverFeat is a  one-stage ,  class-specific  detection pipeline, and ours is a  two-stage cascade  consisting of class-agnostic proposals and class-specific detections", "word_idx": 50080, "sentence_idx": 932, "label": "unlabeled"}, {"type": "text", "expr": " In OverFeat, the region-wise features come from a sliding window of one aspect ratio over a scale pyramid", "word_idx": 50247, "sentence_idx": 933, "label": "unlabeled"}, {"type": "text", "expr": " These features are used to simultaneously determine the location and category of objects", "word_idx": 50353, "sentence_idx": 934, "label": "unlabeled"}, {"type": "text", "expr": " In RPN, the features are from square (3 $\\times$ 3) sliding windows and predict proposals relative to anchors with different scales and aspect ratios", "word_idx": 50442, "sentence_idx": 935, "label": "unlabeled"}, {"type": "text", "expr": " Though both methods use sliding windows, the region proposal task is only the first stage of Faster R-CNN\u2014the downstream Fast R-CNN detector  attends  to the proposals to refine them", "word_idx": 50592, "sentence_idx": 936, "label": "unlabeled"}, {"type": "text", "expr": " In the second stage of our cascade, the region-wise features are adaptively pooled   from proposal boxes that more faithfully cover the features of the regions", "word_idx": 50775, "sentence_idx": 937, "label": "unlabeled"}, {"type": "text", "expr": " We believe these features lead to more accurate detections", "word_idx": 50935, "sentence_idx": 938, "label": "unlabeled"}, {"type": "text", "expr": "One-Stage Detection  \\vs \u00a0Two-Stage Proposal + Detection", "word_idx": 50994, "sentence_idx": 939, "label": "unlabeled"}, {"type": "text", "expr": "one-stage", "word_idx": 51050, "sentence_idx": 940, "label": "unlabeled"}, {"type": "text", "expr": "class-specific", "word_idx": 51059, "sentence_idx": 941, "label": "unlabeled"}, {"type": "text", "expr": "two-stage cascade", "word_idx": 51073, "sentence_idx": 942, "label": "unlabeled"}, {"type": "math", "expr": "$$\\times$$", "word_idx": 51090, "sentence_idx": 943, "label": "unlabeled"}, {"type": "text", "expr": "attends", "word_idx": 51096, "sentence_idx": 944, "label": "unlabeled"}, {"type": "text", "expr": "To compare the one-stage and two-stage systems, we  emulate  the OverFeat system (and thus also circumvent other differences of implementation details) by  one-stage  Fast R-CNN", "word_idx": 51103, "sentence_idx": 945, "label": "unlabeled"}, {"type": "text", "expr": " In this system, the \u201cproposals\u201d are dense sliding windows of 3 scales (128, 256, 512) and 3 aspect ratios (1:1, 1:2, 2:1)", "word_idx": 51280, "sentence_idx": 946, "label": "unlabeled"}, {"type": "text", "expr": "\nFast R-CNN is trained to predict class-specific scores and regress box locations from these sliding windows", "word_idx": 51402, "sentence_idx": 947, "label": "unlabeled"}, {"type": "text", "expr": " Because the OverFeat system adopts an image pyramid, we also evaluate using convolutional features extracted from 5 scales", "word_idx": 51510, "sentence_idx": 948, "label": "unlabeled"}, {"type": "text", "expr": " We use those 5 scales as in  ", "word_idx": 51633, "sentence_idx": 949, "label": "unlabeled"}, {"type": "text", "expr": "emulate", "word_idx": 51663, "sentence_idx": 950, "label": "unlabeled"}, {"type": "text", "expr": "one-stage", "word_idx": 51670, "sentence_idx": 951, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a0 10  compares the two-stage system and two variants of the one-stage system", "word_idx": 51679, "sentence_idx": 952, "label": "unlabeled"}, {"type": "text", "expr": " Using the ZF model, the one-stage system has an mAP of 53", "word_idx": 51760, "sentence_idx": 953, "label": "unlabeled"}, {"type": "text", "expr": " This is lower than the two-stage system (58", "word_idx": 51818, "sentence_idx": 954, "label": "unlabeled"}, {"type": "text", "expr": "7%) by 4", "word_idx": 51862, "sentence_idx": 955, "label": "unlabeled"}, {"type": "text", "expr": " This experiment justifies the effectiveness of cascaded region proposals and object detection", "word_idx": 51870, "sentence_idx": 956, "label": "unlabeled"}, {"type": "text", "expr": " Similar observations are reported in  , where replacing SS region proposals with sliding windows leads to  $\\sim$ 6% degradation in both papers", "word_idx": 51964, "sentence_idx": 957, "label": "unlabeled"}, {"type": "text", "expr": "\nWe also note that the one-stage system is slower as it has considerably more proposals to process", "word_idx": 52108, "sentence_idx": 958, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sim$$", "word_idx": 52206, "sentence_idx": 959, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a011:  Object detection results (%) on the  MS COCO  dataset", "word_idx": 52210, "sentence_idx": 960, "label": "unlabeled"}, {"type": "text", "expr": " The model is VGG-16", "word_idx": 52274, "sentence_idx": 961, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a011:", "word_idx": 52294, "sentence_idx": 962, "label": "unlabeled"}, {"type": "text", "expr": "MS COCO", "word_idx": 52303, "sentence_idx": 963, "label": "unlabeled"}, {"type": "text", "expr": "COCO val COCO test-dev", "word_idx": 52310, "sentence_idx": 964, "label": "unlabeled"}, {"type": "text", "expr": "COCO val", "word_idx": 52332, "sentence_idx": 965, "label": "unlabeled"}, {"type": "text", "expr": "COCO test-dev", "word_idx": 52340, "sentence_idx": 966, "label": "unlabeled"}, {"type": "text", "expr": "method proposals training data \u00a0\u00a0\u00a0mAP@", "word_idx": 52353, "sentence_idx": 967, "label": "unlabeled"}, {"type": "text", "expr": "5 mAP@[", "word_idx": 52391, "sentence_idx": 968, "label": "unlabeled"}, {"type": "text", "expr": "95] \u00a0\u00a0\u00a0mAP@", "word_idx": 52398, "sentence_idx": 969, "label": "unlabeled"}, {"type": "text", "expr": "5 mAP@[", "word_idx": 52409, "sentence_idx": 970, "label": "unlabeled"}, {"type": "text", "expr": "method", "word_idx": 52416, "sentence_idx": 971, "label": "unlabeled"}, {"type": "text", "expr": "proposals", "word_idx": 52422, "sentence_idx": 972, "label": "unlabeled"}, {"type": "text", "expr": "training data", "word_idx": 52431, "sentence_idx": 973, "label": "unlabeled"}, {"type": "text", "expr": "mAP@[", "word_idx": 52444, "sentence_idx": 974, "label": "unlabeled"}, {"type": "text", "expr": "mAP@[", "word_idx": 52449, "sentence_idx": 975, "label": "unlabeled"}, {"type": "text", "expr": "SS, 2000 COCO train - - 35", "word_idx": 52454, "sentence_idx": 976, "label": "unlabeled"}, {"type": "text", "expr": "Fast R-CNN", "word_idx": 52480, "sentence_idx": 977, "label": "unlabeled"}, {"type": "text", "expr": "Fast R-CNN", "word_idx": 52490, "sentence_idx": 978, "label": "unlabeled"}, {"type": "text", "expr": "SS, 2000", "word_idx": 52500, "sentence_idx": 979, "label": "unlabeled"}, {"type": "text", "expr": "SS, 2000", "word_idx": 52508, "sentence_idx": 980, "label": "unlabeled"}, {"type": "text", "expr": "COCO train", "word_idx": 52516, "sentence_idx": 981, "label": "unlabeled"}, {"type": "text", "expr": "COCO train", "word_idx": 52526, "sentence_idx": 982, "label": "unlabeled"}, {"type": "text", "expr": "SS, 2000 COCO train 38", "word_idx": 52536, "sentence_idx": 983, "label": "unlabeled"}, {"type": "text", "expr": "Fast R-CNN  [impl", "word_idx": 52558, "sentence_idx": 984, "label": "unlabeled"}, {"type": "text", "expr": " in this paper]", "word_idx": 52575, "sentence_idx": 985, "label": "unlabeled"}, {"type": "text", "expr": "Fast R-CNN", "word_idx": 52590, "sentence_idx": 986, "label": "unlabeled"}, {"type": "text", "expr": "[impl", "word_idx": 52600, "sentence_idx": 987, "label": "unlabeled"}, {"type": "text", "expr": " in this paper]", "word_idx": 52605, "sentence_idx": 988, "label": "unlabeled"}, {"type": "text", "expr": "SS, 2000", "word_idx": 52620, "sentence_idx": 989, "label": "unlabeled"}, {"type": "text", "expr": "SS, 2000", "word_idx": 52628, "sentence_idx": 990, "label": "unlabeled"}, {"type": "text", "expr": "COCO train", "word_idx": 52636, "sentence_idx": 991, "label": "unlabeled"}, {"type": "text", "expr": "COCO train", "word_idx": 52646, "sentence_idx": 992, "label": "unlabeled"}, {"type": "text", "expr": "Faster R-CNN RPN, 300 COCO train 41", "word_idx": 52656, "sentence_idx": 993, "label": "unlabeled"}, {"type": "text", "expr": "Faster R-CNN", "word_idx": 52691, "sentence_idx": 994, "label": "unlabeled"}, {"type": "text", "expr": "Faster R-CNN", "word_idx": 52703, "sentence_idx": 995, "label": "unlabeled"}, {"type": "text", "expr": "RPN, 300", "word_idx": 52715, "sentence_idx": 996, "label": "unlabeled"}, {"type": "text", "expr": "RPN, 300", "word_idx": 52723, "sentence_idx": 997, "label": "unlabeled"}, {"type": "text", "expr": "COCO train", "word_idx": 52731, "sentence_idx": 998, "label": "unlabeled"}, {"type": "text", "expr": "COCO train", "word_idx": 52741, "sentence_idx": 999, "label": "unlabeled"}, {"type": "text", "expr": "Faster R-CNN RPN, 300 COCO trainval - - 42", "word_idx": 52751, "sentence_idx": 1000, "label": "unlabeled"}, {"type": "text", "expr": "Faster R-CNN", "word_idx": 52793, "sentence_idx": 1001, "label": "unlabeled"}, {"type": "text", "expr": "Faster R-CNN", "word_idx": 52805, "sentence_idx": 1002, "label": "unlabeled"}, {"type": "text", "expr": "RPN, 300", "word_idx": 52817, "sentence_idx": 1003, "label": "unlabeled"}, {"type": "text", "expr": "RPN, 300", "word_idx": 52825, "sentence_idx": 1004, "label": "unlabeled"}, {"type": "text", "expr": "COCO trainval", "word_idx": 52833, "sentence_idx": 1005, "label": "unlabeled"}, {"type": "text", "expr": "COCO trainval", "word_idx": 52846, "sentence_idx": 1006, "label": "unlabeled"}, {"type": "text", "expr": "2  Experiments on MS COCO", "word_idx": 52859, "sentence_idx": 1007, "label": "unlabeled"}, {"type": "text", "expr": "We present more results on the Microsoft COCO object detection dataset  ", "word_idx": 52884, "sentence_idx": 1008, "label": "unlabeled"}, {"type": "text", "expr": " This dataset involves 80 object categories", "word_idx": 52956, "sentence_idx": 1009, "label": "unlabeled"}, {"type": "text", "expr": " We experiment with the 80k images on the training set, 40k images on the validation set, and 20k images on the test-dev set", "word_idx": 52999, "sentence_idx": 1010, "label": "unlabeled"}, {"type": "text", "expr": "\nWe evaluate the mAP averaged for IoU  $\\in[05:005:095]$  (COCO\u2019s standard metric, simply denoted as mAP@[", "word_idx": 53123, "sentence_idx": 1011, "label": "unlabeled"}, {"type": "text", "expr": "95]) and mAP@0", "word_idx": 53229, "sentence_idx": 1012, "label": "unlabeled"}, {"type": "text", "expr": "5 (PASCAL VOC\u2019s metric)", "word_idx": 53243, "sentence_idx": 1013, "label": "unlabeled"}, {"type": "math", "expr": "$$\\in[0.5:0.05:0.95]$$", "word_idx": 53266, "sentence_idx": 1014, "label": "unlabeled"}, {"type": "text", "expr": "There are a few minor changes of our system made for this dataset", "word_idx": 53284, "sentence_idx": 1015, "label": "unlabeled"}, {"type": "text", "expr": " We train our models on an 8-GPU implementation, and the effective mini-batch size becomes 8 for RPN (1 per GPU) and 16 for Fast R-CNN (2 per GPU)", "word_idx": 53349, "sentence_idx": 1016, "label": "unlabeled"}, {"type": "text", "expr": " The RPN step and Fast R-CNN step are both trained for 240k iterations with a learning rate of 0", "word_idx": 53495, "sentence_idx": 1017, "label": "unlabeled"}, {"type": "text", "expr": "003 and then for 80k iterations with 0", "word_idx": 53591, "sentence_idx": 1018, "label": "unlabeled"}, {"type": "text", "expr": " We modify the learning rates (starting with 0", "word_idx": 53629, "sentence_idx": 1019, "label": "unlabeled"}, {"type": "text", "expr": "003 instead of 0", "word_idx": 53675, "sentence_idx": 1020, "label": "unlabeled"}, {"type": "text", "expr": "001) because the mini-batch size is changed", "word_idx": 53691, "sentence_idx": 1021, "label": "unlabeled"}, {"type": "text", "expr": "\nFor the anchors, we use 3 aspect ratios and 4 scales (adding  $64^{2}$ ), mainly motivated by handling small objects on this dataset", "word_idx": 53734, "sentence_idx": 1022, "label": "unlabeled"}, {"type": "text", "expr": " In addition, in our Fast R-CNN step, the negative samples are defined as those with a maximum IoU with ground truth in the interval of  $[0,05)$ , instead of  $[01,05)$  used in  ", "word_idx": 53867, "sentence_idx": 1023, "label": "unlabeled"}, {"type": "text", "expr": " We note that in the SPPnet system  , the negative samples in  $[01,05)$  are used for network fine-tuning, but the negative samples in  $[0,05)$  are still visited in the SVM step with hard-negative mining", "word_idx": 54047, "sentence_idx": 1024, "label": "unlabeled"}, {"type": "text", "expr": " But the Fast R-CNN system   abandons the SVM step, so the negative samples in  $[0,01)$  are never visited", "word_idx": 54253, "sentence_idx": 1025, "label": "unlabeled"}, {"type": "text", "expr": " Including these  $[0,01)$  samples improves mAP@0", "word_idx": 54360, "sentence_idx": 1026, "label": "unlabeled"}, {"type": "text", "expr": "5 on the COCO dataset for both Fast R-CNN and Faster R-CNN systems (but the impact is negligible on PASCAL VOC)", "word_idx": 54410, "sentence_idx": 1027, "label": "unlabeled"}, {"type": "math", "expr": "$$64^{2}$$", "word_idx": 54521, "sentence_idx": 1028, "label": "unlabeled"}, {"type": "math", "expr": "$$[0,0.5)$$", "word_idx": 54527, "sentence_idx": 1029, "label": "unlabeled"}, {"type": "math", "expr": "$$[0.1,0.5)$$", "word_idx": 54534, "sentence_idx": 1030, "label": "unlabeled"}, {"type": "math", "expr": "$$[0.1,0.5)$$", "word_idx": 54543, "sentence_idx": 1031, "label": "unlabeled"}, {"type": "math", "expr": "$$[0,0.5)$$", "word_idx": 54552, "sentence_idx": 1032, "label": "unlabeled"}, {"type": "math", "expr": "$$[0,0.1)$$", "word_idx": 54559, "sentence_idx": 1033, "label": "unlabeled"}, {"type": "math", "expr": "$$[0,0.1)$$", "word_idx": 54566, "sentence_idx": 1034, "label": "unlabeled"}, {"type": "text", "expr": "The rest of the implementation details are the same as on PASCAL VOC", "word_idx": 54573, "sentence_idx": 1035, "label": "unlabeled"}, {"type": "text", "expr": " In particular, we keep using 300 proposals and single-scale ( $s=600$ ) testing", "word_idx": 54641, "sentence_idx": 1036, "label": "unlabeled"}, {"type": "text", "expr": " The testing time is still about 200ms per image on the COCO dataset", "word_idx": 54721, "sentence_idx": 1037, "label": "unlabeled"}, {"type": "math", "expr": "$$s=600$$", "word_idx": 54789, "sentence_idx": 1038, "label": "unlabeled"}, {"type": "text", "expr": "In Table\u00a0 11  we first report the results of the Fast R-CNN system   using the implementation in this paper", "word_idx": 54794, "sentence_idx": 1039, "label": "unlabeled"}, {"type": "text", "expr": " Our Fast R-CNN baseline has 39", "word_idx": 54901, "sentence_idx": 1040, "label": "unlabeled"}, {"type": "text", "expr": "3% mAP@0", "word_idx": 54932, "sentence_idx": 1041, "label": "unlabeled"}, {"type": "text", "expr": "5 on the test-dev set, higher than that reported in  ", "word_idx": 54940, "sentence_idx": 1042, "label": "unlabeled"}, {"type": "text", "expr": " We conjecture that the reason for this gap is mainly due to the definition of the negative samples and also the changes of the mini-batch sizes", "word_idx": 54993, "sentence_idx": 1043, "label": "unlabeled"}, {"type": "text", "expr": " We also note that the mAP@[", "word_idx": 55137, "sentence_idx": 1044, "label": "unlabeled"}, {"type": "text", "expr": "95] is just comparable", "word_idx": 55165, "sentence_idx": 1045, "label": "unlabeled"}, {"type": "text", "expr": "Next we evaluate our Faster R-CNN system", "word_idx": 55187, "sentence_idx": 1046, "label": "unlabeled"}, {"type": "text", "expr": "\nUsing the COCO training set to train, Faster R-CNN has 42", "word_idx": 55227, "sentence_idx": 1047, "label": "unlabeled"}, {"type": "text", "expr": "1% mAP@0", "word_idx": 55285, "sentence_idx": 1048, "label": "unlabeled"}, {"type": "text", "expr": "5 and 21", "word_idx": 55293, "sentence_idx": 1049, "label": "unlabeled"}, {"type": "text", "expr": "5% mAP@[", "word_idx": 55301, "sentence_idx": 1050, "label": "unlabeled"}, {"type": "text", "expr": "95] on the COCO test-dev set", "word_idx": 55309, "sentence_idx": 1051, "label": "unlabeled"}, {"type": "text", "expr": " This is 2", "word_idx": 55337, "sentence_idx": 1052, "label": "unlabeled"}, {"type": "text", "expr": "8% higher for mAP@0", "word_idx": 55347, "sentence_idx": 1053, "label": "unlabeled"}, {"type": "text", "expr": "5 and  2", "word_idx": 55366, "sentence_idx": 1054, "label": "unlabeled"}, {"type": "text", "expr": "2% higher for mAP@[", "word_idx": 55374, "sentence_idx": 1055, "label": "unlabeled"}, {"type": "text", "expr": "95]  than the Fast R-CNN counterpart under the same protocol (Table\u00a0 11 )", "word_idx": 55393, "sentence_idx": 1056, "label": "unlabeled"}, {"type": "text", "expr": " This indicates that RPN performs excellent for improving the localization accuracy at higher IoU thresholds", "word_idx": 55466, "sentence_idx": 1057, "label": "unlabeled"}, {"type": "text", "expr": "\nUsing the COCO trainval set to train, Faster R-CNN has 42", "word_idx": 55574, "sentence_idx": 1058, "label": "unlabeled"}, {"type": "text", "expr": "7% mAP@0", "word_idx": 55632, "sentence_idx": 1059, "label": "unlabeled"}, {"type": "text", "expr": "5 and 21", "word_idx": 55640, "sentence_idx": 1060, "label": "unlabeled"}, {"type": "text", "expr": "9% mAP@[", "word_idx": 55648, "sentence_idx": 1061, "label": "unlabeled"}, {"type": "text", "expr": "95] on the COCO test-dev set", "word_idx": 55656, "sentence_idx": 1062, "label": "unlabeled"}, {"type": "text", "expr": "\nFigure\u00a0 6  shows some results on the MS COCO test-dev set", "word_idx": 55684, "sentence_idx": 1063, "label": "unlabeled"}, {"type": "text", "expr": "2% higher for mAP@[", "word_idx": 55742, "sentence_idx": 1064, "label": "unlabeled"}, {"type": "text", "expr": "Faster R-CNN in ILSVRC & COCO 2015 competitions \nWe have demonstrated that Faster R-CNN benefits more from better features, thanks to the fact that the RPN completely learns to propose regions by neural networks", "word_idx": 55761, "sentence_idx": 1065, "label": "unlabeled"}, {"type": "text", "expr": " This observation is still valid even when one increases the depth substantially to over 100 layers  ", "word_idx": 55972, "sentence_idx": 1066, "label": "unlabeled"}, {"type": "text", "expr": " Only by replacing VGG-16 with a 101-layer residual net (ResNet-101)  , the Faster R-CNN system increases the mAP from 41", "word_idx": 56073, "sentence_idx": 1067, "label": "unlabeled"}, {"type": "text", "expr": "5%/21", "word_idx": 56194, "sentence_idx": 1068, "label": "unlabeled"}, {"type": "text", "expr": "2% (VGG-16) to 48", "word_idx": 56199, "sentence_idx": 1069, "label": "unlabeled"}, {"type": "text", "expr": "4%/27", "word_idx": 56216, "sentence_idx": 1070, "label": "unlabeled"}, {"type": "text", "expr": "2% (ResNet-101) on the COCO val set", "word_idx": 56221, "sentence_idx": 1071, "label": "unlabeled"}, {"type": "text", "expr": " With other improvements orthogonal to Faster R-CNN, He  \\etal  obtained a single-model result of 55", "word_idx": 56256, "sentence_idx": 1072, "label": "unlabeled"}, {"type": "text", "expr": "7%/34", "word_idx": 56356, "sentence_idx": 1073, "label": "unlabeled"}, {"type": "text", "expr": "9% and an ensemble result of 59", "word_idx": 56361, "sentence_idx": 1074, "label": "unlabeled"}, {"type": "text", "expr": "0%/37", "word_idx": 56392, "sentence_idx": 1075, "label": "unlabeled"}, {"type": "text", "expr": "4% on the COCO test-dev set, which won the 1st place in the COCO 2015 object detection competition", "word_idx": 56397, "sentence_idx": 1076, "label": "unlabeled"}, {"type": "text", "expr": " The same system   also won the 1st place in the ILSVRC 2015 object detection competition, surpassing the second place by absolute 8", "word_idx": 56495, "sentence_idx": 1077, "label": "unlabeled"}, {"type": "text", "expr": " RPN is also a building block of the 1st-place winning entries in ILSVRC 2015 localization and COCO 2015 segmentation competitions, for which the details are available in   and   respectively", "word_idx": 56627, "sentence_idx": 1078, "label": "unlabeled"}, {"type": "text", "expr": "Faster R-CNN in ILSVRC & COCO 2015 competitions", "word_idx": 56818, "sentence_idx": 1079, "label": "unlabeled"}, {"type": "text", "expr": "\\etal", "word_idx": 56865, "sentence_idx": 1080, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a05:  Selected examples of object detection results on the PASCAL VOC 2007 test set using the Faster R-CNN system", "word_idx": 56870, "sentence_idx": 1081, "label": "unlabeled"}, {"type": "text", "expr": " The model is VGG-16 and the training data is 07+12 trainval (73", "word_idx": 56988, "sentence_idx": 1082, "label": "unlabeled"}, {"type": "text", "expr": "2% mAP on the 2007 test set)", "word_idx": 57052, "sentence_idx": 1083, "label": "unlabeled"}, {"type": "text", "expr": " Our method detects objects of a wide range of scales and aspect ratios", "word_idx": 57080, "sentence_idx": 1084, "label": "unlabeled"}, {"type": "text", "expr": " Each output box is associated with a category label and a softmax score in  $[0,1]$ ", "word_idx": 57151, "sentence_idx": 1085, "label": "unlabeled"}, {"type": "text", "expr": " A score threshold of 0", "word_idx": 57236, "sentence_idx": 1086, "label": "unlabeled"}, {"type": "text", "expr": "6 is used to display these images", "word_idx": 57259, "sentence_idx": 1087, "label": "unlabeled"}, {"type": "text", "expr": " The running time for obtaining these results is  198ms  per image,  including all steps ", "word_idx": 57292, "sentence_idx": 1088, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a05:", "word_idx": 57381, "sentence_idx": 1089, "label": "unlabeled"}, {"type": "math", "expr": "$$[0,1]$$", "word_idx": 57390, "sentence_idx": 1090, "label": "unlabeled"}, {"type": "text", "expr": "198ms", "word_idx": 57395, "sentence_idx": 1091, "label": "unlabeled"}, {"type": "text", "expr": "including all steps", "word_idx": 57400, "sentence_idx": 1092, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a06:  Selected examples of object detection results on the MS COCO test-dev set using the Faster R-CNN system", "word_idx": 57419, "sentence_idx": 1093, "label": "unlabeled"}, {"type": "text", "expr": " The model is VGG-16 and the training data is COCO trainval (42", "word_idx": 57533, "sentence_idx": 1094, "label": "unlabeled"}, {"type": "text", "expr": "7% mAP@0", "word_idx": 57596, "sentence_idx": 1095, "label": "unlabeled"}, {"type": "text", "expr": "5 on the test-dev set)", "word_idx": 57604, "sentence_idx": 1096, "label": "unlabeled"}, {"type": "text", "expr": " Each output box is associated with a category label and a softmax score in  $[0,1]$ ", "word_idx": 57626, "sentence_idx": 1097, "label": "unlabeled"}, {"type": "text", "expr": " A score threshold of 0", "word_idx": 57711, "sentence_idx": 1098, "label": "unlabeled"}, {"type": "text", "expr": "6 is used to display these images", "word_idx": 57734, "sentence_idx": 1099, "label": "unlabeled"}, {"type": "text", "expr": " For each image, one color represents one object category in that image", "word_idx": 57767, "sentence_idx": 1100, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a06:", "word_idx": 57838, "sentence_idx": 1101, "label": "unlabeled"}, {"type": "math", "expr": "$$[0,1]$$", "word_idx": 57847, "sentence_idx": 1102, "label": "unlabeled"}, {"type": "text", "expr": "3  From MS COCO to PASCAL VOC", "word_idx": 57852, "sentence_idx": 1103, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a012:  Detection mAP (%) of Faster R-CNN on PASCAL VOC 2007 test set and 2012 test set using different training data", "word_idx": 57881, "sentence_idx": 1104, "label": "unlabeled"}, {"type": "text", "expr": " The model is VGG-16", "word_idx": 58001, "sentence_idx": 1105, "label": "unlabeled"}, {"type": "text", "expr": " \u201cCOCO\u201d denotes that the COCO trainval set is used for training", "word_idx": 58021, "sentence_idx": 1106, "label": "unlabeled"}, {"type": "text", "expr": " See also Table\u00a0 6  and Table\u00a0 7 ", "word_idx": 58084, "sentence_idx": 1107, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a012:", "word_idx": 58117, "sentence_idx": 1108, "label": "unlabeled"}, {"type": "text", "expr": "training data 2007 test 2012 test", "word_idx": 58126, "sentence_idx": 1109, "label": "unlabeled"}, {"type": "text", "expr": "training data", "word_idx": 58159, "sentence_idx": 1110, "label": "unlabeled"}, {"type": "text", "expr": "2007 test", "word_idx": 58172, "sentence_idx": 1111, "label": "unlabeled"}, {"type": "text", "expr": "2012 test", "word_idx": 58181, "sentence_idx": 1112, "label": "unlabeled"}, {"type": "text", "expr": "VOC07 69", "word_idx": 58190, "sentence_idx": 1113, "label": "unlabeled"}, {"type": "text", "expr": "VOC07", "word_idx": 58198, "sentence_idx": 1114, "label": "unlabeled"}, {"type": "text", "expr": "VOC07+12 73", "word_idx": 58203, "sentence_idx": 1115, "label": "unlabeled"}, {"type": "text", "expr": "VOC07+12", "word_idx": 58214, "sentence_idx": 1116, "label": "unlabeled"}, {"type": "text", "expr": "VOC07++12 - 70", "word_idx": 58222, "sentence_idx": 1117, "label": "unlabeled"}, {"type": "text", "expr": "VOC07++12", "word_idx": 58236, "sentence_idx": 1118, "label": "unlabeled"}, {"type": "text", "expr": "COCO (no VOC) 76", "word_idx": 58245, "sentence_idx": 1119, "label": "unlabeled"}, {"type": "text", "expr": "COCO (no VOC)", "word_idx": 58261, "sentence_idx": 1120, "label": "unlabeled"}, {"type": "text", "expr": "COCO+VOC07+12 78", "word_idx": 58274, "sentence_idx": 1121, "label": "unlabeled"}, {"type": "text", "expr": "COCO+VOC07+12", "word_idx": 58290, "sentence_idx": 1122, "label": "unlabeled"}, {"type": "text", "expr": "COCO+VOC07++12 - 75", "word_idx": 58303, "sentence_idx": 1123, "label": "unlabeled"}, {"type": "text", "expr": "COCO+VOC07++12", "word_idx": 58322, "sentence_idx": 1124, "label": "unlabeled"}, {"type": "text", "expr": "Large-scale data is of crucial importance for improving deep neural networks", "word_idx": 58336, "sentence_idx": 1125, "label": "unlabeled"}, {"type": "text", "expr": " Next, we investigate how the MS COCO dataset can help with the detection performance on PASCAL VOC", "word_idx": 58412, "sentence_idx": 1126, "label": "unlabeled"}, {"type": "text", "expr": "Large-scale data is of crucial importance for improving deep neural networks", "word_idx": 58511, "sentence_idx": 1127, "label": "unlabeled"}, {"type": "text", "expr": " Next, we investigate how the MS COCO dataset can help with the detection performance on PASCAL VOC", "word_idx": 58587, "sentence_idx": 1128, "label": "unlabeled"}, {"type": "text", "expr": "As a simple baseline, we directly evaluate the COCO detection model on the PASCAL VOC dataset,  without fine-tuning on any PASCAL VOC data ", "word_idx": 58686, "sentence_idx": 1129, "label": "unlabeled"}, {"type": "text", "expr": " This evaluation is possible because the categories on COCO are a superset of those on PASCAL VOC", "word_idx": 58825, "sentence_idx": 1130, "label": "unlabeled"}, {"type": "text", "expr": " The categories that are exclusive on COCO are ignored in this experiment, and the softmax layer is performed only on the 20 categories plus background", "word_idx": 58922, "sentence_idx": 1131, "label": "unlabeled"}, {"type": "text", "expr": "\nThe mAP under this setting is 76", "word_idx": 59073, "sentence_idx": 1132, "label": "unlabeled"}, {"type": "text", "expr": "1% on the PASCAL VOC 2007 test set (Table\u00a0 12 )", "word_idx": 59106, "sentence_idx": 1133, "label": "unlabeled"}, {"type": "text", "expr": " This result is better than that trained on VOC07+12 (73", "word_idx": 59153, "sentence_idx": 1134, "label": "unlabeled"}, {"type": "text", "expr": "2%) by a good margin, even though the PASCAL VOC data are not exploited", "word_idx": 59209, "sentence_idx": 1135, "label": "unlabeled"}, {"type": "text", "expr": "without fine-tuning on any PASCAL VOC data", "word_idx": 59280, "sentence_idx": 1136, "label": "unlabeled"}, {"type": "text", "expr": "Then we fine-tune the COCO detection model on the VOC dataset", "word_idx": 59322, "sentence_idx": 1137, "label": "unlabeled"}, {"type": "text", "expr": " In this experiment, the COCO model is in place of the ImageNet-pre-trained model (that is used to initialize the network weights), and the Faster R-CNN system is fine-tuned as described in Section\u00a0 3", "word_idx": 59383, "sentence_idx": 1138, "label": "unlabeled"}, {"type": "text", "expr": " Doing so leads to 78", "word_idx": 59583, "sentence_idx": 1139, "label": "unlabeled"}, {"type": "text", "expr": "8% mAP on the PASCAL VOC 2007 test set", "word_idx": 59604, "sentence_idx": 1140, "label": "unlabeled"}, {"type": "text", "expr": " The extra data from the COCO set increases the mAP by 5", "word_idx": 59642, "sentence_idx": 1141, "label": "unlabeled"}, {"type": "text", "expr": " Table\u00a0 6  shows that the model trained on COCO+VOC has the best AP for every individual category on PASCAL VOC 2007", "word_idx": 59698, "sentence_idx": 1142, "label": "unlabeled"}, {"type": "text", "expr": "\nSimilar improvements are observed on the PASCAL VOC 2012 test set (Table\u00a0 12  and Table\u00a0 7 )", "word_idx": 59814, "sentence_idx": 1143, "label": "unlabeled"}, {"type": "text", "expr": " We note that the test-time speed of obtaining these strong results is still about  200ms per image ", "word_idx": 59907, "sentence_idx": 1144, "label": "unlabeled"}, {"type": "text", "expr": "200ms per image", "word_idx": 60007, "sentence_idx": 1145, "label": "unlabeled"}, {"type": "text", "expr": "5  Conclusion", "word_idx": 60022, "sentence_idx": 1146, "label": "unlabeled"}, {"type": "text", "expr": "We have presented RPNs for efficient and accurate region proposal generation", "word_idx": 60035, "sentence_idx": 1147, "label": "unlabeled"}, {"type": "text", "expr": " By sharing convolutional features with the down-stream detection network, the region proposal step is nearly cost-free", "word_idx": 60111, "sentence_idx": 1148, "label": "unlabeled"}, {"type": "text", "expr": " Our method enables a unified, deep-learning-based object detection system to run at near real-time frame rates", "word_idx": 60230, "sentence_idx": 1149, "label": "unlabeled"}, {"type": "text", "expr": " The learned RPN also improves region proposal quality and thus the overall object detection accuracy", "word_idx": 60341, "sentence_idx": 1150, "label": "unlabeled"}, {"type": "text", "expr": "We have presented RPNs for efficient and accurate region proposal generation", "word_idx": 60442, "sentence_idx": 1151, "label": "unlabeled"}, {"type": "text", "expr": " By sharing convolutional features with the down-stream detection network, the region proposal step is nearly cost-free", "word_idx": 60518, "sentence_idx": 1152, "label": "unlabeled"}, {"type": "text", "expr": " Our method enables a unified, deep-learning-based object detection system to run at near real-time frame rates", "word_idx": 60637, "sentence_idx": 1153, "label": "unlabeled"}, {"type": "text", "expr": " The learned RPN also improves region proposal quality and thus the overall object detection accuracy", "word_idx": 60748, "sentence_idx": 1154, "label": "unlabeled"}, {"type": "text", "expr": "References", "word_idx": 60849, "sentence_idx": 1155, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, X", "word_idx": 60859, "sentence_idx": 1156, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang, S", "word_idx": 60865, "sentence_idx": 1157, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ren, and J", "word_idx": 60874, "sentence_idx": 1158, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sun, \u201cSpatial pyramid pooling in deep\nconvolutional networks for visual recognition,\u201d in  European Conference\non Computer Vision (ECCV) , 2014", "word_idx": 60885, "sentence_idx": 1159, "label": "unlabeled"}, {"type": "text", "expr": "European Conference\non Computer Vision (ECCV)", "word_idx": 61028, "sentence_idx": 1160, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Girshick, \u201cFast R-CNN,\u201d in  IEEE International Conference on\nComputer Vision (ICCV) , 2015", "word_idx": 61073, "sentence_idx": 1161, "label": "unlabeled"}, {"type": "text", "expr": "IEEE International Conference on\nComputer Vision (ICCV)", "word_idx": 61164, "sentence_idx": 1162, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Simonyan and A", "word_idx": 61219, "sentence_idx": 1163, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zisserman, \u201cVery deep convolutional networks for\nlarge-scale image recognition,\u201d in  International Conference on\nLearning Representations (ICLR) , 2015", "word_idx": 61234, "sentence_idx": 1164, "label": "unlabeled"}, {"type": "text", "expr": "International Conference on\nLearning Representations (ICLR)", "word_idx": 61386, "sentence_idx": 1165, "label": "unlabeled"}, {"type": "text", "expr": " Uijlings, K", "word_idx": 61445, "sentence_idx": 1166, "label": "unlabeled"}, {"type": "text", "expr": " van\u00a0de Sande, T", "word_idx": 61457, "sentence_idx": 1167, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gevers, and A", "word_idx": 61473, "sentence_idx": 1168, "label": "unlabeled"}, {"type": "text", "expr": " Smeulders, \u201cSelective\nsearch for object recognition,\u201d  International Journal of Computer\nVision (IJCV) , 2013", "word_idx": 61487, "sentence_idx": 1169, "label": "unlabeled"}, {"type": "text", "expr": "International Journal of Computer\nVision (IJCV)", "word_idx": 61597, "sentence_idx": 1170, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Girshick, J", "word_idx": 61644, "sentence_idx": 1171, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Donahue, T", "word_idx": 61656, "sentence_idx": 1172, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Darrell, and J", "word_idx": 61667, "sentence_idx": 1173, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Malik, \u201cRich feature hierarchies\nfor accurate object detection and semantic segmentation,\u201d in  IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) , 2014", "word_idx": 61682, "sentence_idx": 1174, "label": "unlabeled"}, {"type": "text", "expr": "IEEE\nConference on Computer Vision and Pattern Recognition (CVPR)", "word_idx": 61850, "sentence_idx": 1175, "label": "unlabeled"}, {"type": "text", "expr": " Zitnick and P", "word_idx": 61915, "sentence_idx": 1176, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Doll\u00e1r, \u201cEdge boxes: Locating object proposals from\nedges,\u201d in  European Conference on Computer Vision (ECCV) , 2014", "word_idx": 61929, "sentence_idx": 1177, "label": "unlabeled"}, {"type": "text", "expr": "European Conference on Computer Vision (ECCV)", "word_idx": 62046, "sentence_idx": 1178, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Long, E", "word_idx": 62091, "sentence_idx": 1179, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Shelhamer, and T", "word_idx": 62099, "sentence_idx": 1180, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Darrell, \u201cFully convolutional networks for\nsemantic segmentation,\u201d in  IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) , 2015", "word_idx": 62116, "sentence_idx": 1181, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Conference on Computer Vision and\nPattern Recognition (CVPR)", "word_idx": 62260, "sentence_idx": 1182, "label": "unlabeled"}, {"type": "text", "expr": " Felzenszwalb, R", "word_idx": 62325, "sentence_idx": 1183, "label": "unlabeled"}, {"type": "text", "expr": " Girshick, D", "word_idx": 62341, "sentence_idx": 1184, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0McAllester, and D", "word_idx": 62353, "sentence_idx": 1185, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ramanan, \u201cObject\ndetection with discriminatively trained part-based models,\u201d  IEEE\nTransactions on Pattern Analysis and Machine Intelligence (TPAMI) , 2010", "word_idx": 62371, "sentence_idx": 1186, "label": "unlabeled"}, {"type": "text", "expr": "IEEE\nTransactions on Pattern Analysis and Machine Intelligence (TPAMI)", "word_idx": 62527, "sentence_idx": 1187, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sermanet, D", "word_idx": 62597, "sentence_idx": 1188, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Eigen, X", "word_idx": 62609, "sentence_idx": 1189, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang, M", "word_idx": 62618, "sentence_idx": 1190, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mathieu, R", "word_idx": 62627, "sentence_idx": 1191, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fergus, and Y", "word_idx": 62638, "sentence_idx": 1192, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0LeCun,\n\u201cOverfeat: Integrated recognition, localization and detection using\nconvolutional networks,\u201d in  International Conference on Learning\nRepresentations (ICLR) , 2014", "word_idx": 62652, "sentence_idx": 1193, "label": "unlabeled"}, {"type": "text", "expr": "International Conference on Learning\nRepresentations (ICLR)", "word_idx": 62823, "sentence_idx": 1194, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ren, K", "word_idx": 62882, "sentence_idx": 1195, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, R", "word_idx": 62889, "sentence_idx": 1196, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Girshick, and J", "word_idx": 62895, "sentence_idx": 1197, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sun, \u201cFaster R-CNN: Towards real-time\nobject detection with region proposal networks,\u201d in  Neural\nInformation Processing Systems (NIPS) , 2015", "word_idx": 62911, "sentence_idx": 1198, "label": "unlabeled"}, {"type": "text", "expr": "Neural\nInformation Processing Systems (NIPS)", "word_idx": 63054, "sentence_idx": 1199, "label": "unlabeled"}, {"type": "text", "expr": "11 \nM", "word_idx": 63098, "sentence_idx": 1200, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Everingham, L", "word_idx": 63103, "sentence_idx": 1201, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Van\u00a0Gool, C", "word_idx": 63117, "sentence_idx": 1202, "label": "unlabeled"}, {"type": "text", "expr": " Williams, J", "word_idx": 63129, "sentence_idx": 1203, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Winn, and A", "word_idx": 63141, "sentence_idx": 1204, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zisserman, \u201cThe\nPASCAL Visual Object Classes Challenge 2007 (VOC2007)\nResults,\u201d 2007", "word_idx": 63153, "sentence_idx": 1205, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Everingham, L", "word_idx": 63238, "sentence_idx": 1206, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Van\u00a0Gool, C", "word_idx": 63252, "sentence_idx": 1207, "label": "unlabeled"}, {"type": "text", "expr": " Williams, J", "word_idx": 63264, "sentence_idx": 1208, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Winn, and A", "word_idx": 63276, "sentence_idx": 1209, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zisserman, \u201cThe\nPASCAL Visual Object Classes Challenge 2007 (VOC2007)\nResults,\u201d 2007", "word_idx": 63288, "sentence_idx": 1210, "label": "unlabeled"}, {"type": "text", "expr": " Lin, M", "word_idx": 63373, "sentence_idx": 1211, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Maire, S", "word_idx": 63380, "sentence_idx": 1212, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Belongie, J", "word_idx": 63389, "sentence_idx": 1213, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hays, P", "word_idx": 63401, "sentence_idx": 1214, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Perona, D", "word_idx": 63409, "sentence_idx": 1215, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ramanan,\nP", "word_idx": 63419, "sentence_idx": 1216, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Doll\u00e1r, and C", "word_idx": 63430, "sentence_idx": 1217, "label": "unlabeled"}, {"type": "text", "expr": " Zitnick, \u201cMicrosoft COCO: Common Objects in\nContext,\u201d in  European Conference on Computer Vision (ECCV) , 2014", "word_idx": 63444, "sentence_idx": 1218, "label": "unlabeled"}, {"type": "text", "expr": "European Conference on Computer Vision (ECCV)", "word_idx": 63555, "sentence_idx": 1219, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Song and J", "word_idx": 63600, "sentence_idx": 1220, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Xiao, \u201cDeep sliding shapes for amodal 3d object detection in\nrgb-d images,\u201d  arXiv:1511", "word_idx": 63611, "sentence_idx": 1221, "label": "unlabeled"}, {"type": "text", "expr": "02300 , 2015", "word_idx": 63699, "sentence_idx": 1222, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1511", "word_idx": 63711, "sentence_idx": 1223, "label": "unlabeled"}, {"type": "text", "expr": "02300", "word_idx": 63721, "sentence_idx": 1224, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhu, X", "word_idx": 63726, "sentence_idx": 1225, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chen, and A", "word_idx": 63733, "sentence_idx": 1226, "label": "unlabeled"}, {"type": "text", "expr": " Yuille, \u201cDeePM: A deep part-based model for\nobject detection and semantic part localization,\u201d  arXiv:1511", "word_idx": 63745, "sentence_idx": 1227, "label": "unlabeled"}, {"type": "text", "expr": "07131 ,\n2015", "word_idx": 63851, "sentence_idx": 1228, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1511", "word_idx": 63863, "sentence_idx": 1229, "label": "unlabeled"}, {"type": "text", "expr": "07131", "word_idx": 63873, "sentence_idx": 1230, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dai, K", "word_idx": 63878, "sentence_idx": 1231, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, and J", "word_idx": 63885, "sentence_idx": 1232, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sun, \u201cInstance-aware semantic segmentation via\nmulti-task network cascades,\u201d  arXiv:1512", "word_idx": 63895, "sentence_idx": 1233, "label": "unlabeled"}, {"type": "text", "expr": "04412 , 2015", "word_idx": 63984, "sentence_idx": 1234, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1512", "word_idx": 63996, "sentence_idx": 1235, "label": "unlabeled"}, {"type": "text", "expr": "04412", "word_idx": 64006, "sentence_idx": 1236, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Johnson, A", "word_idx": 64011, "sentence_idx": 1237, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Karpathy, and L", "word_idx": 64022, "sentence_idx": 1238, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fei-Fei, \u201cDensecap: Fully convolutional\nlocalization networks for dense captioning,\u201d  arXiv:1511", "word_idx": 64038, "sentence_idx": 1239, "label": "unlabeled"}, {"type": "text", "expr": "07571 , 2015", "word_idx": 64135, "sentence_idx": 1240, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1511", "word_idx": 64147, "sentence_idx": 1241, "label": "unlabeled"}, {"type": "text", "expr": "07571", "word_idx": 64157, "sentence_idx": 1242, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kislyuk, Y", "word_idx": 64162, "sentence_idx": 1243, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Liu, D", "word_idx": 64173, "sentence_idx": 1244, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Liu, E", "word_idx": 64180, "sentence_idx": 1245, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Tzeng, and Y", "word_idx": 64187, "sentence_idx": 1246, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Jing, \u201cHuman curation and\nconvnets: Powering item-to-item recommendations on pinterest,\u201d\n arXiv:1511", "word_idx": 64200, "sentence_idx": 1247, "label": "unlabeled"}, {"type": "text", "expr": "04003 , 2015", "word_idx": 64301, "sentence_idx": 1248, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1511", "word_idx": 64313, "sentence_idx": 1249, "label": "unlabeled"}, {"type": "text", "expr": "04003", "word_idx": 64323, "sentence_idx": 1250, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, X", "word_idx": 64328, "sentence_idx": 1251, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang, S", "word_idx": 64334, "sentence_idx": 1252, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ren, and J", "word_idx": 64343, "sentence_idx": 1253, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sun, \u201cDeep residual learning for image\nrecognition,\u201d  arXiv:1512", "word_idx": 64354, "sentence_idx": 1254, "label": "unlabeled"}, {"type": "text", "expr": "03385 , 2015", "word_idx": 64419, "sentence_idx": 1255, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1512", "word_idx": 64431, "sentence_idx": 1256, "label": "unlabeled"}, {"type": "text", "expr": "03385", "word_idx": 64441, "sentence_idx": 1257, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hosang, R", "word_idx": 64446, "sentence_idx": 1258, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Benenson, and B", "word_idx": 64456, "sentence_idx": 1259, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Schiele, \u201cHow good are detection proposals,\nreally?\u201d in  British Machine Vision Conference (BMVC) , 2014", "word_idx": 64472, "sentence_idx": 1260, "label": "unlabeled"}, {"type": "text", "expr": "British Machine Vision Conference (BMVC)", "word_idx": 64577, "sentence_idx": 1261, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hosang, R", "word_idx": 64617, "sentence_idx": 1262, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Benenson, P", "word_idx": 64627, "sentence_idx": 1263, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Doll\u00e1r, and B", "word_idx": 64639, "sentence_idx": 1264, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Schiele, \u201cWhat makes for\neffective detection proposals?\u201d  IEEE Transactions on Pattern Analysis\nand Machine Intelligence (TPAMI) , 2015", "word_idx": 64653, "sentence_idx": 1265, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Transactions on Pattern Analysis\nand Machine Intelligence (TPAMI)", "word_idx": 64789, "sentence_idx": 1266, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chavali, H", "word_idx": 64859, "sentence_idx": 1267, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Agrawal, A", "word_idx": 64870, "sentence_idx": 1268, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Mahendru, and D", "word_idx": 64881, "sentence_idx": 1269, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Batra, \u201cObject-Proposal\nEvaluation Protocol is \u2019Gameable\u2019,\u201d  arXiv: 1505", "word_idx": 64897, "sentence_idx": 1270, "label": "unlabeled"}, {"type": "text", "expr": "05836 , 2015", "word_idx": 64970, "sentence_idx": 1271, "label": "unlabeled"}, {"type": "text", "expr": "arXiv: 1505", "word_idx": 64982, "sentence_idx": 1272, "label": "unlabeled"}, {"type": "text", "expr": "05836", "word_idx": 64993, "sentence_idx": 1273, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Carreira and C", "word_idx": 64998, "sentence_idx": 1274, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sminchisescu, \u201cCPMC: Automatic object segmentation using\nconstrained parametric min-cuts,\u201d  IEEE Transactions on Pattern\nAnalysis and Machine Intelligence (TPAMI) , 2012", "word_idx": 65013, "sentence_idx": 1275, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Transactions on Pattern\nAnalysis and Machine Intelligence (TPAMI)", "word_idx": 65183, "sentence_idx": 1276, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Arbel\u00e1ez, J", "word_idx": 65253, "sentence_idx": 1277, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Pont-Tuset, J", "word_idx": 65265, "sentence_idx": 1278, "label": "unlabeled"}, {"type": "text", "expr": " Barron, F", "word_idx": 65279, "sentence_idx": 1279, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Marques, and J", "word_idx": 65289, "sentence_idx": 1280, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Malik,\n\u201cMultiscale combinatorial grouping,\u201d in  IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) , 2014", "word_idx": 65304, "sentence_idx": 1281, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Conference on Computer\nVision and Pattern Recognition (CVPR)", "word_idx": 65425, "sentence_idx": 1282, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Alexe, T", "word_idx": 65490, "sentence_idx": 1283, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Deselaers, and V", "word_idx": 65499, "sentence_idx": 1284, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ferrari, \u201cMeasuring the objectness of image\nwindows,\u201d  IEEE Transactions on Pattern Analysis and Machine\nIntelligence (TPAMI) , 2012", "word_idx": 65516, "sentence_idx": 1285, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Transactions on Pattern Analysis and Machine\nIntelligence (TPAMI)", "word_idx": 65649, "sentence_idx": 1286, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Szegedy, A", "word_idx": 65719, "sentence_idx": 1287, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Toshev, and D", "word_idx": 65730, "sentence_idx": 1288, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Erhan, \u201cDeep neural networks for object\ndetection,\u201d in  Neural Information Processing Systems (NIPS) , 2013", "word_idx": 65744, "sentence_idx": 1289, "label": "unlabeled"}, {"type": "text", "expr": "Neural Information Processing Systems (NIPS)", "word_idx": 65852, "sentence_idx": 1290, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Erhan, C", "word_idx": 65896, "sentence_idx": 1291, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Szegedy, A", "word_idx": 65905, "sentence_idx": 1292, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Toshev, and D", "word_idx": 65916, "sentence_idx": 1293, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Anguelov, \u201cScalable object detection\nusing deep neural networks,\u201d in  IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) , 2014", "word_idx": 65930, "sentence_idx": 1294, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Conference on Computer Vision and\nPattern Recognition (CVPR)", "word_idx": 66073, "sentence_idx": 1295, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Szegedy, S", "word_idx": 66138, "sentence_idx": 1296, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Reed, D", "word_idx": 66149, "sentence_idx": 1297, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Erhan, and D", "word_idx": 66157, "sentence_idx": 1298, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Anguelov, \u201cScalable, high-quality object\ndetection,\u201d  arXiv:1412", "word_idx": 66170, "sentence_idx": 1299, "label": "unlabeled"}, {"type": "text", "expr": "1441 (v1) , 2015", "word_idx": 66235, "sentence_idx": 1300, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1412", "word_idx": 66251, "sentence_idx": 1301, "label": "unlabeled"}, {"type": "text", "expr": "1441 (v1)", "word_idx": 66261, "sentence_idx": 1302, "label": "unlabeled"}, {"type": "text", "expr": " Pinheiro, R", "word_idx": 66270, "sentence_idx": 1303, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Collobert, and P", "word_idx": 66282, "sentence_idx": 1304, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dollar, \u201cLearning to segment object\ncandidates,\u201d in  Neural Information Processing Systems (NIPS) , 2015", "word_idx": 66299, "sentence_idx": 1305, "label": "unlabeled"}, {"type": "text", "expr": "Neural Information Processing Systems (NIPS)", "word_idx": 66404, "sentence_idx": 1306, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dai, K", "word_idx": 66448, "sentence_idx": 1307, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, and J", "word_idx": 66455, "sentence_idx": 1308, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sun, \u201cConvolutional feature masking for joint object and\nstuff segmentation,\u201d in  IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , 2015", "word_idx": 66465, "sentence_idx": 1309, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR)", "word_idx": 66620, "sentence_idx": 1310, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ren, K", "word_idx": 66685, "sentence_idx": 1311, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, R", "word_idx": 66692, "sentence_idx": 1312, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Girshick, X", "word_idx": 66698, "sentence_idx": 1313, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang, and J", "word_idx": 66710, "sentence_idx": 1314, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sun, \u201cObject detection networks\non convolutional feature maps,\u201d  arXiv:1504", "word_idx": 66723, "sentence_idx": 1315, "label": "unlabeled"}, {"type": "text", "expr": "06066 , 2015", "word_idx": 66799, "sentence_idx": 1316, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1504", "word_idx": 66811, "sentence_idx": 1317, "label": "unlabeled"}, {"type": "text", "expr": "06066", "word_idx": 66821, "sentence_idx": 1318, "label": "unlabeled"}, {"type": "text", "expr": " Chorowski, D", "word_idx": 66826, "sentence_idx": 1319, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bahdanau, D", "word_idx": 66839, "sentence_idx": 1320, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Serdyuk, K", "word_idx": 66851, "sentence_idx": 1321, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Cho, and Y", "word_idx": 66862, "sentence_idx": 1322, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bengio,\n\u201cAttention-based models for speech recognition,\u201d in  Neural\nInformation Processing Systems (NIPS) , 2015", "word_idx": 66873, "sentence_idx": 1323, "label": "unlabeled"}, {"type": "text", "expr": "Neural\nInformation Processing Systems (NIPS)", "word_idx": 66986, "sentence_idx": 1324, "label": "unlabeled"}, {"type": "text", "expr": " Zeiler and R", "word_idx": 67030, "sentence_idx": 1325, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fergus, \u201cVisualizing and understanding convolutional\nneural networks,\u201d in  European Conference on Computer Vision (ECCV) ,\n2014", "word_idx": 67043, "sentence_idx": 1326, "label": "unlabeled"}, {"type": "text", "expr": "European Conference on Computer Vision (ECCV)", "word_idx": 67171, "sentence_idx": 1327, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Nair and G", "word_idx": 67216, "sentence_idx": 1328, "label": "unlabeled"}, {"type": "text", "expr": " Hinton, \u201cRectified linear units improve restricted boltzmann\nmachines,\u201d in  International Conference on Machine Learning (ICML) ,\n2010", "word_idx": 67227, "sentence_idx": 1329, "label": "unlabeled"}, {"type": "text", "expr": "International Conference on Machine Learning (ICML)", "word_idx": 67362, "sentence_idx": 1330, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Szegedy, W", "word_idx": 67413, "sentence_idx": 1331, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Liu, Y", "word_idx": 67424, "sentence_idx": 1332, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Jia, P", "word_idx": 67431, "sentence_idx": 1333, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sermanet, S", "word_idx": 67438, "sentence_idx": 1334, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Reed, D", "word_idx": 67450, "sentence_idx": 1335, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Anguelov, D", "word_idx": 67458, "sentence_idx": 1336, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Erhan, and\nA", "word_idx": 67470, "sentence_idx": 1337, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rabinovich, \u201cGoing deeper with convolutions,\u201d in  IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) , 2015", "word_idx": 67483, "sentence_idx": 1338, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Conference\non Computer Vision and Pattern Recognition (CVPR)", "word_idx": 67606, "sentence_idx": 1339, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0LeCun, B", "word_idx": 67671, "sentence_idx": 1340, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Boser, J", "word_idx": 67680, "sentence_idx": 1341, "label": "unlabeled"}, {"type": "text", "expr": " Denker, D", "word_idx": 67689, "sentence_idx": 1342, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Henderson, R", "word_idx": 67699, "sentence_idx": 1343, "label": "unlabeled"}, {"type": "text", "expr": " Howard, W", "word_idx": 67712, "sentence_idx": 1344, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hubbard, and\nL", "word_idx": 67722, "sentence_idx": 1345, "label": "unlabeled"}, {"type": "text", "expr": " Jackel, \u201cBackpropagation applied to handwritten zip code\nrecognition,\u201d  Neural computation , 1989", "word_idx": 67737, "sentence_idx": 1346, "label": "unlabeled"}, {"type": "text", "expr": "Neural computation", "word_idx": 67835, "sentence_idx": 1347, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Russakovsky, J", "word_idx": 67853, "sentence_idx": 1348, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Deng, H", "word_idx": 67868, "sentence_idx": 1349, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Su, J", "word_idx": 67876, "sentence_idx": 1350, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Krause, S", "word_idx": 67882, "sentence_idx": 1351, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Satheesh, S", "word_idx": 67892, "sentence_idx": 1352, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ma, Z", "word_idx": 67904, "sentence_idx": 1353, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Huang,\nA", "word_idx": 67910, "sentence_idx": 1354, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Karpathy, A", "word_idx": 67919, "sentence_idx": 1355, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Khosla, M", "word_idx": 67931, "sentence_idx": 1356, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Bernstein, A", "word_idx": 67941, "sentence_idx": 1357, "label": "unlabeled"}, {"type": "text", "expr": " Berg, and L", "word_idx": 67954, "sentence_idx": 1358, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fei-Fei, \u201cImageNet\nLarge Scale Visual Recognition Challenge,\u201d in  International Journal\nof Computer Vision (IJCV) , 2015", "word_idx": 67966, "sentence_idx": 1359, "label": "unlabeled"}, {"type": "text", "expr": "International Journal\nof Computer Vision (IJCV)", "word_idx": 68087, "sentence_idx": 1360, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Krizhevsky, I", "word_idx": 68134, "sentence_idx": 1361, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sutskever, and G", "word_idx": 68148, "sentence_idx": 1362, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hinton, \u201cImagenet classification with deep\nconvolutional neural networks,\u201d in  Neural Information Processing\nSystems (NIPS) , 2012", "word_idx": 68165, "sentence_idx": 1363, "label": "unlabeled"}, {"type": "text", "expr": "Neural Information Processing\nSystems (NIPS)", "word_idx": 68296, "sentence_idx": 1364, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Jia, E", "word_idx": 68340, "sentence_idx": 1365, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Shelhamer, J", "word_idx": 68347, "sentence_idx": 1366, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Donahue, S", "word_idx": 68360, "sentence_idx": 1367, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Karayev, J", "word_idx": 68371, "sentence_idx": 1368, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Long, R", "word_idx": 68382, "sentence_idx": 1369, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Girshick,\nS", "word_idx": 68390, "sentence_idx": 1370, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Guadarrama, and T", "word_idx": 68402, "sentence_idx": 1371, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Darrell, \u201cCaffe: Convolutional architecture for fast\nfeature embedding,\u201d  arXiv:1408", "word_idx": 68420, "sentence_idx": 1372, "label": "unlabeled"}, {"type": "text", "expr": "5093 , 2014", "word_idx": 68505, "sentence_idx": 1373, "label": "unlabeled"}, {"type": "text", "expr": "arXiv:1408", "word_idx": 68516, "sentence_idx": 1374, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lenc and A", "word_idx": 68526, "sentence_idx": 1375, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Vedaldi, \u201cR-CNN minus R,\u201d in  British Machine Vision\nConference (BMVC) , 2015", "word_idx": 68537, "sentence_idx": 1376, "label": "unlabeled"}, {"type": "text", "expr": "British Machine Vision\nConference (BMVC)", "word_idx": 68615, "sentence_idx": 1377, "label": "unlabeled"}, {"type": "text", "expr": "Generated  on Tue Apr  3 11:32:06 2018 by", "word_idx": 68655, "sentence_idx": 1378, "label": "unlabeled"}, {"type": "text", "expr": "LaTeXML", "word_idx": 68696, "sentence_idx": 1379, "label": "unlabeled"}], "face_detection": [{"type": "text", "expr": "Beyond Context: Exploring Semantic Similarity for Tiny Face Detection", "word_idx": 0, "sentence_idx": 0, "label": "unlabeled"}, {"type": "text", "expr": "Beyond Context: Exploring Semantic Similarity for Tiny Face Detection", "word_idx": 69, "sentence_idx": 1, "label": "unlabeled"}, {"type": "text", "expr": "Abstract Tiny face detection aims to find faces with high degrees of variability in scale, resolution and occlusion in cluttered scenes", "word_idx": 138, "sentence_idx": 2, "label": "unlabeled"}, {"type": "text", "expr": " Due to the very little information available on tiny faces, it is not sufficient to detect them merely based on the information presented inside the tiny bounding boxes or their context", "word_idx": 273, "sentence_idx": 3, "label": "unlabeled"}, {"type": "text", "expr": " In this paper, we propose to exploit the semantic similarity among all predicted targets in each image to boost current face detectors", "word_idx": 459, "sentence_idx": 4, "label": "unlabeled"}, {"type": "text", "expr": " To this end, we present a novel framework to model semantic similarity as pairwise constraints within the metric learning scheme, and then refine our predictions with the semantic similarity by utilizing the graph cut techniques", "word_idx": 594, "sentence_idx": 5, "label": "unlabeled"}, {"type": "text", "expr": " Experiments conducted on three widely-used benchmark datasets have demonstrated the improvement over the-state-of-the-arts gained by applying this idea", "word_idx": 823, "sentence_idx": 6, "label": "unlabeled"}, {"type": "text", "expr": "Abstract", "word_idx": 975, "sentence_idx": 7, "label": "unlabeled"}, {"type": "text", "expr": "Tiny face detection aims to find faces with high degrees of variability in scale, resolution and occlusion in cluttered scenes", "word_idx": 983, "sentence_idx": 8, "label": "unlabeled"}, {"type": "text", "expr": " Due to the very little information available on tiny faces, it is not sufficient to detect them merely based on the information presented inside the tiny bounding boxes or their context", "word_idx": 1109, "sentence_idx": 9, "label": "unlabeled"}, {"type": "text", "expr": " In this paper, we propose to exploit the semantic similarity among all predicted targets in each image to boost current face detectors", "word_idx": 1295, "sentence_idx": 10, "label": "unlabeled"}, {"type": "text", "expr": " To this end, we present a novel framework to model semantic similarity as pairwise constraints within the metric learning scheme, and then refine our predictions with the semantic similarity by utilizing the graph cut techniques", "word_idx": 1430, "sentence_idx": 11, "label": "unlabeled"}, {"type": "text", "expr": " Experiments conducted on three widely-used benchmark datasets have demonstrated the improvement over the-state-of-the-arts gained by applying this idea", "word_idx": 1659, "sentence_idx": 12, "label": "unlabeled"}, {"type": "text", "expr": "\\name", "word_idx": 1811, "sentence_idx": 13, "label": "unlabeled"}, {"type": "text", "expr": "YueXi ${}^{1,2}$ ,JiangbinZheng ${}^{1,}$ *,XiangjiaHe ${}^{2,}$ *,WenjingJia ${}^{2}$ ,HanhuiLi ${}^{2,3}$ \\address ${}^{1}$ SchoolofComputerScience,NorthwesternPolytechnicalUniversity,P", "word_idx": 1816, "sentence_idx": 14, "label": "unlabeled"}, {"type": "text", "expr": "China\n ${}^{2}$ GlobalBigDataTechnologiesCentre(GBDTC),UniversityofTechnologySydney,Australia\n ${}^{3}$ SchoolofDataandComputerScience,SunYat-senUniversity,P", "word_idx": 2003, "sentence_idx": 15, "label": "unlabeled"}, {"type": "text", "expr": "China", "word_idx": 2160, "sentence_idx": 16, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{1,2}$$", "word_idx": 2165, "sentence_idx": 17, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{1,}$$", "word_idx": 2173, "sentence_idx": 18, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{2,}$$", "word_idx": 2180, "sentence_idx": 19, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{2}$$", "word_idx": 2187, "sentence_idx": 20, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{2,3}$$", "word_idx": 2193, "sentence_idx": 21, "label": "unlabeled"}, {"type": "text", "expr": "\u2020 thanks:  *XiangjianHeandJiangbinZhengaretheco-correspondingauthorsforthispaper", "word_idx": 2201, "sentence_idx": 22, "label": "unlabeled"}, {"type": "text", "expr": "thanks:", "word_idx": 2281, "sentence_idx": 23, "label": "unlabeled"}, {"type": "text", "expr": "\\address", "word_idx": 2288, "sentence_idx": 24, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{1}$$", "word_idx": 2296, "sentence_idx": 25, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{2}$$", "word_idx": 2302, "sentence_idx": 26, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{3}$$", "word_idx": 2308, "sentence_idx": 27, "label": "unlabeled"}, {"type": "text", "expr": "{keywords}", "word_idx": 2314, "sentence_idx": 28, "label": "unlabeled"}, {"type": "text", "expr": "Tiny face detection, semantic information, metric learning, graph-cut", "word_idx": 2324, "sentence_idx": 29, "label": "unlabeled"}, {"type": "text", "expr": "Tiny face detection, semantic information, metric learning, graph-cut", "word_idx": 2393, "sentence_idx": 30, "label": "unlabeled"}, {"type": "text", "expr": "1  Introduction", "word_idx": 2462, "sentence_idx": 31, "label": "unlabeled"}, {"type": "text", "expr": "Robust face detection is one of the ultimate components to support various facial\nrelated problems, such as face alignment  , face recognition  , face verification  , and face tracking  , etc", "word_idx": 2477, "sentence_idx": 32, "label": "unlabeled"}, {"type": "text", "expr": " From the cornerstone by Viola-Jones   to the recent work by Hu et al", "word_idx": 2668, "sentence_idx": 33, "label": "unlabeled"}, {"type": "text", "expr": "  , the performance of face detection has been\nimproved dramatically", "word_idx": 2737, "sentence_idx": 34, "label": "unlabeled"}, {"type": "text", "expr": "\nThe recent introduction of the WIDER\nface dataset  , which contains a large number of small faces, exposes the performance gap between humans and\nthe current face detection techniques due to a number of challenges in practice", "word_idx": 2805, "sentence_idx": 35, "label": "unlabeled"}, {"type": "text", "expr": "\nDifferent from the classical face detection, tiny face detection mainly focuses on low-resolution, large scale variation and serious occlusion, as shown in Fig", "word_idx": 3031, "sentence_idx": 36, "label": "unlabeled"}, {"type": "text", "expr": "\nAll of these challenges suggest the information on small objects is far too limited", "word_idx": 3191, "sentence_idx": 37, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:  Tiny faces detected with our proposed approach (shown as yellow and green boxes) and the HR approach\u00a0  (shown as green boxes)", "word_idx": 3275, "sentence_idx": 38, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:", "word_idx": 3411, "sentence_idx": 39, "label": "unlabeled"}, {"type": "text", "expr": "The existing methods for finding small objects in imageries can be grouped into three categories", "word_idx": 3420, "sentence_idx": 40, "label": "unlabeled"}, {"type": "text", "expr": " The first group (e", "word_idx": 3516, "sentence_idx": 41, "label": "unlabeled"}, {"type": "text", "expr": ",  ) aims to extract scale-invariant features using pre-trained deep networks", "word_idx": 3535, "sentence_idx": 42, "label": "unlabeled"}, {"type": "text", "expr": " However, their performance drops dramatically as the target faces become too small", "word_idx": 3612, "sentence_idx": 43, "label": "unlabeled"}, {"type": "text", "expr": " Another group tries to generate additional information inside the objects by interpolation", "word_idx": 3695, "sentence_idx": 44, "label": "unlabeled"}, {"type": "text", "expr": " For example, the work in\u00a0  demonstrated that interpolating the lowest layer of image pyramid was significantly beneficial for capturing small objects", "word_idx": 3786, "sentence_idx": 45, "label": "unlabeled"}, {"type": "text", "expr": "\nThe last group (e", "word_idx": 3936, "sentence_idx": 46, "label": "unlabeled"}, {"type": "text", "expr": ",  ) seeks to incorporate information surrounding the objects (i", "word_idx": 3954, "sentence_idx": 47, "label": "unlabeled"}, {"type": "text", "expr": ", context) in order to improve the performance of tiny face detection", "word_idx": 4018, "sentence_idx": 48, "label": "unlabeled"}, {"type": "text", "expr": " It is clear that computer vision needs additional contextual information to accurately classify small faces", "word_idx": 4087, "sentence_idx": 49, "label": "unlabeled"}, {"type": "text", "expr": " Is there another way to improve the performance of small object detection?", "word_idx": 4195, "sentence_idx": 50, "label": "unlabeled"}, {"type": "text", "expr": "Note that, the existing classification-based tiny face detectors simply apply a\nthreshold on a classification score to determine whether the corresponding candidate is face or non-face, as shown in the first stage of Fig", "word_idx": 4270, "sentence_idx": 51, "label": "unlabeled"}, {"type": "text", "expr": "\nHowever, the optimal threshold is often difficult to obtain", "word_idx": 4490, "sentence_idx": 52, "label": "unlabeled"}, {"type": "text", "expr": "\nIn this paper,\nwe propose a novel idea to exploit the semantic information (consisting of spatial locations, scales and textures) of a candidate\u2019s neighbors to classify a target to face or background", "word_idx": 4550, "sentence_idx": 53, "label": "unlabeled"}, {"type": "text", "expr": "\nSpecifically, based on such semantic information, we try to group all of the faces into one cluster, while backgrounds are kept far away from the cluster", "word_idx": 4750, "sentence_idx": 54, "label": "unlabeled"}, {"type": "text", "expr": "\nFor this purpose, we propose a Metric Learning and Graph-Cut (MLGC) framework, which carries out further classification on the candidates produced by other object detectors", "word_idx": 4904, "sentence_idx": 55, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 2  illustrates the framework of this idea", "word_idx": 5077, "sentence_idx": 56, "label": "unlabeled"}, {"type": "text", "expr": "We first obtain a high-recall classifier which aims to retrieve all of the targets in an image, but may unavoidably introduce lots of false positives", "word_idx": 5120, "sentence_idx": 57, "label": "unlabeled"}, {"type": "text", "expr": "\nOur focus is to retrieve faces with low classification scores but remove these false positives", "word_idx": 5269, "sentence_idx": 58, "label": "unlabeled"}, {"type": "text", "expr": "\nIn order to do this, we design a metric learning method to learn a similarity matrix to evaluate the similarity of each pair of candidates", "word_idx": 5364, "sentence_idx": 59, "label": "unlabeled"}, {"type": "text", "expr": "\nA graph model is built to represent the similarity matrix of these candidates", "word_idx": 5503, "sentence_idx": 60, "label": "unlabeled"}, {"type": "text", "expr": "\nThe graph cut technique is utilized to divide the graph into several groups where candidates in the same group are similar and those in different groups are dissimilar to each other", "word_idx": 5581, "sentence_idx": 61, "label": "unlabeled"}, {"type": "text", "expr": "\nFinally, the candidates in each group are classified into faces or non-faces, correspondingly, by voting", "word_idx": 5763, "sentence_idx": 62, "label": "unlabeled"}, {"type": "text", "expr": "We first obtain a high-recall classifier which aims to retrieve all of the targets in an image, but may unavoidably introduce lots of false positives", "word_idx": 5868, "sentence_idx": 63, "label": "unlabeled"}, {"type": "text", "expr": "\nOur focus is to retrieve faces with low classification scores but remove these false positives", "word_idx": 6017, "sentence_idx": 64, "label": "unlabeled"}, {"type": "text", "expr": "\nIn order to do this, we design a metric learning method to learn a similarity matrix to evaluate the similarity of each pair of candidates", "word_idx": 6112, "sentence_idx": 65, "label": "unlabeled"}, {"type": "text", "expr": "\nA graph model is built to represent the similarity matrix of these candidates", "word_idx": 6251, "sentence_idx": 66, "label": "unlabeled"}, {"type": "text", "expr": "\nThe graph cut technique is utilized to divide the graph into several groups where candidates in the same group are similar and those in different groups are dissimilar to each other", "word_idx": 6329, "sentence_idx": 67, "label": "unlabeled"}, {"type": "text", "expr": "\nFinally, the candidates in each group are classified into faces or non-faces, correspondingly, by voting", "word_idx": 6511, "sentence_idx": 68, "label": "unlabeled"}, {"type": "text", "expr": "The main contributions of this paper can be highlighted as follows", "word_idx": 6616, "sentence_idx": 69, "label": "unlabeled"}, {"type": "text", "expr": "\nFirst, aiming to boost the detection performance, we propose a novel metric learning and graph-cut framework to exploit the semantic information between targeting objects\u2019 neighbors", "word_idx": 6682, "sentence_idx": 70, "label": "unlabeled"}, {"type": "text", "expr": "\nSecondly, to depict local neighborhood relationships, we introduce a pairwise constraint into the tiny face detector to improve the detection accuracy", "word_idx": 6864, "sentence_idx": 71, "label": "unlabeled"}, {"type": "text", "expr": "\nThirdly, to realize such a pairwise constraint, we convert the problem of regression that estimates the similarity between different candidates into a classification problem that produces the scores of classification for each pair of candidates", "word_idx": 7015, "sentence_idx": 72, "label": "unlabeled"}, {"type": "text", "expr": "The main contributions of this paper can be highlighted as follows", "word_idx": 7260, "sentence_idx": 73, "label": "unlabeled"}, {"type": "text", "expr": "\nFirst, aiming to boost the detection performance, we propose a novel metric learning and graph-cut framework to exploit the semantic information between targeting objects\u2019 neighbors", "word_idx": 7326, "sentence_idx": 74, "label": "unlabeled"}, {"type": "text", "expr": "\nSecondly, to depict local neighborhood relationships, we introduce a pairwise constraint into the tiny face detector to improve the detection accuracy", "word_idx": 7508, "sentence_idx": 75, "label": "unlabeled"}, {"type": "text", "expr": "\nThirdly, to realize such a pairwise constraint, we convert the problem of regression that estimates the similarity between different candidates into a classification problem that produces the scores of classification for each pair of candidates", "word_idx": 7659, "sentence_idx": 76, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:  The framework of our proposed MLGC for high-density tiny face detection", "word_idx": 7904, "sentence_idx": 77, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:", "word_idx": 7986, "sentence_idx": 78, "label": "unlabeled"}, {"type": "text", "expr": "2  Related Work", "word_idx": 7995, "sentence_idx": 79, "label": "unlabeled"}, {"type": "text", "expr": "Face detection is a classic topic in computer vision", "word_idx": 8010, "sentence_idx": 80, "label": "unlabeled"}, {"type": "text", "expr": "\nThe pioneer work on the topic was published by Viola and\nJones   who designed a cascade of weak classifiers using Haar features and AdaBoost for fast and robust face detection", "word_idx": 8062, "sentence_idx": 81, "label": "unlabeled"}, {"type": "text", "expr": " Similar in spirit, numerous approaches have been developed to\nimprove the performance with more sophisticated hand-crafted features   and more powerful classifiers  ", "word_idx": 8238, "sentence_idx": 82, "label": "unlabeled"}, {"type": "text", "expr": "\nHowever, these methods using non-robust hand-crafted features\nand optimized each components independently, and hence led to sub-optimal face detection results", "word_idx": 8404, "sentence_idx": 83, "label": "unlabeled"}, {"type": "text", "expr": "\nRecently, face detectors based on  CNNs    have greatly bridged the gap between human vision and artificial detectors", "word_idx": 8563, "sentence_idx": 84, "label": "unlabeled"}, {"type": "text", "expr": "Tiny face detection aims to detect a large number of small faces in crowded and cluttered scenes", "word_idx": 8681, "sentence_idx": 85, "label": "unlabeled"}, {"type": "text", "expr": "\nIt is totally different from detecting normal faces, because the cues for detecting a 3-pixel tall face are fundamentally different from those for detecting a 300-pixel tall face  ", "word_idx": 8777, "sentence_idx": 86, "label": "unlabeled"}, {"type": "text", "expr": " Bell\u00a0  presented the Inside-Outside Net (ION) to model the context outside a region of interest and showed improvements on small object detection", "word_idx": 8958, "sentence_idx": 87, "label": "unlabeled"}, {"type": "text", "expr": "\nVery recently, Hu and Ramanan   designed a foveal descriptor that captured both coarse context and high-resolution image features in order to effectively encode context information, which has achieved state-of-the-art performance on the WIDER FACE dataset", "word_idx": 9104, "sentence_idx": 88, "label": "unlabeled"}, {"type": "text", "expr": "\nAs we all know, it is not sufficient to detect small objects merely by extracting deep learning features from the texture inside an object region", "word_idx": 9360, "sentence_idx": 89, "label": "unlabeled"}, {"type": "text", "expr": " One main drawback is that, these approaches have neglected local semantic information", "word_idx": 9506, "sentence_idx": 90, "label": "unlabeled"}, {"type": "text", "expr": " We have observed that there exists local coherent relationships in terms of spatial location, scale, and texture in high-density tiny face detection, ignoring the influence of various viewpoints", "word_idx": 9592, "sentence_idx": 91, "label": "unlabeled"}, {"type": "text", "expr": " For example, as shown in Fig", "word_idx": 9787, "sentence_idx": 92, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 1 , face bounding boxes close to each other are similar in their scales and textures", "word_idx": 9816, "sentence_idx": 93, "label": "unlabeled"}, {"type": "text", "expr": "\nLocal semantic information helps tiny face detectors better eliminate false alarms", "word_idx": 9902, "sentence_idx": 94, "label": "unlabeled"}, {"type": "text", "expr": "\nTo introduce local coherent relationships, we learn a metric to represent this coherence and use the graph-cut algorithm to divide candidates into several groups, where candidates in the same group are similar, and dissimilar when they are in different groups", "word_idx": 9985, "sentence_idx": 95, "label": "unlabeled"}, {"type": "text", "expr": "3  The Proposed Method", "word_idx": 10245, "sentence_idx": 96, "label": "unlabeled"}, {"type": "text", "expr": "Our goal is to integrate local coherent relationships into tiny face detection", "word_idx": 10267, "sentence_idx": 97, "label": "unlabeled"}, {"type": "text", "expr": "\nIn order to represent local coherent relationships, we define pairwise constraints, which are an equivalence\nconstraint for pairs of data points belonging to same classes, and an inequivalence constraint for pairs of data points belonging to different classes", "word_idx": 10345, "sentence_idx": 98, "label": "unlabeled"}, {"type": "text", "expr": "Our goal is to integrate local coherent relationships into tiny face detection", "word_idx": 10605, "sentence_idx": 99, "label": "unlabeled"}, {"type": "text", "expr": "\nIn order to represent local coherent relationships, we define pairwise constraints, which are an equivalence\nconstraint for pairs of data points belonging to same classes, and an inequivalence constraint for pairs of data points belonging to different classes", "word_idx": 10683, "sentence_idx": 100, "label": "unlabeled"}, {"type": "text", "expr": "As shown in Fig", "word_idx": 10943, "sentence_idx": 101, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 2 , we present a metric learning and graph-cut (MLGC) approach for high-density tiny face detection", "word_idx": 10958, "sentence_idx": 102, "label": "unlabeled"}, {"type": "text", "expr": "\nWe first use a linear-SVM to estimate the similarity matrix among all candidates (Sect", "word_idx": 11059, "sentence_idx": 103, "label": "unlabeled"}, {"type": "text", "expr": "1 ) and then we construct a graph model and use the graph-cut algorithm to divide candidates into several groups (Sect", "word_idx": 11146, "sentence_idx": 104, "label": "unlabeled"}, {"type": "text", "expr": "\nFinally, we design a voting method to classify groups (Sect", "word_idx": 11264, "sentence_idx": 105, "label": "unlabeled"}, {"type": "text", "expr": "1  Metric learning based on linear-SVM", "word_idx": 11324, "sentence_idx": 106, "label": "unlabeled"}, {"type": "text", "expr": "Let  $X=\\{x_{1},x_{2},,x_{N}\\}$  denote the set of  $N$  candidates (i", "word_idx": 11362, "sentence_idx": 107, "label": "unlabeled"}, {"type": "text", "expr": ", face or non-face bounding boxes)", "word_idx": 11432, "sentence_idx": 108, "label": "unlabeled"}, {"type": "text", "expr": "\nTo introduce the pairwise constraint,\nwe first build a similarity matrix  $S=s(x_{i},x_{j}),x_{i},x_{j}\\in X,i,j=1,2,N$ , where  $s(x_{i},x_{j})$  represents the similarity between  $x_{i}$  and  $x_{j}$ ", "word_idx": 11466, "sentence_idx": 109, "label": "unlabeled"}, {"type": "text", "expr": "\n $s(x_{i},x_{j})=1$  means that  $x_{i}$  has a strong resemblance of  $x_{j}$ , and  $s(x_{i},x_{j})=0$  means that  $x_{i}$  is completely different from  $x_{j}$ ", "word_idx": 11671, "sentence_idx": 110, "label": "unlabeled"}, {"type": "math", "expr": "$$X=\\{x_{1},x_{2},...,x_{N}\\}$$", "word_idx": 11837, "sentence_idx": 111, "label": "unlabeled"}, {"type": "math", "expr": "$$S=s(x_{i},x_{j}),x_{i},x_{j}\\in X,i,j=1,2,...N$$", "word_idx": 11864, "sentence_idx": 112, "label": "unlabeled"}, {"type": "math", "expr": "$$s(x_{i},x_{j})$$", "word_idx": 11910, "sentence_idx": 113, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{i}$$", "word_idx": 11924, "sentence_idx": 114, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{j}$$", "word_idx": 11929, "sentence_idx": 115, "label": "unlabeled"}, {"type": "math", "expr": "$$s(x_{i},x_{j})=1$$", "word_idx": 11934, "sentence_idx": 116, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{i}$$", "word_idx": 11950, "sentence_idx": 117, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{j}$$", "word_idx": 11955, "sentence_idx": 118, "label": "unlabeled"}, {"type": "math", "expr": "$$s(x_{i},x_{j})=0$$", "word_idx": 11960, "sentence_idx": 119, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{i}$$", "word_idx": 11976, "sentence_idx": 120, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{j}$$", "word_idx": 11981, "sentence_idx": 121, "label": "unlabeled"}, {"type": "text", "expr": "In order to obtain the similarity score between two candidates  $x_{i}$  and  $x_{j}$ , we treat it as a classification problem and propose an unsupervised way to obtain the similarity score between two candidates", "word_idx": 11986, "sentence_idx": 122, "label": "unlabeled"}, {"type": "text", "expr": "\nWe use SVM to compute the similarity score between two candidates  $x_{i}$  and  $x_{j}$  based on multiple cues, i", "word_idx": 12199, "sentence_idx": 123, "label": "unlabeled"}, {"type": "text", "expr": ", the position, scale, classification score and deep features of the candidates, which are concatenated together into a feature vector  $\\phi(x_{i})$ ", "word_idx": 12315, "sentence_idx": 124, "label": "unlabeled"}, {"type": "text", "expr": " Note that, classification scores and deep features of a candidate  $x_{i}$  are obtained from the tiny face detector  ", "word_idx": 12465, "sentence_idx": 125, "label": "unlabeled"}, {"type": "text", "expr": " During the training stage, we sort  $X$  by their scores in descending order", "word_idx": 12584, "sentence_idx": 126, "label": "unlabeled"}, {"type": "text", "expr": " We suppose that  $X_{Top}$  denotes the top  $10\\%$  of  $X$  which are face patches, while  $X_{Bottom}$  denotes the bottom  $10\\%$ \nof the non-face patches in  $X$ ", "word_idx": 12661, "sentence_idx": 127, "label": "unlabeled"}, {"type": "text", "expr": " As shown in Fig", "word_idx": 12829, "sentence_idx": 128, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 2 , in Stage 2 of our MLGC, we build a training set  $\\{(x_{11}^{{}^{\\prime}},y_{11}^{{}^{\\prime}}),(x_{12}^{{}^{\\prime}},y_{12}^{{}%\n^{\\prime}}),(x_{nn}^{{}^{\\prime}},y_{nn}^{{}^{\\prime}})\\}$ ,  $x_{ij}^{{}^{\\prime}}=\\phi(x_{i})-\\phi(x_{j})$ ,  $y_{ij}^{{}^{\\prime}}=\\{0,1\\}$ ", "word_idx": 12845, "sentence_idx": 129, "label": "unlabeled"}, {"type": "text", "expr": " If  $x_{i},x_{j}\\in X_{Top},y_{ij}^{{}^{\\prime}}=1$ ", "word_idx": 13124, "sentence_idx": 130, "label": "unlabeled"}, {"type": "text", "expr": " If  $x_{i}\\in X_{Top},x_{j}\\in X_{Bottom},y_{ij}^{{}^{\\prime}}=0$ ", "word_idx": 13177, "sentence_idx": 131, "label": "unlabeled"}, {"type": "text", "expr": "\nDuring the testing stage, we feed  $x_{ij}^{{}^{\\prime}}=\\phi(x_{i})-\\phi(x_{j})$  to the SVM classifier, and then use the output score as the similarity score  $s(x_{i},x_{j})$  between  $x_{i}$  and  $x_{j}$ ", "word_idx": 13244, "sentence_idx": 132, "label": "unlabeled"}, {"type": "text", "expr": " Thus, we build the similarity matrix  $S$ ", "word_idx": 13455, "sentence_idx": 133, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{i}$$", "word_idx": 13498, "sentence_idx": 134, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{j}$$", "word_idx": 13503, "sentence_idx": 135, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{i}$$", "word_idx": 13508, "sentence_idx": 136, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{j}$$", "word_idx": 13513, "sentence_idx": 137, "label": "unlabeled"}, {"type": "math", "expr": "$$\\phi(x_{i})$$", "word_idx": 13518, "sentence_idx": 138, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{i}$$", "word_idx": 13529, "sentence_idx": 139, "label": "unlabeled"}, {"type": "math", "expr": "$$X_{Top}$$", "word_idx": 13534, "sentence_idx": 140, "label": "unlabeled"}, {"type": "math", "expr": "$$10\\%$$", "word_idx": 13541, "sentence_idx": 141, "label": "unlabeled"}, {"type": "math", "expr": "$$X_{Bottom}$$", "word_idx": 13545, "sentence_idx": 142, "label": "unlabeled"}, {"type": "math", "expr": "$$10\\%$$", "word_idx": 13555, "sentence_idx": 143, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{(x_{11}^{{}^{\\prime}},y_{11}^{{}^{\\prime}}),(x_{12}^{{}^{\\prime}},y_{12}^{{}%\n^{\\prime}}),...(x_{nn}^{{}^{\\prime}},y_{nn}^{{}^{\\prime}})\\}$$", "word_idx": 13559, "sentence_idx": 144, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{ij}^{{}^{\\prime}}=\\phi(x_{i})-\\phi(x_{j})$$", "word_idx": 13699, "sentence_idx": 145, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{ij}^{{}^{\\prime}}=\\{0,1\\}$$", "word_idx": 13743, "sentence_idx": 146, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{i},x_{j}\\in X_{Top},y_{ij}^{{}^{\\prime}}=1$$", "word_idx": 13771, "sentence_idx": 147, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{i}\\in X_{Top},x_{j}\\in X_{Bottom},y_{ij}^{{}^{\\prime}}=0$$", "word_idx": 13816, "sentence_idx": 148, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{ij}^{{}^{\\prime}}=\\phi(x_{i})-\\phi(x_{j})$$", "word_idx": 13875, "sentence_idx": 149, "label": "unlabeled"}, {"type": "math", "expr": "$$s(x_{i},x_{j})$$", "word_idx": 13919, "sentence_idx": 150, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{i}$$", "word_idx": 13933, "sentence_idx": 151, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{j}$$", "word_idx": 13938, "sentence_idx": 152, "label": "unlabeled"}, {"type": "text", "expr": "2  Graph-cut based on spectral clustering", "word_idx": 13943, "sentence_idx": 153, "label": "unlabeled"}, {"type": "text", "expr": "Given a set of candidates  $X=\\{x_{1},x_{2},,x_{N}\\}$  and a similarity matrix  $S$ , our goal is to cluster  $X$  into different groups", "word_idx": 13984, "sentence_idx": 154, "label": "unlabeled"}, {"type": "text", "expr": " Candidates are similar when they are in the same group, and are dissimilar when they are in different groups", "word_idx": 14120, "sentence_idx": 155, "label": "unlabeled"}, {"type": "text", "expr": " In this work, we adopt the graph-cut algorithm for this purpose", "word_idx": 14229, "sentence_idx": 156, "label": "unlabeled"}, {"type": "text", "expr": " First, we build a graph model  $G=(V,E)$  to represent  $X$ , where each vertex  $v_{i}\\in V$  represents a candidate  $x_{i}$ , and  $e_{ij}\\in E$  represents the similarity  $s(x_{i},x_{j})$  between the corresponding candidates  $x_{i}$  and  $x_{j}$ ", "word_idx": 14293, "sentence_idx": 157, "label": "unlabeled"}, {"type": "text", "expr": " Then, clustering  $X$  into groups can be reformulated with the graph model represented in Eq", "word_idx": 14548, "sentence_idx": 158, "label": "unlabeled"}, {"type": "text", "expr": "\nWe want to find a partition of the graph so that the weights of edges between different subgraphs are very low\n(indicating that points in different clusters are dissimilar from each other) and the weights of edges in the same group are very high (meaning that points within the same cluster are similar to each other)", "word_idx": 14642, "sentence_idx": 159, "label": "unlabeled"}, {"type": "text", "expr": "\nFormally,", "word_idx": 14960, "sentence_idx": 160, "label": "unlabeled"}, {"type": "math", "expr": "$$X=\\{x_{1},x_{2},...,x_{N}\\}$$", "word_idx": 14970, "sentence_idx": 161, "label": "unlabeled"}, {"type": "math", "expr": "$$G=(V,E)$$", "word_idx": 14997, "sentence_idx": 162, "label": "unlabeled"}, {"type": "math", "expr": "$$v_{i}\\in V$$", "word_idx": 15004, "sentence_idx": 163, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{i}$$", "word_idx": 15014, "sentence_idx": 164, "label": "unlabeled"}, {"type": "math", "expr": "$$e_{ij}\\in E$$", "word_idx": 15019, "sentence_idx": 165, "label": "unlabeled"}, {"type": "math", "expr": "$$s(x_{i},x_{j})$$", "word_idx": 15030, "sentence_idx": 166, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{i}$$", "word_idx": 15044, "sentence_idx": 167, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{j}$$", "word_idx": 15049, "sentence_idx": 168, "label": "unlabeled"}, {"type": "text", "expr": "$cut(A_{1},A_{2},,A_{k})=\\frac{1}{2}\\sum_{i=1}^{k}W(A_{i},\\bar{A_{i}})$", "word_idx": 15054, "sentence_idx": 169, "label": "unlabeled"}, {"type": "math", "expr": "$$cut(A_{1},A_{2},...,A_{k})=\\frac{1}{2}\\sum_{i=1}^{k}W(A_{i},\\bar{A_{i}})$$", "word_idx": 15125, "sentence_idx": 170, "label": "unlabeled"}, {"type": "text", "expr": "where  $A_{i}\\subset V,A_{i}\\cap A_{j}=\\emptyset$  and  $A_{1}\\cup A_{2}\\cup\\cup A_{k}=V$ ,  $W(A_{i},\\bar{A_{i}})=\\sum_{m\\in{A_{i}},n\\in\\bar{A_{i}}}w_{mn}$ ,  $w_{mn}=exp({-S_{mn}}/{2\\delta^{2}})$  used to boost local neighborhood relationships", "word_idx": 15197, "sentence_idx": 171, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{i}\\subset V,A_{i}\\cap A_{j}=\\emptyset$$", "word_idx": 15442, "sentence_idx": 172, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{1}\\cup A_{2}\\cup...\\cup A_{k}=V$$", "word_idx": 15482, "sentence_idx": 173, "label": "unlabeled"}, {"type": "math", "expr": "$$W(A_{i},\\bar{A_{i}})=\\sum_{m\\in{A_{i}},n\\in\\bar{A_{i}}}w_{mn}$$", "word_idx": 15516, "sentence_idx": 174, "label": "unlabeled"}, {"type": "math", "expr": "$$w_{mn}=exp({-S_{mn}}/{2\\delta^{2}})$$", "word_idx": 15577, "sentence_idx": 175, "label": "unlabeled"}, {"type": "text", "expr": "However, the solution simply separates one individual vertex from the rest of the graph", "word_idx": 15612, "sentence_idx": 176, "label": "unlabeled"}, {"type": "text", "expr": "\nTo avoid unbalanced graph-cut situation that there is a large difference in sizes of subgraphs, we introduce the size of\nsubgraph  $|A|$  which is the number of vertexes in  $A$  to ensure the set of subgraph  $\\{A_{1},A_{2},,A_{k}\\}$  is reasonably large", "word_idx": 15699, "sentence_idx": 177, "label": "unlabeled"}, {"type": "text", "expr": " So, we can transform Eq", "word_idx": 15955, "sentence_idx": 178, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 1  as follows:", "word_idx": 15979, "sentence_idx": 179, "label": "unlabeled"}, {"type": "math", "expr": "$$|A|$$", "word_idx": 15995, "sentence_idx": 180, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{A_{1},A_{2},...,A_{k}\\}$$", "word_idx": 15998, "sentence_idx": 181, "label": "unlabeled"}, {"type": "text", "expr": "$cut(A_{1},A_{2},,A_{k})=\\frac{1}{2}\\sum_{i=1}^{k}\\frac{W(A_{i},\\bar{A_{i}})%\n}{|A_{i}|}$", "word_idx": 16023, "sentence_idx": 182, "label": "unlabeled"}, {"type": "math", "expr": "$$cut(A_{1},A_{2},...,A_{k})=\\frac{1}{2}\\sum_{i=1}^{k}\\frac{W(A_{i},\\bar{A_{i}})%\n}{|A_{i}|}$$", "word_idx": 16112, "sentence_idx": 183, "label": "unlabeled"}, {"type": "text", "expr": "According to  ,", "word_idx": 16202, "sentence_idx": 184, "label": "unlabeled"}, {"type": "text", "expr": "$\\begin{split}\\displaystyle\\arg\\min cut(A_{1},A_{2},,A_{k})=\\mathop{\\arg\\min%\n}_{H}Tr(H^{T}LH)\\end{split}$", "word_idx": 16217, "sentence_idx": 185, "label": "unlabeled"}, {"type": "math", "expr": "$$\\begin{split}\\displaystyle\\arg\\min cut(A_{1},A_{2},...,A_{k})=\\mathop{\\arg\\min%\n}_{H}Tr(H^{T}LH)\\end{split}$$", "word_idx": 16323, "sentence_idx": 186, "label": "unlabeled"}, {"type": "text", "expr": "where\n $L\\textrm{ is the Laplacian matrix},H^{T}H=I$ , and the indicator", "word_idx": 16430, "sentence_idx": 187, "label": "unlabeled"}, {"type": "math", "expr": "$$L\\textrm{ is the Laplacian matrix},H^{T}H=I$$", "word_idx": 16502, "sentence_idx": 188, "label": "unlabeled"}, {"type": "text", "expr": "$H=\\{h_{1},h_{2},,h_{k}\\}$", "word_idx": 16545, "sentence_idx": 189, "label": "unlabeled"}, {"type": "math", "expr": "$$H=\\{h_{1},h_{2},...,h_{k}\\}$$", "word_idx": 16571, "sentence_idx": 190, "label": "unlabeled"}, {"type": "text", "expr": "$h_{i,j}=\\left\\{\\begin{array}[]{ll}\\frac{1}{\\sqrt{A_{j}}}&\\textrm{if $v_{i}\\in A%\n_{j}$}\\\\\n0&\\text{otherwise}\\\\\n\\end{array}\\right$", "word_idx": 16598, "sentence_idx": 191, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{i,j}=\\left\\{\\begin{array}[]{ll}\\frac{1}{\\sqrt{A_{j}}}&\\textrm{if $v_{i}\\in A%\n_{j}$}\\\\\n0&\\text{otherwise}\\\\\n\\end{array}\\right.$$", "word_idx": 16728, "sentence_idx": 192, "label": "unlabeled"}, {"type": "text", "expr": "for  $i=1,2,,N;j=1,2,,k$ ", "word_idx": 16857, "sentence_idx": 193, "label": "unlabeled"}, {"type": "math", "expr": "$$i=1,2,...,N;j=1,2,...,k$$", "word_idx": 16882, "sentence_idx": 194, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 3  is the standard form of a trace minimization\nproblem", "word_idx": 16905, "sentence_idx": 195, "label": "unlabeled"}, {"type": "text", "expr": " According to the Rayleigh-Ritz theorem\u00a0 , the solution is given by choosing the matrix  $U$  which contains the first  $k$  eigenvectors of  $L$  and then uses the  $k$ -means algorithm on  $U$ ", "word_idx": 16962, "sentence_idx": 196, "label": "unlabeled"}, {"type": "text", "expr": " So, we manage to cluster  $X$  into\n $k$  groups  $\\{A_{1},A_{2},,A_{k}\\}$ ", "word_idx": 17157, "sentence_idx": 197, "label": "unlabeled"}, {"type": "text", "expr": "\nFinally, candidates in each group are classified to face or non-face class using voting", "word_idx": 17233, "sentence_idx": 198, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{A_{1},A_{2},...,A_{k}\\}$$", "word_idx": 17321, "sentence_idx": 199, "label": "unlabeled"}, {"type": "text", "expr": "4  Experiments", "word_idx": 17346, "sentence_idx": 200, "label": "unlabeled"}, {"type": "text", "expr": "In this section, we first demonstrate the effectiveness of our proposesd semantic similarity metric and then evaluate the whole model on three widely-used face detection benchmarks, including WIDER FACE\u00a0 , Annotated Faces in the Wild (AFW)\u00a0  and Pascal Faces\u00a0 ", "word_idx": 17360, "sentence_idx": 201, "label": "unlabeled"}, {"type": "text", "expr": "To demonstrate the effectiveness of our proposed semantic metric (see Subsection\u00a0 3", "word_idx": 17620, "sentence_idx": 202, "label": "unlabeled"}, {"type": "text", "expr": "1 ) for similarity measurement, we create positive samples, i", "word_idx": 17703, "sentence_idx": 203, "label": "unlabeled"}, {"type": "text", "expr": ", the ground truth face regions, and negative samples which are patches randomly sampled from background, and evaluate the discriminative ability of the computed similarity matrix on the WIDER FACE validation set", "word_idx": 17764, "sentence_idx": 204, "label": "unlabeled"}, {"type": "text", "expr": "\nThe average precision in each image on the validation set is\n $7958\\%$  in the testing set composed of both positive and negative samples,\n $7225\\%$  in the set of positive samples only,\nand  $8675\\%$  in the set of negative samples only", "word_idx": 17976, "sentence_idx": 205, "label": "unlabeled"}, {"type": "math", "expr": "$$79.58\\%$$", "word_idx": 18214, "sentence_idx": 206, "label": "unlabeled"}, {"type": "math", "expr": "$$72.25\\%$$", "word_idx": 18221, "sentence_idx": 207, "label": "unlabeled"}, {"type": "math", "expr": "$$86.75\\%$$", "word_idx": 18228, "sentence_idx": 208, "label": "unlabeled"}, {"type": "text", "expr": "1  The AFW and PASCAL FACE Dataset Results", "word_idx": 18235, "sentence_idx": 209, "label": "unlabeled"}, {"type": "text", "expr": "The AFW dataset has 205 images containing in total 473 labelled faces", "word_idx": 18277, "sentence_idx": 210, "label": "unlabeled"}, {"type": "text", "expr": "\nWe evaluate our model against the HR\u00a0 , DPM  , Headhunter, SquaresChnFtrs\u00a0 , Structured Models  , Shen et al", "word_idx": 18346, "sentence_idx": 211, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 , TSM\u00a0  and commercial detectors (e", "word_idx": 18455, "sentence_idx": 212, "label": "unlabeled"}, {"type": "text", "expr": ", Face", "word_idx": 18492, "sentence_idx": 213, "label": "unlabeled"}, {"type": "text", "expr": "com, Face++ and Picasa)", "word_idx": 18498, "sentence_idx": 214, "label": "unlabeled"}, {"type": "text", "expr": "\nAs illustrated in Fig", "word_idx": 18521, "sentence_idx": 215, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 LABEL:exp:allb , our MLGC\noutperforms all other detectors on precision-recall (PR) curves", "word_idx": 18543, "sentence_idx": 216, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:exp:allb", "word_idx": 18634, "sentence_idx": 217, "label": "unlabeled"}, {"type": "text", "expr": "The PASCAL FACE dataset contains 1,335 labeled faces in 851 images, which are collected from PASCAL person layout subset", "word_idx": 18648, "sentence_idx": 218, "label": "unlabeled"}, {"type": "text", "expr": " Because this paper focuses on face detection, we ignore images without persons from the original dataset, similar like DPM\u00a0 ", "word_idx": 18768, "sentence_idx": 219, "label": "unlabeled"}, {"type": "text", "expr": "\nWe also evaluate our model against the HR\u00a0 , DPM\u00a0 , Headhunter, SquaresChnFtrs\u00a0 , Structured Models\u00a0 , Shen et al", "word_idx": 18893, "sentence_idx": 220, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 , TSM\u00a0  and commercial detectors (e", "word_idx": 19007, "sentence_idx": 221, "label": "unlabeled"}, {"type": "text", "expr": ", Face++ and Picasa)", "word_idx": 19044, "sentence_idx": 222, "label": "unlabeled"}, {"type": "text", "expr": "\nAs shown in Fig", "word_idx": 19064, "sentence_idx": 223, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 LABEL:exp:allc , our MLGC\noutperforms all other detectors on PR curves", "word_idx": 19080, "sentence_idx": 224, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:exp:allc", "word_idx": 19152, "sentence_idx": 225, "label": "unlabeled"}, {"type": "text", "expr": "2  Results Obtained on the WIDER FACE Dataset", "word_idx": 19166, "sentence_idx": 226, "label": "unlabeled"}, {"type": "text", "expr": "The WIDER FACE Dataset is one of the most challenging public face datasets due to the variety of face scales and occlusion", "word_idx": 19211, "sentence_idx": 227, "label": "unlabeled"}, {"type": "text", "expr": "\nIt contains 32,203 images split into training (40%), validation (10%) and testing (50%) set", "word_idx": 19333, "sentence_idx": 228, "label": "unlabeled"}, {"type": "text", "expr": "\nThe validation set and testing set are divided into \u201ceasy\u201d, \u201cmedium\u201d, and \u201chard\u201d subsets according to the difficulties of the detection", "word_idx": 19425, "sentence_idx": 229, "label": "unlabeled"}, {"type": "text", "expr": "The WIDER FACE Dataset is one of the most challenging public face datasets due to the variety of face scales and occlusion", "word_idx": 19561, "sentence_idx": 230, "label": "unlabeled"}, {"type": "text", "expr": "\nIt contains 32,203 images split into training (40%), validation (10%) and testing (50%) set", "word_idx": 19683, "sentence_idx": 231, "label": "unlabeled"}, {"type": "text", "expr": "\nThe validation set and testing set are divided into \u201ceasy\u201d, \u201cmedium\u201d, and \u201chard\u201d subsets according to the difficulties of the detection", "word_idx": 19775, "sentence_idx": 232, "label": "unlabeled"}, {"type": "text", "expr": "We compare our MLGC with the HR\u00a0 , MSCNN\u00a0 , ScaleFace\u00a0 , CMS-RCNN\u00a0  and Multitask Cascade CNN\u00a0 ", "word_idx": 19911, "sentence_idx": 233, "label": "unlabeled"}, {"type": "text", "expr": "\nThe PR curves on the testing set is presented in Fig", "word_idx": 20006, "sentence_idx": 234, "label": "unlabeled"}, {"type": "text", "expr": " \u00a0 LABEL:exp:alla , and our method outperforms HR by 0", "word_idx": 20059, "sentence_idx": 235, "label": "unlabeled"}, {"type": "text", "expr": "2% in \u201ceasy\u201d subset", "word_idx": 20113, "sentence_idx": 236, "label": "unlabeled"}, {"type": "text", "expr": "\nThe PR curves on the validation set is presented in Fig", "word_idx": 20132, "sentence_idx": 237, "label": "unlabeled"}, {"type": "text", "expr": "  LABEL:fig:overall  and our method outperforms the HR by 0", "word_idx": 20188, "sentence_idx": 238, "label": "unlabeled"}, {"type": "text", "expr": "5%, 0", "word_idx": 20247, "sentence_idx": 239, "label": "unlabeled"}, {"type": "text", "expr": "2%, 0", "word_idx": 20252, "sentence_idx": 240, "label": "unlabeled"}, {"type": "text", "expr": "3%, in \u201ceasy\u201d, \u201cmedium\u201d and \u201chard\u201d subsets respectively", "word_idx": 20257, "sentence_idx": 241, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:exp:alla", "word_idx": 20312, "sentence_idx": 242, "label": "unlabeled"}, {"type": "text", "expr": "LABEL:fig:overall", "word_idx": 20326, "sentence_idx": 243, "label": "unlabeled"}, {"type": "text", "expr": "5  Conclusion", "word_idx": 20343, "sentence_idx": 244, "label": "unlabeled"}, {"type": "text", "expr": "In this paper, aiming to improve the performance of tiny face detection, we have proposed a novel idea to exploit the semantic similarity between targeting objects\u2019 neighbors and created a pairwise constraint to depict such semantic similarity", "word_idx": 20356, "sentence_idx": 245, "label": "unlabeled"}, {"type": "text", "expr": "\nThen, a framework which adopts the metric learning and graph-cut techniques has been formulated to boost the accuracy of existing tiny object classifiers", "word_idx": 20599, "sentence_idx": 246, "label": "unlabeled"}, {"type": "text", "expr": "\nExperiments conducted on three widely-used benchmark datasets for face detection have demonstrated the improvement over the state-of-the-arts by applying this idea", "word_idx": 20753, "sentence_idx": 247, "label": "unlabeled"}, {"type": "text", "expr": "\nThe mechanism of our proposed framework is generic indicating that the framework has a great potential being applied on other small and generic object detectors", "word_idx": 20917, "sentence_idx": 248, "label": "unlabeled"}, {"type": "text", "expr": "\nThis forms our work for the next step", "word_idx": 21078, "sentence_idx": 249, "label": "unlabeled"}, {"type": "text", "expr": "In this paper, aiming to improve the performance of tiny face detection, we have proposed a novel idea to exploit the semantic similarity between targeting objects\u2019 neighbors and created a pairwise constraint to depict such semantic similarity", "word_idx": 21116, "sentence_idx": 250, "label": "unlabeled"}, {"type": "text", "expr": "\nThen, a framework which adopts the metric learning and graph-cut techniques has been formulated to boost the accuracy of existing tiny object classifiers", "word_idx": 21359, "sentence_idx": 251, "label": "unlabeled"}, {"type": "text", "expr": "\nExperiments conducted on three widely-used benchmark datasets for face detection have demonstrated the improvement over the state-of-the-arts by applying this idea", "word_idx": 21513, "sentence_idx": 252, "label": "unlabeled"}, {"type": "text", "expr": "\nThe mechanism of our proposed framework is generic indicating that the framework has a great potential being applied on other small and generic object detectors", "word_idx": 21677, "sentence_idx": 253, "label": "unlabeled"}, {"type": "text", "expr": "\nThis forms our work for the next step", "word_idx": 21838, "sentence_idx": 254, "label": "unlabeled"}, {"type": "text", "expr": "References", "word_idx": 21876, "sentence_idx": 255, "label": "unlabeled"}, {"type": "text", "expr": "1 \nXuehan Xiong and Fernando De\u00a0la Torre,\n\n \u201cSupervised descent method and its applications to face alignment,\u201d", "word_idx": 21886, "sentence_idx": 256, "label": "unlabeled"}, {"type": "text", "expr": "Xuehan Xiong and Fernando De\u00a0la Torre,", "word_idx": 21997, "sentence_idx": 257, "label": "unlabeled"}, {"type": "text", "expr": "\u201cSupervised descent method and its applications to face alignment,\u201d", "word_idx": 22035, "sentence_idx": 258, "label": "unlabeled"}, {"type": "text", "expr": "in  CVPR , 2013", "word_idx": 22102, "sentence_idx": 259, "label": "unlabeled"}, {"type": "text", "expr": "2 \nXiangyu Zhu, Zhen Lei, Xiaoming Liu, Hailin Shi, and Stan\u00a0Z Li,\n\n \u201cFace alignment across large poses: A 3d solution,\u201d", "word_idx": 22117, "sentence_idx": 260, "label": "unlabeled"}, {"type": "text", "expr": "Xiangyu Zhu, Zhen Lei, Xiaoming Liu, Hailin Shi, and Stan\u00a0Z Li,", "word_idx": 22237, "sentence_idx": 261, "label": "unlabeled"}, {"type": "text", "expr": "\u201cFace alignment across large poses: A 3d solution,\u201d", "word_idx": 22300, "sentence_idx": 262, "label": "unlabeled"}, {"type": "text", "expr": "in  CVPR , 2016, pp", "word_idx": 22351, "sentence_idx": 263, "label": "unlabeled"}, {"type": "text", "expr": " 146\u2013155", "word_idx": 22370, "sentence_idx": 264, "label": "unlabeled"}, {"type": "text", "expr": "3 \nOmkar\u00a0M Parkhi, Andrea Vedaldi, Andrew Zisserman, et\u00a0al", "word_idx": 22378, "sentence_idx": 265, "label": "unlabeled"}, {"type": "text", "expr": ",\n\n \u201cDeep face recognition", "word_idx": 22436, "sentence_idx": 266, "label": "unlabeled"}, {"type": "text", "expr": "Omkar\u00a0M Parkhi, Andrea Vedaldi, Andrew Zisserman, et\u00a0al", "word_idx": 22462, "sentence_idx": 267, "label": "unlabeled"}, {"type": "text", "expr": "\u201cDeep face recognition", "word_idx": 22517, "sentence_idx": 268, "label": "unlabeled"}, {"type": "text", "expr": "in  BMVC , 2015", "word_idx": 22539, "sentence_idx": 269, "label": "unlabeled"}, {"type": "text", "expr": "4 \nFlorian Schroff, Dmitry Kalenichenko, and James Philbin,\n\n \u201cFacenet: A unified embedding for face recognition and clustering,\u201d", "word_idx": 22554, "sentence_idx": 270, "label": "unlabeled"}, {"type": "text", "expr": "Florian Schroff, Dmitry Kalenichenko, and James Philbin,", "word_idx": 22683, "sentence_idx": 271, "label": "unlabeled"}, {"type": "text", "expr": "\u201cFacenet: A unified embedding for face recognition and clustering,\u201d", "word_idx": 22739, "sentence_idx": 272, "label": "unlabeled"}, {"type": "text", "expr": "in  CVPR , 2015, pp", "word_idx": 22806, "sentence_idx": 273, "label": "unlabeled"}, {"type": "text", "expr": " 815\u2013823", "word_idx": 22825, "sentence_idx": 274, "label": "unlabeled"}, {"type": "text", "expr": "5 \nXiangyu Zhu, Zhen Lei, Junjie Yan, Dong Yi, and Stan\u00a0Z Li,\n\n \u201cHigh-fidelity pose and expression normalization for face\nrecognition in the wild,\u201d", "word_idx": 22833, "sentence_idx": 275, "label": "unlabeled"}, {"type": "text", "expr": "Xiangyu Zhu, Zhen Lei, Junjie Yan, Dong Yi, and Stan\u00a0Z Li,", "word_idx": 22980, "sentence_idx": 276, "label": "unlabeled"}, {"type": "text", "expr": "\u201cHigh-fidelity pose and expression normalization for face\nrecognition in the wild,\u201d", "word_idx": 23038, "sentence_idx": 277, "label": "unlabeled"}, {"type": "text", "expr": "in  CVPR , 2015", "word_idx": 23121, "sentence_idx": 278, "label": "unlabeled"}, {"type": "text", "expr": "6 \nYi\u00a0Sun, Yuheng Chen, Xiaogang Wang, and Xiaoou Tang,\n\n \u201cDeep learning face representation by joint\nidentification-verification,\u201d", "word_idx": 23136, "sentence_idx": 279, "label": "unlabeled"}, {"type": "text", "expr": "Yi\u00a0Sun, Yuheng Chen, Xiaogang Wang, and Xiaoou Tang,", "word_idx": 23267, "sentence_idx": 280, "label": "unlabeled"}, {"type": "text", "expr": "\u201cDeep learning face representation by joint\nidentification-verification,\u201d", "word_idx": 23319, "sentence_idx": 281, "label": "unlabeled"}, {"type": "text", "expr": "in  NIPS , 2014", "word_idx": 23392, "sentence_idx": 282, "label": "unlabeled"}, {"type": "text", "expr": "7 \nMinyoung Kim, Sanjiv Kumar, Vladimir Pavlovic, and Henry Rowley,\n\n \u201cFace tracking and recognition with visual constraints in real-world\nvideos,\u201d", "word_idx": 23407, "sentence_idx": 283, "label": "unlabeled"}, {"type": "text", "expr": "Minyoung Kim, Sanjiv Kumar, Vladimir Pavlovic, and Henry Rowley,", "word_idx": 23554, "sentence_idx": 284, "label": "unlabeled"}, {"type": "text", "expr": "\u201cFace tracking and recognition with visual constraints in real-world\nvideos,\u201d", "word_idx": 23618, "sentence_idx": 285, "label": "unlabeled"}, {"type": "text", "expr": "in  CVPR ", "word_idx": 23695, "sentence_idx": 286, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 2008, pp", "word_idx": 23704, "sentence_idx": 287, "label": "unlabeled"}, {"type": "text", "expr": "8 \nPaul Viola and Michael Jones,\n\n \u201cRobust real-time face detection,\u201d", "word_idx": 23719, "sentence_idx": 288, "label": "unlabeled"}, {"type": "text", "expr": "Paul Viola and Michael Jones,", "word_idx": 23788, "sentence_idx": 289, "label": "unlabeled"}, {"type": "text", "expr": "\u201cRobust real-time face detection,\u201d", "word_idx": 23817, "sentence_idx": 290, "label": "unlabeled"}, {"type": "text", "expr": "IJCV , vol", "word_idx": 23851, "sentence_idx": 291, "label": "unlabeled"}, {"type": "text", "expr": " 57, no", "word_idx": 23861, "sentence_idx": 292, "label": "unlabeled"}, {"type": "text", "expr": " 2, pp", "word_idx": 23868, "sentence_idx": 293, "label": "unlabeled"}, {"type": "text", "expr": " 137\u2013154, 2004", "word_idx": 23874, "sentence_idx": 294, "label": "unlabeled"}, {"type": "text", "expr": "9 \nPeiyun Hu and Deva Ramanan,\n\n \u201cFinding tiny faces,\u201d", "word_idx": 23888, "sentence_idx": 295, "label": "unlabeled"}, {"type": "text", "expr": "Peiyun Hu and Deva Ramanan,", "word_idx": 23942, "sentence_idx": 296, "label": "unlabeled"}, {"type": "text", "expr": "\u201cFinding tiny faces,\u201d", "word_idx": 23969, "sentence_idx": 297, "label": "unlabeled"}, {"type": "text", "expr": "in  CVPR ", "word_idx": 23990, "sentence_idx": 298, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 2017, pp", "word_idx": 23999, "sentence_idx": 299, "label": "unlabeled"}, {"type": "text", "expr": " 1522\u20131530", "word_idx": 24014, "sentence_idx": 300, "label": "unlabeled"}, {"type": "text", "expr": "10 \nShuo Yang, Ping Luo, Chen-Change Loy, and Xiaoou Tang,\n\n \u201cWider face: A face detection benchmark,\u201d", "word_idx": 24024, "sentence_idx": 301, "label": "unlabeled"}, {"type": "text", "expr": "Shuo Yang, Ping Luo, Chen-Change Loy, and Xiaoou Tang,", "word_idx": 24126, "sentence_idx": 302, "label": "unlabeled"}, {"type": "text", "expr": "\u201cWider face: A face detection benchmark,\u201d", "word_idx": 24180, "sentence_idx": 303, "label": "unlabeled"}, {"type": "text", "expr": "in  CVPR , 2016, pp", "word_idx": 24221, "sentence_idx": 304, "label": "unlabeled"}, {"type": "text", "expr": " 5525\u20135533", "word_idx": 24240, "sentence_idx": 305, "label": "unlabeled"}, {"type": "text", "expr": "11 \nWei Liu, Dragomir Anguelov, Dumitru Erhan, Cheng-Yang Szegedy, and Alexander\u00a0C\nBerg,\n\n \u201cSsd: Single shot multibox detector,\u201d", "word_idx": 24250, "sentence_idx": 306, "label": "unlabeled"}, {"type": "text", "expr": "Wei Liu, Dragomir Anguelov, Dumitru Erhan, Cheng-Yang Szegedy, and Alexander\u00a0C\nBerg,", "word_idx": 24378, "sentence_idx": 307, "label": "unlabeled"}, {"type": "text", "expr": "\u201cSsd: Single shot multibox detector,\u201d", "word_idx": 24462, "sentence_idx": 308, "label": "unlabeled"}, {"type": "text", "expr": "in  ECCV , 2016", "word_idx": 24499, "sentence_idx": 309, "label": "unlabeled"}, {"type": "text", "expr": "12 \nQuoc\u00a0V", "word_idx": 24514, "sentence_idx": 310, "label": "unlabeled"}, {"type": "text", "expr": " Le, Navdeep Jaitly, and Geoffrey\u00a0E", "word_idx": 24524, "sentence_idx": 311, "label": "unlabeled"}, {"type": "text", "expr": " Hinton,\n\n \u201cA simple way to initialize recurrent networks of rectified linear\nunits,\u201d", "word_idx": 24559, "sentence_idx": 312, "label": "unlabeled"}, {"type": "text", "expr": "Quoc\u00a0V", "word_idx": 24644, "sentence_idx": 313, "label": "unlabeled"}, {"type": "text", "expr": " Le, Navdeep Jaitly, and Geoffrey\u00a0E", "word_idx": 24650, "sentence_idx": 314, "label": "unlabeled"}, {"type": "text", "expr": " Hinton,", "word_idx": 24685, "sentence_idx": 315, "label": "unlabeled"}, {"type": "text", "expr": "\u201cA simple way to initialize recurrent networks of rectified linear\nunits,\u201d", "word_idx": 24693, "sentence_idx": 316, "label": "unlabeled"}, {"type": "text", "expr": "CoRR , vol", "word_idx": 24767, "sentence_idx": 317, "label": "unlabeled"}, {"type": "text", "expr": " abs/1504", "word_idx": 24777, "sentence_idx": 318, "label": "unlabeled"}, {"type": "text", "expr": "00941, 2015", "word_idx": 24786, "sentence_idx": 319, "label": "unlabeled"}, {"type": "text", "expr": "13 \nBin Yang, Junjie Yan, Zhen Lei, and Stan\u00a0Z Li,\n\n \u201cAggregate channel features for multi-view face detection,\u201d", "word_idx": 24797, "sentence_idx": 320, "label": "unlabeled"}, {"type": "text", "expr": "Bin Yang, Junjie Yan, Zhen Lei, and Stan\u00a0Z Li,", "word_idx": 24909, "sentence_idx": 321, "label": "unlabeled"}, {"type": "text", "expr": "\u201cAggregate channel features for multi-view face detection,\u201d", "word_idx": 24955, "sentence_idx": 322, "label": "unlabeled"}, {"type": "text", "expr": "in  IJCB ", "word_idx": 25014, "sentence_idx": 323, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 2014", "word_idx": 25023, "sentence_idx": 324, "label": "unlabeled"}, {"type": "text", "expr": "14 \nMinh\u00a0Tri Pham and Tat\u00a0Jen Chain,\n\n \u201cFast training and selection of haar features using statistics in\nboosting-based face detection,\u201d", "word_idx": 25034, "sentence_idx": 325, "label": "unlabeled"}, {"type": "text", "expr": "Minh\u00a0Tri Pham and Tat\u00a0Jen Chain,", "word_idx": 25170, "sentence_idx": 326, "label": "unlabeled"}, {"type": "text", "expr": "\u201cFast training and selection of haar features using statistics in\nboosting-based face detection,\u201d", "word_idx": 25202, "sentence_idx": 327, "label": "unlabeled"}, {"type": "text", "expr": "in  ICCV , 2007, pp", "word_idx": 25299, "sentence_idx": 328, "label": "unlabeled"}, {"type": "text", "expr": "15 \nHaoxiang Li, Zhe Lin, Xiaohui Shen, Jonathan Brandt, and Gang Hua,\n\n \u201cA convolutional neural network cascade for face detection,\u201d", "word_idx": 25318, "sentence_idx": 329, "label": "unlabeled"}, {"type": "text", "expr": "Haoxiang Li, Zhe Lin, Xiaohui Shen, Jonathan Brandt, and Gang Hua,", "word_idx": 25451, "sentence_idx": 330, "label": "unlabeled"}, {"type": "text", "expr": "\u201cA convolutional neural network cascade for face detection,\u201d", "word_idx": 25517, "sentence_idx": 331, "label": "unlabeled"}, {"type": "text", "expr": "in  CVPR , 2015", "word_idx": 25577, "sentence_idx": 332, "label": "unlabeled"}, {"type": "text", "expr": "16 \nBin Yang, Junjie Yan, Zhen Lei, and Stan\u00a0Z Li,\n\n \u201cConvolutional channel features,\u201d", "word_idx": 25592, "sentence_idx": 333, "label": "unlabeled"}, {"type": "text", "expr": "Bin Yang, Junjie Yan, Zhen Lei, and Stan\u00a0Z Li,", "word_idx": 25678, "sentence_idx": 334, "label": "unlabeled"}, {"type": "text", "expr": "\u201cConvolutional channel features,\u201d", "word_idx": 25724, "sentence_idx": 335, "label": "unlabeled"}, {"type": "text", "expr": "in  CVPR , 2015, pp", "word_idx": 25757, "sentence_idx": 336, "label": "unlabeled"}, {"type": "text", "expr": " 82\u201390", "word_idx": 25776, "sentence_idx": 337, "label": "unlabeled"}, {"type": "text", "expr": "17 \nShuo Yang, Ping Luo, Chen-Change Loy, and Xiaoou Tang,\n\n \u201cFrom facial parts responses to face detection: A deep learning\napproach,\u201d", "word_idx": 25782, "sentence_idx": 338, "label": "unlabeled"}, {"type": "text", "expr": "Shuo Yang, Ping Luo, Chen-Change Loy, and Xiaoou Tang,", "word_idx": 25917, "sentence_idx": 339, "label": "unlabeled"}, {"type": "text", "expr": "\u201cFrom facial parts responses to face detection: A deep learning\napproach,\u201d", "word_idx": 25971, "sentence_idx": 340, "label": "unlabeled"}, {"type": "text", "expr": "in  ICCV , 2015", "word_idx": 26045, "sentence_idx": 341, "label": "unlabeled"}, {"type": "text", "expr": "18 \nSean Bell, C\u00a0Lawrence\u00a0Zitnick, Kavita Bala, and Ross Girshick,\n\n \u201cInside-outside net: Detecting objects in context with skip pooling\nand recurrent neural networks,\u201d", "word_idx": 26060, "sentence_idx": 342, "label": "unlabeled"}, {"type": "text", "expr": "Sean Bell, C\u00a0Lawrence\u00a0Zitnick, Kavita Bala, and Ross Girshick,", "word_idx": 26228, "sentence_idx": 343, "label": "unlabeled"}, {"type": "text", "expr": "\u201cInside-outside net: Detecting objects in context with skip pooling\nand recurrent neural networks,\u201d", "word_idx": 26290, "sentence_idx": 344, "label": "unlabeled"}, {"type": "text", "expr": "in  CVPR , 2016, pp", "word_idx": 26389, "sentence_idx": 345, "label": "unlabeled"}, {"type": "text", "expr": " 2874\u20132883", "word_idx": 26408, "sentence_idx": 346, "label": "unlabeled"}, {"type": "text", "expr": "19 \nUlrike Von\u00a0Luxburg,\n\n \u201cA tutorial on spectral clustering,\u201d", "word_idx": 26418, "sentence_idx": 347, "label": "unlabeled"}, {"type": "text", "expr": "Ulrike Von\u00a0Luxburg,", "word_idx": 26480, "sentence_idx": 348, "label": "unlabeled"}, {"type": "text", "expr": "\u201cA tutorial on spectral clustering,\u201d", "word_idx": 26499, "sentence_idx": 349, "label": "unlabeled"}, {"type": "text", "expr": "Statistics and computing , 2007", "word_idx": 26535, "sentence_idx": 350, "label": "unlabeled"}, {"type": "text", "expr": "Statistics and computing", "word_idx": 26566, "sentence_idx": 351, "label": "unlabeled"}, {"type": "text", "expr": "20 \nHelmut Lutkepohl,\n\n \u201cHandbook of matrices", "word_idx": 26590, "sentence_idx": 352, "label": "unlabeled"}, {"type": "text", "expr": "Helmut Lutkepohl,", "word_idx": 26635, "sentence_idx": 353, "label": "unlabeled"}, {"type": "text", "expr": "\u201cHandbook of matrices", "word_idx": 26652, "sentence_idx": 354, "label": "unlabeled"}, {"type": "text", "expr": "Computational Statistics and Data Analysis , 1997", "word_idx": 26673, "sentence_idx": 355, "label": "unlabeled"}, {"type": "text", "expr": "Computational Statistics and Data Analysis", "word_idx": 26722, "sentence_idx": 356, "label": "unlabeled"}, {"type": "text", "expr": "21 \nXiangxin Zhu and Deva Ramanan,\n\n \u201cFace detection, pose estimation, and landmark localization in the\nwild,\u201d", "word_idx": 26764, "sentence_idx": 357, "label": "unlabeled"}, {"type": "text", "expr": "Xiangxin Zhu and Deva Ramanan,", "word_idx": 26874, "sentence_idx": 358, "label": "unlabeled"}, {"type": "text", "expr": "\u201cFace detection, pose estimation, and landmark localization in the\nwild,\u201d", "word_idx": 26904, "sentence_idx": 359, "label": "unlabeled"}, {"type": "text", "expr": "in  CVPR ", "word_idx": 26977, "sentence_idx": 360, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 2012, pp", "word_idx": 26986, "sentence_idx": 361, "label": "unlabeled"}, {"type": "text", "expr": " 2879\u20132886", "word_idx": 27001, "sentence_idx": 362, "label": "unlabeled"}, {"type": "text", "expr": "22 \nJunjie Yan, Xuzong Zhang, Zhen Lei, and Stan\u00a0Z Li,\n\n \u201cFace detection by structural models,\u201d", "word_idx": 27011, "sentence_idx": 363, "label": "unlabeled"}, {"type": "text", "expr": "Junjie Yan, Xuzong Zhang, Zhen Lei, and Stan\u00a0Z Li,", "word_idx": 27106, "sentence_idx": 364, "label": "unlabeled"}, {"type": "text", "expr": "\u201cFace detection by structural models,\u201d", "word_idx": 27156, "sentence_idx": 365, "label": "unlabeled"}, {"type": "text", "expr": "Image and Vision Computing , vol", "word_idx": 27194, "sentence_idx": 366, "label": "unlabeled"}, {"type": "text", "expr": " 32, no", "word_idx": 27226, "sentence_idx": 367, "label": "unlabeled"}, {"type": "text", "expr": " 10, pp", "word_idx": 27233, "sentence_idx": 368, "label": "unlabeled"}, {"type": "text", "expr": " 790\u2013799,\n2014", "word_idx": 27240, "sentence_idx": 369, "label": "unlabeled"}, {"type": "text", "expr": "Image and Vision Computing", "word_idx": 27254, "sentence_idx": 370, "label": "unlabeled"}, {"type": "text", "expr": "23 \nPedro\u00a0F Felzenszwalb, Ross\u00a0B Girshick, David McAllester, and Deva Ramanan,\n\n \u201cObject detection with discriminatively trained part-based models,\u201d", "word_idx": 27280, "sentence_idx": 371, "label": "unlabeled"}, {"type": "text", "expr": "Pedro\u00a0F Felzenszwalb, Ross\u00a0B Girshick, David McAllester, and Deva Ramanan,", "word_idx": 27428, "sentence_idx": 372, "label": "unlabeled"}, {"type": "text", "expr": "\u201cObject detection with discriminatively trained part-based models,\u201d", "word_idx": 27502, "sentence_idx": 373, "label": "unlabeled"}, {"type": "text", "expr": "TPAMI , vol", "word_idx": 27569, "sentence_idx": 374, "label": "unlabeled"}, {"type": "text", "expr": " 32, no", "word_idx": 27580, "sentence_idx": 375, "label": "unlabeled"}, {"type": "text", "expr": " 9, pp", "word_idx": 27587, "sentence_idx": 376, "label": "unlabeled"}, {"type": "text", "expr": " 1627\u20131645, 2010", "word_idx": 27593, "sentence_idx": 377, "label": "unlabeled"}, {"type": "text", "expr": "TPAMI", "word_idx": 27609, "sentence_idx": 378, "label": "unlabeled"}, {"type": "text", "expr": "24 \nMarkus Mathias, Rodrigo Benenson, Marco Pedersoli, and Luc Van\u00a0Gool,\n\n \u201cFace detection without bells and whistles,\u201d", "word_idx": 27614, "sentence_idx": 379, "label": "unlabeled"}, {"type": "text", "expr": "Markus Mathias, Rodrigo Benenson, Marco Pedersoli, and Luc Van\u00a0Gool,", "word_idx": 27733, "sentence_idx": 380, "label": "unlabeled"}, {"type": "text", "expr": "\u201cFace detection without bells and whistles,\u201d", "word_idx": 27801, "sentence_idx": 381, "label": "unlabeled"}, {"type": "text", "expr": "in  ECCV ", "word_idx": 27845, "sentence_idx": 382, "label": "unlabeled"}, {"type": "text", "expr": " Springer, 2014, pp", "word_idx": 27854, "sentence_idx": 383, "label": "unlabeled"}, {"type": "text", "expr": " 720\u2013735", "word_idx": 27873, "sentence_idx": 384, "label": "unlabeled"}, {"type": "text", "expr": "25 \nXiaohui Shen, Zhe Lin, Jonathan Brandt, and Ying Wu,\n\n \u201cDetecting and aligning faces by image retrieval,\u201d", "word_idx": 27881, "sentence_idx": 385, "label": "unlabeled"}, {"type": "text", "expr": "Xiaohui Shen, Zhe Lin, Jonathan Brandt, and Ying Wu,", "word_idx": 27990, "sentence_idx": 386, "label": "unlabeled"}, {"type": "text", "expr": "\u201cDetecting and aligning faces by image retrieval,\u201d", "word_idx": 28042, "sentence_idx": 387, "label": "unlabeled"}, {"type": "text", "expr": "in  CVPR ", "word_idx": 28092, "sentence_idx": 388, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 2013, pp", "word_idx": 28101, "sentence_idx": 389, "label": "unlabeled"}, {"type": "text", "expr": " 3460\u20133467", "word_idx": 28116, "sentence_idx": 390, "label": "unlabeled"}, {"type": "text", "expr": "26 \nZhaowei Cai, Quanfu Fan, Rogerio\u00a0S Feris, and Nuno Vasconcelos,\n\n \u201cA unified multi-scale deep convolutional neural network for fast\nobject detection,\u201d", "word_idx": 28126, "sentence_idx": 391, "label": "unlabeled"}, {"type": "text", "expr": "Zhaowei Cai, Quanfu Fan, Rogerio\u00a0S Feris, and Nuno Vasconcelos,", "word_idx": 28280, "sentence_idx": 392, "label": "unlabeled"}, {"type": "text", "expr": "\u201cA unified multi-scale deep convolutional neural network for fast\nobject detection,\u201d", "word_idx": 28343, "sentence_idx": 393, "label": "unlabeled"}, {"type": "text", "expr": "in  ECCV ", "word_idx": 28427, "sentence_idx": 394, "label": "unlabeled"}, {"type": "text", "expr": " Springer, 2016, pp", "word_idx": 28436, "sentence_idx": 395, "label": "unlabeled"}, {"type": "text", "expr": " 354\u2013370", "word_idx": 28455, "sentence_idx": 396, "label": "unlabeled"}, {"type": "text", "expr": "27 \nShuo Yang, Yuanjun Xiong, Chen\u00a0Change Loy, and Xiaoou Tang,\n\n \u201cFace detection through scale-friendly deep convolutional\nnetworks,\u201d", "word_idx": 28463, "sentence_idx": 397, "label": "unlabeled"}, {"type": "text", "expr": "Shuo Yang, Yuanjun Xiong, Chen\u00a0Change Loy, and Xiaoou Tang,", "word_idx": 28597, "sentence_idx": 398, "label": "unlabeled"}, {"type": "text", "expr": "\u201cFace detection through scale-friendly deep convolutional\nnetworks,\u201d", "word_idx": 28656, "sentence_idx": 399, "label": "unlabeled"}, {"type": "text", "expr": "arXiv , 2017", "word_idx": 28724, "sentence_idx": 400, "label": "unlabeled"}, {"type": "text", "expr": "arXiv", "word_idx": 28736, "sentence_idx": 401, "label": "unlabeled"}, {"type": "text", "expr": "28 \nChenchen Zhu, Yutong Zheng, Khoa Luu, and Marios Savvides,\n\n \u201cCms-rcnn: contextual multi-scale region-based cnn for unconstrained\nface detection,\u201d", "word_idx": 28741, "sentence_idx": 402, "label": "unlabeled"}, {"type": "text", "expr": "Chenchen Zhu, Yutong Zheng, Khoa Luu, and Marios Savvides,", "word_idx": 28891, "sentence_idx": 403, "label": "unlabeled"}, {"type": "text", "expr": "\u201cCms-rcnn: contextual multi-scale region-based cnn for unconstrained\nface detection,\u201d", "word_idx": 28949, "sentence_idx": 404, "label": "unlabeled"}, {"type": "text", "expr": "in  Deep Learning for Biometrics , pp", "word_idx": 29034, "sentence_idx": 405, "label": "unlabeled"}, {"type": "text", "expr": " 57\u201379", "word_idx": 29071, "sentence_idx": 406, "label": "unlabeled"}, {"type": "text", "expr": " Springer, 2017", "word_idx": 29077, "sentence_idx": 407, "label": "unlabeled"}, {"type": "text", "expr": "Deep Learning for Biometrics", "word_idx": 29092, "sentence_idx": 408, "label": "unlabeled"}, {"type": "text", "expr": "29 \nKaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu\u00a0Qiao,\n\n \u201cJoint face detection and alignment using multitask cascaded\nconvolutional networks,\u201d", "word_idx": 29120, "sentence_idx": 409, "label": "unlabeled"}, {"type": "text", "expr": "Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu\u00a0Qiao,", "word_idx": 29267, "sentence_idx": 410, "label": "unlabeled"}, {"type": "text", "expr": "\u201cJoint face detection and alignment using multitask cascaded\nconvolutional networks,\u201d", "word_idx": 29322, "sentence_idx": 411, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Signal Processing Letters , 2016", "word_idx": 29407, "sentence_idx": 412, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Signal Processing Letters", "word_idx": 29444, "sentence_idx": 413, "label": "unlabeled"}, {"type": "text", "expr": "Generated  on Tue Apr  3 11:32:13 2018 by", "word_idx": 29474, "sentence_idx": 414, "label": "unlabeled"}, {"type": "text", "expr": "LaTeXML", "word_idx": 29515, "sentence_idx": 415, "label": "unlabeled"}], "Input_Convex_Neural_Networks": [{"type": "text", "expr": "Untitled Document", "word_idx": 0, "sentence_idx": 0, "label": "unlabeled"}, {"type": "text", "expr": "Abstract This paper presents the input convex neural network\narchitecture", "word_idx": 17, "sentence_idx": 1, "label": "unlabeled"}, {"type": "text", "expr": " These are scalar-valued (potentially deep) neural\nnetworks with constraints on the network parameters such that the\noutput of the network is a convex function of (some of) the inputs", "word_idx": 90, "sentence_idx": 2, "label": "unlabeled"}, {"type": "text", "expr": "\nThe networks allow for efficient inference via optimization over some\ninputs to the network given others, and can be applied to settings\nincluding structured prediction, data imputation, reinforcement\nlearning, and others", "word_idx": 273, "sentence_idx": 3, "label": "unlabeled"}, {"type": "text", "expr": " In this paper we lay the basic groundwork for\nthese models, proposing methods for inference, optimization and\nlearning, and analyze their representational power", "word_idx": 495, "sentence_idx": 4, "label": "unlabeled"}, {"type": "text", "expr": " We show that many\nexisting neural network architectures can be made input-convex with\na minor modification, and develop specialized optimization\nalgorithms tailored to this setting", "word_idx": 656, "sentence_idx": 5, "label": "unlabeled"}, {"type": "text", "expr": " Finally, we highlight the\nperformance of the methods on multi-label prediction, image\ncompletion, and reinforcement learning problems, where we show\nimprovement over the existing state of the art in many cases", "word_idx": 837, "sentence_idx": 6, "label": "unlabeled"}, {"type": "text", "expr": "Abstract", "word_idx": 1047, "sentence_idx": 7, "label": "unlabeled"}, {"type": "text", "expr": "This paper presents the input convex neural network\narchitecture", "word_idx": 1055, "sentence_idx": 8, "label": "unlabeled"}, {"type": "text", "expr": " These are scalar-valued (potentially deep) neural\nnetworks with constraints on the network parameters such that the\noutput of the network is a convex function of (some of) the inputs", "word_idx": 1119, "sentence_idx": 9, "label": "unlabeled"}, {"type": "text", "expr": "\nThe networks allow for efficient inference via optimization over some\ninputs to the network given others, and can be applied to settings\nincluding structured prediction, data imputation, reinforcement\nlearning, and others", "word_idx": 1302, "sentence_idx": 10, "label": "unlabeled"}, {"type": "text", "expr": " In this paper we lay the basic groundwork for\nthese models, proposing methods for inference, optimization and\nlearning, and analyze their representational power", "word_idx": 1524, "sentence_idx": 11, "label": "unlabeled"}, {"type": "text", "expr": " We show that many\nexisting neural network architectures can be made input-convex with\na minor modification, and develop specialized optimization\nalgorithms tailored to this setting", "word_idx": 1685, "sentence_idx": 12, "label": "unlabeled"}, {"type": "text", "expr": " Finally, we highlight the\nperformance of the methods on multi-label prediction, image\ncompletion, and reinforcement learning problems, where we show\nimprovement over the existing state of the art in many cases", "word_idx": 1866, "sentence_idx": 13, "label": "unlabeled"}, {"type": "text", "expr": "\\definecolor", "word_idx": 2076, "sentence_idx": 14, "label": "unlabeled"}, {"type": "text", "expr": "lightgraygray0", "word_idx": 2088, "sentence_idx": 15, "label": "unlabeled"}, {"type": "text", "expr": "95 \\icmltitlerunning InputConvexNeuralNetworks", "word_idx": 2102, "sentence_idx": 16, "label": "unlabeled"}, {"type": "text", "expr": "\\icmltitlerunning", "word_idx": 2148, "sentence_idx": 17, "label": "unlabeled"}, {"type": "text", "expr": "\\printAffiliationsAndNotice", "word_idx": 2165, "sentence_idx": 18, "label": "unlabeled"}, {"type": "text", "expr": "* Work done\nwhile author was at Carnegie Mellon University", "word_idx": 2192, "sentence_idx": 19, "label": "unlabeled"}, {"type": "text", "expr": "1  Introduction", "word_idx": 2250, "sentence_idx": 20, "label": "unlabeled"}, {"type": "text", "expr": "In this paper, we propose a new\nneural network architecture that we call the  input convex neural network \n(ICNN)", "word_idx": 2265, "sentence_idx": 21, "label": "unlabeled"}, {"type": "text", "expr": "These are  scalar-valued  neural networks  $f(x,y;\\theta)$  where  $x$ \nand\n $y$  denotes inputs to the function and  $\\theta$ \ndenotes the parameters, built in such a way that the network is convex in\n(a subset of)  inputs   $y$ ", "word_idx": 2378, "sentence_idx": 22, "label": "unlabeled"}, {"type": "text", "expr": " \u00a0\nThe fundamental benefit to these ICNNs is that we can  optimize  over\nthe convex inputs to the network given some fixed value for other inputs", "word_idx": 2608, "sentence_idx": 23, "label": "unlabeled"}, {"type": "text", "expr": " That\nis, given some fixed  $x$  (and possibly some fixed elements of  $y$ ) we can\nglobally and efficiently (because the problem is convex) solve the optimization\nproblem", "word_idx": 2753, "sentence_idx": 24, "label": "unlabeled"}, {"type": "text", "expr": "input convex neural network", "word_idx": 2924, "sentence_idx": 25, "label": "unlabeled"}, {"type": "text", "expr": "scalar-valued", "word_idx": 2951, "sentence_idx": 26, "label": "unlabeled"}, {"type": "math", "expr": "$$f(x,y;\\theta)$$", "word_idx": 2964, "sentence_idx": 27, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta$$", "word_idx": 2977, "sentence_idx": 28, "label": "unlabeled"}, {"type": "text", "expr": "inputs", "word_idx": 2983, "sentence_idx": 29, "label": "unlabeled"}, {"type": "text", "expr": "3 We emphasize the term \u201cinput convex\u201d\nsince convexity in machine learning typically refers to convexity (of the loss\nminimization learning problem) in the  parameters , which is not the case\nhere", "word_idx": 2989, "sentence_idx": 30, "label": "unlabeled"}, {"type": "text", "expr": " Note that in our notation,  $f$  needs only be a convex function in\n $y$ , and may still be non-convex in the remaining inputs  $x$ ", "word_idx": 3185, "sentence_idx": 31, "label": "unlabeled"}, {"type": "text", "expr": " Training these\nneural networks remains a nonconvex problem, and the\nconvexity is only being exploited at inference time", "word_idx": 3318, "sentence_idx": 32, "label": "unlabeled"}, {"type": "text", "expr": "parameters", "word_idx": 3438, "sentence_idx": 33, "label": "unlabeled"}, {"type": "text", "expr": "optimize", "word_idx": 3448, "sentence_idx": 34, "label": "unlabeled"}, {"type": "text", "expr": "$\\argmin_{y}f(x,y;\\theta)$", "word_idx": 3456, "sentence_idx": 35, "label": "unlabeled"}, {"type": "math", "expr": "$$\\argmin_{y}f(x,y;\\theta).$$", "word_idx": 3482, "sentence_idx": 36, "label": "unlabeled"}, {"type": "text", "expr": "Fundamentally, this\nformalism lets us perform inference in the network via  optimization ", "word_idx": 3507, "sentence_idx": 37, "label": "unlabeled"}, {"type": "text", "expr": "\nThat is, instead of making predictions in a neural network via a purely\nfeedforward process, we can make predictions by optimizing a scalar function\n(which effectively plays the role of an energy function) over some\ninputs to the function given others", "word_idx": 3596, "sentence_idx": 38, "label": "unlabeled"}, {"type": "text", "expr": " There are a number of potential use cases\nfor these networks", "word_idx": 3848, "sentence_idx": 39, "label": "unlabeled"}, {"type": "text", "expr": "optimization", "word_idx": 3909, "sentence_idx": 40, "label": "unlabeled"}, {"type": "text", "expr": "Structured prediction", "word_idx": 3921, "sentence_idx": 41, "label": "unlabeled"}, {"type": "text", "expr": "As is perhaps apparent from our\nnotation above, a key application of this work is in structured prediction", "word_idx": 3942, "sentence_idx": 42, "label": "unlabeled"}, {"type": "text", "expr": "\nGiven (typically high-dimensional) structured input and output spaces  $\\mathcal{X}\\times\\mathcal{Y}$ , we can build a network over  $(x,y)$  pairs\nthat encodes the energy function for this pair, following typical energy-based\nlearning formalisms  \\citep lecun2006tutorial", "word_idx": 4048, "sentence_idx": 43, "label": "unlabeled"}, {"type": "text", "expr": " Prediction involves finding the\n $y\\in\\mathcal{Y}$  that\nminimizes the energy for a given  $x$ , which is exactly\nthe argmin problem in ( 1 )", "word_idx": 4321, "sentence_idx": 44, "label": "unlabeled"}, {"type": "text", "expr": "\nIn our setting, assuming that  $\\mathcal{Y}$  is a convex space (a common\nassumption in structured prediction), this optimization\nproblem is convex", "word_idx": 4463, "sentence_idx": 45, "label": "unlabeled"}, {"type": "text", "expr": "\nThis is similar in nature to the structured prediction energy networks (SPENs)\n \\citep belanger2016structured, which also use deep networks over the input and\noutput spaces, with the difference being that in our setting  $f$  is convex in\n $y$ , so the optimization can be performed globally", "word_idx": 4611, "sentence_idx": 46, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{X}\\times\\mathcal{Y}$$", "word_idx": 4903, "sentence_idx": 47, "label": "unlabeled"}, {"type": "math", "expr": "$$(x,y)$$", "word_idx": 4931, "sentence_idx": 48, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 4936, "sentence_idx": 49, "label": "unlabeled"}, {"type": "math", "expr": "$$y\\in\\mathcal{Y}$$", "word_idx": 4942, "sentence_idx": 50, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{Y}$$", "word_idx": 4957, "sentence_idx": 51, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 4968, "sentence_idx": 52, "label": "unlabeled"}, {"type": "text", "expr": "Data imputation", "word_idx": 4974, "sentence_idx": 53, "label": "unlabeled"}, {"type": "text", "expr": "Similar to structured prediction\nbut slightly more generic, if we are given some space  $\\mathcal{Y}$ \nwe can learn a network  $f(y;\\theta)$  (removing the additional  $x$ \ninputs, though these can be added as well) that, given an example with\nsome subset  $\\mathcal{I}$  missing, imputes the likely values of these variables\nby solving the optimization problem as above  $\\hat{y}_{\\mathcal{I}}=\\argmin_{y_{\\mathcal{I}}}f(y_{\\mathcal{I}},y_{\\bar{%\n\\mathcal{I}}};\\theta)$ \nThis could be used e", "word_idx": 4989, "sentence_idx": 54, "label": "unlabeled"}, {"type": "text", "expr": ", in image inpainting\nwhere the goal is to fill in some arbitrary set of missing pixels given observed\nones", "word_idx": 5481, "sentence_idx": 55, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{Y}$$", "word_idx": 5588, "sentence_idx": 56, "label": "unlabeled"}, {"type": "math", "expr": "$$f(y;\\theta)$$", "word_idx": 5599, "sentence_idx": 57, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{I}$$", "word_idx": 5610, "sentence_idx": 58, "label": "unlabeled"}, {"type": "math", "expr": "$$\\hat{y}_{\\mathcal{I}}=\\argmin_{y_{\\mathcal{I}}}f(y_{\\mathcal{I}},y_{\\bar{%\n\\mathcal{I}}};\\theta)$$", "word_idx": 5621, "sentence_idx": 59, "label": "unlabeled"}, {"type": "text", "expr": "Continuous action reinforcement learning", "word_idx": 5717, "sentence_idx": 60, "label": "unlabeled"}, {"type": "text", "expr": "Given a reinforcement learning problem with potentially continuous\nstate and action spaces  $\\mathcal{S}\\times\\mathcal{A}$ ,\nwe can model the (negative)  $Q$  function,\n $-Q(s,a;\\theta)$  as an input convex neural network", "word_idx": 5757, "sentence_idx": 61, "label": "unlabeled"}, {"type": "text", "expr": " In this case the action\nselection procedure can be formulated as a convex optimization problem\n $a^{\\star}(s)=\\argmin_{a}-Q(s,a;\\theta)$ ", "word_idx": 5978, "sentence_idx": 62, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{S}\\times\\mathcal{A}$$", "word_idx": 6116, "sentence_idx": 63, "label": "unlabeled"}, {"type": "math", "expr": "$$-Q(s,a;\\theta)$$", "word_idx": 6144, "sentence_idx": 64, "label": "unlabeled"}, {"type": "math", "expr": "$$a^{\\star}(s)=\\argmin_{a}-Q(s,a;\\theta)$$", "word_idx": 6158, "sentence_idx": 65, "label": "unlabeled"}, {"type": "text", "expr": "This paper lays the\nfoundation for optimization, inference, and learning in these input convex\nmodels, and explores their performance in the applications above", "word_idx": 6196, "sentence_idx": 66, "label": "unlabeled"}, {"type": "text", "expr": " Our main\ncontributions are: we propose the ICNN\narchitecture and a partially convex variant; we develop\nefficient optimization and inference procedures that are well-suited to the\ncomplexity of these specific models; we propose techniques for training these\nmodels, based upon either max-margin structured prediction or direct\ndifferentiation of the argmin operation; and we evaluate the system on\nmulti-label prediction, image completion, and reinforcement learning domains;\nin many of these settings we show performance that improves upon the\nstate of the art", "word_idx": 6355, "sentence_idx": 67, "label": "unlabeled"}, {"type": "text", "expr": "This paper lays the\nfoundation for optimization, inference, and learning in these input convex\nmodels, and explores their performance in the applications above", "word_idx": 6917, "sentence_idx": 68, "label": "unlabeled"}, {"type": "text", "expr": " Our main\ncontributions are: we propose the ICNN\narchitecture and a partially convex variant; we develop\nefficient optimization and inference procedures that are well-suited to the\ncomplexity of these specific models; we propose techniques for training these\nmodels, based upon either max-margin structured prediction or direct\ndifferentiation of the argmin operation; and we evaluate the system on\nmulti-label prediction, image completion, and reinforcement learning domains;\nin many of these settings we show performance that improves upon the\nstate of the art", "word_idx": 7076, "sentence_idx": 69, "label": "unlabeled"}, {"type": "text", "expr": "2  Background and related work", "word_idx": 7638, "sentence_idx": 70, "label": "unlabeled"}, {"type": "text", "expr": "Energy-based learning", "word_idx": 7668, "sentence_idx": 71, "label": "unlabeled"}, {"type": "text", "expr": "The interplay between inference, optimization,\nand structured prediction has a long history in neural networks", "word_idx": 7689, "sentence_idx": 72, "label": "unlabeled"}, {"type": "text", "expr": " Several early\nincarnations of\nneural networks were explicitly trained to produce structured sequences\n(e", "word_idx": 7799, "sentence_idx": 73, "label": "unlabeled"}, {"type": "text", "expr": "  ),\nand there was an early appreciation that structured models like hidden Markov\nmodels could be combined with the outputs of neural networks\n \\citep bengio-lecun-henderson-94", "word_idx": 7904, "sentence_idx": 74, "label": "unlabeled"}, {"type": "text", "expr": " Much of this earlier work is surveyed and\nsynthesized by  , who give a tutorial on these energy\nbased learning\nmethods", "word_idx": 8081, "sentence_idx": 75, "label": "unlabeled"}, {"type": "text", "expr": "\nIn recent years, there has been a strong push to further incorporate\nstructured prediction methods like conditional random fields as the \u201clast\nlayer\u201d of a deep network architecture\n \\citep peng2009conditional,zheng2015conditional,chen2015learning", "word_idx": 8200, "sentence_idx": 76, "label": "unlabeled"}, {"type": "text", "expr": "\nSeveral methods have proposed to build general neural networks over joint input\nand output spaces, and perform inference over outputs using generic optimization\ntechniques such as Generative Adversarial Networks (GANs)  \\citep goodfellow2014generative\nand Structured Prediction Energy Networks (SPENs)  \\citep belanger2016structured", "word_idx": 8447, "sentence_idx": 77, "label": "unlabeled"}, {"type": "text", "expr": "\nSPENs provide a deep structure over input and output spaces\nthat performs the inference in ( 1 ) as\na non-convex optimization problem", "word_idx": 8780, "sentence_idx": 78, "label": "unlabeled"}, {"type": "text", "expr": "Simard & LeCun(1991)Simard and LeCun", "word_idx": 8914, "sentence_idx": 79, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 8950, "sentence_idx": 80, "label": "unlabeled"}, {"type": "text", "expr": "LeCun et\u00a0al", "word_idx": 8956, "sentence_idx": 81, "label": "unlabeled"}, {"type": "text", "expr": "(2006)LeCun, Chopra, Hadsell, Ranzato, and\nHuang", "word_idx": 8967, "sentence_idx": 82, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 9015, "sentence_idx": 83, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 9021, "sentence_idx": 84, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 9027, "sentence_idx": 85, "label": "unlabeled"}, {"type": "text", "expr": "The current work is highly related to these past approaches, but also differs in\na very particular way", "word_idx": 9033, "sentence_idx": 86, "label": "unlabeled"}, {"type": "text", "expr": " To the best of our knowledge, each of these structured\nprediction methods based upon energy-based models operates in one of two ways,\neither: 1) the architecture is built in a very particular way such that\noptimization over the output is guaranteed to be \u201ceasy\u201d (e", "word_idx": 9135, "sentence_idx": 87, "label": "unlabeled"}, {"type": "text", "expr": " convex, or the\nresult of running some inference procedure), usually by introducing a\nstructured linear objective at the last layer of the network; or 2) no attempt\nis made to make the architecture \u201ceasy\u201d to run inference over, and instead a\ngeneral model is built over the output space", "word_idx": 9400, "sentence_idx": 88, "label": "unlabeled"}, {"type": "text", "expr": " In contrast, our approach lies\nsomewhere in between: by ensuring convexity of the resulting decision space, we\nare constraining the inference problem to be easy in some respect, but we\nspecify very\nlittle about the architecture other than the constraints required to make\nit convex", "word_idx": 9686, "sentence_idx": 89, "label": "unlabeled"}, {"type": "text", "expr": " In particular, as we will show, the network architecture over the\nvariables to be optimized over can be deep and involve multiple\nnon-linearities", "word_idx": 9968, "sentence_idx": 90, "label": "unlabeled"}, {"type": "text", "expr": " The goal of the proposed work is to allow\nfor complex functions over the output without needing to specify them manually\n(exactly analogous to how current deep neural networks treat their input space)", "word_idx": 10114, "sentence_idx": 91, "label": "unlabeled"}, {"type": "text", "expr": "The current work is highly related to these past approaches, but also differs in\na very particular way", "word_idx": 10315, "sentence_idx": 92, "label": "unlabeled"}, {"type": "text", "expr": " To the best of our knowledge, each of these structured\nprediction methods based upon energy-based models operates in one of two ways,\neither: 1) the architecture is built in a very particular way such that\noptimization over the output is guaranteed to be \u201ceasy\u201d (e", "word_idx": 10417, "sentence_idx": 93, "label": "unlabeled"}, {"type": "text", "expr": " convex, or the\nresult of running some inference procedure), usually by introducing a\nstructured linear objective at the last layer of the network; or 2) no attempt\nis made to make the architecture \u201ceasy\u201d to run inference over, and instead a\ngeneral model is built over the output space", "word_idx": 10682, "sentence_idx": 94, "label": "unlabeled"}, {"type": "text", "expr": " In contrast, our approach lies\nsomewhere in between: by ensuring convexity of the resulting decision space, we\nare constraining the inference problem to be easy in some respect, but we\nspecify very\nlittle about the architecture other than the constraints required to make\nit convex", "word_idx": 10968, "sentence_idx": 95, "label": "unlabeled"}, {"type": "text", "expr": " In particular, as we will show, the network architecture over the\nvariables to be optimized over can be deep and involve multiple\nnon-linearities", "word_idx": 11250, "sentence_idx": 96, "label": "unlabeled"}, {"type": "text", "expr": " The goal of the proposed work is to allow\nfor complex functions over the output without needing to specify them manually\n(exactly analogous to how current deep neural networks treat their input space)", "word_idx": 11396, "sentence_idx": 97, "label": "unlabeled"}, {"type": "text", "expr": "Structured prediction and MAP inference", "word_idx": 11597, "sentence_idx": 98, "label": "unlabeled"}, {"type": "text", "expr": "Our work also draws some\nconnection to MAP-inference-based learning and\napproximate inference", "word_idx": 11636, "sentence_idx": 99, "label": "unlabeled"}, {"type": "text", "expr": " There are two broad classes of learning approaches in\nstructured prediction: method that use probabilistic inference techniques\n(typically exploiting the fact that the gradient of log likelihood is given by\nthe actual feature expectations minus their expectation under the learned\nmodel  \\citep [Ch 20]koller2009probabilistic), and methods that rely solely upon\nMAP inference (such as max-margin structured prediction\n \\citep taskar2005learning,tsochantaridis2005large)", "word_idx": 11729, "sentence_idx": 100, "label": "unlabeled"}, {"type": "text", "expr": " MAP inference in particular also\nhas close connections to optimization, as various convex relaxations of the\ngeneral MAP inference problem often perform well in theory and practice", "word_idx": 12199, "sentence_idx": 101, "label": "unlabeled"}, {"type": "text", "expr": "\nThe proposed methods can be viewed as an extreme case of this second class of\nalgorithm, where inference is based  solely  upon a convex optimization\nproblem that may not have any probabilistic semantics at all", "word_idx": 12380, "sentence_idx": 102, "label": "unlabeled"}, {"type": "text", "expr": " Finally, although\nit is more abstract, we feel there is a philosophical similarity between our\nproposed approach and sum-product networks  \\citep poon2011sum; both settings\ndefine networks where inference is accomplished \u201ceasily\u201d either by a\nsum-product message passing algorithm (by construction) or via convex\noptimization", "word_idx": 12591, "sentence_idx": 103, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 12916, "sentence_idx": 104, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 12922, "sentence_idx": 105, "label": "unlabeled"}, {"type": "text", "expr": "solely", "word_idx": 12928, "sentence_idx": 106, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 12934, "sentence_idx": 107, "label": "unlabeled"}, {"type": "text", "expr": "Fitting convex functions", "word_idx": 12940, "sentence_idx": 108, "label": "unlabeled"}, {"type": "text", "expr": "Finally, the proposed work relates to a\ntopic less considered in the machine learning literature, that of fitting convex\nfunctions to data  \\citep [pg", "word_idx": 12964, "sentence_idx": 109, "label": "unlabeled"}, {"type": "text", "expr": " 338]boyd2004convex", "word_idx": 13114, "sentence_idx": 110, "label": "unlabeled"}, {"type": "text", "expr": " Indeed our learning problem\ncan be viewed as\nparameter estimation under a model that is guaranteed to be convex by its\nconstruction", "word_idx": 13133, "sentence_idx": 111, "label": "unlabeled"}, {"type": "text", "expr": " The most similar work of which we\nare aware specifically fits sums of rectified half-planes to data\n \\citep magnani2009convex, which\nis similar to one layer of our rectified linear units", "word_idx": 13265, "sentence_idx": 112, "label": "unlabeled"}, {"type": "text", "expr": " However, the actual\ntraining scheme is much different, and our deep network architecture allows for\na much richer class of representations, while still maintaining convexity", "word_idx": 13452, "sentence_idx": 113, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 13626, "sentence_idx": 114, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 13632, "sentence_idx": 115, "label": "unlabeled"}, {"type": "text", "expr": "3  Convex neural network architectures", "word_idx": 13638, "sentence_idx": 116, "label": "unlabeled"}, {"type": "text", "expr": "Here we more formally present different ICNN architectures and\nprove their convexity properties given certain constraints on the parameter\nspace", "word_idx": 13676, "sentence_idx": 117, "label": "unlabeled"}, {"type": "text", "expr": " Our chief claim is that the class of (full and partial) input convex\nmodels is rich and lets us capture complex joint models over the input to a\nnetwork", "word_idx": 13820, "sentence_idx": 118, "label": "unlabeled"}, {"type": "text", "expr": "Here we more formally present different ICNN architectures and\nprove their convexity properties given certain constraints on the parameter\nspace", "word_idx": 13973, "sentence_idx": 119, "label": "unlabeled"}, {"type": "text", "expr": " Our chief claim is that the class of (full and partial) input convex\nmodels is rich and lets us capture complex joint models over the input to a\nnetwork", "word_idx": 14117, "sentence_idx": 120, "label": "unlabeled"}, {"type": "text", "expr": "1  Fully input convex neural networks", "word_idx": 14270, "sentence_idx": 121, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:  A fully input convex neural network (FICNN)", "word_idx": 14307, "sentence_idx": 122, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:", "word_idx": 14361, "sentence_idx": 123, "label": "unlabeled"}, {"type": "text", "expr": "To begin, we consider a fully convex,  $k$ -layer, fully connected ICNN\nthat we call a FICNN and is shown in Figure  1 ", "word_idx": 14370, "sentence_idx": 124, "label": "unlabeled"}, {"type": "text", "expr": "\nThis model defines a neural network over the input  $y$ \n(i", "word_idx": 14489, "sentence_idx": 125, "label": "unlabeled"}, {"type": "text", "expr": ", omitting any  $x$  term in this function) using the architecture for\n $i=0,\\ldots,k-1$", "word_idx": 14549, "sentence_idx": 126, "label": "unlabeled"}, {"type": "math", "expr": "$$i=0,\\ldots,k-1$$", "word_idx": 14637, "sentence_idx": 127, "label": "unlabeled"}, {"type": "text", "expr": "$\\begin{split}\\displaystyle z_{i+1}&\\displaystyle=g_{i}\\left(W^{(z)}_{i}z_{i}+W%\n^{(y)}_{i}y+b_{i}\\right),\\;\\;f(y;\\theta)=z_{k}\\end{split}$", "word_idx": 14651, "sentence_idx": 128, "label": "unlabeled"}, {"type": "math", "expr": "$$\\begin{split}\\displaystyle z_{i+1}&\\displaystyle=g_{i}\\left(W^{(z)}_{i}z_{i}+W%\n^{(y)}_{i}y+b_{i}\\right),\\;\\;f(y;\\theta)=z_{k}\\end{split}$$", "word_idx": 14790, "sentence_idx": 129, "label": "unlabeled"}, {"type": "text", "expr": "where  $z_{i}$  denotes the layer activations (with  $z_{0},W^{(z)}_{0}\\equiv 0$ ),\n $\\theta=\\{W^{(y)}_{0:k-1},W^{(z)}_{1:k-1},b_{0:k-1}\\}$  are the\nparameters, and  $g_{i}$  are non-linear activation functions", "word_idx": 14927, "sentence_idx": 130, "label": "unlabeled"}, {"type": "text", "expr": " The central result\non convexity of the network is the following:", "word_idx": 15137, "sentence_idx": 131, "label": "unlabeled"}, {"type": "math", "expr": "$$z_{i}$$", "word_idx": 15202, "sentence_idx": 132, "label": "unlabeled"}, {"type": "math", "expr": "$$z_{0},W^{(z)}_{0}\\equiv 0$$", "word_idx": 15207, "sentence_idx": 133, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta=\\{W^{(y)}_{0:k-1},W^{(z)}_{1:k-1},b_{0:k-1}\\}$$", "word_idx": 15232, "sentence_idx": 134, "label": "unlabeled"}, {"type": "math", "expr": "$$g_{i}$$", "word_idx": 15284, "sentence_idx": 135, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 1 ", "word_idx": 15289, "sentence_idx": 136, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 1", "word_idx": 15303, "sentence_idx": 137, "label": "unlabeled"}, {"type": "text", "expr": "The function  $f$  is convex in  $y$  provided that all\n $W^{(z)}_{1:k-1}$  are non-negative, and all functions  $g_{i}$  are convex and\nnon-decreasing", "word_idx": 15316, "sentence_idx": 138, "label": "unlabeled"}, {"type": "math", "expr": "$$W^{(z)}_{1:k-1}$$", "word_idx": 15467, "sentence_idx": 139, "label": "unlabeled"}, {"type": "math", "expr": "$$g_{i}$$", "word_idx": 15482, "sentence_idx": 140, "label": "unlabeled"}, {"type": "text", "expr": "The proof is simple and follows from the fact that\nnon-negative sums of convex functions are also convex and that the composition\nof a convex and convex non-decreasing function is also convex (see e", "word_idx": 15487, "sentence_idx": 141, "label": "unlabeled"}, {"type": "text", "expr": "\n \\citet [3", "word_idx": 15685, "sentence_idx": 142, "label": "unlabeled"}, {"type": "text", "expr": "4]boyd2004convex)", "word_idx": 15696, "sentence_idx": 143, "label": "unlabeled"}, {"type": "text", "expr": " The constraint that the  $g_{i}$  be convex\nnon-decreasing is not particularly restrictive, as current non-linear activation\nunits like the rectified linear unit or max-pooling unit already satisfy this\nconstraint", "word_idx": 15713, "sentence_idx": 144, "label": "unlabeled"}, {"type": "text", "expr": " The constraint that the  $W^{(z)}$  terms be non-negative is\nsomewhat restrictive, but because the bias terms and  $W^{(y)}$  terms can be\nnegative, the network still has substantial representation power, as we will\nshortly demonstrate empirically", "word_idx": 15927, "sentence_idx": 145, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 16175, "sentence_idx": 146, "label": "unlabeled"}, {"type": "math", "expr": "$$g_{i}$$", "word_idx": 16181, "sentence_idx": 147, "label": "unlabeled"}, {"type": "math", "expr": "$$W^{(z)}$$", "word_idx": 16186, "sentence_idx": 148, "label": "unlabeled"}, {"type": "math", "expr": "$$W^{(y)}$$", "word_idx": 16193, "sentence_idx": 149, "label": "unlabeled"}, {"type": "text", "expr": "One notable addition in the ICNN are the \u201cpassthrough\u201d layers that directly\nconnect the input  $y$  to hidden units in deeper layers", "word_idx": 16200, "sentence_idx": 150, "label": "unlabeled"}, {"type": "text", "expr": " Such layers are\nunnecessary in traditional feedforward networks because previous hidden units\ncan always be mapped to subsequent hidden units with the identity mapping;\nhowever, for ICNNs, the non-negativity constraint subsequent  $W^{(z)}$  weights\nrestricts the allowable use of hidden units that mirror the identity mapping,\nand so we explicitly include this additional passthrough", "word_idx": 16332, "sentence_idx": 151, "label": "unlabeled"}, {"type": "text", "expr": " Some\npassthrough layers have been recently explored in the deep residual networks\n \\citep he2015deep and densely connected convolutional\nnetworks  \\citep huang2016densely,\nthough these differ from those of an ICNN as they pass through\nhidden layers deeper in the network, whereas to maintain convexity our\npassthrough layers can only apply to the input directly", "word_idx": 16717, "sentence_idx": 152, "label": "unlabeled"}, {"type": "math", "expr": "$$W^{(z)}$$", "word_idx": 17079, "sentence_idx": 153, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 17086, "sentence_idx": 154, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 17092, "sentence_idx": 155, "label": "unlabeled"}, {"type": "text", "expr": "Other linear operators like convolutions can\nbe included in ICNNs without changing the convexity properties", "word_idx": 17098, "sentence_idx": 156, "label": "unlabeled"}, {"type": "text", "expr": "\nIndeed, modern feedforward architectures such as AlexNet\n \\citep krizhevsky2012imagenet, VGG  \\citep simonyan2014very, and GoogLeNet\n \\citep szegedy2015going with ReLUs  \\citep nair2010rectified can be made\ninput convex with Proposition\u00a0 1 ", "word_idx": 17205, "sentence_idx": 157, "label": "unlabeled"}, {"type": "text", "expr": " In the experiment that follow,\nwe will explore ICNNs with both fully connected and convolutional layers, and we\nprovide more detail about these additional architectures in\nSection\u00a0 A  of the supplement", "word_idx": 17446, "sentence_idx": 158, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 17648, "sentence_idx": 159, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 17654, "sentence_idx": 160, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 17660, "sentence_idx": 161, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 17666, "sentence_idx": 162, "label": "unlabeled"}, {"type": "text", "expr": "2  Partially input convex architectures", "word_idx": 17672, "sentence_idx": 163, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:  A partially input convex neural network (PICNN)", "word_idx": 17711, "sentence_idx": 164, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:", "word_idx": 17769, "sentence_idx": 165, "label": "unlabeled"}, {"type": "text", "expr": "The FICNN provides joint convexity over the entire input to the function, which\nindeed may\nbe a restriction on the allowable class of models", "word_idx": 17778, "sentence_idx": 166, "label": "unlabeled"}, {"type": "text", "expr": " Furthermore, this full joint\nconvexity is unnecessary in settings like structured prediction where the neural\nnetwork is used to build a joint model over an input and output example space\nand only convexity over the outputs is necessary", "word_idx": 17918, "sentence_idx": 167, "label": "unlabeled"}, {"type": "text", "expr": "The FICNN provides joint convexity over the entire input to the function, which\nindeed may\nbe a restriction on the allowable class of models", "word_idx": 18155, "sentence_idx": 168, "label": "unlabeled"}, {"type": "text", "expr": " Furthermore, this full joint\nconvexity is unnecessary in settings like structured prediction where the neural\nnetwork is used to build a joint model over an input and output example space\nand only convexity over the outputs is necessary", "word_idx": 18295, "sentence_idx": 169, "label": "unlabeled"}, {"type": "text", "expr": "In this section we propose an extension to the pure FICNN, the partially\ninput convex neural network (PICNN), that is convex over only some inputs to the\nnetwork (in general ICNNs will refer to this new class)", "word_idx": 18532, "sentence_idx": 170, "label": "unlabeled"}, {"type": "text", "expr": " As we will show, these\nnetworks generalize both\ntraditional feedforward networks and FICNNs, and thus provide substantial\nrepresentational benefits", "word_idx": 18741, "sentence_idx": 171, "label": "unlabeled"}, {"type": "text", "expr": " We define a PICNN to be a network over  $(x,y)$  pairs\n $f(x,y;\\theta)$  where  $f$  is convex in  $y$  but not convex in  $x$ ", "word_idx": 18889, "sentence_idx": 172, "label": "unlabeled"}, {"type": "text", "expr": "\nFigure\u00a0 2  illustrates one potential  $k$ -layer PICNN architecture\ndefined by the recurrences", "word_idx": 19017, "sentence_idx": 173, "label": "unlabeled"}, {"type": "math", "expr": "$$(x,y)$$", "word_idx": 19112, "sentence_idx": 174, "label": "unlabeled"}, {"type": "math", "expr": "$$f(x,y;\\theta)$$", "word_idx": 19117, "sentence_idx": 175, "label": "unlabeled"}, {"type": "text", "expr": "$\\begin{split}\\displaystyle u_{i+1}&\\displaystyle=\\tilde{g}_{i}(\\tilde{W}_{i}u_%\n{i}+\\tilde{b}_{i})\\\\\n\\displaystyle z_{i+1}&\\displaystyle=g_{i}\\left(W^{(z)}_{i}\\left(z_{i}\\circ[W_{%\ni}^{(zu)}u_{i}+b^{(z)}_{i}]_{+}\\right)+\\right\\\\\n&\\displaystyle\\leftW^{(y)}_{i}\\left(y\\circ(W_{i}^{(yu)}u_{i}+b^{(y)}_{i})%\n\\right)+W^{(u)}_{i}u_{i}+b_{i}\\right)\\\\\n\\displaystyle f(x,y;\\theta)&\\displaystyle=z_{k},\\;u_{0}=x\\end{split}$", "word_idx": 19130, "sentence_idx": 176, "label": "unlabeled"}, {"type": "math", "expr": "$$\\begin{split}\\displaystyle u_{i+1}&\\displaystyle=\\tilde{g}_{i}(\\tilde{W}_{i}u_%\n{i}+\\tilde{b}_{i})\\\\\n\\displaystyle z_{i+1}&\\displaystyle=g_{i}\\left(W^{(z)}_{i}\\left(z_{i}\\circ[W_{%\ni}^{(zu)}u_{i}+b^{(z)}_{i}]_{+}\\right)+\\right.\\\\\n&\\displaystyle\\left.W^{(y)}_{i}\\left(y\\circ(W_{i}^{(yu)}u_{i}+b^{(y)}_{i})%\n\\right)+W^{(u)}_{i}u_{i}+b_{i}\\right)\\\\\n\\displaystyle f(x,y;\\theta)&\\displaystyle=z_{k},\\;u_{0}=x\\end{split}$$", "word_idx": 19544, "sentence_idx": 177, "label": "unlabeled"}, {"type": "text", "expr": "where  $u_{i}\\in\\mathbb{R}^{n_{i}}$  and  $z_{i}\\in\\mathbb{R}^{m_{i}}$  denote\nthe hidden units for the \u201c $x$ -path\u201d and \u201c $y$ -path\u201d, where  $y\\in\\mathbb{R}^{p}$ , and where  $\\circ$  denotes the Hadamard product, the\nelementwise product between two vectors", "word_idx": 19958, "sentence_idx": 178, "label": "unlabeled"}, {"type": "text", "expr": " The crucial element here is that\nunlike the FICNN, we only need the  $W^{(z)}$  terms to be non-negative, and we\ncan introduce arbitrary products  between  the  $u_{i}$  hidden units and the\n $z_{i}$  hidden units", "word_idx": 20216, "sentence_idx": 179, "label": "unlabeled"}, {"type": "text", "expr": "\nThe following proposition highlights the representational\npower of the PICNN", "word_idx": 20430, "sentence_idx": 180, "label": "unlabeled"}, {"type": "math", "expr": "$$u_{i}\\in\\mathbb{R}^{n_{i}}$$", "word_idx": 20507, "sentence_idx": 181, "label": "unlabeled"}, {"type": "math", "expr": "$$z_{i}\\in\\mathbb{R}^{m_{i}}$$", "word_idx": 20533, "sentence_idx": 182, "label": "unlabeled"}, {"type": "math", "expr": "$$y\\in\\mathbb{R}^{p}$$", "word_idx": 20559, "sentence_idx": 183, "label": "unlabeled"}, {"type": "math", "expr": "$$\\circ$$", "word_idx": 20577, "sentence_idx": 184, "label": "unlabeled"}, {"type": "math", "expr": "$$W^{(z)}$$", "word_idx": 20582, "sentence_idx": 185, "label": "unlabeled"}, {"type": "text", "expr": "between", "word_idx": 20589, "sentence_idx": 186, "label": "unlabeled"}, {"type": "math", "expr": "$$u_{i}$$", "word_idx": 20596, "sentence_idx": 187, "label": "unlabeled"}, {"type": "math", "expr": "$$z_{i}$$", "word_idx": 20601, "sentence_idx": 188, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 2 ", "word_idx": 20606, "sentence_idx": 189, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 2", "word_idx": 20620, "sentence_idx": 190, "label": "unlabeled"}, {"type": "text", "expr": "A PICNN network with  $k$  layers can represent any FICNN with  $k$  layers and any\npurely feedforward network with  $k$  layers", "word_idx": 20633, "sentence_idx": 191, "label": "unlabeled"}, {"type": "text", "expr": "Proof", "word_idx": 20761, "sentence_idx": 192, "label": "unlabeled"}, {"type": "text", "expr": "Proof", "word_idx": 20766, "sentence_idx": 193, "label": "unlabeled"}, {"type": "text", "expr": "To recover a FICNN we simply set the weights over the entire  $x$  path to be\nzero and set  $b^{(z)}=b^{(y)}=1$ ", "word_idx": 20771, "sentence_idx": 194, "label": "unlabeled"}, {"type": "text", "expr": " We can recover a feedforward network by\nnoting that a traditional feedforward network  $\\hat{f}(x;\\theta)$  where  $f:\\mathcal{X}\\rightarrow\\mathcal{Y}$ ,\ncan be viewed as a network with an inner\nproduct  $f(x;\\theta)^{T}y$  in its last layer\n(see e", "word_idx": 20883, "sentence_idx": 195, "label": "unlabeled"}, {"type": "text", "expr": "   for more details)", "word_idx": 21133, "sentence_idx": 196, "label": "unlabeled"}, {"type": "text", "expr": "\nThus, a feedforward network can be represented as a PICNN\nby setting the  $x$  path to be exactly the feedforward component, then having the\n $y$  path be all zero except  $W_{k-1}^{(yu)}=I$  and  $W^{(y)}_{k-1}=1^{T}$ ", "word_idx": 21153, "sentence_idx": 197, "label": "unlabeled"}, {"type": "math", "expr": "$$b^{(z)}=b^{(y)}=1$$", "word_idx": 21373, "sentence_idx": 198, "label": "unlabeled"}, {"type": "math", "expr": "$$\\hat{f}(x;\\theta)$$", "word_idx": 21390, "sentence_idx": 199, "label": "unlabeled"}, {"type": "math", "expr": "$$f:\\mathcal{X}\\rightarrow\\mathcal{Y}$$", "word_idx": 21407, "sentence_idx": 200, "label": "unlabeled"}, {"type": "math", "expr": "$$f(x;\\theta)^{T}y$$", "word_idx": 21442, "sentence_idx": 201, "label": "unlabeled"}, {"type": "text", "expr": "LeCun et\u00a0al", "word_idx": 21458, "sentence_idx": 202, "label": "unlabeled"}, {"type": "text", "expr": "(2006)LeCun, Chopra, Hadsell, Ranzato, and\nHuang", "word_idx": 21469, "sentence_idx": 203, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{k-1}^{(yu)}=I$$", "word_idx": 21517, "sentence_idx": 204, "label": "unlabeled"}, {"type": "math", "expr": "$$W^{(y)}_{k-1}=1^{T}$$", "word_idx": 21533, "sentence_idx": 205, "label": "unlabeled"}, {"type": "text", "expr": "4  Inference in ICNNs", "word_idx": 21552, "sentence_idx": 206, "label": "unlabeled"}, {"type": "text", "expr": "Prediction in ICNNs (which we also refer to as inference), requires\nsolving the convex optimization problem", "word_idx": 21573, "sentence_idx": 207, "label": "unlabeled"}, {"type": "text", "expr": "Prediction in ICNNs (which we also refer to as inference), requires\nsolving the convex optimization problem", "word_idx": 21680, "sentence_idx": 208, "label": "unlabeled"}, {"type": "text", "expr": "$\\minimize_{y\\in\\mathcal{Y}}f(x,y;\\theta)$", "word_idx": 21787, "sentence_idx": 209, "label": "unlabeled"}, {"type": "math", "expr": "$$\\minimize_{y\\in\\mathcal{Y}}f(x,y;\\theta)$$", "word_idx": 21829, "sentence_idx": 210, "label": "unlabeled"}, {"type": "text", "expr": "While the resulting tasks are convex optimization problems (and thus\n\u201ceasy\u201d to solve in some sense), in practice this still involves the solution\nof a potentially very complex optimization problem", "word_idx": 21869, "sentence_idx": 211, "label": "unlabeled"}, {"type": "text", "expr": "\nWe discuss here several approaches for approximately solving these optimization\nproblems", "word_idx": 22065, "sentence_idx": 212, "label": "unlabeled"}, {"type": "text", "expr": " We can usually obtain reasonably accurate solutions\nin many settings using a procedure that only involves a small number of forward\nand backward passes through the network, and which thus has a complexity that\nis at most a constant factor worse than that for feedforward networks", "word_idx": 22154, "sentence_idx": 213, "label": "unlabeled"}, {"type": "text", "expr": "\nThe same consideration will apply to training such networks, which we will\ndiscuss in Section  5 ", "word_idx": 22434, "sentence_idx": 214, "label": "unlabeled"}, {"type": "text", "expr": "Exact inference in ICNNs", "word_idx": 22532, "sentence_idx": 215, "label": "unlabeled"}, {"type": "text", "expr": "Although it is not a practical approach for\nsolving the optimization tasks,\nthe inference problem for the networks presented above (where the non-linear\nare either ReLU or linear units) can be posed as as linear program", "word_idx": 22556, "sentence_idx": 216, "label": "unlabeled"}, {"type": "text", "expr": "\nWe show how to do this in Section\u00a0 B ", "word_idx": 22775, "sentence_idx": 217, "label": "unlabeled"}, {"type": "text", "expr": "1  Approximate inference in ICNNs", "word_idx": 22813, "sentence_idx": 218, "label": "unlabeled"}, {"type": "text", "expr": "Because of the impracticality of exact inference, we focus on approximate\napproaches to optimizing over the inputs to these networks, but ideally ones\nthat still exploit the convexity of the resulting problem", "word_idx": 22846, "sentence_idx": 219, "label": "unlabeled"}, {"type": "text", "expr": " We\nspecifically focus on gradient-based approaches, which use the fact that we can\neasily compute the gradient of an ICNN with respect to its inputs,  $\\nabla_{y}f(x,y;\\theta)$ , using backpropagation", "word_idx": 23054, "sentence_idx": 220, "label": "unlabeled"}, {"type": "math", "expr": "$$\\nabla_{y}f(x,y;\\theta)$$", "word_idx": 23255, "sentence_idx": 221, "label": "unlabeled"}, {"type": "text", "expr": "Gradient descent ", "word_idx": 23278, "sentence_idx": 222, "label": "unlabeled"}, {"type": "text", "expr": " The simplest gradient-based methods for solving\n( 4 ) is just (projected sub-) gradient descent, or modifications\nsuch as\nthose that use a momentum term  \\citep polyak1964some,rumelhart1988learning, or\nspectral step size modifications  \\citep barzilai1988two,birgin2000nonmonotone", "word_idx": 23295, "sentence_idx": 223, "label": "unlabeled"}, {"type": "text", "expr": "\nThat is, we start with some initial  $\\hat{y}$  and repeat the update", "word_idx": 23576, "sentence_idx": 224, "label": "unlabeled"}, {"type": "text", "expr": "Gradient descent", "word_idx": 23646, "sentence_idx": 225, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 23662, "sentence_idx": 226, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 23668, "sentence_idx": 227, "label": "unlabeled"}, {"type": "math", "expr": "$$\\hat{y}$$", "word_idx": 23674, "sentence_idx": 228, "label": "unlabeled"}, {"type": "text", "expr": "$\\hat{y}\\leftarrow\\mathcal{P}_{\\mathcal{Y}}\\left(\\hat{y}-\\alpha\\nabla_{y}f(x,%\n\\hat{y};\\theta)\\right)$", "word_idx": 23681, "sentence_idx": 229, "label": "unlabeled"}, {"type": "math", "expr": "$$\\hat{y}\\leftarrow\\mathcal{P}_{\\mathcal{Y}}\\left(\\hat{y}-\\alpha\\nabla_{y}f(x,%\n\\hat{y};\\theta)\\right)$$", "word_idx": 23783, "sentence_idx": 230, "label": "unlabeled"}, {"type": "text", "expr": "This method is appealing in its simplicity, but suffers from the typical\nproblems of gradient descent on non-smooth objectives: we need to pick a\nstep size and possibly use a sequence of decreasing step sizes, and don\u2019t have an\nobvious method to assess how accurate of a current solution we have obtained\n(since an ICNN with ReLUs is piecewise linear, it will not have\nzero gradient at the solution)", "word_idx": 23883, "sentence_idx": 231, "label": "unlabeled"}, {"type": "text", "expr": "\nThe method is also more challenging to integrate with some learning\nprocedures, as we often need to differentiate through an entire chain of the\ngradient descent algorithm  ", "word_idx": 24282, "sentence_idx": 232, "label": "unlabeled"}, {"type": "text", "expr": " Thus, while the method can\nsometimes\nwork in practice, we have found that other approaches typically far outperform\nthis method, and we will focus on alternative approximate approaches for the\nremainder of this section", "word_idx": 24456, "sentence_idx": 233, "label": "unlabeled"}, {"type": "text", "expr": "Domke(2012)", "word_idx": 24675, "sentence_idx": 234, "label": "unlabeled"}, {"type": "text", "expr": "2  Approximate inference via the bundle entropy method", "word_idx": 24686, "sentence_idx": 235, "label": "unlabeled"}, {"type": "text", "expr": "An alternative approach to gradient descent is the bundle method\n \\citep smola2007bundle, also known as the epigraph cutting plane approach,\nwhich iteratively optimizes a piecewise lower bound on the function given by the\nmaximum over a set of first-order approximations", "word_idx": 24740, "sentence_idx": 236, "label": "unlabeled"}, {"type": "text", "expr": " However, as, the traditional\nbundle method is not well suited to our setting (we need to evaluate a number\nof gradients equal to the dimension of  $x$ , and solve a complex optimization\nproblem at each step) we have developed a new optimization algorithm for\nthis domain that we term the  bundle entropy method ", "word_idx": 25010, "sentence_idx": 237, "label": "unlabeled"}, {"type": "text", "expr": " This algorithm\nspecifically applies to the (common) case where  $\\mathcal{Y}$ \nis bounded, which we assume to be  $\\mathcal{Y}=[0,1]^{n}$ \n(other upper or lower bounds can be attained through scaling)", "word_idx": 25322, "sentence_idx": 238, "label": "unlabeled"}, {"type": "text", "expr": "\nThe method is also easily extensible to the setting where elements\nof  $\\mathcal{Y}$  belong to a higher-dimensional probability simplex as well", "word_idx": 25523, "sentence_idx": 239, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 25668, "sentence_idx": 240, "label": "unlabeled"}, {"type": "text", "expr": "bundle entropy method", "word_idx": 25674, "sentence_idx": 241, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{Y}$$", "word_idx": 25695, "sentence_idx": 242, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{Y}=[0,1]^{n}$$", "word_idx": 25706, "sentence_idx": 243, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{Y}$$", "word_idx": 25727, "sentence_idx": 244, "label": "unlabeled"}, {"type": "text", "expr": "For this approach, we consider adding an additional \u201cbarrier\u201d function to the\noptimization in the form of the negative entropy  $-H(y)$ , where", "word_idx": 25738, "sentence_idx": 245, "label": "unlabeled"}, {"type": "math", "expr": "$$-H(y)$$", "word_idx": 25881, "sentence_idx": 246, "label": "unlabeled"}, {"type": "text", "expr": "$H(y)=-\\sum_{i=1}^{n}(y_{i}\\log y_{i}+(1-y_{i})\\log(1-y_{i}))$", "word_idx": 25886, "sentence_idx": 247, "label": "unlabeled"}, {"type": "math", "expr": "$$H(y)=-\\sum_{i=1}^{n}(y_{i}\\log y_{i}+(1-y_{i})\\log(1-y_{i})).$$", "word_idx": 25948, "sentence_idx": 248, "label": "unlabeled"}, {"type": "text", "expr": "In other words, we instead want to solve the optimization problem  $\\argmin_{y}\\;\\;f(x,y;\\theta)-H(y)$  (with a possible additional scaling term)", "word_idx": 26009, "sentence_idx": 249, "label": "unlabeled"}, {"type": "text", "expr": "\nThe negative entropy is a convex function, with the limits\nof  $\\lim_{y\\rightarrow 0}H(y)=\\lim_{y\\rightarrow 1}H(y)=0$ , and negative\nvalues in the interior of this range", "word_idx": 26154, "sentence_idx": 250, "label": "unlabeled"}, {"type": "text", "expr": " The function acts as a barrier because,\nalthough it does not approach infinity as it reaches the barrier of the feasible\nset, its gradient  does  approach infinity as it reaches the barrier, and\nthus the optimal solution will always lie in the interior of the unit hypercube\n $\\mathcal{Y}$ ", "word_idx": 26325, "sentence_idx": 251, "label": "unlabeled"}, {"type": "math", "expr": "$$\\argmin_{y}\\;\\;f(x,y;\\theta)-H(y)$$", "word_idx": 26616, "sentence_idx": 252, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lim_{y\\rightarrow 0}H(y)=\\lim_{y\\rightarrow 1}H(y)=0$$", "word_idx": 26649, "sentence_idx": 253, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{Y}$$", "word_idx": 26702, "sentence_idx": 254, "label": "unlabeled"}, {"type": "text", "expr": "An appealing feature of the entropy regularization comes from its close\nconnection with sigmoid units in typical neural networks", "word_idx": 26713, "sentence_idx": 255, "label": "unlabeled"}, {"type": "text", "expr": " It follows easily\nfrom first-order optimality conditions that the optimization problem", "word_idx": 26841, "sentence_idx": 256, "label": "unlabeled"}, {"type": "text", "expr": "An appealing feature of the entropy regularization comes from its close\nconnection with sigmoid units in typical neural networks", "word_idx": 26928, "sentence_idx": 257, "label": "unlabeled"}, {"type": "text", "expr": " It follows easily\nfrom first-order optimality conditions that the optimization problem", "word_idx": 27056, "sentence_idx": 258, "label": "unlabeled"}, {"type": "text", "expr": "$\\minimize_{y}\\;c^{T}y-H(y)$", "word_idx": 27143, "sentence_idx": 259, "label": "unlabeled"}, {"type": "math", "expr": "$$\\minimize_{y}\\;c^{T}y-H(y)$$", "word_idx": 27171, "sentence_idx": 260, "label": "unlabeled"}, {"type": "text", "expr": "is given by  $y^{\\star}=1/(1+\\exp(c))$ ", "word_idx": 27197, "sentence_idx": 261, "label": "unlabeled"}, {"type": "text", "expr": " Thus if we consider the \u201ctrivial\u201d\nPICNN mentioned in Section  3", "word_idx": 27236, "sentence_idx": 262, "label": "unlabeled"}, {"type": "text", "expr": "2 , which simply consists of the\nfunction  $f(x,y;\\theta)=y^{T}\\tilde{f}(x;\\theta)$  for some purely feedforward\nnetwork  $\\tilde{f}(x;\\theta)$ , then the entropy-regularized minimization problem\ngives a solution that is equivalent to simply taking the sigmoid of the neural\nnetwork outputs", "word_idx": 27300, "sentence_idx": 263, "label": "unlabeled"}, {"type": "text", "expr": " Thus, the move to ICNNs can be interpreted as providing a\nmore structured joint energy functional over the linear function implicitly used\nby sigmoid layers", "word_idx": 27590, "sentence_idx": 264, "label": "unlabeled"}, {"type": "math", "expr": "$$y^{\\star}=1/(1+\\exp(c))$$", "word_idx": 27747, "sentence_idx": 265, "label": "unlabeled"}, {"type": "math", "expr": "$$f(x,y;\\theta)=y^{T}\\tilde{f}(x;\\theta)$$", "word_idx": 27770, "sentence_idx": 266, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{f}(x;\\theta)$$", "word_idx": 27808, "sentence_idx": 267, "label": "unlabeled"}, {"type": "text", "expr": "At each iteration of the bundle entropy method, we solve the optimization\nproblem The Lagrangian of the optimization problem is which in turn leads to the dual problem", "word_idx": 27827, "sentence_idx": 268, "label": "unlabeled"}, {"type": "text", "expr": "At each iteration of the bundle entropy method, we solve the optimization\nproblem", "word_idx": 27994, "sentence_idx": 269, "label": "unlabeled"}, {"type": "text", "expr": "$y^{k+1},t^{k+1}:=\\argmin_{y,t}\\;\\;\\{t-H(y)\\mid Gy+h\\leq t1\\}$", "word_idx": 28075, "sentence_idx": 270, "label": "unlabeled"}, {"type": "math", "expr": "$$y^{k+1},t^{k+1}:=\\argmin_{y,t}\\;\\;\\{t-H(y)\\mid Gy+h\\leq t1\\}$$", "word_idx": 28137, "sentence_idx": 271, "label": "unlabeled"}, {"type": "text", "expr": "where  $G\\in\\mathbb{R}^{k\\times n}$  has rows equal to", "word_idx": 28197, "sentence_idx": 272, "label": "unlabeled"}, {"type": "math", "expr": "$$G\\in\\mathbb{R}^{k\\times n}$$", "word_idx": 28251, "sentence_idx": 273, "label": "unlabeled"}, {"type": "text", "expr": "$g_{i}^{T}=\\nabla_{y}f(x,y^{i};\\theta)^{T}$", "word_idx": 28277, "sentence_idx": 274, "label": "unlabeled"}, {"type": "math", "expr": "$$g_{i}^{T}=\\nabla_{y}f(x,y^{i};\\theta)^{T}$$", "word_idx": 28320, "sentence_idx": 275, "label": "unlabeled"}, {"type": "text", "expr": "and  $h\\in\\mathbb{R}^{k}$  has entries equal to", "word_idx": 28361, "sentence_idx": 276, "label": "unlabeled"}, {"type": "math", "expr": "$$h\\in\\mathbb{R}^{k}$$", "word_idx": 28408, "sentence_idx": 277, "label": "unlabeled"}, {"type": "text", "expr": "$h_{i}=f(x,y^{i};\\theta)-\\nabla_{y}f(x,y^{i};\\theta)^{T}y^{i}$", "word_idx": 28426, "sentence_idx": 278, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{i}=f(x,y^{i};\\theta)-\\nabla_{y}f(x,y^{i};\\theta)^{T}y^{i}.$$", "word_idx": 28488, "sentence_idx": 279, "label": "unlabeled"}, {"type": "text", "expr": "The Lagrangian of the optimization problem is", "word_idx": 28549, "sentence_idx": 280, "label": "unlabeled"}, {"type": "text", "expr": "$\\mathcal{L}(y,t,\\lambda)=t-H(y)+\\lambda^{T}(Gy+h-t1)$", "word_idx": 28594, "sentence_idx": 281, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}(y,t,\\lambda)=t-H(y)+\\lambda^{T}(Gy+h-t1)$$", "word_idx": 28648, "sentence_idx": 282, "label": "unlabeled"}, {"type": "text", "expr": "and differentiating with respect to  $y$  and  $t$  gives the optimality conditions", "word_idx": 28700, "sentence_idx": 283, "label": "unlabeled"}, {"type": "text", "expr": "$\\begin{split}\\displaystyle\\nabla_{y}\\mathcal{L}(y,t,\\lambda)=0&\\displaystyle\\;%\n\\Longrightarrow\\;y=\\frac{1}{1+\\exp(G^{T}\\lambda)}\\\\\n\\displaystyle\\nabla_{t}\\mathcal{L}(y,t,\\lambda)=0&\\displaystyle\\;%\n\\Longrightarrow\\;1^{T}\\lambda=1\\end{split}$", "word_idx": 28783, "sentence_idx": 284, "label": "unlabeled"}, {"type": "math", "expr": "$$\\begin{split}\\displaystyle\\nabla_{y}\\mathcal{L}(y,t,\\lambda)=0&\\displaystyle\\;%\n\\Longrightarrow\\;y=\\frac{1}{1+\\exp(G^{T}\\lambda)}\\\\\n\\displaystyle\\nabla_{t}\\mathcal{L}(y,t,\\lambda)=0&\\displaystyle\\;%\n\\Longrightarrow\\;1^{T}\\lambda=1\\end{split}$$", "word_idx": 29026, "sentence_idx": 285, "label": "unlabeled"}, {"type": "text", "expr": "which in turn leads to the dual problem", "word_idx": 29267, "sentence_idx": 286, "label": "unlabeled"}, {"type": "text", "expr": "$\\begin{split}\\displaystyle\\maximize_{\\lambda}&\\displaystyle(G1+h)^{T}\\lambda-1%\n^{T}\\log(1+\\exp(G^{T}\\lambda))\\\\\n\\displaystyle\\subjectto&\\displaystyle\\lambda\\geq 0,1^{T}\\lambda=1\\end{split}$", "word_idx": 29306, "sentence_idx": 287, "label": "unlabeled"}, {"type": "math", "expr": "$$\\begin{split}\\displaystyle\\maximize_{\\lambda}&\\displaystyle(G1+h)^{T}\\lambda-1%\n^{T}\\log(1+\\exp(G^{T}\\lambda))\\\\\n\\displaystyle\\subjectto&\\displaystyle\\lambda\\geq 0,1^{T}\\lambda=1.\\end{split}$$", "word_idx": 29497, "sentence_idx": 288, "label": "unlabeled"}, {"type": "text", "expr": "This is a smooth optimization problem over the unit simplex, and can be solved using\na method like\nthe Projected Newton method of  \\citep [pg", "word_idx": 29687, "sentence_idx": 289, "label": "unlabeled"}, {"type": "text", "expr": " 241, eq", "word_idx": 29828, "sentence_idx": 290, "label": "unlabeled"}, {"type": "text", "expr": " 97]bertsekas1982projected", "word_idx": 29836, "sentence_idx": 291, "label": "unlabeled"}, {"type": "text", "expr": "\nA complete description of the bundle entropy\nmethod is given in Section\u00a0 D ", "word_idx": 29862, "sentence_idx": 292, "label": "unlabeled"}, {"type": "text", "expr": "\nFor lower dimensional problems, the bundle entropy method often attains an exact\nsolution after a relatively small number of iterations", "word_idx": 29938, "sentence_idx": 293, "label": "unlabeled"}, {"type": "text", "expr": " And even for larger\nproblems, we find that the approximate solutions generated by a very\nsmall number of iterations (we typically use 5 iterations), still\nsubstantially outperform gradient descent approaches", "word_idx": 30074, "sentence_idx": 294, "label": "unlabeled"}, {"type": "text", "expr": " Further, because we\nmaintain an explicit lower bound on the function, we can compute an\noptimality gap of our solution, though in practice just using a fixed number of\niterations performs well", "word_idx": 30282, "sentence_idx": 295, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 30475, "sentence_idx": 296, "label": "unlabeled"}, {"type": "text", "expr": "5  Learning ICNNs", "word_idx": 30481, "sentence_idx": 297, "label": "unlabeled"}, {"type": "text", "expr": "Although we only discuss the entropy regularization in this work, we emphasize\nthat other regularizers are also possible", "word_idx": 30498, "sentence_idx": 298, "label": "unlabeled"}, {"type": "text", "expr": " Depending on the setting, there are\nseveral different approaches we can use\nto ensure that the ICNN achieves the desired targets, and we consider three\napproaches below: direct functional fitting,\nmax-margin structured prediction, and argmin differentiation", "word_idx": 30618, "sentence_idx": 299, "label": "unlabeled"}, {"type": "text", "expr": "Generally speaking, ICNN learning shapes the objective\u2019s energy function to\nproduce the desired values when optimizing over the relevant inputs", "word_idx": 30876, "sentence_idx": 300, "label": "unlabeled"}, {"type": "text", "expr": " That is,\nfor a given input output pair  $(x,y^{\\star})$ , our goal is to find ICNN parameters\n $\\theta$  such that", "word_idx": 31019, "sentence_idx": 301, "label": "unlabeled"}, {"type": "math", "expr": "$$(x,y^{\\star})$$", "word_idx": 31134, "sentence_idx": 302, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta$$", "word_idx": 31147, "sentence_idx": 303, "label": "unlabeled"}, {"type": "text", "expr": "$y^{\\star}\\approx\\argmin_{y}\\tilde{f}(x,y;\\theta)$", "word_idx": 31153, "sentence_idx": 304, "label": "unlabeled"}, {"type": "math", "expr": "$$y^{\\star}\\approx\\argmin_{y}\\tilde{f}(x,y;\\theta)$$", "word_idx": 31203, "sentence_idx": 305, "label": "unlabeled"}, {"type": "text", "expr": "where for the entirely of this section, we use the notation  $\\tilde{f}$  to\ndenote the combination of the neural network function  plus  the\nregularization term such as  $-H(y)$ , if it is included, i", "word_idx": 31251, "sentence_idx": 306, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{f}$$", "word_idx": 31452, "sentence_idx": 307, "label": "unlabeled"}, {"type": "math", "expr": "$$-H(y)$$", "word_idx": 31461, "sentence_idx": 308, "label": "unlabeled"}, {"type": "text", "expr": "$\\tilde{f}(x,y;\\theta)=f(x,y;\\theta)-H(y)$", "word_idx": 31466, "sentence_idx": 309, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{f}(x,y;\\theta)=f(x,y;\\theta)-H(y).$$", "word_idx": 31508, "sentence_idx": 310, "label": "unlabeled"}, {"type": "text", "expr": "Although we only discuss the entropy regularization in this work, we emphasize\nthat other regularizers are also possible", "word_idx": 31549, "sentence_idx": 311, "label": "unlabeled"}, {"type": "text", "expr": " Depending on the setting, there are\nseveral different approaches we can use\nto ensure that the ICNN achieves the desired targets, and we consider three\napproaches below: direct functional fitting,\nmax-margin structured prediction, and argmin differentiation", "word_idx": 31669, "sentence_idx": 312, "label": "unlabeled"}, {"type": "text", "expr": "Direct functional fitting", "word_idx": 31927, "sentence_idx": 313, "label": "unlabeled"}, {"type": "text", "expr": "We first note that in some domains, we do not need a specialized procedure for\nfitting ICNNs, but can use existing approaches that directly fit the ICNN", "word_idx": 31952, "sentence_idx": 314, "label": "unlabeled"}, {"type": "text", "expr": "\nAn example of this is the Q-learning setting", "word_idx": 32104, "sentence_idx": 315, "label": "unlabeled"}, {"type": "text", "expr": "\nGiven some observed tuple  $(s,a,r,s^{\\prime})$ , Q learning updates the\nparameters  $\\theta$  with the gradient", "word_idx": 32149, "sentence_idx": 316, "label": "unlabeled"}, {"type": "math", "expr": "$$(s,a,r,s^{\\prime})$$", "word_idx": 32262, "sentence_idx": 317, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta$$", "word_idx": 32280, "sentence_idx": 318, "label": "unlabeled"}, {"type": "text", "expr": "$\\left(Q(s,a)-r-\\gamma\\max_{a^{\\prime}}Q(s^{\\prime},a^{\\prime})\\right)\\nabla_{%\n\\theta}Q(s,a),$", "word_idx": 32286, "sentence_idx": 319, "label": "unlabeled"}, {"type": "math", "expr": "$$\\left(Q(s,a)-r-\\gamma\\max_{a^{\\prime}}Q(s^{\\prime},a^{\\prime})\\right)\\nabla_{%\n\\theta}Q(s,a),$$", "word_idx": 32381, "sentence_idx": 320, "label": "unlabeled"}, {"type": "text", "expr": "where the maximization step is carried out with gradient descent or\nthe bundle entropy method", "word_idx": 32474, "sentence_idx": 321, "label": "unlabeled"}, {"type": "text", "expr": "\nThese updates can be applied to ICNNs with the only additional requirement that\nwe project the weights onto their feasible sets after this update\n(i", "word_idx": 32567, "sentence_idx": 322, "label": "unlabeled"}, {"type": "text", "expr": ", clip or project any  $W$  terms that are required to be positive)", "word_idx": 32716, "sentence_idx": 323, "label": "unlabeled"}, {"type": "text", "expr": " Section\u00a0 E  gives a complete description of\ndeep Q-learning with ICNNs", "word_idx": 32783, "sentence_idx": 324, "label": "unlabeled"}, {"type": "text", "expr": "Max-margin structured prediction", "word_idx": 32854, "sentence_idx": 325, "label": "unlabeled"}, {"type": "text", "expr": "Although max-margin structured prediction is a simple and well-studied approach\n \\citep tsochantaridis2005large,taskar2005learning,\nin our experiences using these methods within an ICNN, we had substantial\ndifficulty choosing the proper margin scaling term (especially for domains with\ncontinuous-valued outputs), or allowing for losses other\nthan the hinge loss", "word_idx": 32886, "sentence_idx": 326, "label": "unlabeled"}, {"type": "text", "expr": " For this reason,\nSection\u00a0 F  discusses max-margin structured\nprediction in more detail, but the majority of our experiments here\nfocus on the next approach, which more directly encodes the\nloss suffered by the full structured-prediction pipeline", "word_idx": 33248, "sentence_idx": 327, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 33494, "sentence_idx": 328, "label": "unlabeled"}, {"type": "text", "expr": "1  Argmin differentiation", "word_idx": 33500, "sentence_idx": 329, "label": "unlabeled"}, {"type": "text", "expr": "In our final proposed approach, that of argmin differentiation, we\npropose to directly minimize a loss function between true outputs and the\noutputs predicted by our model, where these predictions themselves are the\nresult of an optimization problem", "word_idx": 33525, "sentence_idx": 330, "label": "unlabeled"}, {"type": "text", "expr": " We explicitly consider the case where the\napproximate solution to the inference problem is attained via the\npreviously-described bundle entropy method, typically run for some fixed\n(usually small) number of iterations", "word_idx": 33774, "sentence_idx": 331, "label": "unlabeled"}, {"type": "text", "expr": " To simplify notation, in the following we\nwill let", "word_idx": 33992, "sentence_idx": 332, "label": "unlabeled"}, {"type": "text", "expr": "In our final proposed approach, that of argmin differentiation, we\npropose to directly minimize a loss function between true outputs and the\noutputs predicted by our model, where these predictions themselves are the\nresult of an optimization problem", "word_idx": 34043, "sentence_idx": 333, "label": "unlabeled"}, {"type": "text", "expr": " We explicitly consider the case where the\napproximate solution to the inference problem is attained via the\npreviously-described bundle entropy method, typically run for some fixed\n(usually small) number of iterations", "word_idx": 34292, "sentence_idx": 334, "label": "unlabeled"}, {"type": "text", "expr": " To simplify notation, in the following we\nwill let", "word_idx": 34510, "sentence_idx": 335, "label": "unlabeled"}, {"type": "text", "expr": "$\\begin{split}\\displaystyle\\hat{y}(x;\\theta)&\\displaystyle=\\argmin_{y}\\min_{t}%\n\\;\\;\\{t-H(y)\\mid Gy+h\\leq t1\\}\\\\\n&\\displaystyle\\approx\\argmin_{y}\\tilde{f}(x,y;\\theta)\\end{split}$", "word_idx": 34561, "sentence_idx": 336, "label": "unlabeled"}, {"type": "math", "expr": "$$\\begin{split}\\displaystyle\\hat{y}(x;\\theta)&\\displaystyle=\\argmin_{y}\\min_{t}%\n\\;\\;\\{t-H(y)\\mid Gy+h\\leq t1\\}\\\\\n&\\displaystyle\\approx\\argmin_{y}\\tilde{f}(x,y;\\theta)\\end{split}$$", "word_idx": 34739, "sentence_idx": 337, "label": "unlabeled"}, {"type": "text", "expr": "refer to the  approximate  minimization over  $y$  that\nresults from running the bundle entropy method, specifically at the last\niteration of the method", "word_idx": 34915, "sentence_idx": 338, "label": "unlabeled"}, {"type": "text", "expr": "approximate", "word_idx": 35067, "sentence_idx": 339, "label": "unlabeled"}, {"type": "text", "expr": "Given some example  $(x,y^{\\star})$ , our goal is to compute the gradient, with\nrespect to the ICNN parameters, of the loss between  $y^{\\star}$  and  $\\hat{y}(x;\\theta)$ :\n $\\ell(\\hat{y}(x;\\theta),y^{\\star})$ ", "word_idx": 35078, "sentence_idx": 340, "label": "unlabeled"}, {"type": "text", "expr": " This is in some sense the most direct\nanalogue to\ntraditional neural network learning, since we typically optimize networks by\nminimizing some loss between the network\u2019s (feedforward) predictions and the true\ndesired labels", "word_idx": 35288, "sentence_idx": 341, "label": "unlabeled"}, {"type": "text", "expr": " Doing this in the predictions-via-optimization setting\nrequires that we differentiate \u201cthrough\u201d the argmin operator, which can be\naccomplished via implicit differentiation of the KKT optimality conditions", "word_idx": 35512, "sentence_idx": 342, "label": "unlabeled"}, {"type": "text", "expr": "\nAlthough the derivation is somewhat involved, the final result is fairly\ncompact, and is given by the following proposition (for simplicity, we will\nwrite  $\\hat{y}$  below instead of  $\\hat{y}(x;\\theta)$  when the notation should\nbe clear):", "word_idx": 35717, "sentence_idx": 343, "label": "unlabeled"}, {"type": "math", "expr": "$$(x,y^{\\star})$$", "word_idx": 35959, "sentence_idx": 344, "label": "unlabeled"}, {"type": "math", "expr": "$$y^{\\star}$$", "word_idx": 35972, "sentence_idx": 345, "label": "unlabeled"}, {"type": "math", "expr": "$$\\hat{y}(x;\\theta)$$", "word_idx": 35981, "sentence_idx": 346, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ell(\\hat{y}(x;\\theta),y^{\\star})$$", "word_idx": 35998, "sentence_idx": 347, "label": "unlabeled"}, {"type": "math", "expr": "$$\\hat{y}$$", "word_idx": 36031, "sentence_idx": 348, "label": "unlabeled"}, {"type": "math", "expr": "$$\\hat{y}(x;\\theta)$$", "word_idx": 36038, "sentence_idx": 349, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 3 ", "word_idx": 36055, "sentence_idx": 350, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 3", "word_idx": 36069, "sentence_idx": 351, "label": "unlabeled"}, {"type": "text", "expr": "The gradient of the neural network loss for predictions generated through\nthe minimization process is", "word_idx": 36082, "sentence_idx": 352, "label": "unlabeled"}, {"type": "text", "expr": "The gradient of the neural network loss for predictions generated through\nthe minimization process is", "word_idx": 36183, "sentence_idx": 353, "label": "unlabeled"}, {"type": "text", "expr": "The gradient of the neural network loss for predictions generated through\nthe minimization process is", "word_idx": 36284, "sentence_idx": 354, "label": "unlabeled"}, {"type": "text", "expr": "$\\begin{split}&\\displaystyle\\nabla_{\\theta}\\ell(\\hat{y}(x;\\theta),y^{\\star})=%\n\\sum_{i=1}^{k}(c^{\\lambda}_{i}\\nabla_{\\theta}f(x,y^{i};\\theta)+\\\\\n&\\displaystyle\\hskip 5690551pt\\nabla_{\\theta}\\left(\\nabla_{y}f(x,y^{i};\\theta%\n)^{T}\\left(\\lambda_{i}c^{y}+c^{\\lambda}_{i}\\left(\\hat{y}(x;\\theta)-y^{i}\\right%\n)\\right)\\right))\\end{split}$", "word_idx": 36385, "sentence_idx": 355, "label": "unlabeled"}, {"type": "math", "expr": "$$\\begin{split}&\\displaystyle\\nabla_{\\theta}\\ell(\\hat{y}(x;\\theta),y^{\\star})=%\n\\sum_{i=1}^{k}(c^{\\lambda}_{i}\\nabla_{\\theta}f(x,y^{i};\\theta)+\\\\\n&\\displaystyle\\hskip 5.690551pt\\nabla_{\\theta}\\left(\\nabla_{y}f(x,y^{i};\\theta%\n)^{T}\\left(\\lambda_{i}c^{y}+c^{\\lambda}_{i}\\left(\\hat{y}(x;\\theta)-y^{i}\\right%\n)\\right)\\right))\\end{split}$$", "word_idx": 36717, "sentence_idx": 356, "label": "unlabeled"}, {"type": "text", "expr": "where  $y^{i}$  denotes the solution returned by the  $i$ th iteration of the entropy\nbundle method,  $\\lambda$  denotes the dual variable solution of the entropy\nbundle method, and where the  $c$  variables are determined by the solution to the\nlinear system", "word_idx": 37048, "sentence_idx": 357, "label": "unlabeled"}, {"type": "math", "expr": "$$y^{i}$$", "word_idx": 37307, "sentence_idx": 358, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda$$", "word_idx": 37312, "sentence_idx": 359, "label": "unlabeled"}, {"type": "text", "expr": "$\\left[\\begin{array}[]{ccc}H&G^{T}&0\\\\\nG&0&-1\\\\\n0&-1^{T}&0\\end{array}\\right]\\left[\\begin{array}[]{c}c^{y}\\\\\nc^{\\lambda}\\\\\nc^{t}\\end{array}\\right]=\\left[\\begin{array}[]{c}-\\nabla_{\\hat{y}}\\ell(\\hat{y},%\ny^{\\star})\\\\\n0\\\\\n0\\end{array}\\right]$", "word_idx": 37319, "sentence_idx": 360, "label": "unlabeled"}, {"type": "math", "expr": "$$\\left[\\begin{array}[]{ccc}H&G^{T}&0\\\\\nG&0&-1\\\\\n0&-1^{T}&0\\end{array}\\right]\\left[\\begin{array}[]{c}c^{y}\\\\\nc^{\\lambda}\\\\\nc^{t}\\end{array}\\right]=\\left[\\begin{array}[]{c}-\\nabla_{\\hat{y}}\\ell(\\hat{y},%\ny^{\\star})\\\\\n0\\\\\n0\\end{array}\\right].$$", "word_idx": 37558, "sentence_idx": 361, "label": "unlabeled"}, {"type": "text", "expr": "where  $H=\\diag\\left(\\frac{1}{\\hat{y}}+\\frac{1}{1-\\hat{y}}\\right)$ ", "word_idx": 37796, "sentence_idx": 362, "label": "unlabeled"}, {"type": "math", "expr": "$$H=\\diag\\left(\\frac{1}{\\hat{y}}+\\frac{1}{1-\\hat{y}}\\right)$$", "word_idx": 37863, "sentence_idx": 363, "label": "unlabeled"}, {"type": "text", "expr": "The proof of this proposition is given in Section\u00a0 G ,\nbut we highlight a few key points here", "word_idx": 37920, "sentence_idx": 364, "label": "unlabeled"}, {"type": "text", "expr": "\nThe complexity of computing this gradient will be linear in  $k$ , which is the\nnumber of  active  constraints at the solution of the bundle entropy\nmethod", "word_idx": 38013, "sentence_idx": 365, "label": "unlabeled"}, {"type": "text", "expr": " The inverse of this matrix can also be computed efficiently by just\ninverting the  $k\\times k$  matrix  $GH^{-1}G^{T}$  via\na variable elimination procedure, instead of by inverting the full matrix", "word_idx": 38169, "sentence_idx": 366, "label": "unlabeled"}, {"type": "text", "expr": "\nThe gradients  $\\nabla_{\\theta}f(x,y_{i};\\theta)$  are standard neural network\ngradients, and further, can be computed in the same forward/backward pass as we\nuse to compute the gradients for the bundle entropy method", "word_idx": 38367, "sentence_idx": 367, "label": "unlabeled"}, {"type": "text", "expr": "\nThe main challenge of the method is to compute the terms of the form\n $\\nabla_{\\theta}(\\nabla_{y}f(x,y_{i};\\theta)^{T}v)$ \nfor some vector  $v$ ", "word_idx": 38585, "sentence_idx": 368, "label": "unlabeled"}, {"type": "text", "expr": " This quantity can be computed by most autodifferentiation\ntools (the gradient inner product  $\\nabla_{y}f(x,y_{i};\\theta)^{T}v$  itself just\nbecomes a graph computation than can be differentiated itself), or it can be\ncomputed by a finite difference approximation", "word_idx": 38730, "sentence_idx": 369, "label": "unlabeled"}, {"type": "text", "expr": "\nThe complexity of computing this entire gradient is a small\nconstant multiple of computing  $k$  gradients with respect to  $\\theta$ ", "word_idx": 38994, "sentence_idx": 370, "label": "unlabeled"}, {"type": "text", "expr": "active", "word_idx": 39128, "sentence_idx": 371, "label": "unlabeled"}, {"type": "math", "expr": "$$k\\times k$$", "word_idx": 39134, "sentence_idx": 372, "label": "unlabeled"}, {"type": "math", "expr": "$$GH^{-1}G^{T}$$", "word_idx": 39143, "sentence_idx": 373, "label": "unlabeled"}, {"type": "math", "expr": "$$\\nabla_{\\theta}f(x,y_{i};\\theta)$$", "word_idx": 39155, "sentence_idx": 374, "label": "unlabeled"}, {"type": "math", "expr": "$$\\nabla_{\\theta}(\\nabla_{y}f(x,y_{i};\\theta)^{T}v)$$", "word_idx": 39187, "sentence_idx": 375, "label": "unlabeled"}, {"type": "math", "expr": "$$\\nabla_{y}f(x,y_{i};\\theta)^{T}v$$", "word_idx": 39236, "sentence_idx": 376, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta$$", "word_idx": 39268, "sentence_idx": 377, "label": "unlabeled"}, {"type": "text", "expr": "Given this ability to compute gradients with respect to an arbitrary loss\nfunction, we can fit the parameter using traditional stochastic gradient methods\nexamples", "word_idx": 39274, "sentence_idx": 378, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, given an example (or a minibatch of examples)  $x_{i},y_{i}$ , we compute gradients  $\\nabla_{\\theta}\\ell(\\hat{y}(x_{i};\\theta),y_{i})$ \nand update the parameters using e", "word_idx": 39437, "sentence_idx": 379, "label": "unlabeled"}, {"type": "text", "expr": " the ADAM optimizer  \\citep kingma2014adam", "word_idx": 39622, "sentence_idx": 380, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{i},y_{i}$$", "word_idx": 39664, "sentence_idx": 381, "label": "unlabeled"}, {"type": "math", "expr": "$$\\nabla_{\\theta}\\ell(\\hat{y}(x_{i};\\theta),y_{i})$$", "word_idx": 39675, "sentence_idx": 382, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 39723, "sentence_idx": 383, "label": "unlabeled"}, {"type": "text", "expr": "6  Experiments", "word_idx": 39729, "sentence_idx": 384, "label": "unlabeled"}, {"type": "text", "expr": "Our experiments study the representational power of ICNNs to\nbetter understand the interplay between the model\u2019s\nrestrictiveness and accuracy", "word_idx": 39743, "sentence_idx": 385, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, we evaluate the method on\nmulti-label classification on the BibTeX dataset  \\citep katakis2008multilabel,\nimage completion using the Olivetti face dataset  \\citep samaria1994parameterisation,\nand continuous action reinforcement learning in the\nOpenAI Gym  \\citep brockman2016openai", "word_idx": 39884, "sentence_idx": 386, "label": "unlabeled"}, {"type": "text", "expr": " We show that the methods compare\nfavorably to the state of the art in many situations", "word_idx": 40180, "sentence_idx": 387, "label": "unlabeled"}, {"type": "text", "expr": "\nThe full source code for all experiments is available in the\n icml2017  branch at  https://github", "word_idx": 40266, "sentence_idx": 388, "label": "unlabeled"}, {"type": "text", "expr": "com/locuslab/icnn  and\nour implementation is built using\nPython  \\citep van1995python with the numpy  \\citep oliphant2006guide and\nTensorFlow  \\citep abadi2016tensorflow packages", "word_idx": 40364, "sentence_idx": 389, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 40542, "sentence_idx": 390, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 40548, "sentence_idx": 391, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 40554, "sentence_idx": 392, "label": "unlabeled"}, {"type": "text", "expr": "https://github", "word_idx": 40560, "sentence_idx": 393, "label": "unlabeled"}, {"type": "text", "expr": "com/locuslab/icnn", "word_idx": 40574, "sentence_idx": 394, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 40591, "sentence_idx": 395, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 40597, "sentence_idx": 396, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 40603, "sentence_idx": 397, "label": "unlabeled"}, {"type": "text", "expr": "1  Synthetic 2D example", "word_idx": 40609, "sentence_idx": 398, "label": "unlabeled"}, {"type": "text", "expr": "Though we do not discuss it here,\nSection\u00a0 I  presents a simple synthetic classification\nexperiment comparing FICNN and PICNN decision boundaries", "word_idx": 40632, "sentence_idx": 399, "label": "unlabeled"}, {"type": "text", "expr": "2  Multi-Label Classification", "word_idx": 40777, "sentence_idx": 400, "label": "unlabeled"}, {"type": "text", "expr": "We first study how ICNNs perform on multi-label classification with the\nBibTeX dataset and benchmark presented in  ", "word_idx": 40806, "sentence_idx": 401, "label": "unlabeled"}, {"type": "text", "expr": "\nThis benchmark maps text classification from an input space  $\\cal{X}$  of\n1836 bag-of-works indicator (binary) features to an output\nspace  $\\cal{Y}$  of 159 binary labels", "word_idx": 40921, "sentence_idx": 402, "label": "unlabeled"}, {"type": "text", "expr": "\nWe use the train/test split of 4880/2515 from  \\citep katakis2008multilabel\nand evaluate with the macro-F1 score (higher is better)", "word_idx": 41094, "sentence_idx": 403, "label": "unlabeled"}, {"type": "text", "expr": "\nWe use the ARFF version of this dataset from Mulan  \\citep tsoumakas2011mulan", "word_idx": 41226, "sentence_idx": 404, "label": "unlabeled"}, {"type": "text", "expr": "\nOur PICNN architecture for multi-label classification uses fully-connected\nlayers with ReLU activation functions and batch\nnormalization  \\citep ioffe2015batch along the input path", "word_idx": 41304, "sentence_idx": 405, "label": "unlabeled"}, {"type": "text", "expr": "\nAs a baseline, we use a fully-connected neural network with\nbatch normalization and ReLU activation functions", "word_idx": 41485, "sentence_idx": 406, "label": "unlabeled"}, {"type": "text", "expr": "\nBoth architectures have the same structure\n(600 fully connected, 159 (#labels) fully connected)", "word_idx": 41595, "sentence_idx": 407, "label": "unlabeled"}, {"type": "text", "expr": "\nWe optimize our PICNN with 30 iterations of gradient descent\nwith a learning rate of 0", "word_idx": 41691, "sentence_idx": 408, "label": "unlabeled"}, {"type": "text", "expr": "1 and a momentum of 0", "word_idx": 41778, "sentence_idx": 409, "label": "unlabeled"}, {"type": "text", "expr": "Katakis et\u00a0al", "word_idx": 41799, "sentence_idx": 410, "label": "unlabeled"}, {"type": "text", "expr": "(2008)Katakis, Tsoumakas, and\nVlahavas", "word_idx": 41812, "sentence_idx": 411, "label": "unlabeled"}, {"type": "math", "expr": "$$\\cal{X}$$", "word_idx": 41850, "sentence_idx": 412, "label": "unlabeled"}, {"type": "math", "expr": "$$\\cal{Y}$$", "word_idx": 41857, "sentence_idx": 413, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 41864, "sentence_idx": 414, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 41870, "sentence_idx": 415, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 41876, "sentence_idx": 416, "label": "unlabeled"}, {"type": "text", "expr": "Method Test Macro-F1", "word_idx": 41882, "sentence_idx": 417, "label": "unlabeled"}, {"type": "text", "expr": "Test Macro-F1", "word_idx": 41902, "sentence_idx": 418, "label": "unlabeled"}, {"type": "text", "expr": "Feedforward net 0", "word_idx": 41915, "sentence_idx": 419, "label": "unlabeled"}, {"type": "text", "expr": "ICNN 0", "word_idx": 41932, "sentence_idx": 420, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 41938, "sentence_idx": 421, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:  Comparison of approaches on BibTeX multi-label classification task", "word_idx": 41944, "sentence_idx": 422, "label": "unlabeled"}, {"type": "text", "expr": "\n(Higher is better", "word_idx": 42020, "sentence_idx": 423, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:", "word_idx": 42038, "sentence_idx": 424, "label": "unlabeled"}, {"type": "text", "expr": "Table  1  compares several different methods for this problem", "word_idx": 42046, "sentence_idx": 425, "label": "unlabeled"}, {"type": "text", "expr": "\nOur PICNN\u2019s final macro-F1 score of 0", "word_idx": 42107, "sentence_idx": 426, "label": "unlabeled"}, {"type": "text", "expr": "415 outperforms our\nbaseline feedforward network\u2019s score of 0", "word_idx": 42145, "sentence_idx": 427, "label": "unlabeled"}, {"type": "text", "expr": "396,\nwhich indicates PICNNs have the power to learn a robust\nstructure over the output space", "word_idx": 42206, "sentence_idx": 428, "label": "unlabeled"}, {"type": "text", "expr": "\nSPENs obtain a macro-F1 score of 0", "word_idx": 42298, "sentence_idx": 429, "label": "unlabeled"}, {"type": "text", "expr": "422 on this task  \\citep belanger2016structured\nand pose an interesting comparison point to ICNNs as they have\na similar (but not identical) deep structure that is non-convex\nover the input space", "word_idx": 42333, "sentence_idx": 430, "label": "unlabeled"}, {"type": "text", "expr": "\nThe difference of 0", "word_idx": 42528, "sentence_idx": 431, "label": "unlabeled"}, {"type": "text", "expr": "007 between ICNNs and SPENs could be due\nto differences in our experimental setups, architectures,\nand random experimental noise", "word_idx": 42548, "sentence_idx": 432, "label": "unlabeled"}, {"type": "text", "expr": "\nMore details are included in Section\u00a0 J ", "word_idx": 42676, "sentence_idx": 433, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 42717, "sentence_idx": 434, "label": "unlabeled"}, {"type": "text", "expr": "3  Image completion on the Olivetti faces", "word_idx": 42723, "sentence_idx": 435, "label": "unlabeled"}, {"type": "text", "expr": "As a test of the system on a structured prediction task over a\nmuch more complex output space  $\\cal{Y}$ , we apply a\nconvolutional PICNN to face completion on the\nsklearn version  \\citep pedregosa2011scikit of the Olivetti\ndata set  \\citep samaria1994parameterisation, which contains\n400 64x64 grayscale images", "word_idx": 42764, "sentence_idx": 436, "label": "unlabeled"}, {"type": "text", "expr": "\nICNNs for face completion should be invariant to translations\nand other transformations in the input space", "word_idx": 43075, "sentence_idx": 437, "label": "unlabeled"}, {"type": "text", "expr": "\nTo achieve this invariance, our PICNN is inspired by the\nDQN architecture in  \\citet mnih2015human, which preserves\nthis invariance in the different context of reinforcement learning", "word_idx": 43182, "sentence_idx": 438, "label": "unlabeled"}, {"type": "text", "expr": "\nSpecifically, our network\nis over  $(x,y)$  pairs where\n $x$  (32x64) is the left half and  $y$  (32x64)\nis the right half of the image", "word_idx": 43365, "sentence_idx": 439, "label": "unlabeled"}, {"type": "text", "expr": "\nThe input and output paths are:\n32x8x8 conv (stride 4x2), 64x4x4 conv (stride 2x2),\n64x3x3 conv, 512 fully connected", "word_idx": 43501, "sentence_idx": 440, "label": "unlabeled"}, {"type": "math", "expr": "$$\\cal{Y}$$", "word_idx": 43618, "sentence_idx": 441, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 43625, "sentence_idx": 442, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 43631, "sentence_idx": 443, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 43637, "sentence_idx": 444, "label": "unlabeled"}, {"type": "math", "expr": "$$(x,y)$$", "word_idx": 43643, "sentence_idx": 445, "label": "unlabeled"}, {"type": "text", "expr": "This experiment uses the same training/test splits and minimizes\nthe mean squared error (MSE) as in  \\citet poon2011sum so that our\nresults can be directly compared to (a non-exhaustive list of)\nother techniques", "word_idx": 43648, "sentence_idx": 446, "label": "unlabeled"}, {"type": "text", "expr": "\nWe also explore the tradeoffs between the bundle entropy method\nand gradient descent and use a non-convex baseline to\nbetter understand the impacts of convexity", "word_idx": 43859, "sentence_idx": 447, "label": "unlabeled"}, {"type": "text", "expr": "\nWe use a learning rate of 0", "word_idx": 44020, "sentence_idx": 448, "label": "unlabeled"}, {"type": "text", "expr": "01 and momentum of 0", "word_idx": 44048, "sentence_idx": 449, "label": "unlabeled"}, {"type": "text", "expr": "9 with\ngradient descent for the inner optimization in the ICNN", "word_idx": 44068, "sentence_idx": 450, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 44130, "sentence_idx": 451, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:  Example test set image completions of the ICNN with bundle entropy", "word_idx": 44136, "sentence_idx": 452, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:", "word_idx": 44213, "sentence_idx": 453, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a0 2  shows the test MSEs for the different approaches", "word_idx": 44222, "sentence_idx": 454, "label": "unlabeled"}, {"type": "text", "expr": "\nExample image completions are shown in Figure\u00a0 3 ", "word_idx": 44280, "sentence_idx": 455, "label": "unlabeled"}, {"type": "text", "expr": "\nThese results show that the bundle entropy method can leverage\nmore information from these five iterations than gradient descent,\neven when the convexity constraint is relaxed", "word_idx": 44330, "sentence_idx": 456, "label": "unlabeled"}, {"type": "text", "expr": "\nThe PICNN trained with back-optimization with the relaxed convexity\nconstraint slightly outperforms the network with\nthe convexity constraint, but not the network trained with the\nbundle-entropy method", "word_idx": 44506, "sentence_idx": 457, "label": "unlabeled"}, {"type": "text", "expr": "\nThis shows that for image completion with PICNNs, convexity does not seem to\ninhibit the representational power", "word_idx": 44708, "sentence_idx": 458, "label": "unlabeled"}, {"type": "text", "expr": "\nFurthermore, this experiment suggests that a small number of inner optimization iterations\n(five in this case) is sufficient for good performance", "word_idx": 44820, "sentence_idx": 459, "label": "unlabeled"}, {"type": "text", "expr": "Method MSE", "word_idx": 44966, "sentence_idx": 460, "label": "unlabeled"}, {"type": "text", "expr": "Method", "word_idx": 44976, "sentence_idx": 461, "label": "unlabeled"}, {"type": "text", "expr": "ICNN - Bundle Entropy 833", "word_idx": 44982, "sentence_idx": 462, "label": "unlabeled"}, {"type": "text", "expr": "ICNN - Bundle Entropy", "word_idx": 45007, "sentence_idx": 463, "label": "unlabeled"}, {"type": "text", "expr": "ICNN - Gradient Decent 872", "word_idx": 45028, "sentence_idx": 464, "label": "unlabeled"}, {"type": "text", "expr": "ICNN - Gradient Decent", "word_idx": 45054, "sentence_idx": 465, "label": "unlabeled"}, {"type": "text", "expr": "ICNN - Nonconvex 850", "word_idx": 45076, "sentence_idx": 466, "label": "unlabeled"}, {"type": "text", "expr": "ICNN - Nonconvex", "word_idx": 45096, "sentence_idx": 467, "label": "unlabeled"}, {"type": "text", "expr": "Sum-product  \\citep poon2011sum", "word_idx": 45112, "sentence_idx": 468, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 45143, "sentence_idx": 469, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:  Comparisons of reconstruction error on image completion", "word_idx": 45149, "sentence_idx": 470, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:", "word_idx": 45214, "sentence_idx": 471, "label": "unlabeled"}, {"type": "text", "expr": "4  Continuous Action Reinforcement Learning", "word_idx": 45222, "sentence_idx": 472, "label": "unlabeled"}, {"type": "text", "expr": "Finally, we present standard benchmarks in continuous action reinforcement\nlearning from the OpenAI Gym  \\citep brockman2016openai that use the\nMuJoCo physics simulator  \\citep todorov2012mujoco", "word_idx": 45265, "sentence_idx": 473, "label": "unlabeled"}, {"type": "text", "expr": "\nWe model the (negative)  $Q$  function,\n $-Q(s,a;\\theta)$  as an ICNN and select actions with\nthe convex optimization problem\n $a^{\\star}(s)=\\argmin_{a}-Q(s,a;\\theta)$ ", "word_idx": 45459, "sentence_idx": 474, "label": "unlabeled"}, {"type": "text", "expr": "\nWe use Q-learning to optimize the ICNN as described in\nSection\u00a0 5  and Section\u00a0 E ", "word_idx": 45628, "sentence_idx": 475, "label": "unlabeled"}, {"type": "text", "expr": "\nAt test time, the policy is selected by optimizing  $Q(s,a;\\theta)$ ", "word_idx": 45711, "sentence_idx": 476, "label": "unlabeled"}, {"type": "text", "expr": "\nAll of our experiments use a PICNN with two fully-connected\nlayers that each have 200 hidden units", "word_idx": 45780, "sentence_idx": 477, "label": "unlabeled"}, {"type": "text", "expr": "\nWe compare to Deep Deterministic Policy Gradient (DDPG)  \\citep lillicrap2015continuous\nand Normalized Advantage Functions (NAF)  \\citep gu2016continuous\nas state-of-the-art off-policy learning baselines", "word_idx": 45879, "sentence_idx": 478, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 46083, "sentence_idx": 479, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 46089, "sentence_idx": 480, "label": "unlabeled"}, {"type": "math", "expr": "$$-Q(s,a;\\theta)$$", "word_idx": 46095, "sentence_idx": 481, "label": "unlabeled"}, {"type": "math", "expr": "$$a^{\\star}(s)=\\argmin_{a}-Q(s,a;\\theta)$$", "word_idx": 46109, "sentence_idx": 482, "label": "unlabeled"}, {"type": "math", "expr": "$$Q(s,a;\\theta)$$", "word_idx": 46147, "sentence_idx": 483, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 46160, "sentence_idx": 484, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 46166, "sentence_idx": 485, "label": "unlabeled"}, {"type": "text", "expr": "4 Because there are\nnot official DDPG or NAF implementations or\nresults on the OpenAI gym tasks, we use the Simon Ramstedt\u2019s\nDDPG implementation from  https://github", "word_idx": 46172, "sentence_idx": 486, "label": "unlabeled"}, {"type": "text", "expr": "com/SimonRamstedt/ddpg \nand have re-implemented NAF", "word_idx": 46337, "sentence_idx": 487, "label": "unlabeled"}, {"type": "text", "expr": "https://github", "word_idx": 46388, "sentence_idx": 488, "label": "unlabeled"}, {"type": "text", "expr": "com/SimonRamstedt/ddpg", "word_idx": 46402, "sentence_idx": 489, "label": "unlabeled"}, {"type": "text", "expr": "Task DDPG NAF ICNN", "word_idx": 46424, "sentence_idx": 490, "label": "unlabeled"}, {"type": "text", "expr": "Ant 1000", "word_idx": 46442, "sentence_idx": 491, "label": "unlabeled"}, {"type": "text", "expr": "00 999", "word_idx": 46450, "sentence_idx": 492, "label": "unlabeled"}, {"type": "text", "expr": "03 1056", "word_idx": 46456, "sentence_idx": 493, "label": "unlabeled"}, {"type": "text", "expr": "HalfCheetah 2909", "word_idx": 46463, "sentence_idx": 494, "label": "unlabeled"}, {"type": "text", "expr": "77 2575", "word_idx": 46479, "sentence_idx": 495, "label": "unlabeled"}, {"type": "text", "expr": "16 3822", "word_idx": 46486, "sentence_idx": 496, "label": "unlabeled"}, {"type": "text", "expr": "HalfCheetah", "word_idx": 46493, "sentence_idx": 497, "label": "unlabeled"}, {"type": "text", "expr": "Hopper 1501", "word_idx": 46504, "sentence_idx": 498, "label": "unlabeled"}, {"type": "text", "expr": "33 1100", "word_idx": 46515, "sentence_idx": 499, "label": "unlabeled"}, {"type": "text", "expr": "43 831", "word_idx": 46522, "sentence_idx": 500, "label": "unlabeled"}, {"type": "text", "expr": "Hopper", "word_idx": 46528, "sentence_idx": 501, "label": "unlabeled"}, {"type": "text", "expr": "Humanoid 524", "word_idx": 46534, "sentence_idx": 502, "label": "unlabeled"}, {"type": "text", "expr": "09 5000", "word_idx": 46546, "sentence_idx": 503, "label": "unlabeled"}, {"type": "text", "expr": "68 433", "word_idx": 46553, "sentence_idx": 504, "label": "unlabeled"}, {"type": "text", "expr": "Humanoid", "word_idx": 46559, "sentence_idx": 505, "label": "unlabeled"}, {"type": "text", "expr": "HumanoidStandup 134265", "word_idx": 46567, "sentence_idx": 506, "label": "unlabeled"}, {"type": "text", "expr": "96 116399", "word_idx": 46589, "sentence_idx": 507, "label": "unlabeled"}, {"type": "text", "expr": "05 141217", "word_idx": 46598, "sentence_idx": 508, "label": "unlabeled"}, {"type": "text", "expr": "HumanoidStandup", "word_idx": 46607, "sentence_idx": 509, "label": "unlabeled"}, {"type": "text", "expr": "134265", "word_idx": 46622, "sentence_idx": 510, "label": "unlabeled"}, {"type": "text", "expr": "116399", "word_idx": 46628, "sentence_idx": 511, "label": "unlabeled"}, {"type": "text", "expr": "141217", "word_idx": 46634, "sentence_idx": 512, "label": "unlabeled"}, {"type": "text", "expr": "141217", "word_idx": 46640, "sentence_idx": 513, "label": "unlabeled"}, {"type": "text", "expr": "InvDoubPend 9358", "word_idx": 46646, "sentence_idx": 514, "label": "unlabeled"}, {"type": "text", "expr": "81 9359", "word_idx": 46662, "sentence_idx": 515, "label": "unlabeled"}, {"type": "text", "expr": "59 9359", "word_idx": 46669, "sentence_idx": 516, "label": "unlabeled"}, {"type": "text", "expr": "InvDoubPend", "word_idx": 46676, "sentence_idx": 517, "label": "unlabeled"}, {"type": "text", "expr": "InvPend 1000", "word_idx": 46687, "sentence_idx": 518, "label": "unlabeled"}, {"type": "text", "expr": "00 1000", "word_idx": 46699, "sentence_idx": 519, "label": "unlabeled"}, {"type": "text", "expr": "00 1000", "word_idx": 46706, "sentence_idx": 520, "label": "unlabeled"}, {"type": "text", "expr": "InvPend", "word_idx": 46713, "sentence_idx": 521, "label": "unlabeled"}, {"type": "text", "expr": "Reacher -6", "word_idx": 46720, "sentence_idx": 522, "label": "unlabeled"}, {"type": "text", "expr": "10 -6", "word_idx": 46730, "sentence_idx": 523, "label": "unlabeled"}, {"type": "text", "expr": "31 -5", "word_idx": 46735, "sentence_idx": 524, "label": "unlabeled"}, {"type": "text", "expr": "Reacher", "word_idx": 46740, "sentence_idx": 525, "label": "unlabeled"}, {"type": "text", "expr": "Swimmer 49", "word_idx": 46747, "sentence_idx": 526, "label": "unlabeled"}, {"type": "text", "expr": "79 69", "word_idx": 46757, "sentence_idx": 527, "label": "unlabeled"}, {"type": "text", "expr": "71 64", "word_idx": 46762, "sentence_idx": 528, "label": "unlabeled"}, {"type": "text", "expr": "Swimmer", "word_idx": 46767, "sentence_idx": 529, "label": "unlabeled"}, {"type": "text", "expr": "Walker2d 1604", "word_idx": 46774, "sentence_idx": 530, "label": "unlabeled"}, {"type": "text", "expr": "18 1007", "word_idx": 46787, "sentence_idx": 531, "label": "unlabeled"}, {"type": "text", "expr": "25 298", "word_idx": 46794, "sentence_idx": 532, "label": "unlabeled"}, {"type": "text", "expr": "Walker2d", "word_idx": 46800, "sentence_idx": 533, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a03:  Maximum test reward for ICNN algorithm versus alternatives on several\nOpenAI Gym tasks", "word_idx": 46808, "sentence_idx": 534, "label": "unlabeled"}, {"type": "text", "expr": " (All tasks are v1", "word_idx": 46904, "sentence_idx": 535, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a03:", "word_idx": 46922, "sentence_idx": 536, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a0 3  shows the maximum test reward achieved by the\ndifferent algorithms on these tasks", "word_idx": 46930, "sentence_idx": 537, "label": "unlabeled"}, {"type": "text", "expr": " Although no method strictly dominates the\nothers, the ICNN approach has some clear advantages on tasks like HalfCheetah,\nReacher, and HumanoidStandup, and performs comparably on many other tasks,\nthough with also a few notable poor performances in Hopper and Walker2D", "word_idx": 47021, "sentence_idx": 538, "label": "unlabeled"}, {"type": "text", "expr": "\nNonetheless, given the strong baseline, and the fact that the method is\nliterally just a drop-in replacement for a function approximator in Q-learning,\nthese results are overall positive", "word_idx": 47289, "sentence_idx": 539, "label": "unlabeled"}, {"type": "text", "expr": "\nNAF poses a particularly interesting comparison point to ICNNs", "word_idx": 47476, "sentence_idx": 540, "label": "unlabeled"}, {"type": "text", "expr": " In particular,\nNAF decomposes the  $Q$  function in terms of the value function an an advantage\nfunction  $Q(s,a)=V(s)+A(s,a)$  where the advantage function is restricted to\nbe  concave quadratic  in the actions, and thus always has a closed-form\nsolution", "word_idx": 47539, "sentence_idx": 541, "label": "unlabeled"}, {"type": "text", "expr": " In a sense, this closely mirrors the setup of the PICNN architecture:\nlike NAF, we have a separate non-convex path for the  $s$  variables, and an\noverall function that is convex in  $a$ ; however, the distinction is that while\nNAF requires that the convex portion be quadratic, the ICNN architecture allows\nany convex functional form", "word_idx": 47795, "sentence_idx": 542, "label": "unlabeled"}, {"type": "text", "expr": " As our experiments show, this representational\npower does allow for better performance of the resulting system, though the\ntrade-off, of course, is that determining the optimal action in an ICNN is\nsubstantially more computationally complex than for a quadratic", "word_idx": 48130, "sentence_idx": 543, "label": "unlabeled"}, {"type": "math", "expr": "$$Q(s,a)=V(s)+A(s,a)$$", "word_idx": 48392, "sentence_idx": 544, "label": "unlabeled"}, {"type": "text", "expr": "concave quadratic", "word_idx": 48410, "sentence_idx": 545, "label": "unlabeled"}, {"type": "text", "expr": "7  Conclusion and future work", "word_idx": 48427, "sentence_idx": 546, "label": "unlabeled"}, {"type": "text", "expr": "This paper laid the groundwork for the input convex neural network model", "word_idx": 48456, "sentence_idx": 547, "label": "unlabeled"}, {"type": "text", "expr": " By\nincorporating relatively simple constraints into existing network architectures,\nwe can fit very general convex functions and the apply optimization as an\ninference procedure", "word_idx": 48528, "sentence_idx": 548, "label": "unlabeled"}, {"type": "text", "expr": " Since many existing models already fit into this overall\nframework (e", "word_idx": 48706, "sentence_idx": 549, "label": "unlabeled"}, {"type": "text", "expr": ", CRF models perform an optimization over an output space where\nparameters are given by the output of a neural network), the proposed method\npresents an extension where the entire inference procedure is \u201clearned\u201d along\nwith the network itself, without the need for explicitly building typical\nstructured prediction architectures", "word_idx": 48776, "sentence_idx": 550, "label": "unlabeled"}, {"type": "text", "expr": " This work explored only a small subset of\nthe possible applications of these network, and the networks offer promising\ndirections for many additional domains", "word_idx": 49104, "sentence_idx": 551, "label": "unlabeled"}, {"type": "text", "expr": "This paper laid the groundwork for the input convex neural network model", "word_idx": 49262, "sentence_idx": 552, "label": "unlabeled"}, {"type": "text", "expr": " By\nincorporating relatively simple constraints into existing network architectures,\nwe can fit very general convex functions and the apply optimization as an\ninference procedure", "word_idx": 49334, "sentence_idx": 553, "label": "unlabeled"}, {"type": "text", "expr": " Since many existing models already fit into this overall\nframework (e", "word_idx": 49512, "sentence_idx": 554, "label": "unlabeled"}, {"type": "text", "expr": ", CRF models perform an optimization over an output space where\nparameters are given by the output of a neural network), the proposed method\npresents an extension where the entire inference procedure is \u201clearned\u201d along\nwith the network itself, without the need for explicitly building typical\nstructured prediction architectures", "word_idx": 49582, "sentence_idx": 555, "label": "unlabeled"}, {"type": "text", "expr": " This work explored only a small subset of\nthe possible applications of these network, and the networks offer promising\ndirections for many additional domains", "word_idx": 49910, "sentence_idx": 556, "label": "unlabeled"}, {"type": "text", "expr": "Acknowledgments", "word_idx": 50068, "sentence_idx": 557, "label": "unlabeled"}, {"type": "text", "expr": "BA is supported by the National Science Foundation Graduate Research Fellowship\nProgram under Grant No", "word_idx": 50083, "sentence_idx": 558, "label": "unlabeled"}, {"type": "text", "expr": " DGE1252522", "word_idx": 50185, "sentence_idx": 559, "label": "unlabeled"}, {"type": "text", "expr": "\nWe also thank David Belanger for helpful discussions", "word_idx": 50196, "sentence_idx": 560, "label": "unlabeled"}, {"type": "text", "expr": "BA is supported by the National Science Foundation Graduate Research Fellowship\nProgram under Grant No", "word_idx": 50249, "sentence_idx": 561, "label": "unlabeled"}, {"type": "text", "expr": " DGE1252522", "word_idx": 50351, "sentence_idx": 562, "label": "unlabeled"}, {"type": "text", "expr": "\nWe also thank David Belanger for helpful discussions", "word_idx": 50362, "sentence_idx": 563, "label": "unlabeled"}, {"type": "text", "expr": "References", "word_idx": 50415, "sentence_idx": 564, "label": "unlabeled"}, {"type": "text", "expr": "Abadi et\u00a0al", "word_idx": 50425, "sentence_idx": 565, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,\nCorrado, Davis, Dean, Devin, et\u00a0al", "word_idx": 50436, "sentence_idx": 566, "label": "unlabeled"}, {"type": "text", "expr": " \nAbadi, Mart\u0131n, Agarwal, Ashish, Barham, Paul, Brevdo, Eugene, Chen, Zhifeng,\nCitro, Craig, Corrado, Greg\u00a0S, Davis, Andy, Dean, Jeffrey, Devin, Matthieu,\net\u00a0al", "word_idx": 50521, "sentence_idx": 567, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Tensorflow: Large-scale machine learning on heterogeneous distributed\nsystems", "word_idx": 50681, "sentence_idx": 568, "label": "unlabeled"}, {"type": "text", "expr": "Abadi et\u00a0al", "word_idx": 50761, "sentence_idx": 569, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,\nCorrado, Davis, Dean, Devin, et\u00a0al", "word_idx": 50772, "sentence_idx": 570, "label": "unlabeled"}, {"type": "text", "expr": "Abadi, Mart\u0131n, Agarwal, Ashish, Barham, Paul, Brevdo, Eugene, Chen, Zhifeng,\nCitro, Craig, Corrado, Greg\u00a0S, Davis, Andy, Dean, Jeffrey, Devin, Matthieu,\net\u00a0al", "word_idx": 50857, "sentence_idx": 571, "label": "unlabeled"}, {"type": "text", "expr": "Tensorflow: Large-scale machine learning on heterogeneous distributed\nsystems", "word_idx": 51015, "sentence_idx": 572, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1603", "word_idx": 51092, "sentence_idx": 573, "label": "unlabeled"}, {"type": "text", "expr": "04467 , 2016", "word_idx": 51117, "sentence_idx": 574, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1603", "word_idx": 51129, "sentence_idx": 575, "label": "unlabeled"}, {"type": "text", "expr": "04467", "word_idx": 51154, "sentence_idx": 576, "label": "unlabeled"}, {"type": "text", "expr": "Barzilai & Borwein(1988)Barzilai and Borwein \nBarzilai, Jonathan and Borwein, Jonathan\u00a0M", "word_idx": 51159, "sentence_idx": 577, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Two-point step size gradient methods", "word_idx": 51247, "sentence_idx": 578, "label": "unlabeled"}, {"type": "text", "expr": "Barzilai & Borwein(1988)Barzilai and Borwein", "word_idx": 51286, "sentence_idx": 579, "label": "unlabeled"}, {"type": "text", "expr": "Barzilai, Jonathan and Borwein, Jonathan\u00a0M", "word_idx": 51330, "sentence_idx": 580, "label": "unlabeled"}, {"type": "text", "expr": "Two-point step size gradient methods", "word_idx": 51372, "sentence_idx": 581, "label": "unlabeled"}, {"type": "text", "expr": "IMA Journal of Numerical Analysis , 8(1):141\u2013148, 1988", "word_idx": 51408, "sentence_idx": 582, "label": "unlabeled"}, {"type": "text", "expr": "IMA Journal of Numerical Analysis", "word_idx": 51462, "sentence_idx": 583, "label": "unlabeled"}, {"type": "text", "expr": "Belanger & McCallum(2016)Belanger and\nMcCallum \nBelanger, David and McCallum, Andrew", "word_idx": 51495, "sentence_idx": 584, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Structured prediction energy networks", "word_idx": 51579, "sentence_idx": 585, "label": "unlabeled"}, {"type": "text", "expr": "Belanger & McCallum(2016)Belanger and\nMcCallum", "word_idx": 51619, "sentence_idx": 586, "label": "unlabeled"}, {"type": "text", "expr": "Belanger, David and McCallum, Andrew", "word_idx": 51665, "sentence_idx": 587, "label": "unlabeled"}, {"type": "text", "expr": "Structured prediction energy networks", "word_idx": 51701, "sentence_idx": 588, "label": "unlabeled"}, {"type": "text", "expr": "In  Proceedings of the International Conference on Machine\nLearning , 2016", "word_idx": 51738, "sentence_idx": 589, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the International Conference on Machine\nLearning", "word_idx": 51812, "sentence_idx": 590, "label": "unlabeled"}, {"type": "text", "expr": "Bengio et\u00a0al", "word_idx": 51875, "sentence_idx": 591, "label": "unlabeled"}, {"type": "text", "expr": "(1994)Bengio, LeCun, and\nHenderson \nBengio, Yoshua, LeCun, Yann, and Henderson, Donnie", "word_idx": 51887, "sentence_idx": 592, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Globally trained handwritten word recognizer using spatial\nrepresentation, convolutional neural networks, and hidden markov models", "word_idx": 51973, "sentence_idx": 593, "label": "unlabeled"}, {"type": "text", "expr": "Bengio et\u00a0al", "word_idx": 52106, "sentence_idx": 594, "label": "unlabeled"}, {"type": "text", "expr": "(1994)Bengio, LeCun, and\nHenderson", "word_idx": 52118, "sentence_idx": 595, "label": "unlabeled"}, {"type": "text", "expr": "Bengio, Yoshua, LeCun, Yann, and Henderson, Donnie", "word_idx": 52152, "sentence_idx": 596, "label": "unlabeled"}, {"type": "text", "expr": "Globally trained handwritten word recognizer using spatial\nrepresentation, convolutional neural networks, and hidden markov models", "word_idx": 52202, "sentence_idx": 597, "label": "unlabeled"}, {"type": "text", "expr": "Advances in neural information processing systems , pp", "word_idx": 52332, "sentence_idx": 598, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0937\u2013937, 1994", "word_idx": 52386, "sentence_idx": 599, "label": "unlabeled"}, {"type": "text", "expr": "Advances in neural information processing systems", "word_idx": 52400, "sentence_idx": 600, "label": "unlabeled"}, {"type": "text", "expr": "Bertsekas(1982) \nBertsekas, Dimitri\u00a0P", "word_idx": 52449, "sentence_idx": 601, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Projected newton methods for optimization problems with simple\nconstraints", "word_idx": 52486, "sentence_idx": 602, "label": "unlabeled"}, {"type": "text", "expr": "Bertsekas(1982)", "word_idx": 52563, "sentence_idx": 603, "label": "unlabeled"}, {"type": "text", "expr": "Bertsekas, Dimitri\u00a0P", "word_idx": 52578, "sentence_idx": 604, "label": "unlabeled"}, {"type": "text", "expr": "Projected newton methods for optimization problems with simple\nconstraints", "word_idx": 52598, "sentence_idx": 605, "label": "unlabeled"}, {"type": "text", "expr": "SIAM Journal on control and Optimization , 20(2):221\u2013246, 1982", "word_idx": 52672, "sentence_idx": 606, "label": "unlabeled"}, {"type": "text", "expr": "SIAM Journal on control and Optimization", "word_idx": 52734, "sentence_idx": 607, "label": "unlabeled"}, {"type": "text", "expr": "Birgin et\u00a0al", "word_idx": 52774, "sentence_idx": 608, "label": "unlabeled"}, {"type": "text", "expr": "(2000)Birgin, Mart\u00ednez, and\nRaydan \nBirgin, Ernesto\u00a0G, Mart\u00ednez, Jos\u00e9\u00a0Mario, and Raydan, Marcos", "word_idx": 52786, "sentence_idx": 609, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Nonmonotone spectral projected gradient methods on convex sets", "word_idx": 52881, "sentence_idx": 610, "label": "unlabeled"}, {"type": "text", "expr": "Birgin et\u00a0al", "word_idx": 52946, "sentence_idx": 611, "label": "unlabeled"}, {"type": "text", "expr": "(2000)Birgin, Mart\u00ednez, and\nRaydan", "word_idx": 52958, "sentence_idx": 612, "label": "unlabeled"}, {"type": "text", "expr": "Birgin, Ernesto\u00a0G, Mart\u00ednez, Jos\u00e9\u00a0Mario, and Raydan, Marcos", "word_idx": 52992, "sentence_idx": 613, "label": "unlabeled"}, {"type": "text", "expr": "Nonmonotone spectral projected gradient methods on convex sets", "word_idx": 53051, "sentence_idx": 614, "label": "unlabeled"}, {"type": "text", "expr": "SIAM Journal on Optimization , 10(4):1196\u20131211, 2000", "word_idx": 53113, "sentence_idx": 615, "label": "unlabeled"}, {"type": "text", "expr": "SIAM Journal on Optimization", "word_idx": 53165, "sentence_idx": 616, "label": "unlabeled"}, {"type": "text", "expr": "Boyd & Vandenberghe(2004)Boyd and Vandenberghe \nBoyd, Stephen and Vandenberghe, Lieven", "word_idx": 53193, "sentence_idx": 617, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Cambridge university press, 2004", "word_idx": 53279, "sentence_idx": 618, "label": "unlabeled"}, {"type": "text", "expr": "Boyd & Vandenberghe(2004)Boyd and Vandenberghe", "word_idx": 53314, "sentence_idx": 619, "label": "unlabeled"}, {"type": "text", "expr": "Boyd, Stephen and Vandenberghe, Lieven", "word_idx": 53360, "sentence_idx": 620, "label": "unlabeled"}, {"type": "text", "expr": "Convex optimization ", "word_idx": 53398, "sentence_idx": 621, "label": "unlabeled"}, {"type": "text", "expr": "Convex optimization", "word_idx": 53418, "sentence_idx": 622, "label": "unlabeled"}, {"type": "text", "expr": "Cambridge university press, 2004", "word_idx": 53437, "sentence_idx": 623, "label": "unlabeled"}, {"type": "text", "expr": "Boyd et\u00a0al", "word_idx": 53469, "sentence_idx": 624, "label": "unlabeled"}, {"type": "text", "expr": "(2011)Boyd, Parikh, Chu, Peleato, and\nEckstein \nBoyd, Stephen, Parikh, Neal, Chu, Eric, Peleato, Borja, and Eckstein, Jonathan", "word_idx": 53479, "sentence_idx": 625, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Distributed optimization and statistical learning via the alternating\ndirection method of multipliers", "word_idx": 53605, "sentence_idx": 626, "label": "unlabeled"}, {"type": "text", "expr": "Boyd et\u00a0al", "word_idx": 53709, "sentence_idx": 627, "label": "unlabeled"}, {"type": "text", "expr": "(2011)Boyd, Parikh, Chu, Peleato, and\nEckstein", "word_idx": 53719, "sentence_idx": 628, "label": "unlabeled"}, {"type": "text", "expr": "Boyd, Stephen, Parikh, Neal, Chu, Eric, Peleato, Borja, and Eckstein, Jonathan", "word_idx": 53765, "sentence_idx": 629, "label": "unlabeled"}, {"type": "text", "expr": "Distributed optimization and statistical learning via the alternating\ndirection method of multipliers", "word_idx": 53843, "sentence_idx": 630, "label": "unlabeled"}, {"type": "text", "expr": "Foundations and Trends\u00ae in Machine Learning ,\n3(1):1\u2013122, 2011", "word_idx": 53944, "sentence_idx": 631, "label": "unlabeled"}, {"type": "text", "expr": "Foundations and Trends\u00ae in Machine Learning", "word_idx": 54006, "sentence_idx": 632, "label": "unlabeled"}, {"type": "text", "expr": "Brockman et\u00a0al", "word_idx": 54049, "sentence_idx": 633, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Brockman, Cheung, Pettersson, Schneider,\nSchulman, Tang, and Zaremba \nBrockman, Greg, Cheung, Vicki, Pettersson, Ludwig, Schneider, Jonas, Schulman,\nJohn, Tang, Jie, and Zaremba, Wojciech", "word_idx": 54063, "sentence_idx": 634, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Openai gym", "word_idx": 54256, "sentence_idx": 635, "label": "unlabeled"}, {"type": "text", "expr": "Brockman et\u00a0al", "word_idx": 54269, "sentence_idx": 636, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Brockman, Cheung, Pettersson, Schneider,\nSchulman, Tang, and Zaremba", "word_idx": 54283, "sentence_idx": 637, "label": "unlabeled"}, {"type": "text", "expr": "Brockman, Greg, Cheung, Vicki, Pettersson, Ludwig, Schneider, Jonas, Schulman,\nJohn, Tang, Jie, and Zaremba, Wojciech", "word_idx": 54357, "sentence_idx": 638, "label": "unlabeled"}, {"type": "text", "expr": "Openai gym", "word_idx": 54474, "sentence_idx": 639, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1606", "word_idx": 54484, "sentence_idx": 640, "label": "unlabeled"}, {"type": "text", "expr": "01540 , 2016", "word_idx": 54509, "sentence_idx": 641, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1606", "word_idx": 54521, "sentence_idx": 642, "label": "unlabeled"}, {"type": "text", "expr": "01540", "word_idx": 54546, "sentence_idx": 643, "label": "unlabeled"}, {"type": "text", "expr": "Chen et\u00a0al", "word_idx": 54551, "sentence_idx": 644, "label": "unlabeled"}, {"type": "text", "expr": "(2015)Chen, Schwing, Yuille, and Urtasun \nChen, Liang-Chieh, Schwing, Alexander\u00a0G, Yuille, Alan\u00a0L, and Urtasun, Raquel", "word_idx": 54561, "sentence_idx": 645, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Learning deep structured models", "word_idx": 54679, "sentence_idx": 646, "label": "unlabeled"}, {"type": "text", "expr": "Chen et\u00a0al", "word_idx": 54713, "sentence_idx": 647, "label": "unlabeled"}, {"type": "text", "expr": "(2015)Chen, Schwing, Yuille, and Urtasun", "word_idx": 54723, "sentence_idx": 648, "label": "unlabeled"}, {"type": "text", "expr": "Chen, Liang-Chieh, Schwing, Alexander\u00a0G, Yuille, Alan\u00a0L, and Urtasun, Raquel", "word_idx": 54763, "sentence_idx": 649, "label": "unlabeled"}, {"type": "text", "expr": "Learning deep structured models", "word_idx": 54839, "sentence_idx": 650, "label": "unlabeled"}, {"type": "text", "expr": "In  Proceedings of the International Conference on Machine\nLearning , 2015", "word_idx": 54870, "sentence_idx": 651, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the International Conference on Machine\nLearning", "word_idx": 54944, "sentence_idx": 652, "label": "unlabeled"}, {"type": "text", "expr": "Domke(2012) \nDomke, Justin", "word_idx": 55007, "sentence_idx": 653, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Generic methods for optimization-based modeling", "word_idx": 55033, "sentence_idx": 654, "label": "unlabeled"}, {"type": "text", "expr": "Domke(2012)", "word_idx": 55083, "sentence_idx": 655, "label": "unlabeled"}, {"type": "text", "expr": "Domke, Justin", "word_idx": 55094, "sentence_idx": 656, "label": "unlabeled"}, {"type": "text", "expr": "Generic methods for optimization-based modeling", "word_idx": 55107, "sentence_idx": 657, "label": "unlabeled"}, {"type": "text", "expr": "In  Proceedings of the Conference on AI and Statistics , pp", "word_idx": 55154, "sentence_idx": 658, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0318\u2013326, 2012", "word_idx": 55213, "sentence_idx": 659, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the Conference on AI and Statistics", "word_idx": 55227, "sentence_idx": 660, "label": "unlabeled"}, {"type": "text", "expr": "Duchi et\u00a0al", "word_idx": 55277, "sentence_idx": 661, "label": "unlabeled"}, {"type": "text", "expr": "(2011)Duchi, Hazan, and Singer \nDuchi, John, Hazan, Elad, and Singer, Yoram", "word_idx": 55288, "sentence_idx": 662, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Adaptive subgradient methods for online learning and stochastic\noptimization", "word_idx": 55363, "sentence_idx": 663, "label": "unlabeled"}, {"type": "text", "expr": "Duchi et\u00a0al", "word_idx": 55442, "sentence_idx": 664, "label": "unlabeled"}, {"type": "text", "expr": "(2011)Duchi, Hazan, and Singer", "word_idx": 55453, "sentence_idx": 665, "label": "unlabeled"}, {"type": "text", "expr": "Duchi, John, Hazan, Elad, and Singer, Yoram", "word_idx": 55483, "sentence_idx": 666, "label": "unlabeled"}, {"type": "text", "expr": "Adaptive subgradient methods for online learning and stochastic\noptimization", "word_idx": 55526, "sentence_idx": 667, "label": "unlabeled"}, {"type": "text", "expr": "The Journal of Machine Learning Research , 12:2121\u20132159, 2011", "word_idx": 55602, "sentence_idx": 668, "label": "unlabeled"}, {"type": "text", "expr": "The Journal of Machine Learning Research", "word_idx": 55663, "sentence_idx": 669, "label": "unlabeled"}, {"type": "text", "expr": "Goodfellow et\u00a0al", "word_idx": 55703, "sentence_idx": 670, "label": "unlabeled"}, {"type": "text", "expr": "(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,\nWarde-Farley, Ozair, Courville, and Bengio \nGoodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley,\nDavid, Ozair, Sherjil, Courville, Aaron, and Bengio, Yoshua", "word_idx": 55719, "sentence_idx": 671, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Generative adversarial nets", "word_idx": 55942, "sentence_idx": 672, "label": "unlabeled"}, {"type": "text", "expr": "Goodfellow et\u00a0al", "word_idx": 55972, "sentence_idx": 673, "label": "unlabeled"}, {"type": "text", "expr": "(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,\nWarde-Farley, Ozair, Courville, and Bengio", "word_idx": 55988, "sentence_idx": 674, "label": "unlabeled"}, {"type": "text", "expr": "Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley,\nDavid, Ozair, Sherjil, Courville, Aaron, and Bengio, Yoshua", "word_idx": 56074, "sentence_idx": 675, "label": "unlabeled"}, {"type": "text", "expr": "Generative adversarial nets", "word_idx": 56209, "sentence_idx": 676, "label": "unlabeled"}, {"type": "text", "expr": "In  Advances in Neural Information Processing Systems , pp", "word_idx": 56236, "sentence_idx": 677, "label": "unlabeled"}, {"type": "text", "expr": "\u00a02672\u20132680, 2014", "word_idx": 56294, "sentence_idx": 678, "label": "unlabeled"}, {"type": "text", "expr": "Advances in Neural Information Processing Systems", "word_idx": 56310, "sentence_idx": 679, "label": "unlabeled"}, {"type": "text", "expr": "Gu et\u00a0al", "word_idx": 56359, "sentence_idx": 680, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Gu, Lillicrap, Sutskever, and Levine \nGu, Shixiang, Lillicrap, Timothy, Sutskever, Ilya, and Levine, Sergey", "word_idx": 56367, "sentence_idx": 681, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Continuous deep q-learning with model-based acceleration", "word_idx": 56480, "sentence_idx": 682, "label": "unlabeled"}, {"type": "text", "expr": "Gu et\u00a0al", "word_idx": 56539, "sentence_idx": 683, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Gu, Lillicrap, Sutskever, and Levine", "word_idx": 56547, "sentence_idx": 684, "label": "unlabeled"}, {"type": "text", "expr": "Gu, Shixiang, Lillicrap, Timothy, Sutskever, Ilya, and Levine, Sergey", "word_idx": 56589, "sentence_idx": 685, "label": "unlabeled"}, {"type": "text", "expr": "Continuous deep q-learning with model-based acceleration", "word_idx": 56658, "sentence_idx": 686, "label": "unlabeled"}, {"type": "text", "expr": "In  Proceedings of the International Conference on Machine\nLearning , 2016", "word_idx": 56714, "sentence_idx": 687, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the International Conference on Machine\nLearning", "word_idx": 56788, "sentence_idx": 688, "label": "unlabeled"}, {"type": "text", "expr": "He et\u00a0al", "word_idx": 56851, "sentence_idx": 689, "label": "unlabeled"}, {"type": "text", "expr": "(2015)He, Zhang, Ren, and Sun \nHe, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian", "word_idx": 56859, "sentence_idx": 690, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Deep residual learning for image recognition", "word_idx": 56947, "sentence_idx": 691, "label": "unlabeled"}, {"type": "text", "expr": "He et\u00a0al", "word_idx": 56994, "sentence_idx": 692, "label": "unlabeled"}, {"type": "text", "expr": "(2015)He, Zhang, Ren, and Sun", "word_idx": 57002, "sentence_idx": 693, "label": "unlabeled"}, {"type": "text", "expr": "He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian", "word_idx": 57031, "sentence_idx": 694, "label": "unlabeled"}, {"type": "text", "expr": "Deep residual learning for image recognition", "word_idx": 57088, "sentence_idx": 695, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1512", "word_idx": 57132, "sentence_idx": 696, "label": "unlabeled"}, {"type": "text", "expr": "03385 , 2015", "word_idx": 57157, "sentence_idx": 697, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1512", "word_idx": 57169, "sentence_idx": 698, "label": "unlabeled"}, {"type": "text", "expr": "03385", "word_idx": 57194, "sentence_idx": 699, "label": "unlabeled"}, {"type": "text", "expr": "Huang et\u00a0al", "word_idx": 57199, "sentence_idx": 700, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Huang, Liu, and Weinberger \nHuang, Gao, Liu, Zhuang, and Weinberger, Kilian\u00a0Q", "word_idx": 57210, "sentence_idx": 701, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Densely connected convolutional networks", "word_idx": 57293, "sentence_idx": 702, "label": "unlabeled"}, {"type": "text", "expr": "Huang et\u00a0al", "word_idx": 57336, "sentence_idx": 703, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Huang, Liu, and Weinberger", "word_idx": 57347, "sentence_idx": 704, "label": "unlabeled"}, {"type": "text", "expr": "Huang, Gao, Liu, Zhuang, and Weinberger, Kilian\u00a0Q", "word_idx": 57379, "sentence_idx": 705, "label": "unlabeled"}, {"type": "text", "expr": "Densely connected convolutional networks", "word_idx": 57428, "sentence_idx": 706, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1608", "word_idx": 57468, "sentence_idx": 707, "label": "unlabeled"}, {"type": "text", "expr": "06993 , 2016", "word_idx": 57493, "sentence_idx": 708, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1608", "word_idx": 57505, "sentence_idx": 709, "label": "unlabeled"}, {"type": "text", "expr": "06993", "word_idx": 57530, "sentence_idx": 710, "label": "unlabeled"}, {"type": "text", "expr": "Ioffe & Szegedy(2015)Ioffe and Szegedy \nIoffe, Sergey and Szegedy, Christian", "word_idx": 57535, "sentence_idx": 711, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift", "word_idx": 57611, "sentence_idx": 712, "label": "unlabeled"}, {"type": "text", "expr": "Ioffe & Szegedy(2015)Ioffe and Szegedy", "word_idx": 57706, "sentence_idx": 713, "label": "unlabeled"}, {"type": "text", "expr": "Ioffe, Sergey and Szegedy, Christian", "word_idx": 57744, "sentence_idx": 714, "label": "unlabeled"}, {"type": "text", "expr": "Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift", "word_idx": 57780, "sentence_idx": 715, "label": "unlabeled"}, {"type": "text", "expr": "In  Proceedings of The 32nd International Conference on Machine\nLearning , pp", "word_idx": 57872, "sentence_idx": 716, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0448\u2013456, 2015", "word_idx": 57949, "sentence_idx": 717, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of The 32nd International Conference on Machine\nLearning", "word_idx": 57963, "sentence_idx": 718, "label": "unlabeled"}, {"type": "text", "expr": "Katakis et\u00a0al", "word_idx": 58031, "sentence_idx": 719, "label": "unlabeled"}, {"type": "text", "expr": "(2008)Katakis, Tsoumakas, and\nVlahavas \nKatakis, Ioannis, Tsoumakas, Grigorios, and Vlahavas, Ioannis", "word_idx": 58044, "sentence_idx": 720, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Multilabel text classification for automated tag suggestion", "word_idx": 58145, "sentence_idx": 721, "label": "unlabeled"}, {"type": "text", "expr": "Katakis et\u00a0al", "word_idx": 58207, "sentence_idx": 722, "label": "unlabeled"}, {"type": "text", "expr": "(2008)Katakis, Tsoumakas, and\nVlahavas", "word_idx": 58220, "sentence_idx": 723, "label": "unlabeled"}, {"type": "text", "expr": "Katakis, Ioannis, Tsoumakas, Grigorios, and Vlahavas, Ioannis", "word_idx": 58258, "sentence_idx": 724, "label": "unlabeled"}, {"type": "text", "expr": "Multilabel text classification for automated tag suggestion", "word_idx": 58319, "sentence_idx": 725, "label": "unlabeled"}, {"type": "text", "expr": "ECML PKDD discovery challenge , 75, 2008", "word_idx": 58378, "sentence_idx": 726, "label": "unlabeled"}, {"type": "text", "expr": "ECML PKDD discovery challenge", "word_idx": 58418, "sentence_idx": 727, "label": "unlabeled"}, {"type": "text", "expr": "Kingma & Ba(2014)Kingma and Ba \nKingma, Diederik and Ba, Jimmy", "word_idx": 58447, "sentence_idx": 728, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Adam: A method for stochastic optimization", "word_idx": 58509, "sentence_idx": 729, "label": "unlabeled"}, {"type": "text", "expr": "Kingma & Ba(2014)Kingma and Ba", "word_idx": 58554, "sentence_idx": 730, "label": "unlabeled"}, {"type": "text", "expr": "Kingma, Diederik and Ba, Jimmy", "word_idx": 58584, "sentence_idx": 731, "label": "unlabeled"}, {"type": "text", "expr": "Adam: A method for stochastic optimization", "word_idx": 58614, "sentence_idx": 732, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1412", "word_idx": 58656, "sentence_idx": 733, "label": "unlabeled"}, {"type": "text", "expr": "6980 , 2014", "word_idx": 58681, "sentence_idx": 734, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1412", "word_idx": 58692, "sentence_idx": 735, "label": "unlabeled"}, {"type": "text", "expr": "Koller & Friedman(2009)Koller and Friedman \nKoller, Daphne and Friedman, Nir", "word_idx": 58717, "sentence_idx": 736, "label": "unlabeled"}, {"type": "text", "expr": "\n\n MIT press, 2009", "word_idx": 58793, "sentence_idx": 737, "label": "unlabeled"}, {"type": "text", "expr": "Koller & Friedman(2009)Koller and Friedman", "word_idx": 58811, "sentence_idx": 738, "label": "unlabeled"}, {"type": "text", "expr": "Koller, Daphne and Friedman, Nir", "word_idx": 58853, "sentence_idx": 739, "label": "unlabeled"}, {"type": "text", "expr": "Probabilistic graphical models: principles and techniques ", "word_idx": 58885, "sentence_idx": 740, "label": "unlabeled"}, {"type": "text", "expr": "Probabilistic graphical models: principles and techniques", "word_idx": 58943, "sentence_idx": 741, "label": "unlabeled"}, {"type": "text", "expr": "MIT press, 2009", "word_idx": 59000, "sentence_idx": 742, "label": "unlabeled"}, {"type": "text", "expr": "Krizhevsky et\u00a0al", "word_idx": 59015, "sentence_idx": 743, "label": "unlabeled"}, {"type": "text", "expr": "(2012)Krizhevsky, Sutskever, and\nHinton \nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey\u00a0E", "word_idx": 59031, "sentence_idx": 744, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Imagenet classification with deep convolutional neural networks", "word_idx": 59129, "sentence_idx": 745, "label": "unlabeled"}, {"type": "text", "expr": "Krizhevsky et\u00a0al", "word_idx": 59195, "sentence_idx": 746, "label": "unlabeled"}, {"type": "text", "expr": "(2012)Krizhevsky, Sutskever, and\nHinton", "word_idx": 59211, "sentence_idx": 747, "label": "unlabeled"}, {"type": "text", "expr": "Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey\u00a0E", "word_idx": 59250, "sentence_idx": 748, "label": "unlabeled"}, {"type": "text", "expr": "Imagenet classification with deep convolutional neural networks", "word_idx": 59307, "sentence_idx": 749, "label": "unlabeled"}, {"type": "text", "expr": "In  Advances in neural information processing systems , pp", "word_idx": 59370, "sentence_idx": 750, "label": "unlabeled"}, {"type": "text", "expr": "\u00a01097\u20131105, 2012", "word_idx": 59428, "sentence_idx": 751, "label": "unlabeled"}, {"type": "text", "expr": "Advances in neural information processing systems", "word_idx": 59444, "sentence_idx": 752, "label": "unlabeled"}, {"type": "text", "expr": "LeCun et\u00a0al", "word_idx": 59493, "sentence_idx": 753, "label": "unlabeled"}, {"type": "text", "expr": "(2006)LeCun, Chopra, Hadsell, Ranzato, and\nHuang \nLeCun, Yann, Chopra, Sumit, Hadsell, Raia, Ranzato, M, and Huang, F", "word_idx": 59504, "sentence_idx": 754, "label": "unlabeled"}, {"type": "text", "expr": "\n\n A tutorial on energy-based learning", "word_idx": 59621, "sentence_idx": 755, "label": "unlabeled"}, {"type": "text", "expr": "LeCun et\u00a0al", "word_idx": 59659, "sentence_idx": 756, "label": "unlabeled"}, {"type": "text", "expr": "(2006)LeCun, Chopra, Hadsell, Ranzato, and\nHuang", "word_idx": 59670, "sentence_idx": 757, "label": "unlabeled"}, {"type": "text", "expr": "LeCun, Yann, Chopra, Sumit, Hadsell, Raia, Ranzato, M, and Huang, F", "word_idx": 59718, "sentence_idx": 758, "label": "unlabeled"}, {"type": "text", "expr": "A tutorial on energy-based learning", "word_idx": 59785, "sentence_idx": 759, "label": "unlabeled"}, {"type": "text", "expr": "Predicting structured data , 1:0, 2006", "word_idx": 59820, "sentence_idx": 760, "label": "unlabeled"}, {"type": "text", "expr": "Predicting structured data", "word_idx": 59858, "sentence_idx": 761, "label": "unlabeled"}, {"type": "text", "expr": "Lillicrap et\u00a0al", "word_idx": 59884, "sentence_idx": 762, "label": "unlabeled"}, {"type": "text", "expr": "(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,\nSilver, and Wierstra \nLillicrap, Timothy\u00a0P, Hunt, Jonathan\u00a0J, Pritzel, Alexander, Heess, Nicolas,\nErez, Tom, Tassa, Yuval, Silver, David, and Wierstra, Daan", "word_idx": 59899, "sentence_idx": 763, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Continuous control with deep reinforcement learning", "word_idx": 60107, "sentence_idx": 764, "label": "unlabeled"}, {"type": "text", "expr": "Lillicrap et\u00a0al", "word_idx": 60161, "sentence_idx": 765, "label": "unlabeled"}, {"type": "text", "expr": "(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,\nSilver, and Wierstra", "word_idx": 60176, "sentence_idx": 766, "label": "unlabeled"}, {"type": "text", "expr": "Lillicrap, Timothy\u00a0P, Hunt, Jonathan\u00a0J, Pritzel, Alexander, Heess, Nicolas,\nErez, Tom, Tassa, Yuval, Silver, David, and Wierstra, Daan", "word_idx": 60248, "sentence_idx": 767, "label": "unlabeled"}, {"type": "text", "expr": "Continuous control with deep reinforcement learning", "word_idx": 60382, "sentence_idx": 768, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1509", "word_idx": 60433, "sentence_idx": 769, "label": "unlabeled"}, {"type": "text", "expr": "02971 , 2015", "word_idx": 60458, "sentence_idx": 770, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1509", "word_idx": 60470, "sentence_idx": 771, "label": "unlabeled"}, {"type": "text", "expr": "02971", "word_idx": 60495, "sentence_idx": 772, "label": "unlabeled"}, {"type": "text", "expr": "Magnani & Boyd(2009)Magnani and Boyd \nMagnani, Alessandro and Boyd, Stephen\u00a0P", "word_idx": 60500, "sentence_idx": 773, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Convex piecewise-linear fitting", "word_idx": 60577, "sentence_idx": 774, "label": "unlabeled"}, {"type": "text", "expr": "Magnani & Boyd(2009)Magnani and Boyd", "word_idx": 60611, "sentence_idx": 775, "label": "unlabeled"}, {"type": "text", "expr": "Magnani, Alessandro and Boyd, Stephen\u00a0P", "word_idx": 60647, "sentence_idx": 776, "label": "unlabeled"}, {"type": "text", "expr": "Convex piecewise-linear fitting", "word_idx": 60686, "sentence_idx": 777, "label": "unlabeled"}, {"type": "text", "expr": "Optimization and Engineering , 10(1):1\u201317,\n2009", "word_idx": 60717, "sentence_idx": 778, "label": "unlabeled"}, {"type": "text", "expr": "Optimization and Engineering", "word_idx": 60764, "sentence_idx": 779, "label": "unlabeled"}, {"type": "text", "expr": "Mnih et\u00a0al", "word_idx": 60792, "sentence_idx": 780, "label": "unlabeled"}, {"type": "text", "expr": "(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\nGraves, Riedmiller, Fidjeland, Ostrovski, et\u00a0al", "word_idx": 60802, "sentence_idx": 781, "label": "unlabeled"}, {"type": "text", "expr": " \nMnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei\u00a0A, Veness,\nJoel, Bellemare, Marc\u00a0G, Graves, Alex, Riedmiller, Martin, Fidjeland,\nAndreas\u00a0K, Ostrovski, Georg, et\u00a0al", "word_idx": 60907, "sentence_idx": 782, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Human-level control through deep reinforcement learning", "word_idx": 61089, "sentence_idx": 783, "label": "unlabeled"}, {"type": "text", "expr": "Mnih et\u00a0al", "word_idx": 61147, "sentence_idx": 784, "label": "unlabeled"}, {"type": "text", "expr": "(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\nGraves, Riedmiller, Fidjeland, Ostrovski, et\u00a0al", "word_idx": 61157, "sentence_idx": 785, "label": "unlabeled"}, {"type": "text", "expr": "Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei\u00a0A, Veness,\nJoel, Bellemare, Marc\u00a0G, Graves, Alex, Riedmiller, Martin, Fidjeland,\nAndreas\u00a0K, Ostrovski, Georg, et\u00a0al", "word_idx": 61262, "sentence_idx": 786, "label": "unlabeled"}, {"type": "text", "expr": "Human-level control through deep reinforcement learning", "word_idx": 61442, "sentence_idx": 787, "label": "unlabeled"}, {"type": "text", "expr": "Nature , 518(7540):529\u2013533, 2015", "word_idx": 61497, "sentence_idx": 788, "label": "unlabeled"}, {"type": "text", "expr": "Nature", "word_idx": 61529, "sentence_idx": 789, "label": "unlabeled"}, {"type": "text", "expr": "Nair & Hinton(2010)Nair and Hinton \nNair, Vinod and Hinton, Geoffrey\u00a0E", "word_idx": 61535, "sentence_idx": 790, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Rectified linear units improve restricted boltzmann machines", "word_idx": 61605, "sentence_idx": 791, "label": "unlabeled"}, {"type": "text", "expr": "Nair & Hinton(2010)Nair and Hinton", "word_idx": 61668, "sentence_idx": 792, "label": "unlabeled"}, {"type": "text", "expr": "Nair, Vinod and Hinton, Geoffrey\u00a0E", "word_idx": 61702, "sentence_idx": 793, "label": "unlabeled"}, {"type": "text", "expr": "Rectified linear units improve restricted boltzmann machines", "word_idx": 61736, "sentence_idx": 794, "label": "unlabeled"}, {"type": "text", "expr": "In  Proceedings of the 27th International Conference on Machine\nLearning (ICML-10) , pp", "word_idx": 61796, "sentence_idx": 795, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0807\u2013814, 2010", "word_idx": 61883, "sentence_idx": 796, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the 27th International Conference on Machine\nLearning (ICML-10)", "word_idx": 61897, "sentence_idx": 797, "label": "unlabeled"}, {"type": "text", "expr": "Oliphant(2006) \nOliphant, Travis\u00a0E", "word_idx": 61975, "sentence_idx": 798, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Trelgol Publishing USA, 2006", "word_idx": 62009, "sentence_idx": 799, "label": "unlabeled"}, {"type": "text", "expr": "Oliphant(2006)", "word_idx": 62040, "sentence_idx": 800, "label": "unlabeled"}, {"type": "text", "expr": "Oliphant, Travis\u00a0E", "word_idx": 62054, "sentence_idx": 801, "label": "unlabeled"}, {"type": "text", "expr": "A guide to NumPy , volume\u00a01", "word_idx": 62072, "sentence_idx": 802, "label": "unlabeled"}, {"type": "text", "expr": "A guide to NumPy", "word_idx": 62099, "sentence_idx": 803, "label": "unlabeled"}, {"type": "text", "expr": "Trelgol Publishing USA, 2006", "word_idx": 62115, "sentence_idx": 804, "label": "unlabeled"}, {"type": "text", "expr": "Pedregosa et\u00a0al", "word_idx": 62143, "sentence_idx": 805, "label": "unlabeled"}, {"type": "text", "expr": "(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion,\nGrisel, Blondel, Prettenhofer, Weiss, Dubourg, et\u00a0al", "word_idx": 62158, "sentence_idx": 806, "label": "unlabeled"}, {"type": "text", "expr": " \nPedregosa, Fabian, Varoquaux, Ga\u00ebl, Gramfort, Alexandre, Michel, Vincent,\nThirion, Bertrand, Grisel, Olivier, Blondel, Mathieu, Prettenhofer, Peter,\nWeiss, Ron, Dubourg, Vincent, et\u00a0al", "word_idx": 62265, "sentence_idx": 807, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Scikit-learn: Machine learning in python", "word_idx": 62451, "sentence_idx": 808, "label": "unlabeled"}, {"type": "text", "expr": "Pedregosa et\u00a0al", "word_idx": 62494, "sentence_idx": 809, "label": "unlabeled"}, {"type": "text", "expr": "(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion,\nGrisel, Blondel, Prettenhofer, Weiss, Dubourg, et\u00a0al", "word_idx": 62509, "sentence_idx": 810, "label": "unlabeled"}, {"type": "text", "expr": "Pedregosa, Fabian, Varoquaux, Ga\u00ebl, Gramfort, Alexandre, Michel, Vincent,\nThirion, Bertrand, Grisel, Olivier, Blondel, Mathieu, Prettenhofer, Peter,\nWeiss, Ron, Dubourg, Vincent, et\u00a0al", "word_idx": 62616, "sentence_idx": 811, "label": "unlabeled"}, {"type": "text", "expr": "Scikit-learn: Machine learning in python", "word_idx": 62800, "sentence_idx": 812, "label": "unlabeled"}, {"type": "text", "expr": "The Journal of Machine Learning Research , 12:2825\u20132830, 2011", "word_idx": 62840, "sentence_idx": 813, "label": "unlabeled"}, {"type": "text", "expr": "The Journal of Machine Learning Research", "word_idx": 62901, "sentence_idx": 814, "label": "unlabeled"}, {"type": "text", "expr": "Peng et\u00a0al", "word_idx": 62941, "sentence_idx": 815, "label": "unlabeled"}, {"type": "text", "expr": "(2009)Peng, Bo, and Xu \nPeng, Jian, Bo, Liefeng, and Xu, Jinbo", "word_idx": 62951, "sentence_idx": 816, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Conditional neural fields", "word_idx": 63013, "sentence_idx": 817, "label": "unlabeled"}, {"type": "text", "expr": "Peng et\u00a0al", "word_idx": 63041, "sentence_idx": 818, "label": "unlabeled"}, {"type": "text", "expr": "(2009)Peng, Bo, and Xu", "word_idx": 63051, "sentence_idx": 819, "label": "unlabeled"}, {"type": "text", "expr": "Peng, Jian, Bo, Liefeng, and Xu, Jinbo", "word_idx": 63073, "sentence_idx": 820, "label": "unlabeled"}, {"type": "text", "expr": "Conditional neural fields", "word_idx": 63111, "sentence_idx": 821, "label": "unlabeled"}, {"type": "text", "expr": "In  Advances in neural information processing systems , pp", "word_idx": 63136, "sentence_idx": 822, "label": "unlabeled"}, {"type": "text", "expr": "\u00a01419\u20131427, 2009", "word_idx": 63194, "sentence_idx": 823, "label": "unlabeled"}, {"type": "text", "expr": "Advances in neural information processing systems", "word_idx": 63210, "sentence_idx": 824, "label": "unlabeled"}, {"type": "text", "expr": "Polyak(1964) \nPolyak, Boris\u00a0T", "word_idx": 63259, "sentence_idx": 825, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Some methods of speeding up the convergence of iteration methods", "word_idx": 63288, "sentence_idx": 826, "label": "unlabeled"}, {"type": "text", "expr": "Polyak(1964)", "word_idx": 63355, "sentence_idx": 827, "label": "unlabeled"}, {"type": "text", "expr": "Polyak, Boris\u00a0T", "word_idx": 63367, "sentence_idx": 828, "label": "unlabeled"}, {"type": "text", "expr": "Some methods of speeding up the convergence of iteration methods", "word_idx": 63382, "sentence_idx": 829, "label": "unlabeled"}, {"type": "text", "expr": "USSR Computational Mathematics and Mathematical Physics ,\n4(5):1\u201317, 1964", "word_idx": 63446, "sentence_idx": 830, "label": "unlabeled"}, {"type": "text", "expr": "USSR Computational Mathematics and Mathematical Physics", "word_idx": 63519, "sentence_idx": 831, "label": "unlabeled"}, {"type": "text", "expr": "Poon & Domingos(2011)Poon and Domingos \nPoon, Hoifung and Domingos, Pedro", "word_idx": 63574, "sentence_idx": 832, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Sum-product networks: A new deep architecture", "word_idx": 63647, "sentence_idx": 833, "label": "unlabeled"}, {"type": "text", "expr": "Poon & Domingos(2011)Poon and Domingos", "word_idx": 63695, "sentence_idx": 834, "label": "unlabeled"}, {"type": "text", "expr": "Poon, Hoifung and Domingos, Pedro", "word_idx": 63733, "sentence_idx": 835, "label": "unlabeled"}, {"type": "text", "expr": "Sum-product networks: A new deep architecture", "word_idx": 63766, "sentence_idx": 836, "label": "unlabeled"}, {"type": "text", "expr": "In  UAI 2011, Proceedings of the Twenty-Seventh Conference on\nUncertainty in Artificial Intelligence, Barcelona, Spain, July 14-17, 2011 ,\npp", "word_idx": 63811, "sentence_idx": 837, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0337\u2013346, 2011", "word_idx": 63952, "sentence_idx": 838, "label": "unlabeled"}, {"type": "text", "expr": "UAI 2011, Proceedings of the Twenty-Seventh Conference on\nUncertainty in Artificial Intelligence, Barcelona, Spain, July 14-17, 2011", "word_idx": 63966, "sentence_idx": 839, "label": "unlabeled"}, {"type": "text", "expr": "Ratliff et\u00a0al", "word_idx": 64098, "sentence_idx": 840, "label": "unlabeled"}, {"type": "text", "expr": "(2007)Ratliff, Bagnell, and\nZinkevich \nRatliff, Nathan\u00a0D, Bagnell, J\u00a0Andrew, and Zinkevich, Martin", "word_idx": 64111, "sentence_idx": 841, "label": "unlabeled"}, {"type": "text", "expr": "\n\n (Approximate) subgradient methods for structured prediction", "word_idx": 64209, "sentence_idx": 842, "label": "unlabeled"}, {"type": "text", "expr": "Ratliff et\u00a0al", "word_idx": 64271, "sentence_idx": 843, "label": "unlabeled"}, {"type": "text", "expr": "(2007)Ratliff, Bagnell, and\nZinkevich", "word_idx": 64284, "sentence_idx": 844, "label": "unlabeled"}, {"type": "text", "expr": "Ratliff, Nathan\u00a0D, Bagnell, J\u00a0Andrew, and Zinkevich, Martin", "word_idx": 64321, "sentence_idx": 845, "label": "unlabeled"}, {"type": "text", "expr": "(Approximate) subgradient methods for structured prediction", "word_idx": 64380, "sentence_idx": 846, "label": "unlabeled"}, {"type": "text", "expr": "In  International Conference on Artificial Intelligence and\nStatistics , pp", "word_idx": 64439, "sentence_idx": 847, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0380\u2013387, 2007", "word_idx": 64514, "sentence_idx": 848, "label": "unlabeled"}, {"type": "text", "expr": "International Conference on Artificial Intelligence and\nStatistics", "word_idx": 64528, "sentence_idx": 849, "label": "unlabeled"}, {"type": "text", "expr": "Rumelhart et\u00a0al", "word_idx": 64594, "sentence_idx": 850, "label": "unlabeled"}, {"type": "text", "expr": "(1988)Rumelhart, Hinton, and\nWilliams \nRumelhart, David\u00a0E, Hinton, Geoffrey\u00a0E, and Williams, Ronald\u00a0J", "word_idx": 64609, "sentence_idx": 851, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Learning representations by back-propagating errors", "word_idx": 64710, "sentence_idx": 852, "label": "unlabeled"}, {"type": "text", "expr": "Rumelhart et\u00a0al", "word_idx": 64764, "sentence_idx": 853, "label": "unlabeled"}, {"type": "text", "expr": "(1988)Rumelhart, Hinton, and\nWilliams", "word_idx": 64779, "sentence_idx": 854, "label": "unlabeled"}, {"type": "text", "expr": "Rumelhart, David\u00a0E, Hinton, Geoffrey\u00a0E, and Williams, Ronald\u00a0J", "word_idx": 64816, "sentence_idx": 855, "label": "unlabeled"}, {"type": "text", "expr": "Learning representations by back-propagating errors", "word_idx": 64878, "sentence_idx": 856, "label": "unlabeled"}, {"type": "text", "expr": "Cognitive modeling , 5(3):1, 1988", "word_idx": 64929, "sentence_idx": 857, "label": "unlabeled"}, {"type": "text", "expr": "Cognitive modeling", "word_idx": 64962, "sentence_idx": 858, "label": "unlabeled"}, {"type": "text", "expr": "Samaria & Harter(1994)Samaria and\nHarter \nSamaria, Ferdinando\u00a0S and Harter, Andy\u00a0C", "word_idx": 64980, "sentence_idx": 859, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Parameterisation of a stochastic model for human face identification", "word_idx": 65062, "sentence_idx": 860, "label": "unlabeled"}, {"type": "text", "expr": "Samaria & Harter(1994)Samaria and\nHarter", "word_idx": 65133, "sentence_idx": 861, "label": "unlabeled"}, {"type": "text", "expr": "Samaria, Ferdinando\u00a0S and Harter, Andy\u00a0C", "word_idx": 65173, "sentence_idx": 862, "label": "unlabeled"}, {"type": "text", "expr": "Parameterisation of a stochastic model for human face identification", "word_idx": 65213, "sentence_idx": 863, "label": "unlabeled"}, {"type": "text", "expr": "In  Applications of Computer Vision, 1994", "word_idx": 65281, "sentence_idx": 864, "label": "unlabeled"}, {"type": "text", "expr": ", Proceedings of the\nSecond IEEE Workshop on , pp", "word_idx": 65322, "sentence_idx": 865, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0138\u2013142", "word_idx": 65371, "sentence_idx": 866, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 1994", "word_idx": 65379, "sentence_idx": 867, "label": "unlabeled"}, {"type": "text", "expr": "Applications of Computer Vision, 1994", "word_idx": 65390, "sentence_idx": 868, "label": "unlabeled"}, {"type": "text", "expr": ", Proceedings of the\nSecond IEEE Workshop on", "word_idx": 65427, "sentence_idx": 869, "label": "unlabeled"}, {"type": "text", "expr": "Simard & LeCun(1991)Simard and LeCun \nSimard, Patrice and LeCun, Yann", "word_idx": 65471, "sentence_idx": 870, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Reverse tdnn: an architecture for trajectory generation", "word_idx": 65540, "sentence_idx": 871, "label": "unlabeled"}, {"type": "text", "expr": "Simard & LeCun(1991)Simard and LeCun", "word_idx": 65598, "sentence_idx": 872, "label": "unlabeled"}, {"type": "text", "expr": "Simard, Patrice and LeCun, Yann", "word_idx": 65634, "sentence_idx": 873, "label": "unlabeled"}, {"type": "text", "expr": "Reverse tdnn: an architecture for trajectory generation", "word_idx": 65665, "sentence_idx": 874, "label": "unlabeled"}, {"type": "text", "expr": "In  Advances in Neural Information Processing Systems , pp", "word_idx": 65720, "sentence_idx": 875, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0579\u2013588", "word_idx": 65778, "sentence_idx": 876, "label": "unlabeled"}, {"type": "text", "expr": " Citeseer, 1991", "word_idx": 65786, "sentence_idx": 877, "label": "unlabeled"}, {"type": "text", "expr": "Advances in Neural Information Processing Systems", "word_idx": 65801, "sentence_idx": 878, "label": "unlabeled"}, {"type": "text", "expr": "Simonyan & Zisserman(2014)Simonyan and Zisserman \nSimonyan, Karen and Zisserman, Andrew", "word_idx": 65850, "sentence_idx": 879, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Very deep convolutional networks for large-scale image recognition", "word_idx": 65937, "sentence_idx": 880, "label": "unlabeled"}, {"type": "text", "expr": "Simonyan & Zisserman(2014)Simonyan and Zisserman", "word_idx": 66006, "sentence_idx": 881, "label": "unlabeled"}, {"type": "text", "expr": "Simonyan, Karen and Zisserman, Andrew", "word_idx": 66054, "sentence_idx": 882, "label": "unlabeled"}, {"type": "text", "expr": "Very deep convolutional networks for large-scale image recognition", "word_idx": 66091, "sentence_idx": 883, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1409", "word_idx": 66157, "sentence_idx": 884, "label": "unlabeled"}, {"type": "text", "expr": "1556 , 2014", "word_idx": 66182, "sentence_idx": 885, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1409", "word_idx": 66193, "sentence_idx": 886, "label": "unlabeled"}, {"type": "text", "expr": "Smola et\u00a0al", "word_idx": 66218, "sentence_idx": 887, "label": "unlabeled"}, {"type": "text", "expr": "(2008)Smola, Vishwanathan, and Le \nSmola, Alex\u00a0J", "word_idx": 66229, "sentence_idx": 888, "label": "unlabeled"}, {"type": "text", "expr": ", Vishwanathan, S", "word_idx": 66277, "sentence_idx": 889, "label": "unlabeled"}, {"type": "text", "expr": ", and Le, Quoc\u00a0V", "word_idx": 66294, "sentence_idx": 890, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Bundle methods for machine learning", "word_idx": 66310, "sentence_idx": 891, "label": "unlabeled"}, {"type": "text", "expr": "Smola et\u00a0al", "word_idx": 66348, "sentence_idx": 892, "label": "unlabeled"}, {"type": "text", "expr": "(2008)Smola, Vishwanathan, and Le", "word_idx": 66359, "sentence_idx": 893, "label": "unlabeled"}, {"type": "text", "expr": "Smola, Alex\u00a0J", "word_idx": 66392, "sentence_idx": 894, "label": "unlabeled"}, {"type": "text", "expr": ", Vishwanathan, S", "word_idx": 66405, "sentence_idx": 895, "label": "unlabeled"}, {"type": "text", "expr": ", and Le, Quoc\u00a0V", "word_idx": 66422, "sentence_idx": 896, "label": "unlabeled"}, {"type": "text", "expr": "Bundle methods for machine learning", "word_idx": 66438, "sentence_idx": 897, "label": "unlabeled"}, {"type": "text", "expr": "In Platt, J", "word_idx": 66473, "sentence_idx": 898, "label": "unlabeled"}, {"type": "text", "expr": ", Koller, D", "word_idx": 66484, "sentence_idx": 899, "label": "unlabeled"}, {"type": "text", "expr": ", Singer, Y", "word_idx": 66495, "sentence_idx": 900, "label": "unlabeled"}, {"type": "text", "expr": ", and Roweis, S", "word_idx": 66506, "sentence_idx": 901, "label": "unlabeled"}, {"type": "text", "expr": " (eds", "word_idx": 66521, "sentence_idx": 902, "label": "unlabeled"}, {"type": "text", "expr": "),\n Advances in Neural Information Processing Systems 20 , pp", "word_idx": 66526, "sentence_idx": 903, "label": "unlabeled"}, {"type": "text", "expr": "\u00a01377\u20131384", "word_idx": 66587, "sentence_idx": 904, "label": "unlabeled"}, {"type": "text", "expr": " Curran Associates, Inc", "word_idx": 66597, "sentence_idx": 905, "label": "unlabeled"}, {"type": "text", "expr": ", 2008", "word_idx": 66620, "sentence_idx": 906, "label": "unlabeled"}, {"type": "text", "expr": "Advances in Neural Information Processing Systems 20", "word_idx": 66626, "sentence_idx": 907, "label": "unlabeled"}, {"type": "text", "expr": "Szegedy et\u00a0al", "word_idx": 66678, "sentence_idx": 908, "label": "unlabeled"}, {"type": "text", "expr": "(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov,\nErhan, Vanhoucke, and Rabinovich \nSzegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott,\nAnguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich,\nAndrew", "word_idx": 66691, "sentence_idx": 909, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Going deeper with convolutions", "word_idx": 66930, "sentence_idx": 910, "label": "unlabeled"}, {"type": "text", "expr": "Szegedy et\u00a0al", "word_idx": 66963, "sentence_idx": 911, "label": "unlabeled"}, {"type": "text", "expr": "(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov,\nErhan, Vanhoucke, and Rabinovich", "word_idx": 66976, "sentence_idx": 912, "label": "unlabeled"}, {"type": "text", "expr": "Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott,\nAnguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich,\nAndrew", "word_idx": 67059, "sentence_idx": 913, "label": "unlabeled"}, {"type": "text", "expr": "Going deeper with convolutions", "word_idx": 67213, "sentence_idx": 914, "label": "unlabeled"}, {"type": "text", "expr": "In  Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition , pp", "word_idx": 67243, "sentence_idx": 915, "label": "unlabeled"}, {"type": "text", "expr": "\u00a01\u20139, 2015", "word_idx": 67329, "sentence_idx": 916, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition", "word_idx": 67339, "sentence_idx": 917, "label": "unlabeled"}, {"type": "text", "expr": "Taskar et\u00a0al", "word_idx": 67416, "sentence_idx": 918, "label": "unlabeled"}, {"type": "text", "expr": "(2005)Taskar, Chatalbashev, Koller, and\nGuestrin \nTaskar, Ben, Chatalbashev, Vassil, Koller, Daphne, and Guestrin, Carlos", "word_idx": 67428, "sentence_idx": 919, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Learning structured prediction models: A large margin approach", "word_idx": 67549, "sentence_idx": 920, "label": "unlabeled"}, {"type": "text", "expr": "Taskar et\u00a0al", "word_idx": 67614, "sentence_idx": 921, "label": "unlabeled"}, {"type": "text", "expr": "(2005)Taskar, Chatalbashev, Koller, and\nGuestrin", "word_idx": 67626, "sentence_idx": 922, "label": "unlabeled"}, {"type": "text", "expr": "Taskar, Ben, Chatalbashev, Vassil, Koller, Daphne, and Guestrin, Carlos", "word_idx": 67674, "sentence_idx": 923, "label": "unlabeled"}, {"type": "text", "expr": "Learning structured prediction models: A large margin approach", "word_idx": 67745, "sentence_idx": 924, "label": "unlabeled"}, {"type": "text", "expr": "In  Proceedings of the 22nd International Conference on Machine\nLearning , pp", "word_idx": 67807, "sentence_idx": 925, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0896\u2013903", "word_idx": 67884, "sentence_idx": 926, "label": "unlabeled"}, {"type": "text", "expr": " ACM, 2005", "word_idx": 67892, "sentence_idx": 927, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the 22nd International Conference on Machine\nLearning", "word_idx": 67902, "sentence_idx": 928, "label": "unlabeled"}, {"type": "text", "expr": "Todorov et\u00a0al", "word_idx": 67970, "sentence_idx": 929, "label": "unlabeled"}, {"type": "text", "expr": "(2012)Todorov, Erez, and Tassa \nTodorov, Emanuel, Erez, Tom, and Tassa, Yuval", "word_idx": 67983, "sentence_idx": 930, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Mujoco: A physics engine for model-based control", "word_idx": 68060, "sentence_idx": 931, "label": "unlabeled"}, {"type": "text", "expr": "Todorov et\u00a0al", "word_idx": 68111, "sentence_idx": 932, "label": "unlabeled"}, {"type": "text", "expr": "(2012)Todorov, Erez, and Tassa", "word_idx": 68124, "sentence_idx": 933, "label": "unlabeled"}, {"type": "text", "expr": "Todorov, Emanuel, Erez, Tom, and Tassa, Yuval", "word_idx": 68154, "sentence_idx": 934, "label": "unlabeled"}, {"type": "text", "expr": "Mujoco: A physics engine for model-based control", "word_idx": 68199, "sentence_idx": 935, "label": "unlabeled"}, {"type": "text", "expr": "In  2012 IEEE/RSJ International Conference on Intelligent Robots\nand Systems , pp", "word_idx": 68247, "sentence_idx": 936, "label": "unlabeled"}, {"type": "text", "expr": "\u00a05026\u20135033", "word_idx": 68328, "sentence_idx": 937, "label": "unlabeled"}, {"type": "text", "expr": " IEEE, 2012", "word_idx": 68338, "sentence_idx": 938, "label": "unlabeled"}, {"type": "text", "expr": "2012 IEEE/RSJ International Conference on Intelligent Robots\nand Systems", "word_idx": 68349, "sentence_idx": 939, "label": "unlabeled"}, {"type": "text", "expr": "Tsochantaridis et\u00a0al", "word_idx": 68421, "sentence_idx": 940, "label": "unlabeled"}, {"type": "text", "expr": "(2005)Tsochantaridis, Joachims, Hofmann, and\nAltun \nTsochantaridis, Ioannis, Joachims, Thorsten, Hofmann, Thomas, and Altun,\nYasemin", "word_idx": 68441, "sentence_idx": 941, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Large margin methods for structured and interdependent output\nvariables", "word_idx": 68573, "sentence_idx": 942, "label": "unlabeled"}, {"type": "text", "expr": "Tsochantaridis et\u00a0al", "word_idx": 68647, "sentence_idx": 943, "label": "unlabeled"}, {"type": "text", "expr": "(2005)Tsochantaridis, Joachims, Hofmann, and\nAltun", "word_idx": 68667, "sentence_idx": 944, "label": "unlabeled"}, {"type": "text", "expr": "Tsochantaridis, Ioannis, Joachims, Thorsten, Hofmann, Thomas, and Altun,\nYasemin", "word_idx": 68717, "sentence_idx": 945, "label": "unlabeled"}, {"type": "text", "expr": "Large margin methods for structured and interdependent output\nvariables", "word_idx": 68797, "sentence_idx": 946, "label": "unlabeled"}, {"type": "text", "expr": "Journal of Machine Learning Research , 6:1453\u20131484,\n2005", "word_idx": 68868, "sentence_idx": 947, "label": "unlabeled"}, {"type": "text", "expr": "Journal of Machine Learning Research", "word_idx": 68924, "sentence_idx": 948, "label": "unlabeled"}, {"type": "text", "expr": "Tsoumakas et\u00a0al", "word_idx": 68960, "sentence_idx": 949, "label": "unlabeled"}, {"type": "text", "expr": "(2011)Tsoumakas, Spyromitros-Xioufis, Vilcek, and\nVlahavas \nTsoumakas, Grigorios, Spyromitros-Xioufis, Eleftherios, Vilcek, Jozef, and\nVlahavas, Ioannis", "word_idx": 68975, "sentence_idx": 950, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Mulan: A java library for multi-label learning", "word_idx": 69127, "sentence_idx": 951, "label": "unlabeled"}, {"type": "text", "expr": "Tsoumakas et\u00a0al", "word_idx": 69176, "sentence_idx": 952, "label": "unlabeled"}, {"type": "text", "expr": "(2011)Tsoumakas, Spyromitros-Xioufis, Vilcek, and\nVlahavas", "word_idx": 69191, "sentence_idx": 953, "label": "unlabeled"}, {"type": "text", "expr": "Tsoumakas, Grigorios, Spyromitros-Xioufis, Eleftherios, Vilcek, Jozef, and\nVlahavas, Ioannis", "word_idx": 69249, "sentence_idx": 954, "label": "unlabeled"}, {"type": "text", "expr": "Mulan: A java library for multi-label learning", "word_idx": 69341, "sentence_idx": 955, "label": "unlabeled"}, {"type": "text", "expr": "Journal of Machine Learning Research , 12(Jul):2411\u20132414, 2011", "word_idx": 69387, "sentence_idx": 956, "label": "unlabeled"}, {"type": "text", "expr": "Journal of Machine Learning Research", "word_idx": 69449, "sentence_idx": 957, "label": "unlabeled"}, {"type": "text", "expr": "Van\u00a0Rossum & Drake\u00a0Jr(1995)Van\u00a0Rossum and Drake\u00a0Jr \nVan\u00a0Rossum, Guido and Drake\u00a0Jr, Fred\u00a0L", "word_idx": 69485, "sentence_idx": 958, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Centrum voor Wiskunde en Informatica Amsterdam, 1995", "word_idx": 69575, "sentence_idx": 959, "label": "unlabeled"}, {"type": "text", "expr": "Van\u00a0Rossum & Drake\u00a0Jr(1995)Van\u00a0Rossum and Drake\u00a0Jr", "word_idx": 69630, "sentence_idx": 960, "label": "unlabeled"}, {"type": "text", "expr": "Van\u00a0Rossum, Guido and Drake\u00a0Jr, Fred\u00a0L", "word_idx": 69680, "sentence_idx": 961, "label": "unlabeled"}, {"type": "text", "expr": "Python reference manual ", "word_idx": 69718, "sentence_idx": 962, "label": "unlabeled"}, {"type": "text", "expr": "Python reference manual", "word_idx": 69742, "sentence_idx": 963, "label": "unlabeled"}, {"type": "text", "expr": "Centrum voor Wiskunde en Informatica Amsterdam, 1995", "word_idx": 69765, "sentence_idx": 964, "label": "unlabeled"}, {"type": "text", "expr": "Wright(1997) \nWright, Stephen\u00a0J", "word_idx": 69817, "sentence_idx": 965, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Siam, 1997", "word_idx": 69848, "sentence_idx": 966, "label": "unlabeled"}, {"type": "text", "expr": "Wright(1997)", "word_idx": 69861, "sentence_idx": 967, "label": "unlabeled"}, {"type": "text", "expr": "Wright, Stephen\u00a0J", "word_idx": 69873, "sentence_idx": 968, "label": "unlabeled"}, {"type": "text", "expr": "Primal-dual interior-point methods ", "word_idx": 69890, "sentence_idx": 969, "label": "unlabeled"}, {"type": "text", "expr": "Primal-dual interior-point methods", "word_idx": 69925, "sentence_idx": 970, "label": "unlabeled"}, {"type": "text", "expr": "Siam, 1997", "word_idx": 69959, "sentence_idx": 971, "label": "unlabeled"}, {"type": "text", "expr": "Zheng et\u00a0al", "word_idx": 69969, "sentence_idx": 972, "label": "unlabeled"}, {"type": "text", "expr": "(2015)Zheng, Jayasumana, Romera-Paredes, Vineet, Su, Du,\nHuang, and Torr \nZheng, Shuai, Jayasumana, Sadeep, Romera-Paredes, Bernardino, Vineet, Vibhav,\nSu, Zhizhong, Du, Dalong, Huang, Chang, and Torr, Philip\u00a0HS", "word_idx": 69980, "sentence_idx": 973, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Conditional random fields as recurrent neural networks", "word_idx": 70191, "sentence_idx": 974, "label": "unlabeled"}, {"type": "text", "expr": "Zheng et\u00a0al", "word_idx": 70248, "sentence_idx": 975, "label": "unlabeled"}, {"type": "text", "expr": "(2015)Zheng, Jayasumana, Romera-Paredes, Vineet, Su, Du,\nHuang, and Torr", "word_idx": 70259, "sentence_idx": 976, "label": "unlabeled"}, {"type": "text", "expr": "Zheng, Shuai, Jayasumana, Sadeep, Romera-Paredes, Bernardino, Vineet, Vibhav,\nSu, Zhizhong, Du, Dalong, Huang, Chang, and Torr, Philip\u00a0HS", "word_idx": 70331, "sentence_idx": 977, "label": "unlabeled"}, {"type": "text", "expr": "Conditional random fields as recurrent neural networks", "word_idx": 70468, "sentence_idx": 978, "label": "unlabeled"}, {"type": "text", "expr": "In  Proceedings of the IEEE International Conference on Computer\nVision , pp", "word_idx": 70522, "sentence_idx": 979, "label": "unlabeled"}, {"type": "text", "expr": "\u00a01529\u20131537, 2015", "word_idx": 70598, "sentence_idx": 980, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the IEEE International Conference on Computer\nVision", "word_idx": 70614, "sentence_idx": 981, "label": "unlabeled"}, {"type": "text", "expr": "\\icmltitlerunning", "word_idx": 70681, "sentence_idx": 982, "label": "unlabeled"}, {"type": "text", "expr": "Input Convex Neural Networks: Supplementary Material", "word_idx": 70698, "sentence_idx": 983, "label": "unlabeled"}, {"type": "text", "expr": "Input Convex Neural Networks: Supplementary Material", "word_idx": 70750, "sentence_idx": 984, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0A  Additional architectures", "word_idx": 70802, "sentence_idx": 985, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0A", "word_idx": 70838, "sentence_idx": 986, "label": "unlabeled"}, {"type": "text", "expr": "1  Convolutional architectures", "word_idx": 70848, "sentence_idx": 987, "label": "unlabeled"}, {"type": "text", "expr": "Convolutions are important to many visual structured tasks", "word_idx": 70878, "sentence_idx": 988, "label": "unlabeled"}, {"type": "text", "expr": "\nWe have left convolutions out to keep the prior ICNN notation\nlight by using matrix-vector operations", "word_idx": 70936, "sentence_idx": 989, "label": "unlabeled"}, {"type": "text", "expr": "\nICNNs can be similarly created with convolutions because\nthe convolution is a linear operator", "word_idx": 71038, "sentence_idx": 990, "label": "unlabeled"}, {"type": "text", "expr": "Convolutions are important to many visual structured tasks", "word_idx": 71132, "sentence_idx": 991, "label": "unlabeled"}, {"type": "text", "expr": "\nWe have left convolutions out to keep the prior ICNN notation\nlight by using matrix-vector operations", "word_idx": 71190, "sentence_idx": 992, "label": "unlabeled"}, {"type": "text", "expr": "\nICNNs can be similarly created with convolutions because\nthe convolution is a linear operator", "word_idx": 71292, "sentence_idx": 993, "label": "unlabeled"}, {"type": "text", "expr": "The construction of convolutional layers in ICNNs depends\non the type of input and output space", "word_idx": 71386, "sentence_idx": 994, "label": "unlabeled"}, {"type": "text", "expr": "\nIf the input and output space are similarly\nstructured (e", "word_idx": 71481, "sentence_idx": 995, "label": "unlabeled"}, {"type": "text", "expr": " both spatial), the  $j$ th feature map\nof a convolutional PICNN layer  $i$  can be defined by", "word_idx": 71539, "sentence_idx": 996, "label": "unlabeled"}, {"type": "text", "expr": "$\\begin{split}\\displaystyle z_{i+1}^{j}&\\displaystyle=g_{i}\\left(z_{i}\\ast W_{i%\n,j}^{(z)}+(Sx)\\ast W_{i,j}^{(x)}+(Sy)\\ast W_{i,j}^{(y)}+b_{i,j}\\right)\\\\\n\\end{split}$", "word_idx": 71633, "sentence_idx": 997, "label": "unlabeled"}, {"type": "math", "expr": "$$\\begin{split}\\displaystyle z_{i+1}^{j}&\\displaystyle=g_{i}\\left(z_{i}\\ast W_{i%\n,j}^{(z)}+(Sx)\\ast W_{i,j}^{(x)}+(Sy)\\ast W_{i,j}^{(y)}+b_{i,j}\\right)\\\\\n\\end{split}$$", "word_idx": 71799, "sentence_idx": 998, "label": "unlabeled"}, {"type": "text", "expr": "where the convolution kernels  $W$  are the same size and\n $S$  scales the input and output to be the same size as\nthe previous feature map, and were we omit some of the Hadamard product terms\nthat can appear above for simplicity of presentation", "word_idx": 71963, "sentence_idx": 999, "label": "unlabeled"}, {"type": "text", "expr": "If the input space is spatial, but the output space has another structure\n(e", "word_idx": 72208, "sentence_idx": 1000, "label": "unlabeled"}, {"type": "text", "expr": " the simplex), the convolution over the output space can\nbe replaced by a matrix-vector operation, such as", "word_idx": 72284, "sentence_idx": 1001, "label": "unlabeled"}, {"type": "text", "expr": "If the input space is spatial, but the output space has another structure\n(e", "word_idx": 72390, "sentence_idx": 1002, "label": "unlabeled"}, {"type": "text", "expr": " the simplex), the convolution over the output space can\nbe replaced by a matrix-vector operation, such as", "word_idx": 72466, "sentence_idx": 1003, "label": "unlabeled"}, {"type": "text", "expr": "$\\begin{split}\\displaystyle z_{i+1}^{j}&\\displaystyle=g_{i}\\left(z_{i}\\ast W_{i%\n,j}^{(z)}+(Sx)\\ast W_{i,j}^{(x)}+B_{i,j}^{(y)}y+b_{i,j}\\right)\\\\\n\\end{split}$", "word_idx": 72572, "sentence_idx": 1004, "label": "unlabeled"}, {"type": "math", "expr": "$$\\begin{split}\\displaystyle z_{i+1}^{j}&\\displaystyle=g_{i}\\left(z_{i}\\ast W_{i%\n,j}^{(z)}+(Sx)\\ast W_{i,j}^{(x)}+B_{i,j}^{(y)}y+b_{i,j}\\right)\\\\\n\\end{split}$$", "word_idx": 72730, "sentence_idx": 1005, "label": "unlabeled"}, {"type": "text", "expr": "where the product  $B_{i,j}^{(y)}y$  is a scalar", "word_idx": 72886, "sentence_idx": 1006, "label": "unlabeled"}, {"type": "math", "expr": "$$B_{i,j}^{(y)}y$$", "word_idx": 72934, "sentence_idx": 1007, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0B  Exact inference in ICNNs", "word_idx": 72948, "sentence_idx": 1008, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0B", "word_idx": 72984, "sentence_idx": 1009, "label": "unlabeled"}, {"type": "text", "expr": "Although it is not a practical approach for solving the optimization tasks, we\nfirst highlight the fact that the inference problem for the networks\npresented above (where the non-linear are either ReLU or linear units) can be\nposed as as linear program", "word_idx": 72994, "sentence_idx": 1010, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, considering the FICNN network in\n( 2 ) can be written as the optimization problem", "word_idx": 73246, "sentence_idx": 1011, "label": "unlabeled"}, {"type": "text", "expr": "$\\begin{split}\\displaystyle\\minimize_{y,z_{1},\\ldots,z_{k}}&\\displaystyle z_{k}%\n\\\\\n\\displaystyle\\subjectto&\\displaystyle z_{i+1}\\geq W^{(z)}_{i}z_{i}+W^{(y)}_{i}%\ny+b_{i},\\;\\;i=0,\\ldots,k-1\\\\\n&\\displaystyle z_{i}\\geq 0,\\;\\;i=1,\\ldots,k-1\\end{split}$", "word_idx": 73342, "sentence_idx": 1012, "label": "unlabeled"}, {"type": "math", "expr": "$$\\begin{split}\\displaystyle\\minimize_{y,z_{1},\\ldots,z_{k}}&\\displaystyle z_{k}%\n\\\\\n\\displaystyle\\subjectto&\\displaystyle z_{i+1}\\geq W^{(z)}_{i}z_{i}+W^{(y)}_{i}%\ny+b_{i},\\;\\;i=0,\\ldots,k-1\\\\\n&\\displaystyle z_{i}\\geq 0,\\;\\;i=1,\\ldots,k-1.\\end{split}$$", "word_idx": 73592, "sentence_idx": 1013, "label": "unlabeled"}, {"type": "text", "expr": "This problem exactly replicates the equations of the FICNN, with the exception\nthat we have replaced ReLU and the equality constraint between layers with a\npositivity constraint on the  $z_{i}$  terms and an inequality", "word_idx": 73841, "sentence_idx": 1014, "label": "unlabeled"}, {"type": "text", "expr": " However, because we\nare minimizing the final  $z_{k}$  term, and because each inequality constraint is\nconvex, at the solution one of these constraints must be tight, i", "word_idx": 74059, "sentence_idx": 1015, "label": "unlabeled"}, {"type": "text", "expr": ",  $(z_{i})_{j}=(W^{(z)}_{i}z_{i}+W^{(y)}_{i}y+b_{i})_{j}$  or  $(z_{i})_{j}=0$ , which recovers the\nReLU non-linearity exactly", "word_idx": 74228, "sentence_idx": 1016, "label": "unlabeled"}, {"type": "text", "expr": " The exact same procedure can be used to write to\ncreate an exact inference procedure for the PICNN", "word_idx": 74355, "sentence_idx": 1017, "label": "unlabeled"}, {"type": "math", "expr": "$$z_{i}$$", "word_idx": 74454, "sentence_idx": 1018, "label": "unlabeled"}, {"type": "math", "expr": "$$z_{k}$$", "word_idx": 74459, "sentence_idx": 1019, "label": "unlabeled"}, {"type": "math", "expr": "$$(z_{i})_{j}=(W^{(z)}_{i}z_{i}+W^{(y)}_{i}y+b_{i})_{j}$$", "word_idx": 74464, "sentence_idx": 1020, "label": "unlabeled"}, {"type": "math", "expr": "$$(z_{i})_{j}=0$$", "word_idx": 74517, "sentence_idx": 1021, "label": "unlabeled"}, {"type": "text", "expr": "Although the LP formulation is appealing in its simplicity, in practice these\noptimization problems will have a number of variables equal to the  total \nnumber of activations in the entire network", "word_idx": 74530, "sentence_idx": 1022, "label": "unlabeled"}, {"type": "text", "expr": " Furthermore, most LP solution\nmethods to solve such problems require that we form  and invert \nstructured matrices with blocks such as  $W_{i}^{T}W_{i}$ \n\u2014 the case for most interior-point methods  \\citep wright1997primal or\neven approximate algorithms such as the\nalternating direction method of multipliers  \\citep boyd2011distributed \u2014\nwhich are large dense matrices or have structured forms such as\nnon-cyclic convolutions that are expensive to invert", "word_idx": 74726, "sentence_idx": 1023, "label": "unlabeled"}, {"type": "text", "expr": "\nEven incremental approaches like the Simplex method\nrequire that we form inverses of subsets of columns of these matrices, which are\nadditionally different for structured operations like convolutions, and which\noverall still involve substantially more computation than a single forward pass", "word_idx": 75182, "sentence_idx": 1024, "label": "unlabeled"}, {"type": "text", "expr": "\nFurthermore, such solvers typically do not exploit the substantial effort that\nhas gone in to accelerating the forward and backward computation passes for\nneural networks using hardware such as GPUs", "word_idx": 75473, "sentence_idx": 1025, "label": "unlabeled"}, {"type": "text", "expr": " Thus, as a whole, these do not\npresent a viable option for optimizing the networks", "word_idx": 75672, "sentence_idx": 1026, "label": "unlabeled"}, {"type": "text", "expr": "total", "word_idx": 75755, "sentence_idx": 1027, "label": "unlabeled"}, {"type": "text", "expr": "and invert", "word_idx": 75760, "sentence_idx": 1028, "label": "unlabeled"}, {"type": "math", "expr": "$$W_{i}^{T}W_{i}$$", "word_idx": 75770, "sentence_idx": 1029, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 75784, "sentence_idx": 1030, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 75790, "sentence_idx": 1031, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0C  The bundle method for approximate inference in ICNNs", "word_idx": 75796, "sentence_idx": 1032, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0C", "word_idx": 75860, "sentence_idx": 1033, "label": "unlabeled"}, {"type": "text", "expr": "and update the next iteration by solving the optimization problem A bit more concretely, the optimization problem can be written via a set of linear\ninequality constraints", "word_idx": 75870, "sentence_idx": 1034, "label": "unlabeled"}, {"type": "text", "expr": "We here review the basic bundle method  \\citep smola2007bundle that we build\nupon in our bundle entropy method", "word_idx": 76041, "sentence_idx": 1035, "label": "unlabeled"}, {"type": "text", "expr": "\nThe bundle method takes\nadvantage of the fact that for a convex objective, the first-order approximation\nat any point is a global  underestimator  of the function; this lets us\nmaintain a piecewise linear lower bound on the function by adding\ncutting planes formed by this first order approximation, and then repeatedly\noptimizing this lower bound", "word_idx": 76151, "sentence_idx": 1036, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, the process follows the procedure\nshown in Algorithm\u00a0 1 ", "word_idx": 76499, "sentence_idx": 1037, "label": "unlabeled"}, {"type": "text", "expr": " Denoting the iterates of the algorithm as\n $y^{k}$ , at each iteration of the algorithm, we compute the first order\napproximation to the function", "word_idx": 76570, "sentence_idx": 1038, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 76716, "sentence_idx": 1039, "label": "unlabeled"}, {"type": "text", "expr": "underestimator", "word_idx": 76722, "sentence_idx": 1040, "label": "unlabeled"}, {"type": "math", "expr": "$$y^{k}$$", "word_idx": 76736, "sentence_idx": 1041, "label": "unlabeled"}, {"type": "text", "expr": "$f(x,y^{k};\\theta)+\\nabla_{y}f(x,y^{k};\\theta)^{T}(y-y^{k})$", "word_idx": 76741, "sentence_idx": 1042, "label": "unlabeled"}, {"type": "math", "expr": "$$f(x,y^{k};\\theta)+\\nabla_{y}f(x,y^{k};\\theta)^{T}(y-y^{k})$$", "word_idx": 76801, "sentence_idx": 1043, "label": "unlabeled"}, {"type": "text", "expr": "and update the next iteration by solving the optimization problem", "word_idx": 76859, "sentence_idx": 1044, "label": "unlabeled"}, {"type": "text", "expr": "$y^{k+1}:=\\argmin_{y\\in\\mathcal{Y}}\\max_{1\\leq i\\leq k}\\{f(x,y^{i};\\theta)+%\n\\nabla_{y}f(x,y^{i};\\theta)^{T}(y-y^{i})\\}$", "word_idx": 76924, "sentence_idx": 1045, "label": "unlabeled"}, {"type": "math", "expr": "$$y^{k+1}:=\\argmin_{y\\in\\mathcal{Y}}\\max_{1\\leq i\\leq k}\\{f(x,y^{i};\\theta)+%\n\\nabla_{y}f(x,y^{i};\\theta)^{T}(y-y^{i})\\}.$$", "word_idx": 77044, "sentence_idx": 1046, "label": "unlabeled"}, {"type": "text", "expr": "A bit more concretely, the optimization problem can be written via a set of linear\ninequality constraints", "word_idx": 77163, "sentence_idx": 1047, "label": "unlabeled"}, {"type": "text", "expr": "$y^{k+1},t^{k+1}:=\\argmin_{y\\in\\mathcal{Y},t}\\;\\;\\{t\\mid Gy+h\\leq t1\\}$", "word_idx": 77268, "sentence_idx": 1048, "label": "unlabeled"}, {"type": "math", "expr": "$$y^{k+1},t^{k+1}:=\\argmin_{y\\in\\mathcal{Y},t}\\;\\;\\{t\\mid Gy+h\\leq t1\\}$$", "word_idx": 77339, "sentence_idx": 1049, "label": "unlabeled"}, {"type": "text", "expr": "where  $G\\in\\mathbb{R}^{k\\times n}$  has rows equal to", "word_idx": 77408, "sentence_idx": 1050, "label": "unlabeled"}, {"type": "math", "expr": "$$G\\in\\mathbb{R}^{k\\times n}$$", "word_idx": 77462, "sentence_idx": 1051, "label": "unlabeled"}, {"type": "text", "expr": "$g_{i}^{T}=\\nabla_{y}f(x,y^{i};\\theta)^{T}$", "word_idx": 77488, "sentence_idx": 1052, "label": "unlabeled"}, {"type": "math", "expr": "$$g_{i}^{T}=\\nabla_{y}f(x,y^{i};\\theta)^{T}$$", "word_idx": 77531, "sentence_idx": 1053, "label": "unlabeled"}, {"type": "text", "expr": "and  $h\\in\\mathbb{R}^{k}$  has entries equal to", "word_idx": 77572, "sentence_idx": 1054, "label": "unlabeled"}, {"type": "math", "expr": "$$h\\in\\mathbb{R}^{k}$$", "word_idx": 77619, "sentence_idx": 1055, "label": "unlabeled"}, {"type": "text", "expr": "$h_{i}=f(x,y^{i};\\theta)-\\nabla_{y}f(x,y^{i};\\theta)^{T}y^{i}$", "word_idx": 77637, "sentence_idx": 1056, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{i}=f(x,y^{i};\\theta)-\\nabla_{y}f(x,y^{i};\\theta)^{T}y^{i}.$$", "word_idx": 77699, "sentence_idx": 1057, "label": "unlabeled"}, {"type": "text", "expr": "Algorithm\u00a01  A typical bundle method to optimize  $f:\\mathbb{R}^{m\\times n}\\rightarrow\\mathbb{R}$ \nover  $\\mathbb{R}^{n}$  for  $K$  iterations with a fixed  $x$  and initial starting point  $y^{1}$ ", "word_idx": 77760, "sentence_idx": 1058, "label": "unlabeled"}, {"type": "text", "expr": "Algorithm\u00a01", "word_idx": 77959, "sentence_idx": 1059, "label": "unlabeled"}, {"type": "math", "expr": "$$f:\\mathbb{R}^{m\\times n}\\rightarrow\\mathbb{R}$$", "word_idx": 77970, "sentence_idx": 1060, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbb{R}^{n}$$", "word_idx": 78015, "sentence_idx": 1061, "label": "unlabeled"}, {"type": "math", "expr": "$$y^{1}$$", "word_idx": 78029, "sentence_idx": 1062, "label": "unlabeled"}, {"type": "text", "expr": "function \u00a0 BundleMethod ( $f$ ,  $x$ ,  $y^{1}$ ,  $K$ )", "word_idx": 78034, "sentence_idx": 1063, "label": "unlabeled"}, {"type": "text", "expr": "function", "word_idx": 78090, "sentence_idx": 1064, "label": "unlabeled"}, {"type": "text", "expr": "BundleMethod", "word_idx": 78098, "sentence_idx": 1065, "label": "unlabeled"}, {"type": "math", "expr": "$$y^{1}$$", "word_idx": 78110, "sentence_idx": 1066, "label": "unlabeled"}, {"type": "text", "expr": "$G$   $\\leftarrow$   $0\\in\\mathbb{R}^{K\\times n}$", "word_idx": 78115, "sentence_idx": 1067, "label": "unlabeled"}, {"type": "math", "expr": "$$\\leftarrow$$", "word_idx": 78164, "sentence_idx": 1068, "label": "unlabeled"}, {"type": "math", "expr": "$$0\\in\\mathbb{R}^{K\\times n}$$", "word_idx": 78174, "sentence_idx": 1069, "label": "unlabeled"}, {"type": "text", "expr": "$h$   $\\leftarrow$   $0\\in\\mathbb{R}^{K}$", "word_idx": 78200, "sentence_idx": 1070, "label": "unlabeled"}, {"type": "math", "expr": "$$\\leftarrow$$", "word_idx": 78241, "sentence_idx": 1071, "label": "unlabeled"}, {"type": "math", "expr": "$$0\\in\\mathbb{R}^{K}$$", "word_idx": 78251, "sentence_idx": 1072, "label": "unlabeled"}, {"type": "text", "expr": "for \u00a0 $k=1,K$ \u00a0 do", "word_idx": 78269, "sentence_idx": 1073, "label": "unlabeled"}, {"type": "math", "expr": "$$k=1,K$$", "word_idx": 78287, "sentence_idx": 1074, "label": "unlabeled"}, {"type": "text", "expr": "$G_{k}^{T}$   $\\leftarrow$   $\\nabla_{y}f(x,y^{k};\\theta)^{T}$", "word_idx": 78292, "sentence_idx": 1075, "label": "unlabeled"}, {"type": "math", "expr": "$$G_{k}^{T}$$", "word_idx": 78354, "sentence_idx": 1076, "label": "unlabeled"}, {"type": "math", "expr": "$$\\leftarrow$$", "word_idx": 78363, "sentence_idx": 1077, "label": "unlabeled"}, {"type": "math", "expr": "$$\\nabla_{y}f(x,y^{k};\\theta)^{T}$$", "word_idx": 78373, "sentence_idx": 1078, "label": "unlabeled"}, {"type": "text", "expr": "$\\triangleright$   $k$ th row of  $G$", "word_idx": 78404, "sentence_idx": 1079, "label": "unlabeled"}, {"type": "math", "expr": "$$\\triangleright$$", "word_idx": 78441, "sentence_idx": 1080, "label": "unlabeled"}, {"type": "text", "expr": "$h_{k}$   $\\leftarrow$   $f(x,y^{k};\\theta)-\\nabla_{y}f(x,y^{k};\\theta)^{T}y^{k}$", "word_idx": 78455, "sentence_idx": 1081, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{k}$$", "word_idx": 78536, "sentence_idx": 1082, "label": "unlabeled"}, {"type": "math", "expr": "$$\\leftarrow$$", "word_idx": 78541, "sentence_idx": 1083, "label": "unlabeled"}, {"type": "math", "expr": "$$f(x,y^{k};\\theta)-\\nabla_{y}f(x,y^{k};\\theta)^{T}y^{k}$$", "word_idx": 78551, "sentence_idx": 1084, "label": "unlabeled"}, {"type": "text", "expr": "$y^{k+1}$ ,  $t^{k+1}$   $\\leftarrow$ $\\argmin_{y\\in\\mathcal{Y},t}\\;\\;\\{t\\mid G_{1:k}y+h_{1:k}\\leq t1\\}$", "word_idx": 78605, "sentence_idx": 1085, "label": "unlabeled"}, {"type": "math", "expr": "$$y^{k+1}$$", "word_idx": 78709, "sentence_idx": 1086, "label": "unlabeled"}, {"type": "math", "expr": "$$t^{k+1}$$", "word_idx": 78716, "sentence_idx": 1087, "label": "unlabeled"}, {"type": "math", "expr": "$$\\leftarrow$$", "word_idx": 78723, "sentence_idx": 1088, "label": "unlabeled"}, {"type": "math", "expr": "$$\\argmin_{y\\in\\mathcal{Y},t}\\;\\;\\{t\\mid G_{1:k}y+h_{1:k}\\leq t1\\}$$", "word_idx": 78733, "sentence_idx": 1089, "label": "unlabeled"}, {"type": "text", "expr": "end \u00a0 for", "word_idx": 78797, "sentence_idx": 1090, "label": "unlabeled"}, {"type": "text", "expr": "return   $y^{K+1}$", "word_idx": 78806, "sentence_idx": 1091, "label": "unlabeled"}, {"type": "text", "expr": "return", "word_idx": 78824, "sentence_idx": 1092, "label": "unlabeled"}, {"type": "math", "expr": "$$y^{K+1}$$", "word_idx": 78830, "sentence_idx": 1093, "label": "unlabeled"}, {"type": "text", "expr": "end \u00a0 function", "word_idx": 78837, "sentence_idx": 1094, "label": "unlabeled"}, {"type": "text", "expr": "function", "word_idx": 78851, "sentence_idx": 1095, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0D  Bundle Entropy Algorithm", "word_idx": 78859, "sentence_idx": 1096, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0D", "word_idx": 78895, "sentence_idx": 1097, "label": "unlabeled"}, {"type": "text", "expr": "In Algorithm\u00a0 2 ", "word_idx": 78905, "sentence_idx": 1098, "label": "unlabeled"}, {"type": "text", "expr": "Algorithm\u00a02  Our bundle entropy method to optimize  $f:\\mathbb{R}^{m}\\times[0,1]^{n}\\rightarrow\\mathbb{R}$ \nover  $[0,1]^{n}$ \nfor  $K$  iterations with a fixed  $x$  and initial starting point  $y^{1}$ ", "word_idx": 78921, "sentence_idx": 1099, "label": "unlabeled"}, {"type": "text", "expr": "Algorithm\u00a02", "word_idx": 79124, "sentence_idx": 1100, "label": "unlabeled"}, {"type": "math", "expr": "$$f:\\mathbb{R}^{m}\\times[0,1]^{n}\\rightarrow\\mathbb{R}$$", "word_idx": 79135, "sentence_idx": 1101, "label": "unlabeled"}, {"type": "math", "expr": "$$[0,1]^{n}$$", "word_idx": 79187, "sentence_idx": 1102, "label": "unlabeled"}, {"type": "math", "expr": "$$y^{1}$$", "word_idx": 79196, "sentence_idx": 1103, "label": "unlabeled"}, {"type": "text", "expr": "function \u00a0 BundleEntropyMethod ( $f$ ,  $x$ ,  $y^{1}$ ,  $K$ )", "word_idx": 79201, "sentence_idx": 1104, "label": "unlabeled"}, {"type": "text", "expr": "function", "word_idx": 79264, "sentence_idx": 1105, "label": "unlabeled"}, {"type": "text", "expr": "BundleEntropyMethod", "word_idx": 79272, "sentence_idx": 1106, "label": "unlabeled"}, {"type": "math", "expr": "$$y^{1}$$", "word_idx": 79291, "sentence_idx": 1107, "label": "unlabeled"}, {"type": "text", "expr": "$G_{\\ell}$   $\\leftarrow$   $[\\ ]$", "word_idx": 79296, "sentence_idx": 1108, "label": "unlabeled"}, {"type": "math", "expr": "$$G_{\\ell}$$", "word_idx": 79330, "sentence_idx": 1109, "label": "unlabeled"}, {"type": "math", "expr": "$$\\leftarrow$$", "word_idx": 79338, "sentence_idx": 1110, "label": "unlabeled"}, {"type": "math", "expr": "$$[\\ ]$$", "word_idx": 79348, "sentence_idx": 1111, "label": "unlabeled"}, {"type": "text", "expr": "$h_{\\ell}$   $\\leftarrow$   $[\\ ]$", "word_idx": 79352, "sentence_idx": 1112, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{\\ell}$$", "word_idx": 79386, "sentence_idx": 1113, "label": "unlabeled"}, {"type": "math", "expr": "$$\\leftarrow$$", "word_idx": 79394, "sentence_idx": 1114, "label": "unlabeled"}, {"type": "math", "expr": "$$[\\ ]$$", "word_idx": 79404, "sentence_idx": 1115, "label": "unlabeled"}, {"type": "text", "expr": "for \u00a0 $k=1,K$ \u00a0 do", "word_idx": 79408, "sentence_idx": 1116, "label": "unlabeled"}, {"type": "math", "expr": "$$k=1,K$$", "word_idx": 79426, "sentence_idx": 1117, "label": "unlabeled"}, {"type": "text", "expr": "Append ( $G_{\\ell}$ ,  $\\nabla_{y}f(x,y^{k};\\theta)^{T}$ )", "word_idx": 79431, "sentence_idx": 1118, "label": "unlabeled"}, {"type": "text", "expr": "Append", "word_idx": 79489, "sentence_idx": 1119, "label": "unlabeled"}, {"type": "math", "expr": "$$G_{\\ell}$$", "word_idx": 79495, "sentence_idx": 1120, "label": "unlabeled"}, {"type": "math", "expr": "$$\\nabla_{y}f(x,y^{k};\\theta)^{T}$$", "word_idx": 79503, "sentence_idx": 1121, "label": "unlabeled"}, {"type": "text", "expr": "Append ( $h_{\\ell}$ ,  $f(x,y^{k};\\theta)-\\nabla_{y}f(x,y^{k};\\theta)^{T}y^{k}$ )", "word_idx": 79534, "sentence_idx": 1122, "label": "unlabeled"}, {"type": "text", "expr": "Append", "word_idx": 79615, "sentence_idx": 1123, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{\\ell}$$", "word_idx": 79621, "sentence_idx": 1124, "label": "unlabeled"}, {"type": "math", "expr": "$$f(x,y^{k};\\theta)-\\nabla_{y}f(x,y^{k};\\theta)^{T}y^{k}$$", "word_idx": 79629, "sentence_idx": 1125, "label": "unlabeled"}, {"type": "text", "expr": "$a_{k}$   $\\leftarrow$   Length ( $G_{\\ell}$ )", "word_idx": 79683, "sentence_idx": 1126, "label": "unlabeled"}, {"type": "math", "expr": "$$a_{k}$$", "word_idx": 79729, "sentence_idx": 1127, "label": "unlabeled"}, {"type": "math", "expr": "$$\\leftarrow$$", "word_idx": 79734, "sentence_idx": 1128, "label": "unlabeled"}, {"type": "text", "expr": "Length", "word_idx": 79744, "sentence_idx": 1129, "label": "unlabeled"}, {"type": "math", "expr": "$$G_{\\ell}$$", "word_idx": 79750, "sentence_idx": 1130, "label": "unlabeled"}, {"type": "text", "expr": "$\\triangleright$  The number of active constraints", "word_idx": 79758, "sentence_idx": 1131, "label": "unlabeled"}, {"type": "math", "expr": "$$\\triangleright$$", "word_idx": 79808, "sentence_idx": 1132, "label": "unlabeled"}, {"type": "text", "expr": "$G_{k}$   $\\leftarrow$   Concat ( $G_{\\ell}$ ) $\\in\\mathbb{R}^{a_{k}\\times n}$", "word_idx": 79822, "sentence_idx": 1133, "label": "unlabeled"}, {"type": "math", "expr": "$$G_{k}$$", "word_idx": 79900, "sentence_idx": 1134, "label": "unlabeled"}, {"type": "math", "expr": "$$\\leftarrow$$", "word_idx": 79905, "sentence_idx": 1135, "label": "unlabeled"}, {"type": "text", "expr": "Concat", "word_idx": 79915, "sentence_idx": 1136, "label": "unlabeled"}, {"type": "math", "expr": "$$G_{\\ell}$$", "word_idx": 79921, "sentence_idx": 1137, "label": "unlabeled"}, {"type": "math", "expr": "$$\\in\\mathbb{R}^{a_{k}\\times n}$$", "word_idx": 79929, "sentence_idx": 1138, "label": "unlabeled"}, {"type": "text", "expr": "$h_{k}$   $\\leftarrow$   Concat ( $h_{\\ell}$ ) $\\in\\mathbb{R}^{a_{k}}$", "word_idx": 79958, "sentence_idx": 1139, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{k}$$", "word_idx": 80028, "sentence_idx": 1140, "label": "unlabeled"}, {"type": "math", "expr": "$$\\leftarrow$$", "word_idx": 80033, "sentence_idx": 1141, "label": "unlabeled"}, {"type": "text", "expr": "Concat", "word_idx": 80043, "sentence_idx": 1142, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{\\ell}$$", "word_idx": 80049, "sentence_idx": 1143, "label": "unlabeled"}, {"type": "math", "expr": "$$\\in\\mathbb{R}^{a_{k}}$$", "word_idx": 80057, "sentence_idx": 1144, "label": "unlabeled"}, {"type": "text", "expr": "if \u00a0 $a_{k}=1$ \u00a0 then", "word_idx": 80078, "sentence_idx": 1145, "label": "unlabeled"}, {"type": "math", "expr": "$$a_{k}=1$$", "word_idx": 80099, "sentence_idx": 1146, "label": "unlabeled"}, {"type": "text", "expr": "$\\lambda_{k}$   $\\leftarrow$  1", "word_idx": 80106, "sentence_idx": 1147, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda_{k}$$", "word_idx": 80137, "sentence_idx": 1148, "label": "unlabeled"}, {"type": "math", "expr": "$$\\leftarrow$$", "word_idx": 80148, "sentence_idx": 1149, "label": "unlabeled"}, {"type": "text", "expr": "$\\lambda_{k}$   $\\leftarrow$   ProjNewtonLogistic ( $G_{k}$ ,  $h_{k}$ )", "word_idx": 80158, "sentence_idx": 1150, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda_{k}$$", "word_idx": 80230, "sentence_idx": 1151, "label": "unlabeled"}, {"type": "math", "expr": "$$\\leftarrow$$", "word_idx": 80241, "sentence_idx": 1152, "label": "unlabeled"}, {"type": "text", "expr": "ProjNewtonLogistic", "word_idx": 80251, "sentence_idx": 1153, "label": "unlabeled"}, {"type": "math", "expr": "$$G_{k}$$", "word_idx": 80269, "sentence_idx": 1154, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{k}$$", "word_idx": 80274, "sentence_idx": 1155, "label": "unlabeled"}, {"type": "text", "expr": "end \u00a0 if", "word_idx": 80279, "sentence_idx": 1156, "label": "unlabeled"}, {"type": "text", "expr": "$y^{k+1}$   $\\leftarrow$   $(1+\\exp(G_{k}^{T}\\lambda_{k}))^{-1}$", "word_idx": 80287, "sentence_idx": 1157, "label": "unlabeled"}, {"type": "math", "expr": "$$y^{k+1}$$", "word_idx": 80351, "sentence_idx": 1158, "label": "unlabeled"}, {"type": "math", "expr": "$$\\leftarrow$$", "word_idx": 80358, "sentence_idx": 1159, "label": "unlabeled"}, {"type": "math", "expr": "$$(1+\\exp(G_{k}^{T}\\lambda_{k}))^{-1}$$", "word_idx": 80368, "sentence_idx": 1160, "label": "unlabeled"}, {"type": "text", "expr": "Delete ( $G_{\\ell}[i]$  and  $h_{\\ell}[i]$  where  $\\lambda_{i}\\leq 0$ )", "word_idx": 80403, "sentence_idx": 1161, "label": "unlabeled"}, {"type": "text", "expr": "Delete", "word_idx": 80475, "sentence_idx": 1162, "label": "unlabeled"}, {"type": "math", "expr": "$$G_{\\ell}[i]$$", "word_idx": 80481, "sentence_idx": 1163, "label": "unlabeled"}, {"type": "math", "expr": "$$h_{\\ell}[i]$$", "word_idx": 80492, "sentence_idx": 1164, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda_{i}\\leq 0$$", "word_idx": 80503, "sentence_idx": 1165, "label": "unlabeled"}, {"type": "text", "expr": "$\\triangleright$  Prune inactive constraints", "word_idx": 80520, "sentence_idx": 1166, "label": "unlabeled"}, {"type": "math", "expr": "$$\\triangleright$$", "word_idx": 80564, "sentence_idx": 1167, "label": "unlabeled"}, {"type": "text", "expr": "end \u00a0 for", "word_idx": 80578, "sentence_idx": 1168, "label": "unlabeled"}, {"type": "text", "expr": "return   $y^{K+1}$", "word_idx": 80587, "sentence_idx": 1169, "label": "unlabeled"}, {"type": "text", "expr": "return", "word_idx": 80605, "sentence_idx": 1170, "label": "unlabeled"}, {"type": "math", "expr": "$$y^{K+1}$$", "word_idx": 80611, "sentence_idx": 1171, "label": "unlabeled"}, {"type": "text", "expr": "end \u00a0 function", "word_idx": 80618, "sentence_idx": 1172, "label": "unlabeled"}, {"type": "text", "expr": "function", "word_idx": 80632, "sentence_idx": 1173, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0E  Deep Q-learning with ICNNs", "word_idx": 80640, "sentence_idx": 1174, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0E", "word_idx": 80678, "sentence_idx": 1175, "label": "unlabeled"}, {"type": "text", "expr": "In Algorithm\u00a0 3 ", "word_idx": 80688, "sentence_idx": 1176, "label": "unlabeled"}, {"type": "text", "expr": "Algorithm\u00a03  Deep Q-learning with ICNNs", "word_idx": 80704, "sentence_idx": 1177, "label": "unlabeled"}, {"type": "text", "expr": "\n Opt-Alg  is a convex minimization algorithm such as\ngradient descent or the bundle entropy method", "word_idx": 80743, "sentence_idx": 1178, "label": "unlabeled"}, {"type": "text", "expr": "\n $\\tilde{Q}_{\\theta}$  is the objective the optimization algorithm solves", "word_idx": 80842, "sentence_idx": 1179, "label": "unlabeled"}, {"type": "text", "expr": "\nIn gradient descent,  $\\tilde{Q}_{\\theta}(s,a)=Q(s,a|\\theta)$  and\nwith the bundle entropy method,  $\\tilde{Q}_{\\theta}(s,a)=Q(s,a|\\theta)+H(a)$ ", "word_idx": 80916, "sentence_idx": 1180, "label": "unlabeled"}, {"type": "text", "expr": "Algorithm\u00a03", "word_idx": 81062, "sentence_idx": 1181, "label": "unlabeled"}, {"type": "text", "expr": "Opt-Alg", "word_idx": 81073, "sentence_idx": 1182, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{Q}_{\\theta}$$", "word_idx": 81080, "sentence_idx": 1183, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{Q}_{\\theta}(s,a)=Q(s,a|\\theta)$$", "word_idx": 81098, "sentence_idx": 1184, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{Q}_{\\theta}(s,a)=Q(s,a|\\theta)+H(a)$$", "word_idx": 81135, "sentence_idx": 1185, "label": "unlabeled"}, {"type": "text", "expr": "Select a discount factor  $\\gamma\\in(0,1)$  and moving average factor\n $\\tau\\in(0,1)$", "word_idx": 81177, "sentence_idx": 1186, "label": "unlabeled"}, {"type": "math", "expr": "$$\\gamma\\in(0,1)$$", "word_idx": 81262, "sentence_idx": 1187, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tau\\in(0,1)$$", "word_idx": 81276, "sentence_idx": 1188, "label": "unlabeled"}, {"type": "text", "expr": "Initialize the ICNN  $-Q(s,a|\\theta)$  with\ntarget network parameters  $\\theta^{\\prime}\\leftarrow\\theta$ \nand a replay buffer  $R\\leftarrow\\emptyset$", "word_idx": 81288, "sentence_idx": 1189, "label": "unlabeled"}, {"type": "math", "expr": "$$-Q(s,a|\\theta)$$", "word_idx": 81437, "sentence_idx": 1190, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta^{\\prime}\\leftarrow\\theta$$", "word_idx": 81451, "sentence_idx": 1191, "label": "unlabeled"}, {"type": "math", "expr": "$$R\\leftarrow\\emptyset$$", "word_idx": 81482, "sentence_idx": 1192, "label": "unlabeled"}, {"type": "text", "expr": "for \u00a0each episode  $e=1,E$ \u00a0 do", "word_idx": 81502, "sentence_idx": 1193, "label": "unlabeled"}, {"type": "math", "expr": "$$e=1,E$$", "word_idx": 81533, "sentence_idx": 1194, "label": "unlabeled"}, {"type": "text", "expr": "Initialize a random process  $\\mathcal{N}$  for action exploration", "word_idx": 81538, "sentence_idx": 1195, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{N}$$", "word_idx": 81604, "sentence_idx": 1196, "label": "unlabeled"}, {"type": "text", "expr": "Receive initial observation state  $s_{1}$", "word_idx": 81615, "sentence_idx": 1197, "label": "unlabeled"}, {"type": "math", "expr": "$$s_{1}$$", "word_idx": 81657, "sentence_idx": 1198, "label": "unlabeled"}, {"type": "text", "expr": "for \u00a0 $i=1,I$ \u00a0 do", "word_idx": 81662, "sentence_idx": 1199, "label": "unlabeled"}, {"type": "math", "expr": "$$i=1,I$$", "word_idx": 81680, "sentence_idx": 1200, "label": "unlabeled"}, {"type": "text", "expr": "$a_{i}$   $\\leftarrow$   Opt-Alg ( $-Q_{\\theta}$ ,  $s_{i}$ ,  $a_{i,0}$ )+ $\\mathcal{N}_{i}$", "word_idx": 81685, "sentence_idx": 1201, "label": "unlabeled"}, {"type": "math", "expr": "$$a_{i}$$", "word_idx": 81778, "sentence_idx": 1202, "label": "unlabeled"}, {"type": "math", "expr": "$$\\leftarrow$$", "word_idx": 81783, "sentence_idx": 1203, "label": "unlabeled"}, {"type": "text", "expr": "Opt-Alg", "word_idx": 81793, "sentence_idx": 1204, "label": "unlabeled"}, {"type": "math", "expr": "$$-Q_{\\theta}$$", "word_idx": 81800, "sentence_idx": 1205, "label": "unlabeled"}, {"type": "math", "expr": "$$s_{i}$$", "word_idx": 81811, "sentence_idx": 1206, "label": "unlabeled"}, {"type": "math", "expr": "$$a_{i,0}$$", "word_idx": 81816, "sentence_idx": 1207, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{N}_{i}$$", "word_idx": 81823, "sentence_idx": 1208, "label": "unlabeled"}, {"type": "text", "expr": "$\\triangleright$  For some initial action  $a_{i,0}$", "word_idx": 81838, "sentence_idx": 1209, "label": "unlabeled"}, {"type": "math", "expr": "$$\\triangleright$$", "word_idx": 81890, "sentence_idx": 1210, "label": "unlabeled"}, {"type": "math", "expr": "$$a_{i,0}$$", "word_idx": 81904, "sentence_idx": 1211, "label": "unlabeled"}, {"type": "text", "expr": "Execute  $a_{i}$  and observe  $r_{i+1}$  and  $s_{i+1}$", "word_idx": 81911, "sentence_idx": 1212, "label": "unlabeled"}, {"type": "math", "expr": "$$a_{i}$$", "word_idx": 81967, "sentence_idx": 1213, "label": "unlabeled"}, {"type": "math", "expr": "$$r_{i+1}$$", "word_idx": 81972, "sentence_idx": 1214, "label": "unlabeled"}, {"type": "math", "expr": "$$s_{i+1}$$", "word_idx": 81979, "sentence_idx": 1215, "label": "unlabeled"}, {"type": "text", "expr": "Insert ( $R$ ,  $(s_{i},a_{i},s_{i+1},r_{i+1})$ )", "word_idx": 81986, "sentence_idx": 1216, "label": "unlabeled"}, {"type": "text", "expr": "Insert", "word_idx": 82035, "sentence_idx": 1217, "label": "unlabeled"}, {"type": "math", "expr": "$$(s_{i},a_{i},s_{i+1},r_{i+1})$$", "word_idx": 82041, "sentence_idx": 1218, "label": "unlabeled"}, {"type": "text", "expr": "Sample a random minibatch from the replay buffer:  $R_{M}\\subseteq R$", "word_idx": 82070, "sentence_idx": 1219, "label": "unlabeled"}, {"type": "math", "expr": "$$R_{M}\\subseteq R$$", "word_idx": 82139, "sentence_idx": 1220, "label": "unlabeled"}, {"type": "text", "expr": "for \u00a0 $(s_{m},a_{m},s_{m}^{+},r_{m}^{+})\\in R_{M}$ \u00a0 do", "word_idx": 82155, "sentence_idx": 1221, "label": "unlabeled"}, {"type": "math", "expr": "$$(s_{m},a_{m},s_{m}^{+},r_{m}^{+})\\in R_{M}$$", "word_idx": 82210, "sentence_idx": 1222, "label": "unlabeled"}, {"type": "text", "expr": "$a_{m}^{+}$   $\\leftarrow$   Opt-Alg ( $-Q_{\\theta^{\\prime}}$ , $s_{m}^{+}$ , $a_{m,0}^{+}$ )", "word_idx": 82252, "sentence_idx": 1223, "label": "unlabeled"}, {"type": "math", "expr": "$$a_{m}^{+}$$", "word_idx": 82345, "sentence_idx": 1224, "label": "unlabeled"}, {"type": "math", "expr": "$$\\leftarrow$$", "word_idx": 82354, "sentence_idx": 1225, "label": "unlabeled"}, {"type": "text", "expr": "Opt-Alg", "word_idx": 82364, "sentence_idx": 1226, "label": "unlabeled"}, {"type": "math", "expr": "$$-Q_{\\theta^{\\prime}}$$", "word_idx": 82371, "sentence_idx": 1227, "label": "unlabeled"}, {"type": "math", "expr": "$$s_{m}^{+}$$", "word_idx": 82391, "sentence_idx": 1228, "label": "unlabeled"}, {"type": "math", "expr": "$$a_{m,0}^{+}$$", "word_idx": 82400, "sentence_idx": 1229, "label": "unlabeled"}, {"type": "text", "expr": "$\\triangleright$  Uses the target parameters  $\\theta^{\\prime}$", "word_idx": 82411, "sentence_idx": 1230, "label": "unlabeled"}, {"type": "math", "expr": "$$\\triangleright$$", "word_idx": 82474, "sentence_idx": 1231, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta^{\\prime}$$", "word_idx": 82488, "sentence_idx": 1232, "label": "unlabeled"}, {"type": "text", "expr": "$y_{m}$   $\\leftarrow$   $r_{m}^{+}+\\gamma Q(s_{m}^{+},a_{m}^{+}|\\theta^{\\prime})$", "word_idx": 82503, "sentence_idx": 1233, "label": "unlabeled"}, {"type": "math", "expr": "$$y_{m}$$", "word_idx": 82585, "sentence_idx": 1234, "label": "unlabeled"}, {"type": "math", "expr": "$$\\leftarrow$$", "word_idx": 82590, "sentence_idx": 1235, "label": "unlabeled"}, {"type": "math", "expr": "$$r_{m}^{+}+\\gamma Q(s_{m}^{+},a_{m}^{+}|\\theta^{\\prime})$$", "word_idx": 82600, "sentence_idx": 1236, "label": "unlabeled"}, {"type": "text", "expr": "end \u00a0 for", "word_idx": 82655, "sentence_idx": 1237, "label": "unlabeled"}, {"type": "text", "expr": "Update  $\\theta$  with a gradient step to minimize\n $\\mathcal{L}=\\frac{1}{|R_{M}|}\\sum_{m}\\big{(}\\tilde{Q}(s_{m},a_{m}|\\theta)-y_{m%\n}\\big{)}^{2}$", "word_idx": 82664, "sentence_idx": 1238, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta$$", "word_idx": 82810, "sentence_idx": 1239, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}=\\frac{1}{|R_{M}|}\\sum_{m}\\big{(}\\tilde{Q}(s_{m},a_{m}|\\theta)-y_{m%\n}\\big{)}^{2}$$", "word_idx": 82816, "sentence_idx": 1240, "label": "unlabeled"}, {"type": "text", "expr": "$\\theta^{\\prime}$   $\\leftarrow$   $\\tau\\theta+(1-\\tau)\\theta^{\\prime}$", "word_idx": 82908, "sentence_idx": 1241, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta^{\\prime}$$", "word_idx": 82979, "sentence_idx": 1242, "label": "unlabeled"}, {"type": "math", "expr": "$$\\leftarrow$$", "word_idx": 82994, "sentence_idx": 1243, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tau\\theta+(1-\\tau)\\theta^{\\prime}$$", "word_idx": 83004, "sentence_idx": 1244, "label": "unlabeled"}, {"type": "text", "expr": "$\\triangleright$  Update the target network", "word_idx": 83038, "sentence_idx": 1245, "label": "unlabeled"}, {"type": "math", "expr": "$$\\triangleright$$", "word_idx": 83081, "sentence_idx": 1246, "label": "unlabeled"}, {"type": "text", "expr": "end \u00a0 for", "word_idx": 83095, "sentence_idx": 1247, "label": "unlabeled"}, {"type": "text", "expr": "end \u00a0 for", "word_idx": 83104, "sentence_idx": 1248, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0F  Max-margin structured prediction", "word_idx": 83113, "sentence_idx": 1249, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0F", "word_idx": 83157, "sentence_idx": 1250, "label": "unlabeled"}, {"type": "text", "expr": "In the more traditional structured prediction setting, where we do not aim to\nfit the energy function directly but fit the predictions made by the system to\nsome target outputs, there are different possibilities for learning the ICNN\nparameters", "word_idx": 83167, "sentence_idx": 1251, "label": "unlabeled"}, {"type": "text", "expr": " One such method is based upon the max-margin structured\nprediction framework  \\citep tsochantaridis2005large,taskar2005learning", "word_idx": 83411, "sentence_idx": 1252, "label": "unlabeled"}, {"type": "text", "expr": " Given\nsome training example  $(x,y^{\\star})$ , we would like to require that this example\nhas a joint energy that is lower than all other possible values for  $y$ ", "word_idx": 83539, "sentence_idx": 1253, "label": "unlabeled"}, {"type": "text", "expr": " That\nis, we want the function  $\\tilde{f}$  to satisfy the constraint", "word_idx": 83703, "sentence_idx": 1254, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 83773, "sentence_idx": 1255, "label": "unlabeled"}, {"type": "math", "expr": "$$(x,y^{\\star})$$", "word_idx": 83779, "sentence_idx": 1256, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{f}$$", "word_idx": 83792, "sentence_idx": 1257, "label": "unlabeled"}, {"type": "text", "expr": "$\\tilde{f}(x,y^{\\star};\\theta)\\leq\\min_{y}\\tilde{f}(x,y;\\theta)$", "word_idx": 83801, "sentence_idx": 1258, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{f}(x,y^{\\star};\\theta)\\leq\\min_{y}\\tilde{f}(x,y;\\theta)$$", "word_idx": 83865, "sentence_idx": 1259, "label": "unlabeled"}, {"type": "text", "expr": "Unfortunately, these conditions can be trivially fit by choosing a constant\n $\\tilde{f}$  (although the entropy term alleviates this problem slightly, we can\nstill choose an approximately constant function), so instead the max-margin\napproach adds a margin-scaling term that requires this gap to be larger for  $y$ \nfurther from  $y^{\\star}$ , as measured by some loss function  $\\Delta(y,y^{\\star})$ ", "word_idx": 83927, "sentence_idx": 1260, "label": "unlabeled"}, {"type": "text", "expr": "\nAdditionally adding slack variables to allow for potential violation of these\nconstraints, we arrive at the typical max-margin structured prediction\noptimization problem", "word_idx": 84328, "sentence_idx": 1261, "label": "unlabeled"}, {"type": "math", "expr": "$$\\tilde{f}$$", "word_idx": 84498, "sentence_idx": 1262, "label": "unlabeled"}, {"type": "math", "expr": "$$y^{\\star}$$", "word_idx": 84507, "sentence_idx": 1263, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Delta(y,y^{\\star})$$", "word_idx": 84516, "sentence_idx": 1264, "label": "unlabeled"}, {"type": "text", "expr": "$\\begin{split}\\displaystyle\\minimize_{\\theta,\\xi\\geq 0}&\\displaystyle\\frac{%\n\\lambda}{2}\\|\\theta\\|_{2}^{2}+\\sum_{i=1}^{m}\\xi_{i}\\\\\n\\displaystyle\\subjectto&\\displaystyle\\tilde{f}(x_{i},y_{i};\\theta)\\leq\\min_{y%\n\\in\\mathcal{Y}}\\left(\\tilde{f}(x_{i},y;\\theta)-\\Delta(y_{i},y)\\right)-\\xi_{i}%\n\\end{split}$", "word_idx": 84535, "sentence_idx": 1265, "label": "unlabeled"}, {"type": "math", "expr": "$$\\begin{split}\\displaystyle\\minimize_{\\theta,\\xi\\geq 0}&\\displaystyle\\frac{%\n\\lambda}{2}\\|\\theta\\|_{2}^{2}+\\sum_{i=1}^{m}\\xi_{i}\\\\\n\\displaystyle\\subjectto&\\displaystyle\\tilde{f}(x_{i},y_{i};\\theta)\\leq\\min_{y%\n\\in\\mathcal{Y}}\\left(\\tilde{f}(x_{i},y;\\theta)-\\Delta(y_{i},y)\\right)-\\xi_{i}%\n\\end{split}$$", "word_idx": 84836, "sentence_idx": 1266, "label": "unlabeled"}, {"type": "text", "expr": "As a simple example, for multiclass classification tasks where  $y^{\\star}$  denotes\na \u201cone-hot\u201d encoding of examples, we can use a multi-variate entropy term and\nlet  $\\Delta(y,y^{\\star})={y^{\\star}}^{T}(1-y)$ ", "word_idx": 85135, "sentence_idx": 1267, "label": "unlabeled"}, {"type": "text", "expr": " Training\nrequires solving this \u201closs-augmented\u201d inference problem, which is convex\nfor suitable choices of the margin scaling term", "word_idx": 85346, "sentence_idx": 1268, "label": "unlabeled"}, {"type": "math", "expr": "$$y^{\\star}$$", "word_idx": 85477, "sentence_idx": 1269, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Delta(y,y^{\\star})={y^{\\star}}^{T}(1-y)$$", "word_idx": 85486, "sentence_idx": 1270, "label": "unlabeled"}, {"type": "text", "expr": "and 2) if the margin is violated, updates the network\u2019s parameters according\nto the subgradient", "word_idx": 85526, "sentence_idx": 1271, "label": "unlabeled"}, {"type": "text", "expr": "The optimization problem ( 29 ) is naturally still\n not convex  in  $\\theta$ , but can be solved via the subgradient method\nfor structured prediction  \\citep ratliff2007approximate", "word_idx": 85621, "sentence_idx": 1272, "label": "unlabeled"}, {"type": "text", "expr": " This algorithm\niteratively selects a training example  $x_{i},y_{i}$ , then 1) solves the\noptimization problem", "word_idx": 85801, "sentence_idx": 1273, "label": "unlabeled"}, {"type": "text", "expr": "not convex", "word_idx": 85912, "sentence_idx": 1274, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta$$", "word_idx": 85922, "sentence_idx": 1275, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 85928, "sentence_idx": 1276, "label": "unlabeled"}, {"type": "math", "expr": "$$x_{i},y_{i}$$", "word_idx": 85934, "sentence_idx": 1277, "label": "unlabeled"}, {"type": "text", "expr": "$y^{\\star}=\\argmin_{y\\in\\mathcal{Y}}f(x_{i},y;\\theta)-\\Delta(y_{i},y)$", "word_idx": 85945, "sentence_idx": 1278, "label": "unlabeled"}, {"type": "math", "expr": "$$y^{\\star}=\\argmin_{y\\in\\mathcal{Y}}f(x_{i},y;\\theta)-\\Delta(y_{i},y)$$", "word_idx": 86015, "sentence_idx": 1279, "label": "unlabeled"}, {"type": "text", "expr": "and 2) if the margin is violated, updates the network\u2019s parameters according\nto the subgradient", "word_idx": 86083, "sentence_idx": 1280, "label": "unlabeled"}, {"type": "text", "expr": "$\\theta:=\\mathcal{P}_{+}\\left[\\theta-\\alpha\\left(\\lambda\\theta+\\nabla_{\\theta}f%\n(x_{i},y_{i},\\theta)-\\nabla_{\\theta}f(x_{i},y^{\\star};\\theta)\\right)\\right]$", "word_idx": 86178, "sentence_idx": 1281, "label": "unlabeled"}, {"type": "math", "expr": "$$\\theta:=\\mathcal{P}_{+}\\left[\\theta-\\alpha\\left(\\lambda\\theta+\\nabla_{\\theta}f%\n(x_{i},y_{i},\\theta)-\\nabla_{\\theta}f(x_{i},y^{\\star};\\theta)\\right)\\right]$$", "word_idx": 86335, "sentence_idx": 1282, "label": "unlabeled"}, {"type": "text", "expr": "where  $\\mathcal{P}_{+}$  denotes the projection of  $W^{(z)}_{1:k-1}$  onto the non-negative\northant", "word_idx": 86490, "sentence_idx": 1283, "label": "unlabeled"}, {"type": "text", "expr": " This method can be easily adapted to use mini-batches instead of a\nsingle example per subgradient step, and also adapted to alternative optimization\nmethods like AdaGrad  \\citep duchi2011adaptive or ADAM  \\citep kingma2014adam", "word_idx": 86591, "sentence_idx": 1284, "label": "unlabeled"}, {"type": "text", "expr": "\nFurther, a fast approximate solution to  $y^{\\star}$  can be used instead\nof the exact solution", "word_idx": 86818, "sentence_idx": 1285, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{P}_{+}$$", "word_idx": 86914, "sentence_idx": 1286, "label": "unlabeled"}, {"type": "math", "expr": "$$W^{(z)}_{1:k-1}$$", "word_idx": 86929, "sentence_idx": 1287, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 86944, "sentence_idx": 1288, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 86950, "sentence_idx": 1289, "label": "unlabeled"}, {"type": "math", "expr": "$$y^{\\star}$$", "word_idx": 86956, "sentence_idx": 1290, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0G  Proof of Proposition  3", "word_idx": 86965, "sentence_idx": 1291, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0G", "word_idx": 87000, "sentence_idx": 1292, "label": "unlabeled"}, {"type": "text", "expr": "Proof (of Proposition  3 )", "word_idx": 87010, "sentence_idx": 1293, "label": "unlabeled"}, {"type": "text", "expr": "We have by the chain rule that or in matrix form and we have the the simple formula for the Jacobian product", "word_idx": 87036, "sentence_idx": 1294, "label": "unlabeled"}, {"type": "text", "expr": "We have by the chain rule that", "word_idx": 87144, "sentence_idx": 1295, "label": "unlabeled"}, {"type": "text", "expr": "$\\frac{\\partial\\ell}{\\partial\\theta}=\\frac{\\partial\\ell}{\\partial\\hat{y}}\\left(%\n\\frac{\\partial\\hat{y}}{\\partial G}\\frac{\\partial G}{\\partial\\theta}+\\frac{%\n\\partial\\hat{y}}{\\partial h}\\frac{\\partial h}{\\partial\\theta}\\right)$", "word_idx": 87174, "sentence_idx": 1296, "label": "unlabeled"}, {"type": "math", "expr": "$$\\frac{\\partial\\ell}{\\partial\\theta}=\\frac{\\partial\\ell}{\\partial\\hat{y}}\\left(%\n\\frac{\\partial\\hat{y}}{\\partial G}\\frac{\\partial G}{\\partial\\theta}+\\frac{%\n\\partial\\hat{y}}{\\partial h}\\frac{\\partial h}{\\partial\\theta}\\right).$$", "word_idx": 87400, "sentence_idx": 1297, "label": "unlabeled"}, {"type": "text", "expr": "The challenging terms to compute in this equation are the  $\\frac{\\partial\\hat{y}}{\\partial G}$  and  $\\frac{\\partial\\hat{y}}{\\partial h}$  terms", "word_idx": 87625, "sentence_idx": 1298, "label": "unlabeled"}, {"type": "text", "expr": " These can be\ncomputed (although we will ultimately not compute them explicitly, but just\ncompute the product of these matrices and other terms in the Jacobian), by\nimplicit differentiation of the KKT conditions", "word_idx": 87770, "sentence_idx": 1299, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, the\nKKT conditions of the bundle entropy method (considering only the active\nconstraints at the solution) are given by", "word_idx": 87981, "sentence_idx": 1300, "label": "unlabeled"}, {"type": "math", "expr": "$$\\frac{\\partial\\hat{y}}{\\partial G}$$", "word_idx": 88114, "sentence_idx": 1301, "label": "unlabeled"}, {"type": "math", "expr": "$$\\frac{\\partial\\hat{y}}{\\partial h}$$", "word_idx": 88148, "sentence_idx": 1302, "label": "unlabeled"}, {"type": "text", "expr": "$\\begin{split}\\displaystyle 1+\\log\\hat{y}-\\log(1-\\hat{y})+G^{T}\\lambda&%\n\\displaystyle=0\\\\\n\\displaystyle G\\hat{y}+h-t1&\\displaystyle=0\\\\\n\\displaystyle 1^{T}\\lambda&\\displaystyle=1\\end{split}$", "word_idx": 88182, "sentence_idx": 1303, "label": "unlabeled"}, {"type": "math", "expr": "$$\\begin{split}\\displaystyle 1+\\log\\hat{y}-\\log(1-\\hat{y})+G^{T}\\lambda&%\n\\displaystyle=0\\\\\n\\displaystyle G\\hat{y}+h-t1&\\displaystyle=0\\\\\n\\displaystyle 1^{T}\\lambda&\\displaystyle=1.\\end{split}$$", "word_idx": 88373, "sentence_idx": 1304, "label": "unlabeled"}, {"type": "text", "expr": "For simplicity of presentation, we consider first the Jacobian with respect to\n $h$ ", "word_idx": 88563, "sentence_idx": 1305, "label": "unlabeled"}, {"type": "text", "expr": " Taking differentials of these equations with respect to  $h$  gives", "word_idx": 88647, "sentence_idx": 1306, "label": "unlabeled"}, {"type": "text", "expr": "$\\begin{split}\\displaystyle\\diag\\left(\\frac{1}{\\hat{y}}+\\frac{1}{1-\\hat{y}}%\n\\right)\\mathsf{d}y+G^{T}\\mathsf{d}\\lambda&\\displaystyle=0\\\\\n\\displaystyle G\\mathsf{d}y+\\mathsf{d}h-\\mathsf{d}t1&\\displaystyle=0\\\\\n\\displaystyle 1^{T}\\mathsf{d}\\lambda&\\displaystyle=0\\end{split}$", "word_idx": 88715, "sentence_idx": 1307, "label": "unlabeled"}, {"type": "math", "expr": "$$\\begin{split}\\displaystyle\\diag\\left(\\frac{1}{\\hat{y}}+\\frac{1}{1-\\hat{y}}%\n\\right)\\mathsf{d}y+G^{T}\\mathsf{d}\\lambda&\\displaystyle=0\\\\\n\\displaystyle G\\mathsf{d}y+\\mathsf{d}h-\\mathsf{d}t1&\\displaystyle=0\\\\\n\\displaystyle 1^{T}\\mathsf{d}\\lambda&\\displaystyle=0\\end{split}$$", "word_idx": 88986, "sentence_idx": 1308, "label": "unlabeled"}, {"type": "text", "expr": "or in matrix form", "word_idx": 89255, "sentence_idx": 1309, "label": "unlabeled"}, {"type": "text", "expr": "$\\left[\\begin{array}[]{ccc}\\diag\\left(\\frac{1}{\\hat{y}}+\\frac{1}{1-\\hat{y}}%\n\\right)&G^{T}&0\\\\\nG&0&-1\\\\\n0&-1^{T}&0\\end{array}\\right]\\left[\\begin{array}[]{c}\\mathsf{d}y\\\\\n\\mathsf{d}\\lambda\\\\\n\\mathsf{d}t\\end{array}\\right]=\\left[\\begin{array}[]{c}0\\\\\n-\\mathsf{d}h\\\\\n0\\end{array}\\right]$", "word_idx": 89272, "sentence_idx": 1310, "label": "unlabeled"}, {"type": "math", "expr": "$$\\left[\\begin{array}[]{ccc}\\diag\\left(\\frac{1}{\\hat{y}}+\\frac{1}{1-\\hat{y}}%\n\\right)&G^{T}&0\\\\\nG&0&-1\\\\\n0&-1^{T}&0\\end{array}\\right]\\left[\\begin{array}[]{c}\\mathsf{d}y\\\\\n\\mathsf{d}\\lambda\\\\\n\\mathsf{d}t\\end{array}\\right]=\\left[\\begin{array}[]{c}0\\\\\n-\\mathsf{d}h\\\\\n0\\end{array}\\right].$$", "word_idx": 89555, "sentence_idx": 1311, "label": "unlabeled"}, {"type": "text", "expr": "To compute the Jacobian  $\\frac{\\partial\\hat{y}}{\\partial h}$  we can solve the\nsystem above with the right hand side given by  $\\mathsf{d}h=I$ , and the resulting\n $\\mathsf{d}y$  term will be the corresponding Jacobian", "word_idx": 89837, "sentence_idx": 1312, "label": "unlabeled"}, {"type": "text", "expr": " However, in our ultimate\nobjective we always left-multiply the proper terms in the above equation by\n $\\frac{\\partial\\ell}{\\partial\\hat{y}}$ ", "word_idx": 90056, "sentence_idx": 1313, "label": "unlabeled"}, {"type": "text", "expr": " Thus, we instead define", "word_idx": 90198, "sentence_idx": 1314, "label": "unlabeled"}, {"type": "math", "expr": "$$\\frac{\\partial\\hat{y}}{\\partial h}$$", "word_idx": 90222, "sentence_idx": 1315, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathsf{d}h=I$$", "word_idx": 90256, "sentence_idx": 1316, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathsf{d}y$$", "word_idx": 90269, "sentence_idx": 1317, "label": "unlabeled"}, {"type": "math", "expr": "$$\\frac{\\partial\\ell}{\\partial\\hat{y}}$$", "word_idx": 90280, "sentence_idx": 1318, "label": "unlabeled"}, {"type": "text", "expr": "$\\left[\\begin{array}[]{c}c^{y}\\\\\nc^{\\lambda}\\\\\nc^{t}\\end{array}\\right]=\\left[\\begin{array}[]{ccc}\\diag\\left(\\frac{1}{\\hat{y}}%\n+\\frac{1}{1-\\hat{y}}\\right)&G^{T}&0\\\\\nG&0&-1\\\\\n0&-1^{T}&0\\end{array}\\right]^{-1}\\left[\\begin{array}[]{c}-(\\frac{\\partial\\ell}%\n{\\partial\\hat{y}})^{T}\\\\\n0\\\\\n0\\end{array}\\right]$", "word_idx": 90316, "sentence_idx": 1319, "label": "unlabeled"}, {"type": "math", "expr": "$$\\left[\\begin{array}[]{c}c^{y}\\\\\nc^{\\lambda}\\\\\nc^{t}\\end{array}\\right]=\\left[\\begin{array}[]{ccc}\\diag\\left(\\frac{1}{\\hat{y}}%\n+\\frac{1}{1-\\hat{y}}\\right)&G^{T}&0\\\\\nG&0&-1\\\\\n0&-1^{T}&0\\end{array}\\right]^{-1}\\left[\\begin{array}[]{c}-(\\frac{\\partial\\ell}%\n{\\partial\\hat{y}})^{T}\\\\\n0\\\\\n0\\end{array}\\right]$$", "word_idx": 90619, "sentence_idx": 1320, "label": "unlabeled"}, {"type": "text", "expr": "and we have the the simple formula for the Jacobian product", "word_idx": 90920, "sentence_idx": 1321, "label": "unlabeled"}, {"type": "text", "expr": "$\\frac{\\partial\\ell}{\\partial\\hat{y}}\\frac{\\partial\\hat{y}}{\\partial h}=(c^{%\n\\lambda})^{T}$", "word_idx": 90979, "sentence_idx": 1322, "label": "unlabeled"}, {"type": "math", "expr": "$$\\frac{\\partial\\ell}{\\partial\\hat{y}}\\frac{\\partial\\hat{y}}{\\partial h}=(c^{%\n\\lambda})^{T}.$$", "word_idx": 91071, "sentence_idx": 1323, "label": "unlabeled"}, {"type": "text", "expr": "and the corresponding Jacobian products / gradients are given by Finally, using the definitions that we recover the formula presented in the proposition", "word_idx": 91162, "sentence_idx": 1324, "label": "unlabeled"}, {"type": "text", "expr": "A similar set of operations taking differentials with respect to  $G$  leads to\nthe matrix equations", "word_idx": 91314, "sentence_idx": 1325, "label": "unlabeled"}, {"type": "text", "expr": "$\\left[\\begin{array}[]{ccc}\\diag\\left(\\frac{1}{\\hat{y}}+\\frac{1}{1-\\hat{y}}%\n\\right)&G^{T}&0\\\\\nG&0&-1\\\\\n0&-1^{T}&0\\end{array}\\right]\\left[\\begin{array}[]{c}\\mathsf{d}y\\\\\n\\mathsf{d}\\lambda\\\\\n\\mathsf{d}t\\end{array}\\right]=\\left[\\begin{array}[]{c}-\\mathsf{d}G^{T}\\lambda%\n\\\\\n-\\mathsf{d}Gy\\\\\n0\\end{array}\\right]$", "word_idx": 91414, "sentence_idx": 1326, "label": "unlabeled"}, {"type": "math", "expr": "$$\\left[\\begin{array}[]{ccc}\\diag\\left(\\frac{1}{\\hat{y}}+\\frac{1}{1-\\hat{y}}%\n\\right)&G^{T}&0\\\\\nG&0&-1\\\\\n0&-1^{T}&0\\end{array}\\right]\\left[\\begin{array}[]{c}\\mathsf{d}y\\\\\n\\mathsf{d}\\lambda\\\\\n\\mathsf{d}t\\end{array}\\right]=\\left[\\begin{array}[]{c}-\\mathsf{d}G^{T}\\lambda%\n\\\\\n-\\mathsf{d}Gy\\\\\n0\\end{array}\\right]$$", "word_idx": 91722, "sentence_idx": 1327, "label": "unlabeled"}, {"type": "text", "expr": "and the corresponding Jacobian products / gradients are given by", "word_idx": 92028, "sentence_idx": 1328, "label": "unlabeled"}, {"type": "text", "expr": "$\\frac{\\partial\\ell}{\\partial\\hat{y}}\\frac{\\partial\\hat{y}}{\\partial G}=c^{y}%\n\\lambda^{T}+\\hat{y}(c^{\\lambda})^{T}$", "word_idx": 92092, "sentence_idx": 1329, "label": "unlabeled"}, {"type": "math", "expr": "$$\\frac{\\partial\\ell}{\\partial\\hat{y}}\\frac{\\partial\\hat{y}}{\\partial G}=c^{y}%\n\\lambda^{T}+\\hat{y}(c^{\\lambda})^{T}.$$", "word_idx": 92208, "sentence_idx": 1330, "label": "unlabeled"}, {"type": "text", "expr": "Finally, using the definitions that", "word_idx": 92323, "sentence_idx": 1331, "label": "unlabeled"}, {"type": "text", "expr": "$g_{i}^{T}=\\nabla_{y}f(x,y^{i};\\theta)^{T},\\;\\;h_{i}=f(x,y^{k};\\theta)-\\nabla_{%\ny}f(x,y^{i};\\theta)^{T}y^{i}$", "word_idx": 92358, "sentence_idx": 1332, "label": "unlabeled"}, {"type": "math", "expr": "$$g_{i}^{T}=\\nabla_{y}f(x,y^{i};\\theta)^{T},\\;\\;h_{i}=f(x,y^{k};\\theta)-\\nabla_{%\ny}f(x,y^{i};\\theta)^{T}y^{i}$$", "word_idx": 92468, "sentence_idx": 1333, "label": "unlabeled"}, {"type": "text", "expr": "we recover the formula presented in the proposition", "word_idx": 92576, "sentence_idx": 1334, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0H  State and action space sizes in the OpenAI gym MuJoCo benchmarks", "word_idx": 92627, "sentence_idx": 1335, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0H", "word_idx": 92703, "sentence_idx": 1336, "label": "unlabeled"}, {"type": "text", "expr": "Environment # State # Action", "word_idx": 92713, "sentence_idx": 1337, "label": "unlabeled"}, {"type": "text", "expr": "InvertedPendulum-v1 4 1", "word_idx": 92741, "sentence_idx": 1338, "label": "unlabeled"}, {"type": "text", "expr": "InvertedPendulum-v1", "word_idx": 92764, "sentence_idx": 1339, "label": "unlabeled"}, {"type": "text", "expr": "InvertedDoublePendulum-v1 11 1", "word_idx": 92783, "sentence_idx": 1340, "label": "unlabeled"}, {"type": "text", "expr": "InvertedDoublePendulum-v1", "word_idx": 92813, "sentence_idx": 1341, "label": "unlabeled"}, {"type": "text", "expr": "Reacher-v1 11 2", "word_idx": 92838, "sentence_idx": 1342, "label": "unlabeled"}, {"type": "text", "expr": "Reacher-v1", "word_idx": 92853, "sentence_idx": 1343, "label": "unlabeled"}, {"type": "text", "expr": "HalfCheetah-v1 17 6", "word_idx": 92863, "sentence_idx": 1344, "label": "unlabeled"}, {"type": "text", "expr": "HalfCheetah-v1", "word_idx": 92882, "sentence_idx": 1345, "label": "unlabeled"}, {"type": "text", "expr": "Swimmer-v1 8 2", "word_idx": 92896, "sentence_idx": 1346, "label": "unlabeled"}, {"type": "text", "expr": "Swimmer-v1", "word_idx": 92910, "sentence_idx": 1347, "label": "unlabeled"}, {"type": "text", "expr": "Hopper-v1 11 3", "word_idx": 92920, "sentence_idx": 1348, "label": "unlabeled"}, {"type": "text", "expr": "Hopper-v1", "word_idx": 92934, "sentence_idx": 1349, "label": "unlabeled"}, {"type": "text", "expr": "Walker2d-v1 17 6", "word_idx": 92943, "sentence_idx": 1350, "label": "unlabeled"}, {"type": "text", "expr": "Walker2d-v1", "word_idx": 92959, "sentence_idx": 1351, "label": "unlabeled"}, {"type": "text", "expr": "Ant-v1 111 8", "word_idx": 92970, "sentence_idx": 1352, "label": "unlabeled"}, {"type": "text", "expr": "Ant-v1", "word_idx": 92982, "sentence_idx": 1353, "label": "unlabeled"}, {"type": "text", "expr": "Humanoid-v1 376 17", "word_idx": 92988, "sentence_idx": 1354, "label": "unlabeled"}, {"type": "text", "expr": "Humanoid-v1", "word_idx": 93006, "sentence_idx": 1355, "label": "unlabeled"}, {"type": "text", "expr": "HumanoidStandup-v1 376 17", "word_idx": 93017, "sentence_idx": 1356, "label": "unlabeled"}, {"type": "text", "expr": "HumanoidStandup-v1", "word_idx": 93042, "sentence_idx": 1357, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a04:  State and action space sizes in the OpenAI gym MuJoCo benchmarks", "word_idx": 93060, "sentence_idx": 1358, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a04:", "word_idx": 93134, "sentence_idx": 1359, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0I  Synthetic classification examples", "word_idx": 93142, "sentence_idx": 1360, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0I", "word_idx": 93187, "sentence_idx": 1361, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:  \nFICNN (top) and PICNN (bottom) classification of synthetic non-convex\ndecision boundaries", "word_idx": 93197, "sentence_idx": 1362, "label": "unlabeled"}, {"type": "text", "expr": " Best viewed in color", "word_idx": 93298, "sentence_idx": 1363, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:", "word_idx": 93319, "sentence_idx": 1364, "label": "unlabeled"}, {"type": "text", "expr": "We begin with a simple example to illustrate the classification performance of a\ntwo-hidden-layer FICNN and PICNN on two-dimensional binary classification\ntasks from the scikit-learn toolkit  \\citep pedregosa2011scikit", "word_idx": 93328, "sentence_idx": 1365, "label": "unlabeled"}, {"type": "text", "expr": "\nFigure\u00a0 4  shows the classification performance on the dataset", "word_idx": 93546, "sentence_idx": 1366, "label": "unlabeled"}, {"type": "text", "expr": "\nThe FICNN\u2019s energy function which is fully convex in  $\\cal{X}\\times\\cal{Y}$  jointly is\nable to capture complex, but sometimes restrictive decision boundaries", "word_idx": 93609, "sentence_idx": 1367, "label": "unlabeled"}, {"type": "text", "expr": "\nThe PICNN, which is nonconvex over  $\\cal{X}$  but convex over  $\\cal{Y}$ \novercomes these restrictions and can capture more complex decision boundaries", "word_idx": 93769, "sentence_idx": 1368, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 93922, "sentence_idx": 1369, "label": "unlabeled"}, {"type": "math", "expr": "$$\\cal{X}\\times\\cal{Y}$$", "word_idx": 93928, "sentence_idx": 1370, "label": "unlabeled"}, {"type": "math", "expr": "$$\\cal{X}$$", "word_idx": 93948, "sentence_idx": 1371, "label": "unlabeled"}, {"type": "math", "expr": "$$\\cal{Y}$$", "word_idx": 93955, "sentence_idx": 1372, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0J  Multi-Label Classification Training Plots", "word_idx": 93962, "sentence_idx": 1373, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0J", "word_idx": 94015, "sentence_idx": 1374, "label": "unlabeled"}, {"type": "text", "expr": "In Figure\u00a0 5 ", "word_idx": 94025, "sentence_idx": 1375, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a05:  \nTraining (blue) and test (red) macro-F1 score of\na feedforward network (left) and PICNN (right) on the BibTeX\nmulti-label classification dataset", "word_idx": 94038, "sentence_idx": 1376, "label": "unlabeled"}, {"type": "text", "expr": "\nThe final test F1 scores are 0", "word_idx": 94194, "sentence_idx": 1377, "label": "unlabeled"}, {"type": "text", "expr": "396 and 0", "word_idx": 94225, "sentence_idx": 1378, "label": "unlabeled"}, {"type": "text", "expr": "415, respectively", "word_idx": 94234, "sentence_idx": 1379, "label": "unlabeled"}, {"type": "text", "expr": "\n(Higher is better", "word_idx": 94251, "sentence_idx": 1380, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a05:", "word_idx": 94269, "sentence_idx": 1381, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0K  Image Completion", "word_idx": 94278, "sentence_idx": 1382, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0K", "word_idx": 94306, "sentence_idx": 1383, "label": "unlabeled"}, {"type": "text", "expr": "The losses are in Figure\u00a0 6 ", "word_idx": 94316, "sentence_idx": 1384, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a06:  Mean Squared Error (MSE) on the train\n(blue, rolling over 1 epoch) and test (red) images\nfrom Olivetti faces for PICNNs trained with the\nbundle entropy method (left) and back optimization (center),\nand back optimization with the convexity constraint relaxed (right)", "word_idx": 94344, "sentence_idx": 1385, "label": "unlabeled"}, {"type": "text", "expr": "\nThe minimum test MSEs are 833", "word_idx": 94620, "sentence_idx": 1386, "label": "unlabeled"}, {"type": "text", "expr": "0, 872", "word_idx": 94650, "sentence_idx": 1387, "label": "unlabeled"}, {"type": "text", "expr": "0, and 850", "word_idx": 94656, "sentence_idx": 1388, "label": "unlabeled"}, {"type": "text", "expr": "9, respectively", "word_idx": 94666, "sentence_idx": 1389, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a06:", "word_idx": 94681, "sentence_idx": 1390, "label": "unlabeled"}, {"type": "text", "expr": "Generated  on Tue Apr  3 11:32:35 2018 by", "word_idx": 94690, "sentence_idx": 1391, "label": "unlabeled"}, {"type": "text", "expr": "LaTeXML", "word_idx": 94731, "sentence_idx": 1392, "label": "unlabeled"}], "transfer_learning": [{"type": "text", "expr": "Untitled Document", "word_idx": 0, "sentence_idx": 0, "label": "unlabeled"}, {"type": "text", "expr": "Abstract Classical distillation methods transfer representations from a \u201cteacher\u201d neural network to a \u201cstudent\u201d network by matching their output activations", "word_idx": 17, "sentence_idx": 1, "label": "unlabeled"}, {"type": "text", "expr": " Recent methods also match the Jacobians, or the gradient of output activations with the input", "word_idx": 173, "sentence_idx": 2, "label": "unlabeled"}, {"type": "text", "expr": " However, this involves making some ad hoc decisions, in particular, the choice of the loss function", "word_idx": 267, "sentence_idx": 3, "label": "unlabeled"}, {"type": "text", "expr": " In this paper, we first establish an equivalence between Jacobian matching and distillation with input noise, from which we derive appropriate loss functions for Jacobian matching", "word_idx": 367, "sentence_idx": 4, "label": "unlabeled"}, {"type": "text", "expr": " We then rely on this analysis to apply Jacobian matching to transfer learning by establishing equivalence of a recent transfer learning procedure to distillation", "word_idx": 547, "sentence_idx": 5, "label": "unlabeled"}, {"type": "text", "expr": " We then show experimentally on standard image datasets that Jacobian-based penalties improve distillation, robustness to noisy inputs, and transfer learning", "word_idx": 709, "sentence_idx": 6, "label": "unlabeled"}, {"type": "text", "expr": "Abstract", "word_idx": 866, "sentence_idx": 7, "label": "unlabeled"}, {"type": "text", "expr": "Classical distillation methods transfer representations from a \u201cteacher\u201d neural network to a \u201cstudent\u201d network by matching their output activations", "word_idx": 874, "sentence_idx": 8, "label": "unlabeled"}, {"type": "text", "expr": " Recent methods also match the Jacobians, or the gradient of output activations with the input", "word_idx": 1021, "sentence_idx": 9, "label": "unlabeled"}, {"type": "text", "expr": " However, this involves making some ad hoc decisions, in particular, the choice of the loss function", "word_idx": 1115, "sentence_idx": 10, "label": "unlabeled"}, {"type": "text", "expr": "In this paper, we first establish an equivalence between Jacobian matching and distillation with input noise, from which we derive appropriate loss functions for Jacobian matching", "word_idx": 1215, "sentence_idx": 11, "label": "unlabeled"}, {"type": "text", "expr": " We then rely on this analysis to apply Jacobian matching to transfer learning by establishing equivalence of a recent transfer learning procedure to distillation", "word_idx": 1394, "sentence_idx": 12, "label": "unlabeled"}, {"type": "text", "expr": "We then show experimentally on standard image datasets that Jacobian-based penalties improve distillation, robustness to noisy inputs, and transfer learning", "word_idx": 1556, "sentence_idx": 13, "label": "unlabeled"}, {"type": "text", "expr": "\\icmltitlerunning", "word_idx": 1712, "sentence_idx": 14, "label": "unlabeled"}, {"type": "text", "expr": "KnowledgeTransferwithJacobianMatching", "word_idx": 1729, "sentence_idx": 15, "label": "unlabeled"}, {"type": "text", "expr": "KnowledgeTransferwithJacobianMatching", "word_idx": 1766, "sentence_idx": 16, "label": "unlabeled"}, {"type": "text", "expr": "\\printAffiliationsAndNotice", "word_idx": 1803, "sentence_idx": 17, "label": "unlabeled"}, {"type": "text", "expr": "1  Introduction", "word_idx": 1830, "sentence_idx": 18, "label": "unlabeled"}, {"type": "text", "expr": "Consider that we are given a neural network  $\\mathcal{A}$  trained on a particular dataset, and want to train another neural network  $\\mathcal{B}$  on a similar (or related) dataset", "word_idx": 1845, "sentence_idx": 19, "label": "unlabeled"}, {"type": "text", "expr": " Is it possible to leverage  $\\mathcal{A}$  to train  $\\mathcal{B}$  more efficiently? We call this the problem of  knowledge transfer ", "word_idx": 2028, "sentence_idx": 20, "label": "unlabeled"}, {"type": "text", "expr": " Distillation\u00a0 \\citep hinton2015distilling is a form of knowledge transfer where  $\\mathcal{A}$  and  $\\mathcal{B}$  are trained on the same dataset, but have different architectures", "word_idx": 2163, "sentence_idx": 21, "label": "unlabeled"}, {"type": "text", "expr": " Transfer Learning\u00a0 \\citep pan2010survey is another form of knowledge transfer where  $\\mathcal{A}$  and  $\\mathcal{B}$  are trained on different (but related) datasets", "word_idx": 2345, "sentence_idx": 22, "label": "unlabeled"}, {"type": "text", "expr": " If the architectures are the same, we can in both cases simply copy the weights from  $\\mathcal{A}$  to  $\\mathcal{B}$ ", "word_idx": 2513, "sentence_idx": 23, "label": "unlabeled"}, {"type": "text", "expr": " The problem becomes more challenging when  $\\mathcal{A}$  and  $\\mathcal{B}$  have different architectures", "word_idx": 2633, "sentence_idx": 24, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{A}$$", "word_idx": 2740, "sentence_idx": 25, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{B}$$", "word_idx": 2751, "sentence_idx": 26, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{A}$$", "word_idx": 2762, "sentence_idx": 27, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{B}$$", "word_idx": 2773, "sentence_idx": 28, "label": "unlabeled"}, {"type": "text", "expr": "knowledge transfer", "word_idx": 2784, "sentence_idx": 29, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 2802, "sentence_idx": 30, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{A}$$", "word_idx": 2808, "sentence_idx": 31, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{B}$$", "word_idx": 2819, "sentence_idx": 32, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 2830, "sentence_idx": 33, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{A}$$", "word_idx": 2836, "sentence_idx": 34, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{B}$$", "word_idx": 2847, "sentence_idx": 35, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{A}$$", "word_idx": 2858, "sentence_idx": 36, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{B}$$", "word_idx": 2869, "sentence_idx": 37, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{A}$$", "word_idx": 2880, "sentence_idx": 38, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{B}$$", "word_idx": 2891, "sentence_idx": 39, "label": "unlabeled"}, {"type": "text", "expr": "A perfect distillation method would enable us to transform one neural network architecture into another, while preserving generalization", "word_idx": 2902, "sentence_idx": 40, "label": "unlabeled"}, {"type": "text", "expr": " This would allow us to easily explore the space of neural network architectures, which can be used for neural network architecture search, model compression, or creating diverse ensembles", "word_idx": 3038, "sentence_idx": 41, "label": "unlabeled"}, {"type": "text", "expr": " A perfect transfer learning method, on the other hand, would use little data to train  $\\mathcal{B}$ , optimally using the limited samples at it\u2019s disposal", "word_idx": 3226, "sentence_idx": 42, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{B}$$", "word_idx": 3382, "sentence_idx": 43, "label": "unlabeled"}, {"type": "text", "expr": "This paper deals with the problem of knowledge transfer based on matching the Jacobian of the network\u2019s output which, like the output itself, has a dimension independent of the network\u2019s architecture", "word_idx": 3393, "sentence_idx": 44, "label": "unlabeled"}, {"type": "text", "expr": " This approach has also been recently explored for the case of distillation by\u00a0 \\citet czarnecki2017sobolev, who considered the general idea of matching Jacobians, and by\u00a0 \\citet zagoruyko2016paying who view Jacobians as attention maps", "word_idx": 3592, "sentence_idx": 45, "label": "unlabeled"}, {"type": "text", "expr": " However, in both of these cases, it was not clear what loss function must be used to match Jacobians", "word_idx": 3827, "sentence_idx": 46, "label": "unlabeled"}, {"type": "text", "expr": " It was also unclear how these methods relate to classical distillation approaches\u00a0 \\citep ba2014deep, hinton2015distilling", "word_idx": 3928, "sentence_idx": 47, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 4051, "sentence_idx": 48, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 4057, "sentence_idx": 49, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 4063, "sentence_idx": 50, "label": "unlabeled"}, {"type": "text", "expr": "Recently\u00a0 \\citet li2016learning proposed a distillation-like approach to perform transfer learning", "word_idx": 4069, "sentence_idx": 51, "label": "unlabeled"}, {"type": "text", "expr": " While this approach works well in practice, it was not clear how this exactly relates to regular distillation", "word_idx": 4167, "sentence_idx": 52, "label": "unlabeled"}, {"type": "text", "expr": " It is also not clear how this applies to the challenging setting of transfer learning where the architectures of both networks  $\\mathcal{A}$  and  $\\mathcal{B}$  can be arbitrary", "word_idx": 4277, "sentence_idx": 53, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 4457, "sentence_idx": 54, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{A}$$", "word_idx": 4463, "sentence_idx": 55, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{B}$$", "word_idx": 4474, "sentence_idx": 56, "label": "unlabeled"}, {"type": "text", "expr": "The overall contributions of our paper are:", "word_idx": 4485, "sentence_idx": 57, "label": "unlabeled"}, {"type": "text", "expr": "The overall contributions of our paper are:", "word_idx": 4528, "sentence_idx": 58, "label": "unlabeled"}, {"type": "text", "expr": "We show that matching Jacobians is a special case of classical distillation, where noise is added to the inputs", "word_idx": 4571, "sentence_idx": 59, "label": "unlabeled"}, {"type": "text", "expr": "We show that matching Jacobians is a special case of classical distillation, where noise is added to the inputs", "word_idx": 4682, "sentence_idx": 60, "label": "unlabeled"}, {"type": "text", "expr": "We show that a recent transfer learning method (LwF by  \\citealp li2016learning) can be viewed as distillation, which allows us to match Jacobians for this case", "word_idx": 4793, "sentence_idx": 61, "label": "unlabeled"}, {"type": "text", "expr": "\\citealp", "word_idx": 4953, "sentence_idx": 62, "label": "unlabeled"}, {"type": "text", "expr": "We provide methods to match Jacobians of practical deep networks, where architecture of both networks are arbitrary", "word_idx": 4961, "sentence_idx": 63, "label": "unlabeled"}, {"type": "text", "expr": "We provide methods to match Jacobians of practical deep networks, where architecture of both networks are arbitrary", "word_idx": 5076, "sentence_idx": 64, "label": "unlabeled"}, {"type": "text", "expr": "We experimentally validate these results by providing evidence that Jacobian matching helps both regular distillation and transfer learning, and that Jacobian-norm penalties learn models robust to noise", "word_idx": 5191, "sentence_idx": 65, "label": "unlabeled"}, {"type": "text", "expr": "We experimentally validate these results by providing evidence that Jacobian matching helps both regular distillation and transfer learning, and that Jacobian-norm penalties learn models robust to noise", "word_idx": 5393, "sentence_idx": 66, "label": "unlabeled"}, {"type": "text", "expr": "2  Related Work", "word_idx": 5595, "sentence_idx": 67, "label": "unlabeled"}, {"type": "text", "expr": "Several Jacobian-based regularizers have been proposed in recent times", "word_idx": 5610, "sentence_idx": 68, "label": "unlabeled"}, {"type": "text", "expr": " Sobolev training\u00a0 \\citep czarnecki2017sobolev, showed that using higher order derivatives along with the targets can help in training with less data", "word_idx": 5680, "sentence_idx": 69, "label": "unlabeled"}, {"type": "text", "expr": " This work is similar to ours", "word_idx": 5829, "sentence_idx": 70, "label": "unlabeled"}, {"type": "text", "expr": " While we also make similar claims, we clarify the relationship of this method with regular distillation based on matching activations, and show how it can help", "word_idx": 5858, "sentence_idx": 71, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, we show how the loss function used for activation matching also decides the loss function we use for Jacobian matching", "word_idx": 6018, "sentence_idx": 72, "label": "unlabeled"}, {"type": "text", "expr": " Similarly,  \\citet wang2016analysis use the Jacobian for distillation and show that it helps improve performance", "word_idx": 6151, "sentence_idx": 73, "label": "unlabeled"}, {"type": "text", "expr": "  \\citet zagoruyko2016paying introduce the idea of matching attention maps", "word_idx": 6264, "sentence_idx": 74, "label": "unlabeled"}, {"type": "text", "expr": " The Jacobian was also considered as one such attention map", "word_idx": 6338, "sentence_idx": 75, "label": "unlabeled"}, {"type": "text", "expr": " This work also finds that combining both activation matching and Jacobian matching is helpful", "word_idx": 6397, "sentence_idx": 76, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 6491, "sentence_idx": 77, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 6497, "sentence_idx": 78, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 6503, "sentence_idx": 79, "label": "unlabeled"}, {"type": "text", "expr": "Jacobian-norm regularizers were used in early works by\u00a0 \\citet drucker1992improving, where they looked at penalizing the Jacobian norm", "word_idx": 6509, "sentence_idx": 80, "label": "unlabeled"}, {"type": "text", "expr": " The intuition was to make the model more robust to small changes in the input", "word_idx": 6643, "sentence_idx": 81, "label": "unlabeled"}, {"type": "text", "expr": " We find that this conforms to our analysis as well", "word_idx": 6721, "sentence_idx": 82, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 6772, "sentence_idx": 83, "label": "unlabeled"}, {"type": "text", "expr": "Knowledge Distillation\u00a0 \\citep hinton2015distilling first showed that one can use softmax with temperature to perform knowledge transfer with neural nets", "word_idx": 6778, "sentence_idx": 84, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 \\citet ba2014deep found that squared error between logits worked better than the softmax method, and they used this method to train shallow nets with equivalent performance to deep nets", "word_idx": 6931, "sentence_idx": 85, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 \\citet romero2014fitnets and\u00a0 \\citet zagoruyko2016paying showed how to enhance distillation by matching intermediate features along with the outputs, but use different methods to do so", "word_idx": 7118, "sentence_idx": 86, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 \\citet sau2016deep found that adding noise to logits helps during teacher-student training", "word_idx": 7304, "sentence_idx": 87, "label": "unlabeled"}, {"type": "text", "expr": " We show that the use of the Jacobian can be interpreted as adding such noise to the inputs analytically", "word_idx": 7396, "sentence_idx": 88, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 7500, "sentence_idx": 89, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 7506, "sentence_idx": 90, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 7512, "sentence_idx": 91, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 7518, "sentence_idx": 92, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 7524, "sentence_idx": 93, "label": "unlabeled"}, {"type": "text", "expr": "3  Jacobians of Neural Networks", "word_idx": 7530, "sentence_idx": 94, "label": "unlabeled"}, {"type": "text", "expr": "Let us consider the first order Taylor series expansion of a function  $f:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}$  around a small neighborhood  $\\{\\mathbf{x}+\\Delta\\mathbf{x}:\\|\\Delta\\mathbf{x}\\|\\leq\\epsilon\\}$ ", "word_idx": 7561, "sentence_idx": 95, "label": "unlabeled"}, {"type": "text", "expr": " It can be written as", "word_idx": 7769, "sentence_idx": 96, "label": "unlabeled"}, {"type": "math", "expr": "$$f:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}$$", "word_idx": 7790, "sentence_idx": 97, "label": "unlabeled"}, {"type": "math", "expr": "$$\\{\\mathbf{x}+\\Delta\\mathbf{x}:\\|\\Delta\\mathbf{x}\\|\\leq\\epsilon\\}$$", "word_idx": 7827, "sentence_idx": 98, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle f(\\mathbf{x}+\\Delta\\mathbf{x})$", "word_idx": 7891, "sentence_idx": 99, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle f(\\mathbf{x}+\\Delta\\mathbf{x})$$", "word_idx": 7937, "sentence_idx": 100, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=f(\\mathbf{x})+\\nabla_{x}f(\\mathbf{x})^{T}(\\Delta\\mathbf{x})+%\n\\mathcal{O}(\\epsilon^{2})$", "word_idx": 7981, "sentence_idx": 101, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=f(\\mathbf{x})+\\nabla_{x}f(\\mathbf{x})^{T}(\\Delta\\mathbf{x})+%\n\\mathcal{O}(\\epsilon^{2})$$", "word_idx": 8084, "sentence_idx": 102, "label": "unlabeled"}, {"type": "text", "expr": "We can apply this linearization to neural nets", "word_idx": 8185, "sentence_idx": 103, "label": "unlabeled"}, {"type": "text", "expr": " The source of non-linearity for neural nets lie in the elementwise non-linear activations (like ReLU, sigmoid) and pooling operators", "word_idx": 8231, "sentence_idx": 104, "label": "unlabeled"}, {"type": "text", "expr": "  It is easy to see that to linearize the entire neural network, one must linearize all such non-linearities in the network", "word_idx": 8364, "sentence_idx": 105, "label": "unlabeled"}, {"type": "text", "expr": "It is easy to see that to linearize the entire neural network, one must linearize all such non-linearities in the network", "word_idx": 8487, "sentence_idx": 106, "label": "unlabeled"}, {"type": "text", "expr": "1  Special case: ReLU and MaxPool", "word_idx": 8608, "sentence_idx": 107, "label": "unlabeled"}, {"type": "text", "expr": "For the ReLU nonlinearity, the Taylor approximation is locally exact and simple to compute, as the derivative  $\\frac{\\mathrm{d}\\sigma(z)}{\\mathrm{d}z}$  is either 0 or 1 (except at  $z=0$ , where it is undefined)", "word_idx": 8641, "sentence_idx": 108, "label": "unlabeled"}, {"type": "text", "expr": " A similar statement holds for max-pooling", "word_idx": 8854, "sentence_idx": 109, "label": "unlabeled"}, {"type": "text", "expr": " Going back to the definition in Equation  1 , for piecewise linear nets there exist  $\\epsilon>0$  such that the super-linear terms are exactly zero,  i", "word_idx": 8896, "sentence_idx": 110, "label": "unlabeled"}, {"type": "text", "expr": " ;  $f(\\mathbf{x}+\\Delta\\mathbf{x})=f(\\mathbf{x})+\\nabla_{x}f(\\mathbf{x})^{T}(%\n\\Delta\\mathbf{x})$ ,  i", "word_idx": 9049, "sentence_idx": 111, "label": "unlabeled"}, {"type": "text", "expr": " ;  $\\mathbf{x}$  and  $\\Delta\\mathbf{x}$  lie on the same linear surface", "word_idx": 9152, "sentence_idx": 112, "label": "unlabeled"}, {"type": "math", "expr": "$$\\frac{\\mathrm{d}\\sigma(z)}{\\mathrm{d}z}$$", "word_idx": 9225, "sentence_idx": 113, "label": "unlabeled"}, {"type": "math", "expr": "$$z=0$$", "word_idx": 9264, "sentence_idx": 114, "label": "unlabeled"}, {"type": "math", "expr": "$$\\epsilon>0$$", "word_idx": 9267, "sentence_idx": 115, "label": "unlabeled"}, {"type": "math", "expr": "$$f(\\mathbf{x}+\\Delta\\mathbf{x})=f(\\mathbf{x})+\\nabla_{x}f(\\mathbf{x})^{T}(%\n\\Delta\\mathbf{x})$$", "word_idx": 9277, "sentence_idx": 116, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}$$", "word_idx": 9369, "sentence_idx": 117, "label": "unlabeled"}, {"type": "math", "expr": "$$\\Delta\\mathbf{x}$$", "word_idx": 9379, "sentence_idx": 118, "label": "unlabeled"}, {"type": "text", "expr": "2  Invariance to weight and architecture specification", "word_idx": 9395, "sentence_idx": 119, "label": "unlabeled"}, {"type": "text", "expr": "One useful property of the Jacobian is that its size does not depend on the network architecture", "word_idx": 9449, "sentence_idx": 120, "label": "unlabeled"}, {"type": "text", "expr": " For  $k$  output classes, and input dimension  $D$  , the Jacobian of a neural network is of dimension  $D\\times k$ ", "word_idx": 9545, "sentence_idx": 121, "label": "unlabeled"}, {"type": "text", "expr": " This means that one can compare Jacobians of different architectures", "word_idx": 9662, "sentence_idx": 122, "label": "unlabeled"}, {"type": "math", "expr": "$$D\\times k$$", "word_idx": 9731, "sentence_idx": 123, "label": "unlabeled"}, {"type": "text", "expr": "Another useful property is that for a given neural network architecture, different weight configurations can lead to the same Jacobian", "word_idx": 9740, "sentence_idx": 124, "label": "unlabeled"}, {"type": "text", "expr": " One simple example of this is permutation symmetry of neurons in intermediate hidden layers", "word_idx": 9874, "sentence_idx": 125, "label": "unlabeled"}, {"type": "text", "expr": " It is easy to see different permutations of neurons leave the Jacobian unchanged (as they have the same underlying function)", "word_idx": 9966, "sentence_idx": 126, "label": "unlabeled"}, {"type": "text", "expr": " In general, because of redundancy of neural network models and non-convexity of the loss surface, several different weight configurations can end up having similar Jacobians", "word_idx": 10091, "sentence_idx": 127, "label": "unlabeled"}, {"type": "text", "expr": "Another useful property is that for a given neural network architecture, different weight configurations can lead to the same Jacobian", "word_idx": 10265, "sentence_idx": 128, "label": "unlabeled"}, {"type": "text", "expr": " One simple example of this is permutation symmetry of neurons in intermediate hidden layers", "word_idx": 10399, "sentence_idx": 129, "label": "unlabeled"}, {"type": "text", "expr": " It is easy to see different permutations of neurons leave the Jacobian unchanged (as they have the same underlying function)", "word_idx": 10491, "sentence_idx": 130, "label": "unlabeled"}, {"type": "text", "expr": " In general, because of redundancy of neural network models and non-convexity of the loss surface, several different weight configurations can end up having similar Jacobians", "word_idx": 10616, "sentence_idx": 131, "label": "unlabeled"}, {"type": "text", "expr": "Given these desirable properties, we consider using the Jacobian to perform knowledge transfer between neural networks of different architectures", "word_idx": 10790, "sentence_idx": 132, "label": "unlabeled"}, {"type": "text", "expr": " Note that these properties hold trivially for output activations also", "word_idx": 10935, "sentence_idx": 133, "label": "unlabeled"}, {"type": "text", "expr": " Thus it seems sensible that both these quantities must be used for knowledge transfer", "word_idx": 11005, "sentence_idx": 134, "label": "unlabeled"}, {"type": "text", "expr": " However, the important practical question remains: how exactly should this be done?", "word_idx": 11091, "sentence_idx": 135, "label": "unlabeled"}, {"type": "text", "expr": "Given these desirable properties, we consider using the Jacobian to perform knowledge transfer between neural networks of different architectures", "word_idx": 11175, "sentence_idx": 136, "label": "unlabeled"}, {"type": "text", "expr": " Note that these properties hold trivially for output activations also", "word_idx": 11320, "sentence_idx": 137, "label": "unlabeled"}, {"type": "text", "expr": " Thus it seems sensible that both these quantities must be used for knowledge transfer", "word_idx": 11390, "sentence_idx": 138, "label": "unlabeled"}, {"type": "text", "expr": " However, the important practical question remains: how exactly should this be done?", "word_idx": 11476, "sentence_idx": 139, "label": "unlabeled"}, {"type": "text", "expr": "4  Distillation", "word_idx": 11560, "sentence_idx": 140, "label": "unlabeled"}, {"type": "text", "expr": "We consider the problem of improving distillation using Jacobians", "word_idx": 11575, "sentence_idx": 141, "label": "unlabeled"}, {"type": "text", "expr": " This problem of distillation may be posed as follows: given a  teacher  network  $\\mathcal{T}$  which is trained on a dataset  $\\mathcal{D}$ , we wish to enhance the training of a student network  $\\mathcal{S}$  on  $\\mathcal{D}$  using \u201chints\u201d from  $\\mathcal{T}$ ", "word_idx": 11640, "sentence_idx": 142, "label": "unlabeled"}, {"type": "text", "expr": " Classically, such \u201chints\u201d involve activations of the output layer or some intermediate layers", "word_idx": 11906, "sentence_idx": 143, "label": "unlabeled"}, {"type": "text", "expr": " Recent works\u00a0 \\citep czarnecki2017sobolev, zagoruyko2016paying sought to match the Jacobians of  $\\mathcal{S}$  and  $\\mathcal{T}$ ", "word_idx": 12000, "sentence_idx": 144, "label": "unlabeled"}, {"type": "text", "expr": " However, two aspects are not clear in these formalisms: (i) what penalty term must be used between Jacobians, and (ii) how this idea of matching Jacobians relates to simpler methods such as activation matching\u00a0 \\citep ba2014deep, hinton2015distilling", "word_idx": 12132, "sentence_idx": 145, "label": "unlabeled"}, {"type": "text", "expr": " To resolve these issues, we make the following claim", "word_idx": 12383, "sentence_idx": 146, "label": "unlabeled"}, {"type": "text", "expr": "teacher", "word_idx": 12436, "sentence_idx": 147, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{T}$$", "word_idx": 12443, "sentence_idx": 148, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{D}$$", "word_idx": 12454, "sentence_idx": 149, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{S}$$", "word_idx": 12465, "sentence_idx": 150, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{D}$$", "word_idx": 12476, "sentence_idx": 151, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{T}$$", "word_idx": 12487, "sentence_idx": 152, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 12498, "sentence_idx": 153, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{S}$$", "word_idx": 12504, "sentence_idx": 154, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{T}$$", "word_idx": 12515, "sentence_idx": 155, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 12526, "sentence_idx": 156, "label": "unlabeled"}, {"type": "text", "expr": "Claim ", "word_idx": 12532, "sentence_idx": 157, "label": "unlabeled"}, {"type": "text", "expr": "Claim", "word_idx": 12538, "sentence_idx": 158, "label": "unlabeled"}, {"type": "text", "expr": "Matching Jacobians of two networks is equivalent to matching soft targets with noise added to the inputs during training", "word_idx": 12543, "sentence_idx": 159, "label": "unlabeled"}, {"type": "text", "expr": "Matching Jacobians of two networks is equivalent to matching soft targets with noise added to the inputs during training", "word_idx": 12663, "sentence_idx": 160, "label": "unlabeled"}, {"type": "text", "expr": "Matching Jacobians of two networks is equivalent to matching soft targets with noise added to the inputs during training", "word_idx": 12783, "sentence_idx": 161, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:  Illustration of teacher-student learning in a simple 1D case", "word_idx": 12903, "sentence_idx": 162, "label": "unlabeled"}, {"type": "text", "expr": " Here, x-axis is the input data, and y-axis denotes function outputs", "word_idx": 12974, "sentence_idx": 163, "label": "unlabeled"}, {"type": "text", "expr": " Given a limited number of data points, there exist multiple student functions consistent with the data", "word_idx": 13042, "sentence_idx": 164, "label": "unlabeled"}, {"type": "text", "expr": " How do we select the hypothesis closest to the teacher\u2019s? There are two equivalent solutions: either by augmenting the data set by adding noise to the inputs or by directly matching slopes (Jacobians) of the function at the data points", "word_idx": 13145, "sentence_idx": 165, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:", "word_idx": 13381, "sentence_idx": 166, "label": "unlabeled"}, {"type": "text", "expr": "More concretely, we make the following proposition", "word_idx": 13390, "sentence_idx": 167, "label": "unlabeled"}, {"type": "text", "expr": "More concretely, we make the following proposition", "word_idx": 13440, "sentence_idx": 168, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 1 ", "word_idx": 13490, "sentence_idx": 169, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 1", "word_idx": 13504, "sentence_idx": 170, "label": "unlabeled"}, {"type": "text", "expr": "Consider the squared error cost function for matching soft targets of two neural networks with  $k$ -length targets ( $\\in\\mathbb{R}^{k}$ ), given by  $\\ell(\\mathcal{T}(\\mathbf{x}),\\mathcal{S}(\\mathbf{x}))=\\sum_{i=1}^{k}(\\mathcal{%\nT}^{i}(\\mathbf{x})-\\mathcal{S}^{i}(\\mathbf{x}))^{2}$ , where  $\\mathbf{x}\\in\\mathbb{R}^{D}$  is an input data point", "word_idx": 13517, "sentence_idx": 171, "label": "unlabeled"}, {"type": "text", "expr": " Let  $\\bm{\\xi}~{}(\\in\\mathbb{R}^{D})=\\sigma~{}\\mathbf{z}$  be a scaled version of a unit normal random variable  $\\mathbf{z}~{}\\in\\mathbb{R}^{D}$  with scaling factor  $\\sigma\\in\\mathbb{R}$ ", "word_idx": 13864, "sentence_idx": 172, "label": "unlabeled"}, {"type": "text", "expr": " Then the following is true", "word_idx": 14055, "sentence_idx": 173, "label": "unlabeled"}, {"type": "math", "expr": "$$\\in\\mathbb{R}^{k}$$", "word_idx": 14082, "sentence_idx": 174, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ell(\\mathcal{T}(\\mathbf{x}),\\mathcal{S}(\\mathbf{x}))=\\sum_{i=1}^{k}(\\mathcal{%\nT}^{i}(\\mathbf{x})-\\mathcal{S}^{i}(\\mathbf{x}))^{2}$$", "word_idx": 14099, "sentence_idx": 175, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}\\in\\mathbb{R}^{D}$$", "word_idx": 14230, "sentence_idx": 176, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{\\xi}~{}(\\in\\mathbb{R}^{D})=\\sigma~{}\\mathbf{z}$$", "word_idx": 14257, "sentence_idx": 177, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{z}~{}\\in\\mathbb{R}^{D}$$", "word_idx": 14307, "sentence_idx": 178, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma\\in\\mathbb{R}$$", "word_idx": 14337, "sentence_idx": 179, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\mathbb{E}_{\\bm{\\xi}}\\left[\\sum_{i=1}^{k}\\left(\\mathcal{T}^{i}(%\n\\mathbf{x}+\\bm{\\xi})-\\mathcal{S}^{i}(\\mathbf{x}+\\bm{\\xi})\\right)^{2}\\right]$", "word_idx": 14356, "sentence_idx": 180, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\mathbb{E}_{\\bm{\\xi}}\\left[\\sum_{i=1}^{k}\\left(\\mathcal{T}^{i}(%\n\\mathbf{x}+\\bm{\\xi})-\\mathcal{S}^{i}(\\mathbf{x}+\\bm{\\xi})\\right)^{2}\\right]$$", "word_idx": 14511, "sentence_idx": 181, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\sum_{i=1}^{k}\\left(\\mathcal{T}^{i}(\\mathbf{x})-\\mathcal{S}^{i}(%\n\\mathbf{x})\\right)^{2}+\\sigma^{2}\\sum_{i=1}^{k}\\|\\nabla_{x}\\mathcal{T}^{i}(%\n\\mathbf{x})-\\nabla_{x}\\mathcal{S}^{i}(\\mathbf{x})\\|_{2}^{2}$", "word_idx": 14664, "sentence_idx": 182, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\sum_{i=1}^{k}\\left(\\mathcal{T}^{i}(\\mathbf{x})-\\mathcal{S}^{i}(%\n\\mathbf{x})\\right)^{2}+\\sigma^{2}\\sum_{i=1}^{k}\\|\\nabla_{x}\\mathcal{T}^{i}(%\n\\mathbf{x})-\\nabla_{x}\\mathcal{S}^{i}(\\mathbf{x})\\|_{2}^{2}$$", "word_idx": 14882, "sentence_idx": 183, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle+\\mathcal{O}(\\sigma^{4})$", "word_idx": 15098, "sentence_idx": 184, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle+\\mathcal{O}(\\sigma^{4})$$", "word_idx": 15137, "sentence_idx": 185, "label": "unlabeled"}, {"type": "text", "expr": "Notice that in this expression, we have decomposed the loss function into two components: one representing the usual distillation loss on the samples, and the second regularizer term representing the Jacobian matching loss", "word_idx": 15174, "sentence_idx": 186, "label": "unlabeled"}, {"type": "text", "expr": " The higher order error terms are small for small  $\\sigma$  and can be ignored", "word_idx": 15396, "sentence_idx": 187, "label": "unlabeled"}, {"type": "text", "expr": " The above proposition is a simple consequence of using the first-order Taylor series expansion around  $x$ ", "word_idx": 15475, "sentence_idx": 188, "label": "unlabeled"}, {"type": "text", "expr": " Note that the error term is exactly zero for piecewise-linear nets", "word_idx": 15583, "sentence_idx": 189, "label": "unlabeled"}, {"type": "text", "expr": " An analogous statement is true for the case of cross entropy error between soft targets, leading to:", "word_idx": 15650, "sentence_idx": 190, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma$$", "word_idx": 15751, "sentence_idx": 191, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\mathbb{E}_{\\bm{\\xi}}\\left[-\\sum_{i=1}^{k}\\mathcal{T}^{i}_{s}(%\n\\mathbf{x}+\\bm{\\xi})\\log\\left(\\mathcal{S}^{i}_{s}(\\mathbf{x}+\\bm{\\xi})\\right)\\right]$", "word_idx": 15757, "sentence_idx": 192, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\mathbb{E}_{\\bm{\\xi}}\\left[-\\sum_{i=1}^{k}\\mathcal{T}^{i}_{s}(%\n\\mathbf{x}+\\bm{\\xi})\\log\\left(\\mathcal{S}^{i}_{s}(\\mathbf{x}+\\bm{\\xi})\\right)\\right]$$", "word_idx": 15920, "sentence_idx": 193, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\approx-\\sum_{i=1}^{k}\\mathcal{T}^{i}_{s}(\\mathbf{x})\\log(%\n\\mathcal{S}^{i}_{s}(\\mathbf{x}))~{}-~{}\\sigma^{2}\\sum_{i=1}^{k}\\frac{\\nabla_{x%\n}\\mathcal{T}^{i}_{s}(\\mathbf{x})^{T}\\nabla_{x}\\mathcal{S}^{i}_{s}(\\mathbf{x})}%\n{\\mathcal{S}^{i}_{s}(\\mathbf{x})}$", "word_idx": 16081, "sentence_idx": 194, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\approx-\\sum_{i=1}^{k}\\mathcal{T}^{i}_{s}(\\mathbf{x})\\log(%\n\\mathcal{S}^{i}_{s}(\\mathbf{x}))~{}-~{}\\sigma^{2}\\sum_{i=1}^{k}\\frac{\\nabla_{x%\n}\\mathcal{T}^{i}_{s}(\\mathbf{x})^{T}\\nabla_{x}\\mathcal{S}^{i}_{s}(\\mathbf{x})}%\n{\\mathcal{S}^{i}_{s}(\\mathbf{x})}$$", "word_idx": 16349, "sentence_idx": 195, "label": "unlabeled"}, {"type": "text", "expr": "where  $\\mathcal{T}^{i}_{s}(\\mathbf{x})$  denotes the same network  $\\mathcal{T}^{i}(\\mathbf{x})$  but with a softmax or sigmoid (with temperature parameter  $T$  if needed) added at the end", "word_idx": 16615, "sentence_idx": 196, "label": "unlabeled"}, {"type": "text", "expr": " We do not write the super-linear error terms for convenience", "word_idx": 16805, "sentence_idx": 197, "label": "unlabeled"}, {"type": "text", "expr": " This shows that the Jacobian matching loss depends crucially on the loss used to match activations", "word_idx": 16866, "sentence_idx": 198, "label": "unlabeled"}, {"type": "text", "expr": " This observation can be used in practice to pick appropriate loss function by choosing a specific noise model of interest", "word_idx": 16965, "sentence_idx": 199, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{T}^{i}_{s}(\\mathbf{x})$$", "word_idx": 17087, "sentence_idx": 200, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{T}^{i}(\\mathbf{x})$$", "word_idx": 17118, "sentence_idx": 201, "label": "unlabeled"}, {"type": "text", "expr": "To summarize, these statements show that matching Jacobians is a natural consequence of matching not only the raw CNN outputs at the given data points, but also at the infinitely many data points nearby", "word_idx": 17145, "sentence_idx": 202, "label": "unlabeled"}, {"type": "text", "expr": " This is illustrated in Figure  1 , which shows that by matching on a noise-augmented dataset, the student is able to mimic the teacher better", "word_idx": 17347, "sentence_idx": 203, "label": "unlabeled"}, {"type": "text", "expr": "We can use a similar idea to derive regularizers for the case of regular neural network training as well", "word_idx": 17489, "sentence_idx": 204, "label": "unlabeled"}, {"type": "text", "expr": " These regularizers seek to make the underlying model  robust  to small amounts of noise added to the inputs", "word_idx": 17593, "sentence_idx": 205, "label": "unlabeled"}, {"type": "text", "expr": "robust", "word_idx": 17701, "sentence_idx": 206, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 2 ", "word_idx": 17707, "sentence_idx": 207, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 2", "word_idx": 17721, "sentence_idx": 208, "label": "unlabeled"}, {"type": "text", "expr": "Consider the squared error cost function for training a neural network with  $k$  targets, given by  $\\ell(y(\\mathbf{x}),\\mathcal{S}(\\mathbf{x}))=\\sum_{i=1}^{k}(y^{i}(\\mathbf{x})-%\n\\mathcal{S}^{i}(\\mathbf{x}))^{2}$ , where  $\\mathbf{x}\\in\\mathbb{R}^{D}$  is an input data point, and  $y^{i}(\\mathbf{x})$  is the  $i^{th}$  target output", "word_idx": 17734, "sentence_idx": 209, "label": "unlabeled"}, {"type": "text", "expr": " Let  $\\bm{\\xi}~{}(\\in\\mathbb{R}^{D})=\\sigma~{}\\mathbf{z}$  be a scaled version of a unit normal random variable  $\\mathbf{z}~{}\\in\\mathbb{R}^{D}$  with scaling factor  $\\sigma\\in\\mathbb{R}$ ", "word_idx": 18070, "sentence_idx": 210, "label": "unlabeled"}, {"type": "text", "expr": " Then the following is true", "word_idx": 18261, "sentence_idx": 211, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ell(y(\\mathbf{x}),\\mathcal{S}(\\mathbf{x}))=\\sum_{i=1}^{k}(y^{i}(\\mathbf{x})-%\n\\mathcal{S}^{i}(\\mathbf{x}))^{2}$$", "word_idx": 18288, "sentence_idx": 212, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}\\in\\mathbb{R}^{D}$$", "word_idx": 18399, "sentence_idx": 213, "label": "unlabeled"}, {"type": "math", "expr": "$$y^{i}(\\mathbf{x})$$", "word_idx": 18426, "sentence_idx": 214, "label": "unlabeled"}, {"type": "math", "expr": "$$i^{th}$$", "word_idx": 18443, "sentence_idx": 215, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{\\xi}~{}(\\in\\mathbb{R}^{D})=\\sigma~{}\\mathbf{z}$$", "word_idx": 18449, "sentence_idx": 216, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{z}~{}\\in\\mathbb{R}^{D}$$", "word_idx": 18499, "sentence_idx": 217, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma\\in\\mathbb{R}$$", "word_idx": 18529, "sentence_idx": 218, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\mathbb{E}_{\\bm{\\xi}}\\left[\\sum_{i=1}^{k}\\left(y^{i}(\\mathbf{x})-%\n\\mathcal{S}^{i}(\\mathbf{x}+\\bm{\\xi})\\right)^{2}\\right]$", "word_idx": 18548, "sentence_idx": 219, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\mathbb{E}_{\\bm{\\xi}}\\left[\\sum_{i=1}^{k}\\left(y^{i}(\\mathbf{x})-%\n\\mathcal{S}^{i}(\\mathbf{x}+\\bm{\\xi})\\right)^{2}\\right]$$", "word_idx": 18684, "sentence_idx": 220, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\sum_{i=1}^{k}\\left(y^{i}(\\mathbf{x})-\\mathcal{S}^{i}(\\mathbf{x}%\n)\\right)^{2}+\\sigma^{2}\\sum_{i=1}^{k}\\|\\nabla_{x}\\mathcal{S}^{i}(\\mathbf{x})\\|%\n_{2}^{2}+\\mathcal{O}(\\sigma^{4})$", "word_idx": 18818, "sentence_idx": 221, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\sum_{i=1}^{k}\\left(y^{i}(\\mathbf{x})-\\mathcal{S}^{i}(\\mathbf{x}%\n)\\right)^{2}+\\sigma^{2}\\sum_{i=1}^{k}\\|\\nabla_{x}\\mathcal{S}^{i}(\\mathbf{x})\\|%\n_{2}^{2}+\\mathcal{O}(\\sigma^{4})$$", "word_idx": 19012, "sentence_idx": 222, "label": "unlabeled"}, {"type": "text", "expr": "A statement similar to Proposition  2  has been previously derived by\u00a0 \\citet bishop1995training, who observed that the regularizer term for linear models corresponds exactly to the well-known Tikhonov regularizer", "word_idx": 19204, "sentence_idx": 223, "label": "unlabeled"}, {"type": "text", "expr": " This regularizer was also proposed by\u00a0 \\citet drucker1992improving", "word_idx": 19417, "sentence_idx": 224, "label": "unlabeled"}, {"type": "text", "expr": " The  $\\ell_{2}$  weight decay regularizer for neural networks can be derived by applying this regularizer layer-wise separately", "word_idx": 19484, "sentence_idx": 225, "label": "unlabeled"}, {"type": "text", "expr": " However, we see here that a more appropriate way to ensure noise robustness is to penalize the norm of the Jacobian rather than weights", "word_idx": 19612, "sentence_idx": 226, "label": "unlabeled"}, {"type": "text", "expr": " We can derive a similar result for the case of cross-entropy error as well, which is given by -", "word_idx": 19748, "sentence_idx": 227, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 19844, "sentence_idx": 228, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 19850, "sentence_idx": 229, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ell_{2}$$", "word_idx": 19856, "sentence_idx": 230, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\mathbb{E}_{\\bm{\\xi}}\\left[-\\sum_{i=1}^{k}y^{i}(\\mathbf{x})\\log(%\n\\mathcal{S}^{i}_{s}(\\mathbf{x}+\\bm{\\xi}))\\right]$", "word_idx": 19864, "sentence_idx": 231, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\mathbb{E}_{\\bm{\\xi}}\\left[-\\sum_{i=1}^{k}y^{i}(\\mathbf{x})\\log(%\n\\mathcal{S}^{i}_{s}(\\mathbf{x}+\\bm{\\xi}))\\right]$$", "word_idx": 19993, "sentence_idx": 232, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\approx-\\sum_{i=1}^{k}y^{i}(\\mathbf{x})\\log(\\mathcal{S}^{i}_{s}(%\n\\mathbf{x}))~{}+~{}\\sigma^{2}\\sum_{i=1}^{k}y^{i}(\\mathbf{x})\\frac{\\|\\nabla_{x}%\n\\mathcal{S}^{i}_{s}(\\mathbf{x})\\|_{2}^{2}}{\\mathcal{S}^{i}_{s}(\\mathbf{x})^{2}}$", "word_idx": 20120, "sentence_idx": 233, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\approx-\\sum_{i=1}^{k}y^{i}(\\mathbf{x})\\log(\\mathcal{S}^{i}_{s}(%\n\\mathbf{x}))~{}+~{}\\sigma^{2}\\sum_{i=1}^{k}y^{i}(\\mathbf{x})\\frac{\\|\\nabla_{x}%\n\\mathcal{S}^{i}_{s}(\\mathbf{x})\\|_{2}^{2}}{\\mathcal{S}^{i}_{s}(\\mathbf{x})^{2}}$$", "word_idx": 20360, "sentence_idx": 234, "label": "unlabeled"}, {"type": "text", "expr": "We notice here again that the regularizer involves  $\\mathcal{S}^{i}_{s}(\\mathbf{x})$ , which has the sigmoid / softmax nonlinearity applied on top of the final layer of  $\\mathcal{S}^{i}(\\mathbf{x})$ ", "word_idx": 20598, "sentence_idx": 235, "label": "unlabeled"}, {"type": "text", "expr": " Deriving all the above results is a simple matter of using first-order Taylor series expansions, and a second-order expansion in the case of Equation  3 ", "word_idx": 20799, "sentence_idx": 236, "label": "unlabeled"}, {"type": "text", "expr": " Proof is provided in the supplementary material", "word_idx": 20953, "sentence_idx": 237, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{S}^{i}_{s}(\\mathbf{x})$$", "word_idx": 21001, "sentence_idx": 238, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{S}^{i}(\\mathbf{x})$$", "word_idx": 21032, "sentence_idx": 239, "label": "unlabeled"}, {"type": "text", "expr": "In all cases above we see that the regularizers for cross-entropy error term seem more unstable when compared to those for squared error", "word_idx": 21059, "sentence_idx": 240, "label": "unlabeled"}, {"type": "text", "expr": " We find that this is true experimentally as well", "word_idx": 21195, "sentence_idx": 241, "label": "unlabeled"}, {"type": "text", "expr": " As a result, we use squared error loss for distillation", "word_idx": 21244, "sentence_idx": 242, "label": "unlabeled"}, {"type": "text", "expr": "In all cases above we see that the regularizers for cross-entropy error term seem more unstable when compared to those for squared error", "word_idx": 21300, "sentence_idx": 243, "label": "unlabeled"}, {"type": "text", "expr": " We find that this is true experimentally as well", "word_idx": 21436, "sentence_idx": 244, "label": "unlabeled"}, {"type": "text", "expr": " As a result, we use squared error loss for distillation", "word_idx": 21485, "sentence_idx": 245, "label": "unlabeled"}, {"type": "text", "expr": "1  Approximating the Full Jacobian", "word_idx": 21541, "sentence_idx": 246, "label": "unlabeled"}, {"type": "text", "expr": "One can see that both in the case of Proposition  1  and  2 , we are required to compute the full Jacobian", "word_idx": 21575, "sentence_idx": 247, "label": "unlabeled"}, {"type": "text", "expr": " This is computationally expensive, and sometimes unnecessary", "word_idx": 21681, "sentence_idx": 248, "label": "unlabeled"}, {"type": "text", "expr": " For example, Equation  3  requires only the terms where  $y^{i}(\\mathbf{x})$  is non-zero", "word_idx": 21742, "sentence_idx": 249, "label": "unlabeled"}, {"type": "math", "expr": "$$y^{i}(\\mathbf{x})$$", "word_idx": 21832, "sentence_idx": 250, "label": "unlabeled"}, {"type": "text", "expr": "In general, we can approximate the summation of Jacobian terms with the one with largest magnitude", "word_idx": 21849, "sentence_idx": 251, "label": "unlabeled"}, {"type": "text", "expr": " However, we cannot estimate this without computing the Jacobians themselves", "word_idx": 21947, "sentence_idx": 252, "label": "unlabeled"}, {"type": "text", "expr": " As a result, we use a heuristic where the only output variable involving the correct answer  $c\\in[1,k]$  is used for computing the Jacobian", "word_idx": 22023, "sentence_idx": 253, "label": "unlabeled"}, {"type": "text", "expr": " This corresponds to the case of Equation  3 ", "word_idx": 22164, "sentence_idx": 254, "label": "unlabeled"}, {"type": "text", "expr": " Alternately, if we do not want to use the labels, we may instead use the output variable with the largest magnitude, as it often corresponds to the right label (for good models)", "word_idx": 22209, "sentence_idx": 255, "label": "unlabeled"}, {"type": "math", "expr": "$$c\\in[1,k]$$", "word_idx": 22387, "sentence_idx": 256, "label": "unlabeled"}, {"type": "text", "expr": "5  Transfer Learning", "word_idx": 22396, "sentence_idx": 257, "label": "unlabeled"}, {"type": "text", "expr": "We now use our Jacobian matching machinery to transfer learning problems", "word_idx": 22416, "sentence_idx": 258, "label": "unlabeled"}, {"type": "text", "expr": " In computer vision, transfer learning is often done by fine-tuning\u00a0 \\citep yosinski2014transferable, where models pre-trained on a large dataset  $\\mathcal{D}_{l}$ , such as Imagenet\u00a0 \\citep russakovsky2015imagenet, are used as initialization for training on another smaller dataset  $\\mathcal{D}_{s}$ ", "word_idx": 22488, "sentence_idx": 259, "label": "unlabeled"}, {"type": "text", "expr": " We can also refer to these as the source dataset and the target dataset respectively", "word_idx": 22791, "sentence_idx": 260, "label": "unlabeled"}, {"type": "text", "expr": " Practically, this means that the architecture used for fine-tuning must be the same as that of the pre-trained network, which is restrictive", "word_idx": 22876, "sentence_idx": 261, "label": "unlabeled"}, {"type": "text", "expr": " We would like to develop transfer learning methods where the architectures of the pre-trained network and target \u201cfine-tuned\u201d network can be arbitrarily different", "word_idx": 23017, "sentence_idx": 262, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 23180, "sentence_idx": 263, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{D}_{l}$$", "word_idx": 23186, "sentence_idx": 264, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 23201, "sentence_idx": 265, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{D}_{s}$$", "word_idx": 23207, "sentence_idx": 266, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:  Illustration of our proposed method for transfer learning", "word_idx": 23222, "sentence_idx": 267, "label": "unlabeled"}, {"type": "text", "expr": " We match the output activations of a pre-trained Imagenet network similar to LwF\u00a0 \\citep li2016learning", "word_idx": 23290, "sentence_idx": 268, "label": "unlabeled"}, {"type": "text", "expr": " We also match aggregated activations or \u201cattention\u201d maps between networks, similar to the work of\u00a0 \\citet zagoruyko2016paying", "word_idx": 23394, "sentence_idx": 269, "label": "unlabeled"}, {"type": "text", "expr": " We propose to match Jacobians of (aggregated) attention maps w", "word_idx": 23520, "sentence_idx": 270, "label": "unlabeled"}, {"type": "text", "expr": " inputs", "word_idx": 23583, "sentence_idx": 271, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:", "word_idx": 23590, "sentence_idx": 272, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 23599, "sentence_idx": 273, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 23605, "sentence_idx": 274, "label": "unlabeled"}, {"type": "text", "expr": "One way to achieve this is by distillation: we match output activations of a pre-trained teacher network and an untrained student network", "word_idx": 23611, "sentence_idx": 275, "label": "unlabeled"}, {"type": "text", "expr": " However, this procedure is not general as the target dataset may not share the same label space as the large source dataset", "word_idx": 23748, "sentence_idx": 276, "label": "unlabeled"}, {"type": "text", "expr": " This leads to size mismatch between output activations of the teacher and the student", "word_idx": 23872, "sentence_idx": 277, "label": "unlabeled"}, {"type": "text", "expr": " To overcome this, we can design the student network to have two sets of outputs (or two output \u201cbranches\u201d), one with the label space of the smaller target dataset, while the other with that of the larger source dataset", "word_idx": 23958, "sentence_idx": 278, "label": "unlabeled"}, {"type": "text", "expr": " This leads to the method proposed by\u00a0 \\citet li2016learning, called \u201cLearning without Forgetting\u201d ( LwF )", "word_idx": 24177, "sentence_idx": 279, "label": "unlabeled"}, {"type": "text", "expr": " Note that similar methods were concurrently developed by\u00a0 \\citet jung2016less and\u00a0 \\citet furlanello2016active", "word_idx": 24283, "sentence_idx": 280, "label": "unlabeled"}, {"type": "text", "expr": " In this method, the student network is trained with a composite loss function involving two terms, one in each output branch", "word_idx": 24394, "sentence_idx": 281, "label": "unlabeled"}, {"type": "text", "expr": " The two objectives are  (1)  matching ground truth labels on the target dataset, and  (2)  matching the activations of the student network and a pre-trained teacher network on the target dataset", "word_idx": 24519, "sentence_idx": 282, "label": "unlabeled"}, {"type": "text", "expr": " This is illustrated in Figure  2 ", "word_idx": 24714, "sentence_idx": 283, "label": "unlabeled"}, {"type": "text", "expr": " Crucially, these losses are matched only on the target dataset, and the source data is untouched", "word_idx": 24748, "sentence_idx": 284, "label": "unlabeled"}, {"type": "text", "expr": " This is conceptually different from distillation, where the teacher network is trained on the dataset being used", "word_idx": 24845, "sentence_idx": 285, "label": "unlabeled"}, {"type": "text", "expr": " In LwF, the pre-trained teacher is not trained on the target dataset", "word_idx": 24958, "sentence_idx": 286, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 25027, "sentence_idx": 287, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 25033, "sentence_idx": 288, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 25039, "sentence_idx": 289, "label": "unlabeled"}, {"type": "text", "expr": "This makes it problematic to apply our Jacobian matching framework to LwF", "word_idx": 25045, "sentence_idx": 290, "label": "unlabeled"}, {"type": "text", "expr": " For distillation, it is clear that adding input noise (or Jacobian matching) can improve overall matching as shown in Figure  1 ", "word_idx": 25118, "sentence_idx": 291, "label": "unlabeled"}, {"type": "text", "expr": " For the case of LwF, it is not clear whether improving matching between teacher and student will necessarily improve transfer learning performance", "word_idx": 25247, "sentence_idx": 292, "label": "unlabeled"}, {"type": "text", "expr": " This is especially because the teacher is not trained on the target dataset, and can potentially produce noisy or incorrect results on this data", "word_idx": 25394, "sentence_idx": 293, "label": "unlabeled"}, {"type": "text", "expr": " To resolve this ambiguity, we shall now connect LwF with distillation", "word_idx": 25539, "sentence_idx": 294, "label": "unlabeled"}, {"type": "text", "expr": "1  LwF as Distillation", "word_idx": 25609, "sentence_idx": 295, "label": "unlabeled"}, {"type": "text", "expr": "In the discussion below we shall only consider the distillation-like loss of LwF, and ignore the branch which matches ground truth labels", "word_idx": 25631, "sentence_idx": 296, "label": "unlabeled"}, {"type": "text", "expr": " For LwF to work well, it must be the case that the activations of the pre-trained teacher network on the target dataset must contain information about the source dataset ( i", "word_idx": 25768, "sentence_idx": 297, "label": "unlabeled"}, {"type": "text", "expr": " ; Imagenet)", "word_idx": 25942, "sentence_idx": 298, "label": "unlabeled"}, {"type": "text", "expr": " The attractiveness of LwF lies in the fact that this is done without explicitly using Imagenet", "word_idx": 25954, "sentence_idx": 299, "label": "unlabeled"}, {"type": "text", "expr": " Here, we make the claim that  LwF approximates distillation on (a part of) Imagenet", "word_idx": 26049, "sentence_idx": 300, "label": "unlabeled"}, {"type": "text", "expr": "LwF approximates distillation on (a part of) Imagenet", "word_idx": 26133, "sentence_idx": 301, "label": "unlabeled"}, {"type": "text", "expr": "Note that this is no longer a valid distance metric unlike the Hausdorff", "word_idx": 26186, "sentence_idx": 302, "label": "unlabeled"}, {"type": "text", "expr": " Given these assumptions, we are now ready to state our result", "word_idx": 26258, "sentence_idx": 303, "label": "unlabeled"}, {"type": "text", "expr": "Let  $f(\\cdot)$  be an untrained neural network,  $g(\\cdot)$  be a pre-trained network,  $\\mathbf{x},\\mathbf{y}$  be the input image and corresponding ground truth label respectively", "word_idx": 26320, "sentence_idx": 304, "label": "unlabeled"}, {"type": "text", "expr": " Let  $|\\mathcal{D}|$  be the size of the dataset  $\\mathcal{D}$ ", "word_idx": 26502, "sentence_idx": 305, "label": "unlabeled"}, {"type": "text", "expr": " Let us denote  $\\rho(\\mathbf{x})=\\ell(f(\\mathbf{x}),g(\\mathbf{x}))$  for convenience", "word_idx": 26567, "sentence_idx": 306, "label": "unlabeled"}, {"type": "text", "expr": " Assume Lipschitz continuity for  $\\rho(\\mathbf{x})$  with Lipschitz constant  $\\mathrm{K}$ , and distance metric  $\\psi_{\\mathbf{x}}$  in the input space", "word_idx": 26652, "sentence_idx": 307, "label": "unlabeled"}, {"type": "math", "expr": "$$f(\\cdot)$$", "word_idx": 26806, "sentence_idx": 308, "label": "unlabeled"}, {"type": "math", "expr": "$$g(\\cdot)$$", "word_idx": 26814, "sentence_idx": 309, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x},\\mathbf{y}$$", "word_idx": 26822, "sentence_idx": 310, "label": "unlabeled"}, {"type": "math", "expr": "$$|\\mathcal{D}|$$", "word_idx": 26843, "sentence_idx": 311, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{D}$$", "word_idx": 26856, "sentence_idx": 312, "label": "unlabeled"}, {"type": "math", "expr": "$$\\rho(\\mathbf{x})=\\ell(f(\\mathbf{x}),g(\\mathbf{x}))$$", "word_idx": 26867, "sentence_idx": 313, "label": "unlabeled"}, {"type": "math", "expr": "$$\\rho(\\mathbf{x})$$", "word_idx": 26917, "sentence_idx": 314, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{K}$$", "word_idx": 26933, "sentence_idx": 315, "label": "unlabeled"}, {"type": "math", "expr": "$$\\psi_{\\mathbf{x}}$$", "word_idx": 26943, "sentence_idx": 316, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\|\\rho(\\mathbf{x}_{1})-\\rho(\\mathbf{x}_{2})\\|\\leq\\mathrm{K}\\psi_{%\n\\mathbf{x}}(\\mathbf{x}_{1},\\mathbf{x}_{2})$", "word_idx": 26960, "sentence_idx": 317, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\|\\rho(\\mathbf{x}_{1})-\\rho(\\mathbf{x}_{2})\\|\\leq\\mathrm{K}\\psi_{%\n\\mathbf{x}}(\\mathbf{x}_{1},\\mathbf{x}_{2})$$", "word_idx": 27084, "sentence_idx": 318, "label": "unlabeled"}, {"type": "text", "expr": "Note here that the distance in the input space need not be in terms of pixelwise distances, but can also be neural net feature distance, for example", "word_idx": 27206, "sentence_idx": 319, "label": "unlabeled"}, {"type": "text", "expr": " Let us also define an assymetric version of the Hausdorff distance between two sets  $A,B$ :", "word_idx": 27354, "sentence_idx": 320, "label": "unlabeled"}, {"type": "math", "expr": "$$A,B$$", "word_idx": 27447, "sentence_idx": 321, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\mathcal{H}_{a}(A,B)=\\sup_{a\\in A}\\inf_{b\\in B}\\psi_{\\mathbf{x}}(%\na,b)$", "word_idx": 27450, "sentence_idx": 322, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\mathcal{H}_{a}(A,B)=\\sup_{a\\in A}\\inf_{b\\in B}\\psi_{\\mathbf{x}}(%\na,b).$$", "word_idx": 27536, "sentence_idx": 323, "label": "unlabeled"}, {"type": "text", "expr": "Note that this is no longer a valid distance metric unlike the Hausdorff", "word_idx": 27621, "sentence_idx": 324, "label": "unlabeled"}, {"type": "text", "expr": " Given these assumptions, we are now ready to state our result", "word_idx": 27693, "sentence_idx": 325, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 3 ", "word_idx": 27755, "sentence_idx": 326, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 3", "word_idx": 27769, "sentence_idx": 327, "label": "unlabeled"}, {"type": "text", "expr": "Given the assumptions and notations described above, we have", "word_idx": 27782, "sentence_idx": 328, "label": "unlabeled"}, {"type": "text", "expr": "Given the assumptions and notations described above, we have", "word_idx": 27842, "sentence_idx": 329, "label": "unlabeled"}, {"type": "text", "expr": "Given the assumptions and notations described above, we have", "word_idx": 27902, "sentence_idx": 330, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\frac{1}{|\\mathcal{D}_{l}|}\\sum_{\\mathbf{x}\\sim\\mathcal{D}_{l}}%\n\\ell(f(\\mathbf{x}),g(\\mathbf{x}))$", "word_idx": 27962, "sentence_idx": 331, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\frac{1}{|\\mathcal{D}_{l}|}\\sum_{\\mathbf{x}\\sim\\mathcal{D}_{l}}%\n\\ell(f(\\mathbf{x}),g(\\mathbf{x}))$$", "word_idx": 28075, "sentence_idx": 332, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\leq\\max_{\\mathbf{x}\\sim\\mathcal{D}_{s}}\\ell(f(\\mathbf{x}),g(%\n\\mathbf{x}))$", "word_idx": 28186, "sentence_idx": 333, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\leq\\max_{\\mathbf{x}\\sim\\mathcal{D}_{s}}\\ell(f(\\mathbf{x}),g(%\n\\mathbf{x}))$$", "word_idx": 28276, "sentence_idx": 334, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle+\\mathrm{K}\\mathcal{H}_{a}(\\mathcal{D}_{l},\\mathcal{D}_{s})$", "word_idx": 28364, "sentence_idx": 335, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle+\\mathrm{K}\\mathcal{H}_{a}(\\mathcal{D}_{l},\\mathcal{D}_{s})$$", "word_idx": 28438, "sentence_idx": 336, "label": "unlabeled"}, {"type": "text", "expr": "On the left side of  6  we have the distillation loss on the source dataset, and on the right we have a max-loss term on the target dataset", "word_idx": 28510, "sentence_idx": 337, "label": "unlabeled"}, {"type": "text", "expr": " Note that the LwF loss is an average loss on the target dataset", "word_idx": 28649, "sentence_idx": 338, "label": "unlabeled"}, {"type": "text", "expr": " As expected, the slack terms in the inequality depends on the distance between the source and target datasets ( 7 )", "word_idx": 28713, "sentence_idx": 339, "label": "unlabeled"}, {"type": "text", "expr": " This bounds a loss related to the LwF loss ( i", "word_idx": 28829, "sentence_idx": 340, "label": "unlabeled"}, {"type": "text", "expr": "  max-loss instead of average) with the distillation loss", "word_idx": 28876, "sentence_idx": 341, "label": "unlabeled"}, {"type": "text", "expr": " If the Hausdorff distance is small, then reducing the max-loss would reduce the distillation loss as well", "word_idx": 28933, "sentence_idx": 342, "label": "unlabeled"}, {"type": "text", "expr": " A similar theory was previously presented by  \\citet ben2010theory, but with different formalisms", "word_idx": 29039, "sentence_idx": 343, "label": "unlabeled"}, {"type": "text", "expr": " Our formalism allows us to connect with Jacobian matching, which is our primary objective", "word_idx": 29137, "sentence_idx": 344, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 29227, "sentence_idx": 345, "label": "unlabeled"}, {"type": "text", "expr": "In practice, the target dataset is often much smaller than the Imagenet and has different overall statistics", "word_idx": 29233, "sentence_idx": 346, "label": "unlabeled"}, {"type": "text", "expr": " For example, the target dataset could be a restricted dataset of flower images", "word_idx": 29341, "sentence_idx": 347, "label": "unlabeled"}, {"type": "text", "expr": " In such a case, we can restrict the source dataset to its \u201cbest\u201d subset, in particular with all the irrelevant samples (those far from target dataset) removed", "word_idx": 29420, "sentence_idx": 348, "label": "unlabeled"}, {"type": "text", "expr": " This would make the Hausdorff distance smaller, and provide a tighter bound", "word_idx": 29579, "sentence_idx": 349, "label": "unlabeled"}, {"type": "text", "expr": " In our example, this involves keeping only flowers from Imagenet", "word_idx": 29655, "sentence_idx": 350, "label": "unlabeled"}, {"type": "text", "expr": "In practice, the target dataset is often much smaller than the Imagenet and has different overall statistics", "word_idx": 29720, "sentence_idx": 351, "label": "unlabeled"}, {"type": "text", "expr": " For example, the target dataset could be a restricted dataset of flower images", "word_idx": 29828, "sentence_idx": 352, "label": "unlabeled"}, {"type": "text", "expr": " In such a case, we can restrict the source dataset to its \u201cbest\u201d subset, in particular with all the irrelevant samples (those far from target dataset) removed", "word_idx": 29907, "sentence_idx": 353, "label": "unlabeled"}, {"type": "text", "expr": " This would make the Hausdorff distance smaller, and provide a tighter bound", "word_idx": 30066, "sentence_idx": 354, "label": "unlabeled"}, {"type": "text", "expr": " In our example, this involves keeping only flowers from Imagenet", "word_idx": 30142, "sentence_idx": 355, "label": "unlabeled"}, {"type": "text", "expr": "This makes intuitive sense as well: if the source and target datasets are completely different, we do not expect transfer learning (and thus LwF) to help", "word_idx": 30207, "sentence_idx": 356, "label": "unlabeled"}, {"type": "text", "expr": " The greater the overlap between source and target datasets, the smaller is the Hausdorff distance, the tighter is the bound, and the more we expect knowledge transfer to help", "word_idx": 30360, "sentence_idx": 357, "label": "unlabeled"}, {"type": "text", "expr": " Our results capture this intuition in a rigorous manner", "word_idx": 30535, "sentence_idx": 358, "label": "unlabeled"}, {"type": "text", "expr": " In addition, this predicts that Lipschitz-smooth teacher neural nets that produce small feature distance between source and target images are expected to do well in transfer learning", "word_idx": 30591, "sentence_idx": 359, "label": "unlabeled"}, {"type": "text", "expr": " Lipschitz-smoothness of models has been previously related to adversarial noise robustness\u00a0 \\citep cisse2017parseval, and to learning theory as a sufficient condition for generalization\u00a0 \\citep xu2012robustness", "word_idx": 30774, "sentence_idx": 360, "label": "unlabeled"}, {"type": "text", "expr": " It is interesting that this relates to transfer learning as well", "word_idx": 30985, "sentence_idx": 361, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 31050, "sentence_idx": 362, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 31056, "sentence_idx": 363, "label": "unlabeled"}, {"type": "text", "expr": "More importantly, this establishes LwF as a distillation method", "word_idx": 31062, "sentence_idx": 364, "label": "unlabeled"}, {"type": "text", "expr": " The following result motivates input noise added to the target dataset", "word_idx": 31125, "sentence_idx": 365, "label": "unlabeled"}, {"type": "text", "expr": "More importantly, this establishes LwF as a distillation method", "word_idx": 31196, "sentence_idx": 366, "label": "unlabeled"}, {"type": "text", "expr": " The following result motivates input noise added to the target dataset", "word_idx": 31259, "sentence_idx": 367, "label": "unlabeled"}, {"type": "text", "expr": "Corollary ", "word_idx": 31330, "sentence_idx": 368, "label": "unlabeled"}, {"type": "text", "expr": "Corollary", "word_idx": 31340, "sentence_idx": 369, "label": "unlabeled"}, {"type": "text", "expr": "For any superset  $\\mathrm{D^{\\prime}}_{s}\\supseteq\\mathcal{D}_{s}$  of the target dataset,  $\\mathcal{H}_{a}(\\mathcal{D}_{l},\\mathcal{D^{\\prime}}_{s})\\leq\\mathcal{H}_{a}(%\n\\mathcal{D}_{l},\\mathcal{D}_{s})$", "word_idx": 31349, "sentence_idx": 370, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{D^{\\prime}}_{s}\\supseteq\\mathcal{D}_{s}$$", "word_idx": 31555, "sentence_idx": 371, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{H}_{a}(\\mathcal{D}_{l},\\mathcal{D^{\\prime}}_{s})\\leq\\mathcal{H}_{a}(%\n\\mathcal{D}_{l},\\mathcal{D}_{s})$$", "word_idx": 31602, "sentence_idx": 372, "label": "unlabeled"}, {"type": "text", "expr": "Thus if we augment the target dataset  $\\mathcal{D}_{s}$  by adding noise, we expect the bound in Proposition  3  to get tighter", "word_idx": 31713, "sentence_idx": 373, "label": "unlabeled"}, {"type": "text", "expr": " This is because when we add noise to points in  $\\mathcal{D}_{s}$ , the minimum distance between points from  $\\mathcal{D}_{l}$  to  $\\mathcal{D}_{s}$  decreases", "word_idx": 31841, "sentence_idx": 374, "label": "unlabeled"}, {"type": "text", "expr": " Proofs are provided in the supplementary material", "word_idx": 32003, "sentence_idx": 375, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{D}_{s}$$", "word_idx": 32053, "sentence_idx": 376, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{D}_{s}$$", "word_idx": 32068, "sentence_idx": 377, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{D}_{l}$$", "word_idx": 32083, "sentence_idx": 378, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{D}_{s}$$", "word_idx": 32098, "sentence_idx": 379, "label": "unlabeled"}, {"type": "text", "expr": "To summarize, we have showed that a loss related to the LwF loss (max-loss) is an upper bound on the true distillation loss", "word_idx": 32113, "sentence_idx": 380, "label": "unlabeled"}, {"type": "text", "expr": " Thus by minimizing the upper bound, we can expect to reduce the distillation loss also", "word_idx": 32236, "sentence_idx": 381, "label": "unlabeled"}, {"type": "text", "expr": "To summarize, we have showed that a loss related to the LwF loss (max-loss) is an upper bound on the true distillation loss", "word_idx": 32323, "sentence_idx": 382, "label": "unlabeled"}, {"type": "text", "expr": " Thus by minimizing the upper bound, we can expect to reduce the distillation loss also", "word_idx": 32446, "sentence_idx": 383, "label": "unlabeled"}, {"type": "text", "expr": "1  Incorporating Jacobian matching", "word_idx": 32533, "sentence_idx": 384, "label": "unlabeled"}, {"type": "text", "expr": "Now that input noise and thus Jacobian matching is well motivated, we can incorporate these losses into LwF", "word_idx": 32567, "sentence_idx": 385, "label": "unlabeled"}, {"type": "text", "expr": " When we implemented this for practical deep networks we found that the optimizer wasn\u2019t able to reduce the Jacobian loss at all", "word_idx": 32674, "sentence_idx": 386, "label": "unlabeled"}, {"type": "text", "expr": " We conjecture that it might be because of a vanishing gradient effect / network degeneracy on propagation of second order gradients through the network (and not the first)", "word_idx": 32802, "sentence_idx": 387, "label": "unlabeled"}, {"type": "text", "expr": " As a result, we need an alternative way to match Jacobians", "word_idx": 32974, "sentence_idx": 388, "label": "unlabeled"}, {"type": "text", "expr": "Now that input noise and thus Jacobian matching is well motivated, we can incorporate these losses into LwF", "word_idx": 33033, "sentence_idx": 389, "label": "unlabeled"}, {"type": "text", "expr": " When we implemented this for practical deep networks we found that the optimizer wasn\u2019t able to reduce the Jacobian loss at all", "word_idx": 33140, "sentence_idx": 390, "label": "unlabeled"}, {"type": "text", "expr": " We conjecture that it might be because of a vanishing gradient effect / network degeneracy on propagation of second order gradients through the network (and not the first)", "word_idx": 33268, "sentence_idx": 391, "label": "unlabeled"}, {"type": "text", "expr": " As a result, we need an alternative way to match Jacobians", "word_idx": 33440, "sentence_idx": 392, "label": "unlabeled"}, {"type": "text", "expr": "2  Matching attention maps", "word_idx": 33499, "sentence_idx": 393, "label": "unlabeled"}, {"type": "text", "expr": "It is often insufficient to match only output activations between a teacher and a student network, especially when both networks are deep", "word_idx": 33525, "sentence_idx": 394, "label": "unlabeled"}, {"type": "text", "expr": " In such cases we can consider matching intermediate feature maps as well", "word_idx": 33662, "sentence_idx": 395, "label": "unlabeled"}, {"type": "text", "expr": " In general it is not possible to match the full feature maps between an arbitrary teacher and student network as they may have different architectures, and features sizes may never match at any layer", "word_idx": 33735, "sentence_idx": 396, "label": "unlabeled"}, {"type": "text", "expr": " However, for modern convolutional architectures, spatial sizes of certain features often match across architectures even when the number of channels doesn\u2019t", "word_idx": 33935, "sentence_idx": 397, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 \\citet zagoruyko2016paying noticed that it in such cases it helps to match channelwise aggregated activations, which they call  attention  maps", "word_idx": 34092, "sentence_idx": 398, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, this aggregation is performed by summing over squared absolute value of channels of a feature activation  $Z$ , and is given by -", "word_idx": 34237, "sentence_idx": 399, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 34381, "sentence_idx": 400, "label": "unlabeled"}, {"type": "text", "expr": "attention", "word_idx": 34387, "sentence_idx": 401, "label": "unlabeled"}, {"type": "text", "expr": "$A=AttMap(Z)=\\sum_{\\mathclap{{i\\in channels}}}|Z_{i}|^{2}$", "word_idx": 34396, "sentence_idx": 402, "label": "unlabeled"}, {"type": "math", "expr": "$$A=AttMap(Z)=\\sum_{\\mathclap{{i\\in channels}}}|Z_{i}|^{2}$$", "word_idx": 34454, "sentence_idx": 403, "label": "unlabeled"}, {"type": "text", "expr": "Further, the loss function used to match these activations is", "word_idx": 34510, "sentence_idx": 404, "label": "unlabeled"}, {"type": "text", "expr": "Further, the loss function used to match these activations is", "word_idx": 34571, "sentence_idx": 405, "label": "unlabeled"}, {"type": "text", "expr": "$\\mathrm{Match~{}Activations}=\\left|\\left|\\frac{A_{t}}{\\|A_{t}\\|_{2}}-\\frac{A_{%\ns}}{\\|A_{s}\\|_{2}}\\right|\\right|_{2}$", "word_idx": 34632, "sentence_idx": 406, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{Match~{}Activations}=\\left|\\left|\\frac{A_{t}}{\\|A_{t}\\|_{2}}-\\frac{A_{%\ns}}{\\|A_{s}\\|_{2}}\\right|\\right|_{2}$$", "word_idx": 34750, "sentence_idx": 407, "label": "unlabeled"}, {"type": "text", "expr": "Here,  $A_{t},A_{s}$  are the attention maps of the teacher and student respectively", "word_idx": 34866, "sentence_idx": 408, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 \\citet zagoruyko2016paying note that this choice of loss function is especially crucial", "word_idx": 34950, "sentence_idx": 409, "label": "unlabeled"}, {"type": "math", "expr": "$$A_{t},A_{s}$$", "word_idx": 35039, "sentence_idx": 410, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 35050, "sentence_idx": 411, "label": "unlabeled"}, {"type": "text", "expr": "1  Incorporating Jacobian loss", "word_idx": 35056, "sentence_idx": 412, "label": "unlabeled"}, {"type": "text", "expr": "As the activation maps have large spatial dimensions, it is computationally costly to compute the full Jacobians", "word_idx": 35086, "sentence_idx": 413, "label": "unlabeled"}, {"type": "text", "expr": " We hence resort to computing approximating Jacobians in the same manner as previously discussed", "word_idx": 35198, "sentence_idx": 414, "label": "unlabeled"}, {"type": "text", "expr": " In this case, this leads to picking the pixel in the attention map with the largest magnitude, and computing the Jacobian of this quantity w", "word_idx": 35294, "sentence_idx": 415, "label": "unlabeled"}, {"type": "text", "expr": " the input", "word_idx": 35435, "sentence_idx": 416, "label": "unlabeled"}, {"type": "text", "expr": " We compute the index  $(i,j)$  of this maximum-valued pixel for the teacher network and use the same index to compute the student\u2019s Jacobian", "word_idx": 35445, "sentence_idx": 417, "label": "unlabeled"}, {"type": "text", "expr": " We then use a loss function similar to Equation\u00a0 9 , given by", "word_idx": 35586, "sentence_idx": 418, "label": "unlabeled"}, {"type": "math", "expr": "$$(i,j)$$", "word_idx": 35648, "sentence_idx": 419, "label": "unlabeled"}, {"type": "text", "expr": "$\\mathrm{Match~{}Jacobians}=\\left|\\left|\\frac{\\nabla_{x}f(\\mathbf{x})}{\\|\\nabla%\n_{x}f(\\mathbf{x})\\|_{2}}-\\frac{\\nabla_{x}g(\\mathbf{x})}{\\|\\nabla_{x}g(\\mathbf{%\nx})\\|_{2}}\\right|\\right|_{2}^{2}$", "word_idx": 35653, "sentence_idx": 420, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{Match~{}Jacobians}=\\left|\\left|\\frac{\\nabla_{x}f(\\mathbf{x})}{\\|\\nabla%\n_{x}f(\\mathbf{x})\\|_{2}}-\\frac{\\nabla_{x}g(\\mathbf{x})}{\\|\\nabla_{x}g(\\mathbf{%\nx})\\|_{2}}\\right|\\right|_{2}^{2}$$", "word_idx": 35847, "sentence_idx": 421, "label": "unlabeled"}, {"type": "text", "expr": "2  Justification for Jacobian loss", "word_idx": 36039, "sentence_idx": 422, "label": "unlabeled"}, {"type": "text", "expr": "We can show that the above loss term corresponds to adding a noise term  $\\bm{\\xi}_{f}\\propto\\|\\nabla_{x}f(\\mathbf{x})\\|^{-1}_{2}$  for  $f(\\mathbf{x})$  and  $\\bm{\\xi}_{g}\\propto\\|\\nabla_{x}g(\\mathbf{x})\\|^{-1}_{2}$  for  $g(\\mathbf{x})$  for the distillation loss", "word_idx": 36073, "sentence_idx": 423, "label": "unlabeled"}, {"type": "text", "expr": " From the first order Taylor series expansion, we see that  $g(x+\\bm{\\xi})=g(x)+\\bm{\\xi}_{g}\\nabla_{x}g(\\mathbf{x})$ ", "word_idx": 36338, "sentence_idx": 424, "label": "unlabeled"}, {"type": "text", "expr": " Thus for networks  $f(\\cdot)$  and  $g(\\cdot)$  with different Jacobian magnitudes, we expect different responses for the same noisy inputs", "word_idx": 36455, "sentence_idx": 425, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, we see that  $\\mathbb{E}_{\\bm{\\xi}_{g}}\\|g(x+\\bm{\\xi}_{g})-g(x)\\|^{2}_{2}=\\sigma_{g}^{2}\\|%\n\\nabla_{x}g(\\mathbf{x})\\|^{2}_{2}=\\sigma^{2}\\frac{\\|\\nabla_{x}g(\\mathbf{x})\\|^%\n{2}_{2}}{\\|\\nabla_{x}g(\\mathbf{x})\\|^{2}_{2}}=\\sigma^{2}$  for a gaussian model with covariance matrix being  $\\sigma$  times the identity", "word_idx": 36595, "sentence_idx": 426, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{\\xi}_{f}\\propto\\|\\nabla_{x}f(\\mathbf{x})\\|^{-1}_{2}$$", "word_idx": 36920, "sentence_idx": 427, "label": "unlabeled"}, {"type": "math", "expr": "$$f(\\mathbf{x})$$", "word_idx": 36975, "sentence_idx": 428, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{\\xi}_{g}\\propto\\|\\nabla_{x}g(\\mathbf{x})\\|^{-1}_{2}$$", "word_idx": 36988, "sentence_idx": 429, "label": "unlabeled"}, {"type": "math", "expr": "$$g(\\mathbf{x})$$", "word_idx": 37043, "sentence_idx": 430, "label": "unlabeled"}, {"type": "math", "expr": "$$g(x+\\bm{\\xi})=g(x)+\\bm{\\xi}_{g}\\nabla_{x}g(\\mathbf{x})$$", "word_idx": 37056, "sentence_idx": 431, "label": "unlabeled"}, {"type": "math", "expr": "$$f(\\cdot)$$", "word_idx": 37110, "sentence_idx": 432, "label": "unlabeled"}, {"type": "math", "expr": "$$g(\\cdot)$$", "word_idx": 37118, "sentence_idx": 433, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbb{E}_{\\bm{\\xi}_{g}}\\|g(x+\\bm{\\xi}_{g})-g(x)\\|^{2}_{2}=\\sigma_{g}^{2}\\|%\n\\nabla_{x}g(\\mathbf{x})\\|^{2}_{2}=\\sigma^{2}\\frac{\\|\\nabla_{x}g(\\mathbf{x})\\|^%\n{2}_{2}}{\\|\\nabla_{x}g(\\mathbf{x})\\|^{2}_{2}}=\\sigma^{2}$$", "word_idx": 37126, "sentence_idx": 434, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma$$", "word_idx": 37340, "sentence_idx": 435, "label": "unlabeled"}, {"type": "text", "expr": "6  Experiments", "word_idx": 37346, "sentence_idx": 436, "label": "unlabeled"}, {"type": "text", "expr": "We perform three experiments to show the effectiveness of using Jacobians", "word_idx": 37360, "sentence_idx": 437, "label": "unlabeled"}, {"type": "text", "expr": " First, we perform distillation in a limited data setting on the CIFAR100 dataset\u00a0 \\citep krizhevsky2009learning", "word_idx": 37433, "sentence_idx": 438, "label": "unlabeled"}, {"type": "text", "expr": " Second, we show on that same dataset that penalizing Jacobian norm increases stability of networks to random noise", "word_idx": 37545, "sentence_idx": 439, "label": "unlabeled"}, {"type": "text", "expr": " Finally, we perform transfer learning experiments on the MIT Scenes dataset\u00a0 \\citep quattoni2009recognizing", "word_idx": 37660, "sentence_idx": 440, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 37768, "sentence_idx": 441, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 37774, "sentence_idx": 442, "label": "unlabeled"}, {"type": "text", "expr": "1  Distillation", "word_idx": 37780, "sentence_idx": 443, "label": "unlabeled"}, {"type": "text", "expr": "For the distillation experiments, we use VGG-like\u00a0 \\citep simonyan2014very architectures with batch normalization", "word_idx": 37795, "sentence_idx": 444, "label": "unlabeled"}, {"type": "text", "expr": " The main difference is that we keep only the convolutional layers and have only one fully connected layer rather than three", "word_idx": 37908, "sentence_idx": 445, "label": "unlabeled"}, {"type": "text", "expr": " Our workflow is as follows", "word_idx": 38032, "sentence_idx": 446, "label": "unlabeled"}, {"type": "text", "expr": " First, a 9-layer \u201cteacher\u201d network is trained on the full CIFAR100 dataset", "word_idx": 38059, "sentence_idx": 447, "label": "unlabeled"}, {"type": "text", "expr": " Then, a smaller 4-layer \u201cstudent\u201d network is trained, but this time on small subsets rather than the full dataset", "word_idx": 38134, "sentence_idx": 448, "label": "unlabeled"}, {"type": "text", "expr": " As the teacher is trained on much more data than the student, we expect distillation to improve the student\u2019s performance", "word_idx": 38248, "sentence_idx": 449, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 38370, "sentence_idx": 450, "label": "unlabeled"}, {"type": "text", "expr": "A practical scenario where this would be useful is the case of architecture search and ensemble training, where we require to train many candidate neural network architectures on the same task", "word_idx": 38376, "sentence_idx": 451, "label": "unlabeled"}, {"type": "text", "expr": " Distillation methods can help speed up such methods by using already trained networks to accelerate training of newer models", "word_idx": 38568, "sentence_idx": 452, "label": "unlabeled"}, {"type": "text", "expr": " One way to achieve acceleration is by using less data to train the student dataset", "word_idx": 38693, "sentence_idx": 453, "label": "unlabeled"}, {"type": "text", "expr": "A practical scenario where this would be useful is the case of architecture search and ensemble training, where we require to train many candidate neural network architectures on the same task", "word_idx": 38776, "sentence_idx": 454, "label": "unlabeled"}, {"type": "text", "expr": " Distillation methods can help speed up such methods by using already trained networks to accelerate training of newer models", "word_idx": 38968, "sentence_idx": 455, "label": "unlabeled"}, {"type": "text", "expr": " One way to achieve acceleration is by using less data to train the student dataset", "word_idx": 39093, "sentence_idx": 456, "label": "unlabeled"}, {"type": "text", "expr": "We compare our methods against the following baselines", "word_idx": 39176, "sentence_idx": 457, "label": "unlabeled"}, {"type": "text", "expr": "  (1): Cross-Entropy (CE) training  \u2013 Here we train the student using only the ground truth (hard labels) available with the dataset without invoking the teacher network", "word_idx": 39230, "sentence_idx": 458, "label": "unlabeled"}, {"type": "text", "expr": "  (2): CE + match activations  \u2013 This is the classical form of distillation, where the activations of the teacher network are matched with that of the student", "word_idx": 39399, "sentence_idx": 459, "label": "unlabeled"}, {"type": "text", "expr": " This is weighted with the cross-entropy term which uses ground truth targets", "word_idx": 39557, "sentence_idx": 460, "label": "unlabeled"}, {"type": "text", "expr": "  (3): Match activations only  \u2013 Same as above, except that the cross-entropy term is not used in the loss function", "word_idx": 39634, "sentence_idx": 461, "label": "unlabeled"}, {"type": "text", "expr": "(1): Cross-Entropy (CE) training", "word_idx": 39749, "sentence_idx": 462, "label": "unlabeled"}, {"type": "text", "expr": "(2): CE + match activations", "word_idx": 39781, "sentence_idx": 463, "label": "unlabeled"}, {"type": "text", "expr": "(3): Match activations only", "word_idx": 39808, "sentence_idx": 464, "label": "unlabeled"}, {"type": "text", "expr": "We compare these methods by appending the Jacobian matching term in each case", "word_idx": 39835, "sentence_idx": 465, "label": "unlabeled"}, {"type": "text", "expr": " In all cases, we use the squared-error distillation loss shown in Proposition  1 ", "word_idx": 39912, "sentence_idx": 466, "label": "unlabeled"}, {"type": "text", "expr": " We found that squared loss worked much better than the cross-entropy loss for distillation and it was much easier to tune", "word_idx": 39994, "sentence_idx": 467, "label": "unlabeled"}, {"type": "text", "expr": "From Table  1  we can conclude that (1) it is generally beneficial to do any form of distillation to improve performance, (2) matching Jacobians along with activations outperforms matching only activations in low-data settings, (3) not matching Jacobians is often beneficial for large data", "word_idx": 40116, "sentence_idx": 468, "label": "unlabeled"}, {"type": "text", "expr": "One interesting phenomenon we observe is that having a cross-entropy (CE) error term is often not crucial to maintain good performance", "word_idx": 40405, "sentence_idx": 469, "label": "unlabeled"}, {"type": "text", "expr": " It is only slightly worse than using ground truth labels", "word_idx": 40539, "sentence_idx": 470, "label": "unlabeled"}, {"type": "text", "expr": "One interesting phenomenon we observe is that having a cross-entropy (CE) error term is often not crucial to maintain good performance", "word_idx": 40596, "sentence_idx": 471, "label": "unlabeled"}, {"type": "text", "expr": " It is only slightly worse than using ground truth labels", "word_idx": 40730, "sentence_idx": 472, "label": "unlabeled"}, {"type": "text", "expr": "For Table  1 , we see that when training with activations, Jacobians and regular cross-entropy training (fourth row), we reach an accuracy of  $5243\\%$  when training with 100 data points per class", "word_idx": 40787, "sentence_idx": 473, "label": "unlabeled"}, {"type": "text", "expr": " We observe that the overall accuracy of raw training using the full dataset is  $5428\\%$ ", "word_idx": 40984, "sentence_idx": 474, "label": "unlabeled"}, {"type": "text", "expr": " Thus we are able to reach close to the full training accuracy using  $1/5^{th}$  of the data requirement", "word_idx": 41074, "sentence_idx": 475, "label": "unlabeled"}, {"type": "math", "expr": "$$52.43\\%$$", "word_idx": 41179, "sentence_idx": 476, "label": "unlabeled"}, {"type": "math", "expr": "$$54.28\\%$$", "word_idx": 41186, "sentence_idx": 477, "label": "unlabeled"}, {"type": "math", "expr": "$$1/5^{th}$$", "word_idx": 41193, "sentence_idx": 478, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:  Distillation performance on the CIFAR100 dataset", "word_idx": 41201, "sentence_idx": 479, "label": "unlabeled"}, {"type": "text", "expr": " Table shows test accuracy (%)", "word_idx": 41259, "sentence_idx": 480, "label": "unlabeled"}, {"type": "text", "expr": " We find that matching both activations and Jacobians along with cross-entropy error performs the best for limited-data settings", "word_idx": 41289, "sentence_idx": 481, "label": "unlabeled"}, {"type": "text", "expr": " The student network is VGG-4 while the teacher is a VGG-9 network which achieves  $6478\\%$  accuracy", "word_idx": 41417, "sentence_idx": 482, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:", "word_idx": 41518, "sentence_idx": 483, "label": "unlabeled"}, {"type": "math", "expr": "$$64.78\\%$$", "word_idx": 41526, "sentence_idx": 484, "label": "unlabeled"}, {"type": "text", "expr": "1 5 10 50 100 500 (full)", "word_idx": 41533, "sentence_idx": 485, "label": "unlabeled"}, {"type": "text", "expr": "# of Data points per class  $\\rightarrow$", "word_idx": 41557, "sentence_idx": 486, "label": "unlabeled"}, {"type": "math", "expr": "$$\\rightarrow$$", "word_idx": 41598, "sentence_idx": 487, "label": "unlabeled"}, {"type": "text", "expr": "500 (full)", "word_idx": 41609, "sentence_idx": 488, "label": "unlabeled"}, {"type": "text", "expr": "69 13", "word_idx": 41619, "sentence_idx": 489, "label": "unlabeled"}, {"type": "text", "expr": "03 37", "word_idx": 41624, "sentence_idx": 490, "label": "unlabeled"}, {"type": "text", "expr": "92 54", "word_idx": 41629, "sentence_idx": 491, "label": "unlabeled"}, {"type": "text", "expr": "Cross-Entropy (CE) training", "word_idx": 41634, "sentence_idx": 492, "label": "unlabeled"}, {"type": "text", "expr": "Cross-Entropy (CE) training", "word_idx": 41661, "sentence_idx": 493, "label": "unlabeled"}, {"type": "text", "expr": "13 26", "word_idx": 41688, "sentence_idx": 494, "label": "unlabeled"}, {"type": "text", "expr": "97 33", "word_idx": 41693, "sentence_idx": 495, "label": "unlabeled"}, {"type": "text", "expr": "92 46", "word_idx": 41698, "sentence_idx": 496, "label": "unlabeled"}, {"type": "text", "expr": "47 50", "word_idx": 41703, "sentence_idx": 497, "label": "unlabeled"}, {"type": "text", "expr": "92 56", "word_idx": 41708, "sentence_idx": 498, "label": "unlabeled"}, {"type": "text", "expr": "CE + match activations", "word_idx": 41713, "sentence_idx": 499, "label": "unlabeled"}, {"type": "text", "expr": "CE + match activations", "word_idx": 41735, "sentence_idx": 500, "label": "unlabeled"}, {"type": "text", "expr": "CE + match Jacobians 6", "word_idx": 41757, "sentence_idx": 501, "label": "unlabeled"}, {"type": "text", "expr": "78 23", "word_idx": 41779, "sentence_idx": 502, "label": "unlabeled"}, {"type": "text", "expr": "94 32", "word_idx": 41784, "sentence_idx": 503, "label": "unlabeled"}, {"type": "text", "expr": "03 45", "word_idx": 41789, "sentence_idx": 504, "label": "unlabeled"}, {"type": "text", "expr": "71 51", "word_idx": 41794, "sentence_idx": 505, "label": "unlabeled"}, {"type": "text", "expr": "47 53", "word_idx": 41799, "sentence_idx": 506, "label": "unlabeled"}, {"type": "text", "expr": "CE + match Jacobians", "word_idx": 41804, "sentence_idx": 507, "label": "unlabeled"}, {"type": "text", "expr": "CE + match Jacobians", "word_idx": 41824, "sentence_idx": 508, "label": "unlabeled"}, {"type": "text", "expr": "CE + match {activations + Jacobians} 13", "word_idx": 41844, "sentence_idx": 509, "label": "unlabeled"}, {"type": "text", "expr": "78 33", "word_idx": 41883, "sentence_idx": 510, "label": "unlabeled"}, {"type": "text", "expr": "39 39", "word_idx": 41888, "sentence_idx": 511, "label": "unlabeled"}, {"type": "text", "expr": "55 49", "word_idx": 41893, "sentence_idx": 512, "label": "unlabeled"}, {"type": "text", "expr": "49 52", "word_idx": 41898, "sentence_idx": 513, "label": "unlabeled"}, {"type": "text", "expr": "43 54", "word_idx": 41903, "sentence_idx": 514, "label": "unlabeled"}, {"type": "text", "expr": "CE + match {activations + Jacobians}", "word_idx": 41908, "sentence_idx": 515, "label": "unlabeled"}, {"type": "text", "expr": "CE + match {activations + Jacobians}", "word_idx": 41944, "sentence_idx": 516, "label": "unlabeled"}, {"type": "text", "expr": "73 28", "word_idx": 41980, "sentence_idx": 517, "label": "unlabeled"}, {"type": "text", "expr": "56 33", "word_idx": 41985, "sentence_idx": 518, "label": "unlabeled"}, {"type": "text", "expr": "73 50", "word_idx": 41990, "sentence_idx": 519, "label": "unlabeled"}, {"type": "text", "expr": "15 56", "word_idx": 41995, "sentence_idx": 520, "label": "unlabeled"}, {"type": "text", "expr": "Match activations only", "word_idx": 42000, "sentence_idx": 521, "label": "unlabeled"}, {"type": "text", "expr": "Match activations only", "word_idx": 42022, "sentence_idx": 522, "label": "unlabeled"}, {"type": "text", "expr": "Match {activations + Jacobians} 13", "word_idx": 42044, "sentence_idx": 523, "label": "unlabeled"}, {"type": "text", "expr": "09 33", "word_idx": 42078, "sentence_idx": 524, "label": "unlabeled"}, {"type": "text", "expr": "31 38", "word_idx": 42083, "sentence_idx": 525, "label": "unlabeled"}, {"type": "text", "expr": "16 47", "word_idx": 42088, "sentence_idx": 526, "label": "unlabeled"}, {"type": "text", "expr": "79 50", "word_idx": 42093, "sentence_idx": 527, "label": "unlabeled"}, {"type": "text", "expr": "06 51", "word_idx": 42098, "sentence_idx": 528, "label": "unlabeled"}, {"type": "text", "expr": "Match {activations + Jacobians}", "word_idx": 42103, "sentence_idx": 529, "label": "unlabeled"}, {"type": "text", "expr": "Match {activations + Jacobians}", "word_idx": 42134, "sentence_idx": 530, "label": "unlabeled"}, {"type": "text", "expr": "2  Noise robustness", "word_idx": 42165, "sentence_idx": 531, "label": "unlabeled"}, {"type": "text", "expr": "We perform experiments where we penalize the Jacobian norm to improve robustness of models to random noise", "word_idx": 42184, "sentence_idx": 532, "label": "unlabeled"}, {"type": "text", "expr": " We train 9-layer VGG networks on CIFAR100 with varying amount of the regularization strength ( $\\lambda$ ), and measure their classification accuracy in presence of noise added to the normalized images", "word_idx": 42290, "sentence_idx": 533, "label": "unlabeled"}, {"type": "text", "expr": " From Table  2  we find that using higher regularization strengths is better for robustness, even when the initial accuracy at the zero-noise case is lower", "word_idx": 42492, "sentence_idx": 534, "label": "unlabeled"}, {"type": "text", "expr": " This aligns remarkably well with theory", "word_idx": 42647, "sentence_idx": 535, "label": "unlabeled"}, {"type": "text", "expr": " Surprisingly, we find that popular regularizers such as  $\\ell_{2}$  regularization and dropout\u00a0 \\citep srivastava2014dropout are not robust", "word_idx": 42687, "sentence_idx": 536, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda$$", "word_idx": 42828, "sentence_idx": 537, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ell_{2}$$", "word_idx": 42835, "sentence_idx": 538, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 42843, "sentence_idx": 539, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:  Robustness of various VGG-9 models to gaussian noise added to CIFAR100 images at test time", "word_idx": 42849, "sentence_idx": 540, "label": "unlabeled"}, {"type": "text", "expr": " Table shows test accuracy (%)", "word_idx": 42949, "sentence_idx": 541, "label": "unlabeled"}, {"type": "text", "expr": "  $\\lambda$  is the regularization strength of the Jacobian-norm penalty regularizer", "word_idx": 42979, "sentence_idx": 542, "label": "unlabeled"}, {"type": "text", "expr": "  $\\gamma$  is the  $\\ell_{2}$  regularization strength and  $p$  is the dropout value", "word_idx": 43063, "sentence_idx": 543, "label": "unlabeled"}, {"type": "text", "expr": " We see that the Jacobian-norm penalty can be remarkably robust to noise, unlike  $\\ell_{2}$  regularization and dropout", "word_idx": 43149, "sentence_idx": 544, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:", "word_idx": 43269, "sentence_idx": 545, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda$$", "word_idx": 43277, "sentence_idx": 546, "label": "unlabeled"}, {"type": "math", "expr": "$$\\gamma$$", "word_idx": 43284, "sentence_idx": 547, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ell_{2}$$", "word_idx": 43290, "sentence_idx": 548, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ell_{2}$$", "word_idx": 43298, "sentence_idx": 549, "label": "unlabeled"}, {"type": "text", "expr": "Noise std", "word_idx": 43306, "sentence_idx": 550, "label": "unlabeled"}, {"type": "text", "expr": "  $\\rightarrow$", "word_idx": 43315, "sentence_idx": 551, "label": "unlabeled"}, {"type": "math", "expr": "$$\\rightarrow$$", "word_idx": 43330, "sentence_idx": 552, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda=0$$", "word_idx": 43341, "sentence_idx": 553, "label": "unlabeled"}, {"type": "text", "expr": "$619\\pm 007$", "word_idx": 43350, "sentence_idx": 554, "label": "unlabeled"}, {"type": "math", "expr": "$$61.9\\pm 0.07$$", "word_idx": 43362, "sentence_idx": 555, "label": "unlabeled"}, {"type": "text", "expr": "$4753\\pm 023$", "word_idx": 43374, "sentence_idx": 556, "label": "unlabeled"}, {"type": "math", "expr": "$$47.53\\pm 0.23$$", "word_idx": 43387, "sentence_idx": 557, "label": "unlabeled"}, {"type": "text", "expr": "$2963\\pm 016$", "word_idx": 43400, "sentence_idx": 558, "label": "unlabeled"}, {"type": "math", "expr": "$$29.63\\pm 0.16$$", "word_idx": 43413, "sentence_idx": 559, "label": "unlabeled"}, {"type": "text", "expr": "$1769\\pm 017$", "word_idx": 43426, "sentence_idx": 560, "label": "unlabeled"}, {"type": "math", "expr": "$$17.69\\pm 0.17$$", "word_idx": 43439, "sentence_idx": 561, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda=0.1$$", "word_idx": 43452, "sentence_idx": 562, "label": "unlabeled"}, {"type": "text", "expr": "$\\bm{6336\\pm 018}$", "word_idx": 43463, "sentence_idx": 563, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{63.36\\pm 0.18}$$", "word_idx": 43481, "sentence_idx": 564, "label": "unlabeled"}, {"type": "text", "expr": "$5357\\pm 023$", "word_idx": 43499, "sentence_idx": 565, "label": "unlabeled"}, {"type": "math", "expr": "$$53.57\\pm 0.23$$", "word_idx": 43512, "sentence_idx": 566, "label": "unlabeled"}, {"type": "text", "expr": "$3738\\pm 018$", "word_idx": 43525, "sentence_idx": 567, "label": "unlabeled"}, {"type": "math", "expr": "$$37.38\\pm 0.18$$", "word_idx": 43538, "sentence_idx": 568, "label": "unlabeled"}, {"type": "text", "expr": "$2399\\pm 019$", "word_idx": 43551, "sentence_idx": 569, "label": "unlabeled"}, {"type": "math", "expr": "$$23.99\\pm 0.19$$", "word_idx": 43564, "sentence_idx": 570, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda=1.0$$", "word_idx": 43577, "sentence_idx": 571, "label": "unlabeled"}, {"type": "text", "expr": "$6266\\pm 013$", "word_idx": 43588, "sentence_idx": 572, "label": "unlabeled"}, {"type": "math", "expr": "$$62.66\\pm 0.13$$", "word_idx": 43601, "sentence_idx": 573, "label": "unlabeled"}, {"type": "text", "expr": "${5753\\pm 017}$", "word_idx": 43614, "sentence_idx": 574, "label": "unlabeled"}, {"type": "math", "expr": "$${57.53\\pm 0.17}$$", "word_idx": 43629, "sentence_idx": 575, "label": "unlabeled"}, {"type": "text", "expr": "${4748\\pm 014}$", "word_idx": 43644, "sentence_idx": 576, "label": "unlabeled"}, {"type": "math", "expr": "$${47.48\\pm 0.14}$$", "word_idx": 43659, "sentence_idx": 577, "label": "unlabeled"}, {"type": "text", "expr": "${3543\\pm 011}$", "word_idx": 43674, "sentence_idx": 578, "label": "unlabeled"}, {"type": "math", "expr": "$${35.43\\pm 0.11}$$", "word_idx": 43689, "sentence_idx": 579, "label": "unlabeled"}, {"type": "math", "expr": "$$\\lambda=10.0$$", "word_idx": 43704, "sentence_idx": 580, "label": "unlabeled"}, {"type": "text", "expr": "$6078\\pm 005$", "word_idx": 43716, "sentence_idx": 581, "label": "unlabeled"}, {"type": "math", "expr": "$$60.78\\pm 0.05$$", "word_idx": 43729, "sentence_idx": 582, "label": "unlabeled"}, {"type": "text", "expr": "$\\bm{5828\\pm 013}$", "word_idx": 43742, "sentence_idx": 583, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{58.28\\pm 0.13}$$", "word_idx": 43760, "sentence_idx": 584, "label": "unlabeled"}, {"type": "text", "expr": "$\\bm{5282\\pm 010}$", "word_idx": 43778, "sentence_idx": 585, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{52.82\\pm 0.10}$$", "word_idx": 43796, "sentence_idx": 586, "label": "unlabeled"}, {"type": "text", "expr": "$\\bm{4496\\pm 019}$", "word_idx": 43814, "sentence_idx": 587, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{44.96\\pm 0.19}$$", "word_idx": 43832, "sentence_idx": 588, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ell_{2}$$", "word_idx": 43850, "sentence_idx": 589, "label": "unlabeled"}, {"type": "math", "expr": "$$\\gamma=1e-3$$", "word_idx": 43858, "sentence_idx": 590, "label": "unlabeled"}, {"type": "text", "expr": "$6041\\pm 027$", "word_idx": 43869, "sentence_idx": 591, "label": "unlabeled"}, {"type": "math", "expr": "$$60.41\\pm 0.27$$", "word_idx": 43882, "sentence_idx": 592, "label": "unlabeled"}, {"type": "text", "expr": "$3993\\pm 028$", "word_idx": 43895, "sentence_idx": 593, "label": "unlabeled"}, {"type": "math", "expr": "$$39.93\\pm 0.28$$", "word_idx": 43908, "sentence_idx": 594, "label": "unlabeled"}, {"type": "text", "expr": "$2332\\pm 025$", "word_idx": 43921, "sentence_idx": 595, "label": "unlabeled"}, {"type": "math", "expr": "$$23.32\\pm 0.25$$", "word_idx": 43934, "sentence_idx": 596, "label": "unlabeled"}, {"type": "text", "expr": "$1376\\pm 016$", "word_idx": 43947, "sentence_idx": 597, "label": "unlabeled"}, {"type": "math", "expr": "$$13.76\\pm 0.16$$", "word_idx": 43960, "sentence_idx": 598, "label": "unlabeled"}, {"type": "math", "expr": "$$(p=0.25)$$", "word_idx": 43973, "sentence_idx": 599, "label": "unlabeled"}, {"type": "text", "expr": "$6153\\pm 010$", "word_idx": 43981, "sentence_idx": 600, "label": "unlabeled"}, {"type": "math", "expr": "$$61.53\\pm 0.10$$", "word_idx": 43994, "sentence_idx": 601, "label": "unlabeled"}, {"type": "text", "expr": "$4434\\pm 019$", "word_idx": 44007, "sentence_idx": 602, "label": "unlabeled"}, {"type": "math", "expr": "$$44.34\\pm 0.19$$", "word_idx": 44020, "sentence_idx": 603, "label": "unlabeled"}, {"type": "text", "expr": "$2670\\pm 024$", "word_idx": 44033, "sentence_idx": 604, "label": "unlabeled"}, {"type": "math", "expr": "$$26.70\\pm 0.24$$", "word_idx": 44046, "sentence_idx": 605, "label": "unlabeled"}, {"type": "text", "expr": "$1577\\pm 011$", "word_idx": 44059, "sentence_idx": 606, "label": "unlabeled"}, {"type": "math", "expr": "$$15.77\\pm 0.11$$", "word_idx": 44072, "sentence_idx": 607, "label": "unlabeled"}, {"type": "text", "expr": "3  Transfer Learning", "word_idx": 44085, "sentence_idx": 608, "label": "unlabeled"}, {"type": "text", "expr": "For transfer learning, our objective is to improve training on the target dataset (MIT Scenes) by using Imagenet pre-trained models", "word_idx": 44105, "sentence_idx": 609, "label": "unlabeled"}, {"type": "text", "expr": " Crucially, we want our MIT Scenes model to have a different architecture than the Imagenet model", "word_idx": 44236, "sentence_idx": 610, "label": "unlabeled"}, {"type": "text", "expr": " The teacher model we use is a ResNet-34\u00a0 \\citep he2016deep pre-trained on Imagenet, while the student model is an untrained VGG-9 model with batch normalization", "word_idx": 44333, "sentence_idx": 611, "label": "unlabeled"}, {"type": "text", "expr": " We choose VGG-9 because its architecture is fundamentally different from a ResNet", "word_idx": 44494, "sentence_idx": 612, "label": "unlabeled"}, {"type": "text", "expr": " In principle we could use any architecture for the student", "word_idx": 44576, "sentence_idx": 613, "label": "unlabeled"}, {"type": "text", "expr": " We modify this VGG-9 architecture such that it has two sets of outputs, one sharing the label space with Imagenet (1000 classes), and another with MIT Scenes (67 classes,  $\\sim$  6k images)", "word_idx": 44635, "sentence_idx": 614, "label": "unlabeled"}, {"type": "text", "expr": " The pre-final layer is common to both outputs", "word_idx": 44826, "sentence_idx": 615, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 44872, "sentence_idx": 616, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sim$$", "word_idx": 44878, "sentence_idx": 617, "label": "unlabeled"}, {"type": "text", "expr": "Our baselines are the following", "word_idx": 44882, "sentence_idx": 618, "label": "unlabeled"}, {"type": "text", "expr": "  (1): Cross-Entropy (CE) training of student with ground truth  \u2013 Here we ignore the VGG-9 branch with 1000 classes and optimize the cross-entropy error on MIT Scenes data", "word_idx": 44913, "sentence_idx": 619, "label": "unlabeled"}, {"type": "text", "expr": " The loss function on this branch is always the same for all methods", "word_idx": 45085, "sentence_idx": 620, "label": "unlabeled"}, {"type": "text", "expr": "  (2): CE on pre-trained network  \u2013 This is exactly the same as the first baseline, except that the VGG-9 model is initialized from Imagenet pre-trained weights", "word_idx": 45153, "sentence_idx": 621, "label": "unlabeled"}, {"type": "text", "expr": " We consider this as our \u201coracle\u201d method and strive to match it\u2019s performance", "word_idx": 45313, "sentence_idx": 622, "label": "unlabeled"}, {"type": "text", "expr": "  (3): CE + match activations (LwF)  \u2013 This corresponds to the method of\u00a0 \\citet li2016learning, where the Imagenet branch output activations of the student are matched with those of the teacher", "word_idx": 45390, "sentence_idx": 623, "label": "unlabeled"}, {"type": "text", "expr": "  (4): CE + match { activations + attention}   \u2013 This corresponds to the method of\u00a0 \\citet zagoruyko2016paying, where attention maps are matched between some intermediate layers", "word_idx": 45584, "sentence_idx": 624, "label": "unlabeled"}, {"type": "text", "expr": "(1): Cross-Entropy (CE) training of student with ground truth", "word_idx": 45761, "sentence_idx": 625, "label": "unlabeled"}, {"type": "text", "expr": "(2): CE on pre-trained network", "word_idx": 45822, "sentence_idx": 626, "label": "unlabeled"}, {"type": "text", "expr": "(3): CE + match activations (LwF)", "word_idx": 45852, "sentence_idx": 627, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 45885, "sentence_idx": 628, "label": "unlabeled"}, {"type": "text", "expr": "(4): CE + match { activations + attention}", "word_idx": 45891, "sentence_idx": 629, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 45933, "sentence_idx": 630, "label": "unlabeled"}, {"type": "text", "expr": "We add our Jacobian matching terms to the baselines 3 and 4", "word_idx": 45939, "sentence_idx": 631, "label": "unlabeled"}, {"type": "text", "expr": " We provide our results in Table  3 ", "word_idx": 45998, "sentence_idx": 632, "label": "unlabeled"}, {"type": "text", "expr": " In all cases, we vary the number of images per class on MIT Scenes to observe the performance on low-data settings as well", "word_idx": 46034, "sentence_idx": 633, "label": "unlabeled"}, {"type": "text", "expr": " In this case we average our results over two runs by choosing different random subsets", "word_idx": 46157, "sentence_idx": 634, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a03:  Transfer Learning from Imagenet to MIT Scenes dataset", "word_idx": 46244, "sentence_idx": 635, "label": "unlabeled"}, {"type": "text", "expr": " Table shows test accuracy (%)", "word_idx": 46307, "sentence_idx": 636, "label": "unlabeled"}, {"type": "text", "expr": " The student network (VGG9) is trained from scratch unless otherwise mentioned", "word_idx": 46337, "sentence_idx": 637, "label": "unlabeled"}, {"type": "text", "expr": " The teacher network used is a pre-trained ResNet34", "word_idx": 46415, "sentence_idx": 638, "label": "unlabeled"}, {"type": "text", "expr": " Results are averaged over two runs", "word_idx": 46466, "sentence_idx": 639, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a03:", "word_idx": 46501, "sentence_idx": 640, "label": "unlabeled"}, {"type": "text", "expr": "5 10 25 50 Full", "word_idx": 46509, "sentence_idx": 641, "label": "unlabeled"}, {"type": "text", "expr": "# of Data points per class  $\\rightarrow$", "word_idx": 46524, "sentence_idx": 642, "label": "unlabeled"}, {"type": "math", "expr": "$$\\rightarrow$$", "word_idx": 46565, "sentence_idx": 643, "label": "unlabeled"}, {"type": "text", "expr": "64 20", "word_idx": 46576, "sentence_idx": 644, "label": "unlabeled"}, {"type": "text", "expr": "30 35", "word_idx": 46581, "sentence_idx": 645, "label": "unlabeled"}, {"type": "text", "expr": "19 46", "word_idx": 46586, "sentence_idx": 646, "label": "unlabeled"}, {"type": "text", "expr": "38 59", "word_idx": 46591, "sentence_idx": 647, "label": "unlabeled"}, {"type": "text", "expr": "Cross-Entropy (CE) training on untrained student network", "word_idx": 46596, "sentence_idx": 648, "label": "unlabeled"}, {"type": "text", "expr": "Cross-Entropy (CE) training on untrained student network", "word_idx": 46652, "sentence_idx": 649, "label": "unlabeled"}, {"type": "text", "expr": "CE on pre-trained student network (Oracle) 25", "word_idx": 46708, "sentence_idx": 650, "label": "unlabeled"}, {"type": "text", "expr": "93 43", "word_idx": 46753, "sentence_idx": 651, "label": "unlabeled"}, {"type": "text", "expr": "81 57", "word_idx": 46758, "sentence_idx": 652, "label": "unlabeled"}, {"type": "text", "expr": "65 64", "word_idx": 46763, "sentence_idx": 653, "label": "unlabeled"}, {"type": "text", "expr": "18 71", "word_idx": 46768, "sentence_idx": 654, "label": "unlabeled"}, {"type": "text", "expr": "CE on pre-trained student network (Oracle)", "word_idx": 46773, "sentence_idx": 655, "label": "unlabeled"}, {"type": "text", "expr": "CE on pre-trained student network (Oracle)", "word_idx": 46815, "sentence_idx": 656, "label": "unlabeled"}, {"type": "text", "expr": "08 27", "word_idx": 46857, "sentence_idx": 657, "label": "unlabeled"}, {"type": "text", "expr": "13 45", "word_idx": 46862, "sentence_idx": 658, "label": "unlabeled"}, {"type": "text", "expr": "08 55", "word_idx": 46867, "sentence_idx": 659, "label": "unlabeled"}, {"type": "text", "expr": "22 65", "word_idx": 46872, "sentence_idx": 660, "label": "unlabeled"}, {"type": "text", "expr": "CE + match activations\u00a0 \\citep li2016learning", "word_idx": 46877, "sentence_idx": 661, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 46922, "sentence_idx": 662, "label": "unlabeled"}, {"type": "text", "expr": "CE + match {activations + Jacobians} 17", "word_idx": 46928, "sentence_idx": 663, "label": "unlabeled"}, {"type": "text", "expr": "88 28", "word_idx": 46967, "sentence_idx": 664, "label": "unlabeled"}, {"type": "text", "expr": "25 45", "word_idx": 46972, "sentence_idx": 665, "label": "unlabeled"}, {"type": "text", "expr": "26 56", "word_idx": 46977, "sentence_idx": 666, "label": "unlabeled"}, {"type": "text", "expr": "49 66", "word_idx": 46982, "sentence_idx": 667, "label": "unlabeled"}, {"type": "text", "expr": "CE + match {activations + Jacobians}", "word_idx": 46987, "sentence_idx": 668, "label": "unlabeled"}, {"type": "text", "expr": "CE + match {activations + Jacobians}", "word_idx": 47023, "sentence_idx": 669, "label": "unlabeled"}, {"type": "text", "expr": "53 28", "word_idx": 47059, "sentence_idx": 670, "label": "unlabeled"}, {"type": "text", "expr": "35 46", "word_idx": 47064, "sentence_idx": 671, "label": "unlabeled"}, {"type": "text", "expr": "01 57", "word_idx": 47069, "sentence_idx": 672, "label": "unlabeled"}, {"type": "text", "expr": "80 67", "word_idx": 47074, "sentence_idx": 673, "label": "unlabeled"}, {"type": "text", "expr": "CE + match {activations + attention}\u00a0 \\citep zagoruyko2016paying", "word_idx": 47079, "sentence_idx": 674, "label": "unlabeled"}, {"type": "text", "expr": "\\citep", "word_idx": 47143, "sentence_idx": 675, "label": "unlabeled"}, {"type": "text", "expr": "CE + match {activations + attention + Jacobians} 18", "word_idx": 47149, "sentence_idx": 676, "label": "unlabeled"}, {"type": "text", "expr": "02 29", "word_idx": 47200, "sentence_idx": 677, "label": "unlabeled"}, {"type": "text", "expr": "25 47", "word_idx": 47205, "sentence_idx": 678, "label": "unlabeled"}, {"type": "text", "expr": "31 58", "word_idx": 47210, "sentence_idx": 679, "label": "unlabeled"}, {"type": "text", "expr": "35 67", "word_idx": 47215, "sentence_idx": 680, "label": "unlabeled"}, {"type": "text", "expr": "CE + match {activations + attention + Jacobians}", "word_idx": 47220, "sentence_idx": 681, "label": "unlabeled"}, {"type": "text", "expr": "CE + match {activations + attention + Jacobians}", "word_idx": 47268, "sentence_idx": 682, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a04:  Ablation experiments over choice of feature matching depth", "word_idx": 47316, "sentence_idx": 683, "label": "unlabeled"}, {"type": "text", "expr": " ( $\\mathcal{T}$ ,  $\\mathcal{S}$ ) denotes teacher (ResNet34) and student (VGG9) feature depths", "word_idx": 47384, "sentence_idx": 684, "label": "unlabeled"}, {"type": "text", "expr": " These pairs are chosen such that resulting features have same spatial dimensions", "word_idx": 47480, "sentence_idx": 685, "label": "unlabeled"}, {"type": "text", "expr": " We see that matching the shallowest feature works the best", "word_idx": 47561, "sentence_idx": 686, "label": "unlabeled"}, {"type": "text", "expr": " Results are averaged over two runs", "word_idx": 47620, "sentence_idx": 687, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a04:", "word_idx": 47655, "sentence_idx": 688, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{T}$$", "word_idx": 47663, "sentence_idx": 689, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{S}$$", "word_idx": 47674, "sentence_idx": 690, "label": "unlabeled"}, {"type": "text", "expr": "(7, 2) (15, 4) (27, 6) (33, 8)", "word_idx": 47685, "sentence_idx": 691, "label": "unlabeled"}, {"type": "text", "expr": "Feature matching", "word_idx": 47715, "sentence_idx": 692, "label": "unlabeled"}, {"type": "text", "expr": "Feature matching", "word_idx": 47731, "sentence_idx": 693, "label": "unlabeled"}, {"type": "text", "expr": "Feature matching", "word_idx": 47747, "sentence_idx": 694, "label": "unlabeled"}, {"type": "text", "expr": "depth ( $\\mathcal{T}$ ,  $\\mathcal{S}$ )", "word_idx": 47763, "sentence_idx": 695, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{T}$$", "word_idx": 47803, "sentence_idx": 696, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{S}$$", "word_idx": 47814, "sentence_idx": 697, "label": "unlabeled"}, {"type": "text", "expr": "39 21", "word_idx": 47825, "sentence_idx": 698, "label": "unlabeled"}, {"type": "text", "expr": "98 20", "word_idx": 47830, "sentence_idx": 699, "label": "unlabeled"}, {"type": "text", "expr": "45 20", "word_idx": 47835, "sentence_idx": 700, "label": "unlabeled"}, {"type": "text", "expr": "Accuracy (%)", "word_idx": 47840, "sentence_idx": 701, "label": "unlabeled"}, {"type": "text", "expr": "Accuracy (%)", "word_idx": 47852, "sentence_idx": 702, "label": "unlabeled"}, {"type": "text", "expr": "88 15", "word_idx": 47864, "sentence_idx": 703, "label": "unlabeled"}, {"type": "text", "expr": "59 11", "word_idx": 47869, "sentence_idx": 704, "label": "unlabeled"}, {"type": "text", "expr": "Jacobian loss", "word_idx": 47874, "sentence_idx": 705, "label": "unlabeled"}, {"type": "text", "expr": "Jacobian loss", "word_idx": 47887, "sentence_idx": 706, "label": "unlabeled"}, {"type": "text", "expr": "Jacobian loss", "word_idx": 47900, "sentence_idx": 707, "label": "unlabeled"}, {"type": "text", "expr": "reduction (%)", "word_idx": 47913, "sentence_idx": 708, "label": "unlabeled"}, {"type": "text", "expr": "reduction (%)", "word_idx": 47926, "sentence_idx": 709, "label": "unlabeled"}, {"type": "text", "expr": "reduction (%)", "word_idx": 47939, "sentence_idx": 710, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a05:  Ablation experiments over the computation of Jacobian", "word_idx": 47952, "sentence_idx": 711, "label": "unlabeled"}, {"type": "text", "expr": " Here,  $s$  is the size of the attention map", "word_idx": 48015, "sentence_idx": 712, "label": "unlabeled"}, {"type": "text", "expr": " \u201cFull\u201d is global average pooling, and \u201cNone\u201d is no average pooling", "word_idx": 48060, "sentence_idx": 713, "label": "unlabeled"}, {"type": "text", "expr": " We see that using average pooling while computing Jacobians helps performance", "word_idx": 48127, "sentence_idx": 714, "label": "unlabeled"}, {"type": "text", "expr": " Results are averaged over two runs", "word_idx": 48205, "sentence_idx": 715, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a05:", "word_idx": 48240, "sentence_idx": 716, "label": "unlabeled"}, {"type": "text", "expr": "Average Pool", "word_idx": 48248, "sentence_idx": 717, "label": "unlabeled"}, {"type": "text", "expr": "Average Pool", "word_idx": 48260, "sentence_idx": 718, "label": "unlabeled"}, {"type": "text", "expr": "Average Pool", "word_idx": 48272, "sentence_idx": 719, "label": "unlabeled"}, {"type": "text", "expr": "Window size", "word_idx": 48284, "sentence_idx": 720, "label": "unlabeled"}, {"type": "text", "expr": "Window size", "word_idx": 48295, "sentence_idx": 721, "label": "unlabeled"}, {"type": "text", "expr": "Window size", "word_idx": 48306, "sentence_idx": 722, "label": "unlabeled"}, {"type": "math", "expr": "$$s/3$$", "word_idx": 48317, "sentence_idx": 723, "label": "unlabeled"}, {"type": "math", "expr": "$$s/5$$", "word_idx": 48320, "sentence_idx": 724, "label": "unlabeled"}, {"type": "math", "expr": "$$s/7$$", "word_idx": 48323, "sentence_idx": 725, "label": "unlabeled"}, {"type": "text", "expr": "00 21", "word_idx": 48326, "sentence_idx": 726, "label": "unlabeled"}, {"type": "text", "expr": "20 21", "word_idx": 48331, "sentence_idx": 727, "label": "unlabeled"}, {"type": "text", "expr": "87 21", "word_idx": 48336, "sentence_idx": 728, "label": "unlabeled"}, {"type": "text", "expr": "09 19", "word_idx": 48341, "sentence_idx": 729, "label": "unlabeled"}, {"type": "text", "expr": "Accuracy (%)", "word_idx": 48346, "sentence_idx": 730, "label": "unlabeled"}, {"type": "text", "expr": "Accuracy (%)", "word_idx": 48358, "sentence_idx": 731, "label": "unlabeled"}, {"type": "text", "expr": "Experiments show that matching activations and attention maps increases performance at all levels of data size", "word_idx": 48370, "sentence_idx": 732, "label": "unlabeled"}, {"type": "text", "expr": " It also shows that Jacobians improve performance of all these methods", "word_idx": 48480, "sentence_idx": 733, "label": "unlabeled"}, {"type": "text", "expr": " However, we observe that none of the methods match the oracle performance of using a pre-trained model", "word_idx": 48550, "sentence_idx": 734, "label": "unlabeled"}, {"type": "text", "expr": " The gap in performance is especially large at intermediate data sizes of  $10$  and  $25$  images per class", "word_idx": 48653, "sentence_idx": 735, "label": "unlabeled"}, {"type": "text", "expr": "1  Which features to match?", "word_idx": 48761, "sentence_idx": 736, "label": "unlabeled"}, {"type": "text", "expr": "An important practical question is the choice of intermediate features to compute attention maps for matching", "word_idx": 48788, "sentence_idx": 737, "label": "unlabeled"}, {"type": "text", "expr": " The recipe followed by\u00a0 \\citet zagoruyko2016paying for ResNets is to consider features at the end of a residual block", "word_idx": 48897, "sentence_idx": 738, "label": "unlabeled"}, {"type": "text", "expr": "   As there are typically 3-5 max-pooling layers in most modern networks, we have 3-5 intermediate features to match between any typical teacher and student network", "word_idx": 49015, "sentence_idx": 739, "label": "unlabeled"}, {"type": "text", "expr": " Note that we require the attention maps (channelwise aggregated features) to be of similar spatial size to match", "word_idx": 49179, "sentence_idx": 740, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0 \\citet zagoruyko2016paying match at all such possible locations, and we use the same approach", "word_idx": 49292, "sentence_idx": 741, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 49387, "sentence_idx": 742, "label": "unlabeled"}, {"type": "text", "expr": "1 A residual block is the set of all layers in between two consecutive max-pooling layers in a ResNet", "word_idx": 49393, "sentence_idx": 743, "label": "unlabeled"}, {"type": "text", "expr": " All features in a block have the same dimensions", "word_idx": 49494, "sentence_idx": 744, "label": "unlabeled"}, {"type": "text", "expr": "\\citet", "word_idx": 49543, "sentence_idx": 745, "label": "unlabeled"}, {"type": "text", "expr": "However, computing Jacobians at all such locations is computationally demanding and perhaps unnecessary", "word_idx": 49549, "sentence_idx": 746, "label": "unlabeled"}, {"type": "text", "expr": " We observe that if we compute Jacobians at later layers, we are still not able to reduce training Jacobian loss, possibly due to a \u201csecond-order\u201d vanishing gradient effect", "word_idx": 49652, "sentence_idx": 747, "label": "unlabeled"}, {"type": "text", "expr": " At suitable intermediate layers, we see that loss reduction occurs", "word_idx": 49824, "sentence_idx": 748, "label": "unlabeled"}, {"type": "text", "expr": " This is reflected in Table  4 , where we vary the feature matching depth and observe performance", "word_idx": 49891, "sentence_idx": 749, "label": "unlabeled"}, {"type": "text", "expr": " We observe that the Jacobian loss reduction (during training) is highest for the shallowest layers, and this corresponds to good test performance as well", "word_idx": 49988, "sentence_idx": 750, "label": "unlabeled"}, {"type": "text", "expr": " These ablation experiments are done on the MIT Scenes dataset picking only  $10$  points per class", "word_idx": 50142, "sentence_idx": 751, "label": "unlabeled"}, {"type": "text", "expr": "2  Feature Pooling to compute Jacobians", "word_idx": 50241, "sentence_idx": 752, "label": "unlabeled"}, {"type": "text", "expr": "Instead of considering a single pixel per attention map to compute Jacobians, we can aggregate a large number of large-magnitude pixels", "word_idx": 50280, "sentence_idx": 753, "label": "unlabeled"}, {"type": "text", "expr": " One way to do this is by average pooling over the attention map, and then picking the maximum pixel over the average pooled map", "word_idx": 50415, "sentence_idx": 754, "label": "unlabeled"}, {"type": "text", "expr": " In Table  5  we vary the window size for average pooling and observe performance", "word_idx": 50543, "sentence_idx": 755, "label": "unlabeled"}, {"type": "text", "expr": " We observe that it is beneficial to do average pooling, we find that using a window size of  $\\mathrm{(feature~{}size)}/5$  works the best", "word_idx": 50624, "sentence_idx": 756, "label": "unlabeled"}, {"type": "text", "expr": " These ablation experiments are done on the MIT Scenes dataset picking only  $10$  points per class", "word_idx": 50763, "sentence_idx": 757, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{(feature~{}size)}/5$$", "word_idx": 50862, "sentence_idx": 758, "label": "unlabeled"}, {"type": "text", "expr": "7  Conclusion", "word_idx": 50889, "sentence_idx": 759, "label": "unlabeled"}, {"type": "text", "expr": "In this paper we considered matching Jacobians of deep neural networks for knowledge transfer", "word_idx": 50902, "sentence_idx": 760, "label": "unlabeled"}, {"type": "text", "expr": " Viewing Jacobian matching as a form of data augmentation with gaussian noise motivates their usage, and also informs us about the loss functions to use", "word_idx": 50995, "sentence_idx": 761, "label": "unlabeled"}, {"type": "text", "expr": " We also connected a recent transfer learning method (LwF) to distillation, enabling us to incorporate Jacobian matching", "word_idx": 51147, "sentence_idx": 762, "label": "unlabeled"}, {"type": "text", "expr": "In this paper we considered matching Jacobians of deep neural networks for knowledge transfer", "word_idx": 51267, "sentence_idx": 763, "label": "unlabeled"}, {"type": "text", "expr": " Viewing Jacobian matching as a form of data augmentation with gaussian noise motivates their usage, and also informs us about the loss functions to use", "word_idx": 51360, "sentence_idx": 764, "label": "unlabeled"}, {"type": "text", "expr": " We also connected a recent transfer learning method (LwF) to distillation, enabling us to incorporate Jacobian matching", "word_idx": 51512, "sentence_idx": 765, "label": "unlabeled"}, {"type": "text", "expr": "Despite our advances, there is still a large gap between distillation-based methods and the oracle method of using pre-trained nets for transfer learning", "word_idx": 51632, "sentence_idx": 766, "label": "unlabeled"}, {"type": "text", "expr": " Future work can focus on closing this gap by considering more structured forms of data augmentation than simple noise addition", "word_idx": 51785, "sentence_idx": 767, "label": "unlabeled"}, {"type": "text", "expr": "Despite our advances, there is still a large gap between distillation-based methods and the oracle method of using pre-trained nets for transfer learning", "word_idx": 51912, "sentence_idx": 768, "label": "unlabeled"}, {"type": "text", "expr": " Future work can focus on closing this gap by considering more structured forms of data augmentation than simple noise addition", "word_idx": 52065, "sentence_idx": 769, "label": "unlabeled"}, {"type": "text", "expr": "References", "word_idx": 52192, "sentence_idx": 770, "label": "unlabeled"}, {"type": "text", "expr": "Ba & Caruana(2014)Ba and Caruana \nBa, LJ and Caruana, R", "word_idx": 52202, "sentence_idx": 771, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Do deep networks really need to be deep", "word_idx": 52257, "sentence_idx": 772, "label": "unlabeled"}, {"type": "text", "expr": "Ba & Caruana(2014)Ba and Caruana", "word_idx": 52299, "sentence_idx": 773, "label": "unlabeled"}, {"type": "text", "expr": "Ba, LJ and Caruana, R", "word_idx": 52331, "sentence_idx": 774, "label": "unlabeled"}, {"type": "text", "expr": "Do deep networks really need to be deep", "word_idx": 52352, "sentence_idx": 775, "label": "unlabeled"}, {"type": "text", "expr": "Advances in neural information processing systems ,\n27:1\u20139, 2014", "word_idx": 52391, "sentence_idx": 776, "label": "unlabeled"}, {"type": "text", "expr": "Advances in neural information processing systems", "word_idx": 52455, "sentence_idx": 777, "label": "unlabeled"}, {"type": "text", "expr": "Ben-David et\u00a0al", "word_idx": 52504, "sentence_idx": 778, "label": "unlabeled"}, {"type": "text", "expr": "(2010)Ben-David, Blitzer, Crammer, Kulesza, Pereira,\nand Vaughan \nBen-David, Shai, Blitzer, John, Crammer, Koby, Kulesza, Alex, Pereira,\nFernando, and Vaughan, Jennifer\u00a0Wortman", "word_idx": 52519, "sentence_idx": 779, "label": "unlabeled"}, {"type": "text", "expr": "\n\n A theory of learning from different domains", "word_idx": 52695, "sentence_idx": 780, "label": "unlabeled"}, {"type": "text", "expr": "Ben-David et\u00a0al", "word_idx": 52741, "sentence_idx": 781, "label": "unlabeled"}, {"type": "text", "expr": "(2010)Ben-David, Blitzer, Crammer, Kulesza, Pereira,\nand Vaughan", "word_idx": 52756, "sentence_idx": 782, "label": "unlabeled"}, {"type": "text", "expr": "Ben-David, Shai, Blitzer, John, Crammer, Koby, Kulesza, Alex, Pereira,\nFernando, and Vaughan, Jennifer\u00a0Wortman", "word_idx": 52820, "sentence_idx": 783, "label": "unlabeled"}, {"type": "text", "expr": "A theory of learning from different domains", "word_idx": 52930, "sentence_idx": 784, "label": "unlabeled"}, {"type": "text", "expr": "Machine learning , 79(1-2):151\u2013175, 2010", "word_idx": 52973, "sentence_idx": 785, "label": "unlabeled"}, {"type": "text", "expr": "Machine learning", "word_idx": 53013, "sentence_idx": 786, "label": "unlabeled"}, {"type": "text", "expr": "Bishop(1995) \nBishop, Chris\u00a0M", "word_idx": 53029, "sentence_idx": 787, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Training with noise is equivalent to tikhonov regularization", "word_idx": 53058, "sentence_idx": 788, "label": "unlabeled"}, {"type": "text", "expr": "Bishop(1995)", "word_idx": 53121, "sentence_idx": 789, "label": "unlabeled"}, {"type": "text", "expr": "Bishop, Chris\u00a0M", "word_idx": 53133, "sentence_idx": 790, "label": "unlabeled"}, {"type": "text", "expr": "Training with noise is equivalent to tikhonov regularization", "word_idx": 53148, "sentence_idx": 791, "label": "unlabeled"}, {"type": "text", "expr": "Neural Computation , 1995", "word_idx": 53208, "sentence_idx": 792, "label": "unlabeled"}, {"type": "text", "expr": "Neural Computation", "word_idx": 53233, "sentence_idx": 793, "label": "unlabeled"}, {"type": "text", "expr": "Cisse et\u00a0al", "word_idx": 53251, "sentence_idx": 794, "label": "unlabeled"}, {"type": "text", "expr": "(2017)Cisse, Bojanowski, Grave, Dauphin, and\nUsunier \nCisse, Moustapha, Bojanowski, Piotr, Grave, Edouard, Dauphin, Yann, and\nUsunier, Nicolas", "word_idx": 53262, "sentence_idx": 795, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Parseval networks: Improving robustness to adversarial examples", "word_idx": 53404, "sentence_idx": 796, "label": "unlabeled"}, {"type": "text", "expr": "Cisse et\u00a0al", "word_idx": 53470, "sentence_idx": 797, "label": "unlabeled"}, {"type": "text", "expr": "(2017)Cisse, Bojanowski, Grave, Dauphin, and\nUsunier", "word_idx": 53481, "sentence_idx": 798, "label": "unlabeled"}, {"type": "text", "expr": "Cisse, Moustapha, Bojanowski, Piotr, Grave, Edouard, Dauphin, Yann, and\nUsunier, Nicolas", "word_idx": 53533, "sentence_idx": 799, "label": "unlabeled"}, {"type": "text", "expr": "Parseval networks: Improving robustness to adversarial examples", "word_idx": 53621, "sentence_idx": 800, "label": "unlabeled"}, {"type": "text", "expr": "In  International Conference on Machine Learning , pp", "word_idx": 53684, "sentence_idx": 801, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0854\u2013863, 2017", "word_idx": 53737, "sentence_idx": 802, "label": "unlabeled"}, {"type": "text", "expr": "International Conference on Machine Learning", "word_idx": 53751, "sentence_idx": 803, "label": "unlabeled"}, {"type": "text", "expr": "Czarnecki et\u00a0al", "word_idx": 53795, "sentence_idx": 804, "label": "unlabeled"}, {"type": "text", "expr": "(2017)Czarnecki, Osindero, Jaderberg, \u015awirszcz,\nand Pascanu \nCzarnecki, Wojciech\u00a0Marian, Osindero, Simon, Jaderberg, Max, \u015awirszcz,\nGrzegorz, and Pascanu, Razvan", "word_idx": 53810, "sentence_idx": 805, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Sobolev training for neural networks", "word_idx": 53971, "sentence_idx": 806, "label": "unlabeled"}, {"type": "text", "expr": "Czarnecki et\u00a0al", "word_idx": 54010, "sentence_idx": 807, "label": "unlabeled"}, {"type": "text", "expr": "(2017)Czarnecki, Osindero, Jaderberg, \u015awirszcz,\nand Pascanu", "word_idx": 54025, "sentence_idx": 808, "label": "unlabeled"}, {"type": "text", "expr": "Czarnecki, Wojciech\u00a0Marian, Osindero, Simon, Jaderberg, Max, \u015awirszcz,\nGrzegorz, and Pascanu, Razvan", "word_idx": 54084, "sentence_idx": 809, "label": "unlabeled"}, {"type": "text", "expr": "Sobolev training for neural networks", "word_idx": 54184, "sentence_idx": 810, "label": "unlabeled"}, {"type": "text", "expr": "NIPS , 2017", "word_idx": 54220, "sentence_idx": 811, "label": "unlabeled"}, {"type": "text", "expr": "Drucker & Le\u00a0Cun(1992)Drucker and Le\u00a0Cun \nDrucker, Harris and Le\u00a0Cun, Yann", "word_idx": 54231, "sentence_idx": 812, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Improving generalization performance using double backpropagation", "word_idx": 54305, "sentence_idx": 813, "label": "unlabeled"}, {"type": "text", "expr": "Drucker & Le\u00a0Cun(1992)Drucker and Le\u00a0Cun", "word_idx": 54373, "sentence_idx": 814, "label": "unlabeled"}, {"type": "text", "expr": "Drucker, Harris and Le\u00a0Cun, Yann", "word_idx": 54413, "sentence_idx": 815, "label": "unlabeled"}, {"type": "text", "expr": "Improving generalization performance using double backpropagation", "word_idx": 54445, "sentence_idx": 816, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Transactions on Neural Networks , 1992", "word_idx": 54510, "sentence_idx": 817, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Transactions on Neural Networks", "word_idx": 54553, "sentence_idx": 818, "label": "unlabeled"}, {"type": "text", "expr": "Furlanello et\u00a0al", "word_idx": 54589, "sentence_idx": 819, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Furlanello, Zhao, Saxe, Itti, and\nTjan \nFurlanello, Tommaso, Zhao, Jiaping, Saxe, Andrew\u00a0M, Itti, Laurent, and Tjan,\nBosco\u00a0S", "word_idx": 54605, "sentence_idx": 820, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Active long term memory networks", "word_idx": 54735, "sentence_idx": 821, "label": "unlabeled"}, {"type": "text", "expr": "Furlanello et\u00a0al", "word_idx": 54770, "sentence_idx": 822, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Furlanello, Zhao, Saxe, Itti, and\nTjan", "word_idx": 54786, "sentence_idx": 823, "label": "unlabeled"}, {"type": "text", "expr": "Furlanello, Tommaso, Zhao, Jiaping, Saxe, Andrew\u00a0M, Itti, Laurent, and Tjan,\nBosco\u00a0S", "word_idx": 54830, "sentence_idx": 824, "label": "unlabeled"}, {"type": "text", "expr": "Active long term memory networks", "word_idx": 54914, "sentence_idx": 825, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1606", "word_idx": 54946, "sentence_idx": 826, "label": "unlabeled"}, {"type": "text", "expr": "02355 , 2016", "word_idx": 54971, "sentence_idx": 827, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1606", "word_idx": 54983, "sentence_idx": 828, "label": "unlabeled"}, {"type": "text", "expr": "02355", "word_idx": 55008, "sentence_idx": 829, "label": "unlabeled"}, {"type": "text", "expr": "He et\u00a0al", "word_idx": 55013, "sentence_idx": 830, "label": "unlabeled"}, {"type": "text", "expr": "(2016)He, Zhang, Ren, and Sun \nHe, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian", "word_idx": 55021, "sentence_idx": 831, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Deep residual learning for image recognition", "word_idx": 55109, "sentence_idx": 832, "label": "unlabeled"}, {"type": "text", "expr": "He et\u00a0al", "word_idx": 55156, "sentence_idx": 833, "label": "unlabeled"}, {"type": "text", "expr": "(2016)He, Zhang, Ren, and Sun", "word_idx": 55164, "sentence_idx": 834, "label": "unlabeled"}, {"type": "text", "expr": "He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian", "word_idx": 55193, "sentence_idx": 835, "label": "unlabeled"}, {"type": "text", "expr": "Deep residual learning for image recognition", "word_idx": 55250, "sentence_idx": 836, "label": "unlabeled"}, {"type": "text", "expr": "In  Proceedings of the IEEE conference on computer vision and\npattern recognition , pp", "word_idx": 55294, "sentence_idx": 837, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0770\u2013778, 2016", "word_idx": 55380, "sentence_idx": 838, "label": "unlabeled"}, {"type": "text", "expr": "Proceedings of the IEEE conference on computer vision and\npattern recognition", "word_idx": 55394, "sentence_idx": 839, "label": "unlabeled"}, {"type": "text", "expr": "Hinton et\u00a0al", "word_idx": 55471, "sentence_idx": 840, "label": "unlabeled"}, {"type": "text", "expr": "(2015)Hinton, Vinyals, and Dean \nHinton, Geoffrey, Vinyals, Oriol, and Dean, Jeff", "word_idx": 55483, "sentence_idx": 841, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Distilling the knowledge in a neural network", "word_idx": 55564, "sentence_idx": 842, "label": "unlabeled"}, {"type": "text", "expr": "Hinton et\u00a0al", "word_idx": 55611, "sentence_idx": 843, "label": "unlabeled"}, {"type": "text", "expr": "(2015)Hinton, Vinyals, and Dean", "word_idx": 55623, "sentence_idx": 844, "label": "unlabeled"}, {"type": "text", "expr": "Hinton, Geoffrey, Vinyals, Oriol, and Dean, Jeff", "word_idx": 55654, "sentence_idx": 845, "label": "unlabeled"}, {"type": "text", "expr": "Distilling the knowledge in a neural network", "word_idx": 55702, "sentence_idx": 846, "label": "unlabeled"}, {"type": "text", "expr": "NIPS Deep Learning Workshop , 2015", "word_idx": 55746, "sentence_idx": 847, "label": "unlabeled"}, {"type": "text", "expr": "NIPS Deep Learning Workshop", "word_idx": 55780, "sentence_idx": 848, "label": "unlabeled"}, {"type": "text", "expr": "Jung et\u00a0al", "word_idx": 55807, "sentence_idx": 849, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Jung, Ju, Jung, and Kim \nJung, Heechul, Ju, Jeongwoo, Jung, Minju, and Kim, Junmo", "word_idx": 55817, "sentence_idx": 850, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Less-forgetting learning in deep neural networks", "word_idx": 55904, "sentence_idx": 851, "label": "unlabeled"}, {"type": "text", "expr": "Jung et\u00a0al", "word_idx": 55955, "sentence_idx": 852, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Jung, Ju, Jung, and Kim", "word_idx": 55965, "sentence_idx": 853, "label": "unlabeled"}, {"type": "text", "expr": "Jung, Heechul, Ju, Jeongwoo, Jung, Minju, and Kim, Junmo", "word_idx": 55994, "sentence_idx": 854, "label": "unlabeled"}, {"type": "text", "expr": "Less-forgetting learning in deep neural networks", "word_idx": 56050, "sentence_idx": 855, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1607", "word_idx": 56098, "sentence_idx": 856, "label": "unlabeled"}, {"type": "text", "expr": "00122 , 2016", "word_idx": 56123, "sentence_idx": 857, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1607", "word_idx": 56135, "sentence_idx": 858, "label": "unlabeled"}, {"type": "text", "expr": "00122", "word_idx": 56160, "sentence_idx": 859, "label": "unlabeled"}, {"type": "text", "expr": "Krizhevsky & Hinton(2009)Krizhevsky and\nHinton \nKrizhevsky, Alex and Hinton, Geoffrey", "word_idx": 56165, "sentence_idx": 860, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Learning multiple layers of features from tiny images", "word_idx": 56250, "sentence_idx": 861, "label": "unlabeled"}, {"type": "text", "expr": "\n\n 2009", "word_idx": 56306, "sentence_idx": 862, "label": "unlabeled"}, {"type": "text", "expr": "Krizhevsky & Hinton(2009)Krizhevsky and\nHinton", "word_idx": 56313, "sentence_idx": 863, "label": "unlabeled"}, {"type": "text", "expr": "Krizhevsky, Alex and Hinton, Geoffrey", "word_idx": 56359, "sentence_idx": 864, "label": "unlabeled"}, {"type": "text", "expr": "Learning multiple layers of features from tiny images", "word_idx": 56396, "sentence_idx": 865, "label": "unlabeled"}, {"type": "text", "expr": "Li & Hoiem(2016)Li and Hoiem \nLi, Zhizhong and Hoiem, Derek", "word_idx": 56449, "sentence_idx": 866, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Learning without forgetting", "word_idx": 56508, "sentence_idx": 867, "label": "unlabeled"}, {"type": "text", "expr": "Li & Hoiem(2016)Li and Hoiem", "word_idx": 56538, "sentence_idx": 868, "label": "unlabeled"}, {"type": "text", "expr": "Li, Zhizhong and Hoiem, Derek", "word_idx": 56566, "sentence_idx": 869, "label": "unlabeled"}, {"type": "text", "expr": "Learning without forgetting", "word_idx": 56595, "sentence_idx": 870, "label": "unlabeled"}, {"type": "text", "expr": "In  European Conference on Computer Vision , pp", "word_idx": 56622, "sentence_idx": 871, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0614\u2013629", "word_idx": 56669, "sentence_idx": 872, "label": "unlabeled"}, {"type": "text", "expr": "\nSpringer, 2016", "word_idx": 56677, "sentence_idx": 873, "label": "unlabeled"}, {"type": "text", "expr": "European Conference on Computer Vision", "word_idx": 56692, "sentence_idx": 874, "label": "unlabeled"}, {"type": "text", "expr": "Pan & Yang(2010)Pan and Yang \nPan, Sinno\u00a0Jialin and Yang, Qiang", "word_idx": 56730, "sentence_idx": 875, "label": "unlabeled"}, {"type": "text", "expr": "\n\n A survey on transfer learning", "word_idx": 56793, "sentence_idx": 876, "label": "unlabeled"}, {"type": "text", "expr": "Pan & Yang(2010)Pan and Yang", "word_idx": 56825, "sentence_idx": 877, "label": "unlabeled"}, {"type": "text", "expr": "Pan, Sinno\u00a0Jialin and Yang, Qiang", "word_idx": 56853, "sentence_idx": 878, "label": "unlabeled"}, {"type": "text", "expr": "A survey on transfer learning", "word_idx": 56886, "sentence_idx": 879, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Transactions on knowledge and data engineering ,\n22(10):1345\u20131359, 2010", "word_idx": 56915, "sentence_idx": 880, "label": "unlabeled"}, {"type": "text", "expr": "IEEE Transactions on knowledge and data engineering", "word_idx": 56991, "sentence_idx": 881, "label": "unlabeled"}, {"type": "text", "expr": "Quattoni & Torralba(2009)Quattoni and\nTorralba \nQuattoni, A and Torralba, A", "word_idx": 57042, "sentence_idx": 882, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Recognizing indoor scenes", "word_idx": 57117, "sentence_idx": 883, "label": "unlabeled"}, {"type": "text", "expr": "Quattoni & Torralba(2009)Quattoni and\nTorralba", "word_idx": 57145, "sentence_idx": 884, "label": "unlabeled"}, {"type": "text", "expr": "Quattoni, A and Torralba, A", "word_idx": 57191, "sentence_idx": 885, "label": "unlabeled"}, {"type": "text", "expr": "Recognizing indoor scenes", "word_idx": 57218, "sentence_idx": 886, "label": "unlabeled"}, {"type": "text", "expr": "In  2009 IEEE Conference on Computer Vision and Pattern\nRecognition , 2009", "word_idx": 57243, "sentence_idx": 887, "label": "unlabeled"}, {"type": "text", "expr": "2009 IEEE Conference on Computer Vision and Pattern\nRecognition", "word_idx": 57317, "sentence_idx": 888, "label": "unlabeled"}, {"type": "text", "expr": "Romero et\u00a0al", "word_idx": 57380, "sentence_idx": 889, "label": "unlabeled"}, {"type": "text", "expr": "(2014)Romero, Ballas, Kahou, Chassang, Gatta, and\nBengio \nRomero, Adriana, Ballas, Nicolas, Kahou, Samira\u00a0Ebrahimi, Chassang, Antoine,\nGatta, Carlo, and Bengio, Yoshua", "word_idx": 57392, "sentence_idx": 890, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Fitnets: Hints for thin deep nets", "word_idx": 57559, "sentence_idx": 891, "label": "unlabeled"}, {"type": "text", "expr": "Romero et\u00a0al", "word_idx": 57595, "sentence_idx": 892, "label": "unlabeled"}, {"type": "text", "expr": "(2014)Romero, Ballas, Kahou, Chassang, Gatta, and\nBengio", "word_idx": 57607, "sentence_idx": 893, "label": "unlabeled"}, {"type": "text", "expr": "Romero, Adriana, Ballas, Nicolas, Kahou, Samira\u00a0Ebrahimi, Chassang, Antoine,\nGatta, Carlo, and Bengio, Yoshua", "word_idx": 57663, "sentence_idx": 894, "label": "unlabeled"}, {"type": "text", "expr": "Fitnets: Hints for thin deep nets", "word_idx": 57772, "sentence_idx": 895, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1412", "word_idx": 57805, "sentence_idx": 896, "label": "unlabeled"}, {"type": "text", "expr": "6550 , 2014", "word_idx": 57830, "sentence_idx": 897, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1412", "word_idx": 57841, "sentence_idx": 898, "label": "unlabeled"}, {"type": "text", "expr": "Russakovsky et\u00a0al", "word_idx": 57866, "sentence_idx": 899, "label": "unlabeled"}, {"type": "text", "expr": "(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,\nHuang, Karpathy, Khosla, Bernstein, et\u00a0al", "word_idx": 57883, "sentence_idx": 900, "label": "unlabeled"}, {"type": "text", "expr": " \nRussakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan, Satheesh, Sanjeev, Ma,\nSean, Huang, Zhiheng, Karpathy, Andrej, Khosla, Aditya, Bernstein, Michael,\net\u00a0al", "word_idx": 57975, "sentence_idx": 901, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Imagenet large scale visual recognition challenge", "word_idx": 58138, "sentence_idx": 902, "label": "unlabeled"}, {"type": "text", "expr": "Russakovsky et\u00a0al", "word_idx": 58190, "sentence_idx": 903, "label": "unlabeled"}, {"type": "text", "expr": "(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,\nHuang, Karpathy, Khosla, Bernstein, et\u00a0al", "word_idx": 58207, "sentence_idx": 904, "label": "unlabeled"}, {"type": "text", "expr": "Russakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan, Satheesh, Sanjeev, Ma,\nSean, Huang, Zhiheng, Karpathy, Andrej, Khosla, Aditya, Bernstein, Michael,\net\u00a0al", "word_idx": 58299, "sentence_idx": 905, "label": "unlabeled"}, {"type": "text", "expr": "Imagenet large scale visual recognition challenge", "word_idx": 58460, "sentence_idx": 906, "label": "unlabeled"}, {"type": "text", "expr": "International Journal of Computer Vision , 115(3):211\u2013252, 2015", "word_idx": 58509, "sentence_idx": 907, "label": "unlabeled"}, {"type": "text", "expr": "International Journal of Computer Vision", "word_idx": 58572, "sentence_idx": 908, "label": "unlabeled"}, {"type": "text", "expr": "Sau & Balasubramanian(2016)Sau and Balasubramanian \nSau, Bharat\u00a0Bhusan and Balasubramanian, Vineeth\u00a0N", "word_idx": 58612, "sentence_idx": 909, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Deep model compression: Distilling knowledge from noisy teachers", "word_idx": 58713, "sentence_idx": 910, "label": "unlabeled"}, {"type": "text", "expr": "Sau & Balasubramanian(2016)Sau and Balasubramanian", "word_idx": 58780, "sentence_idx": 911, "label": "unlabeled"}, {"type": "text", "expr": "Sau, Bharat\u00a0Bhusan and Balasubramanian, Vineeth\u00a0N", "word_idx": 58830, "sentence_idx": 912, "label": "unlabeled"}, {"type": "text", "expr": "Deep model compression: Distilling knowledge from noisy teachers", "word_idx": 58879, "sentence_idx": 913, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1610", "word_idx": 58943, "sentence_idx": 914, "label": "unlabeled"}, {"type": "text", "expr": "09650 , 2016", "word_idx": 58968, "sentence_idx": 915, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1610", "word_idx": 58980, "sentence_idx": 916, "label": "unlabeled"}, {"type": "text", "expr": "09650", "word_idx": 59005, "sentence_idx": 917, "label": "unlabeled"}, {"type": "text", "expr": "Simonyan & Zisserman(2014)Simonyan and Zisserman \nSimonyan, Karen and Zisserman, Andrew", "word_idx": 59010, "sentence_idx": 918, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Very deep convolutional networks for large-scale image recognition", "word_idx": 59097, "sentence_idx": 919, "label": "unlabeled"}, {"type": "text", "expr": "Simonyan & Zisserman(2014)Simonyan and Zisserman", "word_idx": 59166, "sentence_idx": 920, "label": "unlabeled"}, {"type": "text", "expr": "Simonyan, Karen and Zisserman, Andrew", "word_idx": 59214, "sentence_idx": 921, "label": "unlabeled"}, {"type": "text", "expr": "Very deep convolutional networks for large-scale image recognition", "word_idx": 59251, "sentence_idx": 922, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1409", "word_idx": 59317, "sentence_idx": 923, "label": "unlabeled"}, {"type": "text", "expr": "1556 , 2014", "word_idx": 59342, "sentence_idx": 924, "label": "unlabeled"}, {"type": "text", "expr": "arXiv preprint arXiv:1409", "word_idx": 59353, "sentence_idx": 925, "label": "unlabeled"}, {"type": "text", "expr": "Srivastava et\u00a0al", "word_idx": 59378, "sentence_idx": 926, "label": "unlabeled"}, {"type": "text", "expr": "(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and\nSalakhutdinov \nSrivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex, Sutskever, Ilya, and\nSalakhutdinov, Ruslan", "word_idx": 59394, "sentence_idx": 927, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Dropout: A simple way to prevent neural networks from overfitting", "word_idx": 59560, "sentence_idx": 928, "label": "unlabeled"}, {"type": "text", "expr": "Srivastava et\u00a0al", "word_idx": 59628, "sentence_idx": 929, "label": "unlabeled"}, {"type": "text", "expr": "(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and\nSalakhutdinov", "word_idx": 59644, "sentence_idx": 930, "label": "unlabeled"}, {"type": "text", "expr": "Srivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex, Sutskever, Ilya, and\nSalakhutdinov, Ruslan", "word_idx": 59710, "sentence_idx": 931, "label": "unlabeled"}, {"type": "text", "expr": "Dropout: A simple way to prevent neural networks from overfitting", "word_idx": 59808, "sentence_idx": 932, "label": "unlabeled"}, {"type": "text", "expr": "The Journal of Machine Learning Research , 15(1):1929\u20131958, 2014", "word_idx": 59873, "sentence_idx": 933, "label": "unlabeled"}, {"type": "text", "expr": "The Journal of Machine Learning Research", "word_idx": 59937, "sentence_idx": 934, "label": "unlabeled"}, {"type": "text", "expr": "Wang et\u00a0al", "word_idx": 59977, "sentence_idx": 935, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Wang, Mohamed, Caruana, Bilmes, Plilipose,\nRichardson, Geras, Urban, and Aslan \nWang, Shengjie, Mohamed, Abdel-rahman, Caruana, Rich, Bilmes, Jeff, Plilipose,\nMatthai, Richardson, Matthew, Geras, Krzysztof, Urban, Gregor, and Aslan,\nOzlem", "word_idx": 59987, "sentence_idx": 936, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Analysis of deep neural networks with extended data jacobian matrix", "word_idx": 60231, "sentence_idx": 937, "label": "unlabeled"}, {"type": "text", "expr": "Wang et\u00a0al", "word_idx": 60301, "sentence_idx": 938, "label": "unlabeled"}, {"type": "text", "expr": "(2016)Wang, Mohamed, Caruana, Bilmes, Plilipose,\nRichardson, Geras, Urban, and Aslan", "word_idx": 60311, "sentence_idx": 939, "label": "unlabeled"}, {"type": "text", "expr": "Wang, Shengjie, Mohamed, Abdel-rahman, Caruana, Rich, Bilmes, Jeff, Plilipose,\nMatthai, Richardson, Matthew, Geras, Krzysztof, Urban, Gregor, and Aslan,\nOzlem", "word_idx": 60395, "sentence_idx": 940, "label": "unlabeled"}, {"type": "text", "expr": "Analysis of deep neural networks with extended data jacobian matrix", "word_idx": 60553, "sentence_idx": 941, "label": "unlabeled"}, {"type": "text", "expr": "In  International Conference on Machine Learning , pp", "word_idx": 60620, "sentence_idx": 942, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0718\u2013726, 2016", "word_idx": 60673, "sentence_idx": 943, "label": "unlabeled"}, {"type": "text", "expr": "International Conference on Machine Learning", "word_idx": 60687, "sentence_idx": 944, "label": "unlabeled"}, {"type": "text", "expr": "Xu & Mannor(2012)Xu and Mannor \nXu, Huan and Mannor, Shie", "word_idx": 60731, "sentence_idx": 945, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Robustness and generalization", "word_idx": 60788, "sentence_idx": 946, "label": "unlabeled"}, {"type": "text", "expr": "Xu & Mannor(2012)Xu and Mannor", "word_idx": 60820, "sentence_idx": 947, "label": "unlabeled"}, {"type": "text", "expr": "Xu, Huan and Mannor, Shie", "word_idx": 60850, "sentence_idx": 948, "label": "unlabeled"}, {"type": "text", "expr": "Robustness and generalization", "word_idx": 60875, "sentence_idx": 949, "label": "unlabeled"}, {"type": "text", "expr": "Machine learning , 86(3):391\u2013423, 2012", "word_idx": 60904, "sentence_idx": 950, "label": "unlabeled"}, {"type": "text", "expr": "Machine learning", "word_idx": 60942, "sentence_idx": 951, "label": "unlabeled"}, {"type": "text", "expr": "Yosinski et\u00a0al", "word_idx": 60958, "sentence_idx": 952, "label": "unlabeled"}, {"type": "text", "expr": "(2014)Yosinski, Clune, Bengio, and\nLipson \nYosinski, Jason, Clune, Jeff, Bengio, Yoshua, and Lipson, Hod", "word_idx": 60972, "sentence_idx": 953, "label": "unlabeled"}, {"type": "text", "expr": "\n\n How transferable are features in deep neural networks?", "word_idx": 61076, "sentence_idx": 954, "label": "unlabeled"}, {"type": "text", "expr": "Yosinski et\u00a0al", "word_idx": 61133, "sentence_idx": 955, "label": "unlabeled"}, {"type": "text", "expr": "(2014)Yosinski, Clune, Bengio, and\nLipson", "word_idx": 61147, "sentence_idx": 956, "label": "unlabeled"}, {"type": "text", "expr": "Yosinski, Jason, Clune, Jeff, Bengio, Yoshua, and Lipson, Hod", "word_idx": 61188, "sentence_idx": 957, "label": "unlabeled"}, {"type": "text", "expr": "How transferable are features in deep neural networks?", "word_idx": 61249, "sentence_idx": 958, "label": "unlabeled"}, {"type": "text", "expr": "In  Advances in neural information processing systems , pp", "word_idx": 61303, "sentence_idx": 959, "label": "unlabeled"}, {"type": "text", "expr": "\u00a03320\u20133328, 2014", "word_idx": 61361, "sentence_idx": 960, "label": "unlabeled"}, {"type": "text", "expr": "Advances in neural information processing systems", "word_idx": 61377, "sentence_idx": 961, "label": "unlabeled"}, {"type": "text", "expr": "Zagoruyko & Komodakis(2017)Zagoruyko and\nKomodakis \nZagoruyko, Sergey and Komodakis, Nikos", "word_idx": 61426, "sentence_idx": 962, "label": "unlabeled"}, {"type": "text", "expr": "\n\n Paying more attention to attention: Improving the performance of\nconvolutional neural networks via attention transfer", "word_idx": 61516, "sentence_idx": 963, "label": "unlabeled"}, {"type": "text", "expr": "Zagoruyko & Komodakis(2017)Zagoruyko and\nKomodakis", "word_idx": 61636, "sentence_idx": 964, "label": "unlabeled"}, {"type": "text", "expr": "Zagoruyko, Sergey and Komodakis, Nikos", "word_idx": 61686, "sentence_idx": 965, "label": "unlabeled"}, {"type": "text", "expr": "Paying more attention to attention: Improving the performance of\nconvolutional neural networks via attention transfer", "word_idx": 61724, "sentence_idx": 966, "label": "unlabeled"}, {"type": "text", "expr": "ICLR , 2017", "word_idx": 61841, "sentence_idx": 967, "label": "unlabeled"}, {"type": "text", "expr": "Supplementary Material", "word_idx": 61852, "sentence_idx": 968, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0A  Proof for Proposition 1", "word_idx": 61874, "sentence_idx": 969, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0A", "word_idx": 61909, "sentence_idx": 970, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 1 ", "word_idx": 61919, "sentence_idx": 971, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 1", "word_idx": 61933, "sentence_idx": 972, "label": "unlabeled"}, {"type": "text", "expr": "Consider the squared error cost function for matching soft targets of two neural networks with  $k$ -length targets ( $\\in\\mathbb{R}^{k}$ ), given by  $\\ell(\\mathcal{T}(\\mathbf{x}),\\mathcal{S}(\\mathbf{x}))=\\sum_{i=1}^{k}(\\mathcal{%\nT}^{i}(\\mathbf{x})-\\mathcal{S}^{i}(\\mathbf{x}))^{2}$ , where  $\\mathbf{x}\\in\\mathbb{R}^{D}$  is an input data point", "word_idx": 61946, "sentence_idx": 973, "label": "unlabeled"}, {"type": "text", "expr": " Let  $\\bm{\\xi}~{}(\\in\\mathbb{R}^{D})=\\sigma~{}\\mathbf{z}$  be a scaled version of a unit normal random variable  $\\mathbf{z}~{}\\in\\mathbb{R}^{D}$  with scaling factor  $\\sigma\\in\\mathbb{R}$ ", "word_idx": 62293, "sentence_idx": 974, "label": "unlabeled"}, {"type": "text", "expr": " Then the following is locally true", "word_idx": 62484, "sentence_idx": 975, "label": "unlabeled"}, {"type": "math", "expr": "$$\\in\\mathbb{R}^{k}$$", "word_idx": 62519, "sentence_idx": 976, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ell(\\mathcal{T}(\\mathbf{x}),\\mathcal{S}(\\mathbf{x}))=\\sum_{i=1}^{k}(\\mathcal{%\nT}^{i}(\\mathbf{x})-\\mathcal{S}^{i}(\\mathbf{x}))^{2}$$", "word_idx": 62536, "sentence_idx": 977, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}\\in\\mathbb{R}^{D}$$", "word_idx": 62667, "sentence_idx": 978, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{\\xi}~{}(\\in\\mathbb{R}^{D})=\\sigma~{}\\mathbf{z}$$", "word_idx": 62694, "sentence_idx": 979, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{z}~{}\\in\\mathbb{R}^{D}$$", "word_idx": 62744, "sentence_idx": 980, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma\\in\\mathbb{R}$$", "word_idx": 62774, "sentence_idx": 981, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\mathbb{E}_{\\bm{\\xi}}\\left[\\sum_{i=1}^{k}\\left(\\mathcal{T}^{i}(%\n\\mathbf{x}+\\bm{\\xi})-\\mathcal{S}^{i}(\\mathbf{x}+\\bm{\\xi})\\right)^{2}\\right]$", "word_idx": 62793, "sentence_idx": 982, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\mathbb{E}_{\\bm{\\xi}}\\left[\\sum_{i=1}^{k}\\left(\\mathcal{T}^{i}(%\n\\mathbf{x}+\\bm{\\xi})-\\mathcal{S}^{i}(\\mathbf{x}+\\bm{\\xi})\\right)^{2}\\right]$$", "word_idx": 62948, "sentence_idx": 983, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=\\sum_{i=1}^{k}\\left(\\mathcal{T}^{i}(\\mathbf{x})-\\mathcal{S}^{i}(%\n\\mathbf{x})\\right)^{2}+\\sigma^{2}\\sum_{i=1}^{k}\\|\\nabla_{x}\\mathcal{T}^{i}(%\n\\mathbf{x})-\\nabla_{x}\\mathcal{S}^{i}(\\mathbf{x})\\|_{2}^{2}$", "word_idx": 63101, "sentence_idx": 984, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=\\sum_{i=1}^{k}\\left(\\mathcal{T}^{i}(\\mathbf{x})-\\mathcal{S}^{i}(%\n\\mathbf{x})\\right)^{2}+\\sigma^{2}\\sum_{i=1}^{k}\\|\\nabla_{x}\\mathcal{T}^{i}(%\n\\mathbf{x})-\\nabla_{x}\\mathcal{S}^{i}(\\mathbf{x})\\|_{2}^{2}$$", "word_idx": 63319, "sentence_idx": 985, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle+\\mathcal{O}(\\sigma^{4})$", "word_idx": 63535, "sentence_idx": 986, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle+\\mathcal{O}(\\sigma^{4})$$", "word_idx": 63574, "sentence_idx": 987, "label": "unlabeled"}, {"type": "text", "expr": "Proof", "word_idx": 63611, "sentence_idx": 988, "label": "unlabeled"}, {"type": "text", "expr": "Proof", "word_idx": 63616, "sentence_idx": 989, "label": "unlabeled"}, {"type": "text", "expr": "There exists  $\\sigma$  and  $\\bm{\\xi}$  small enough that first-order Taylor series expansion holds true", "word_idx": 63621, "sentence_idx": 990, "label": "unlabeled"}, {"type": "math", "expr": "$$\\sigma$$", "word_idx": 63726, "sentence_idx": 991, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{\\xi}$$", "word_idx": 63732, "sentence_idx": 992, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\mathbb{E}_{\\bm{\\xi}}\\left[\\sum_{i=1}^{k}\\left(\\mathcal{T}^{i}(%\n\\mathbf{x}+\\bm{\\xi})-\\mathcal{S}^{i}(\\mathbf{x}+\\bm{\\xi})\\right)^{2}\\right]$", "word_idx": 63740, "sentence_idx": 993, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\mathbb{E}_{\\bm{\\xi}}\\left[\\sum_{i=1}^{k}\\left(\\mathcal{T}^{i}(%\n\\mathbf{x}+\\bm{\\xi})-\\mathcal{S}^{i}(\\mathbf{x}+\\bm{\\xi})\\right)^{2}\\right]$$", "word_idx": 63895, "sentence_idx": 994, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=$", "word_idx": 64048, "sentence_idx": 995, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=$$", "word_idx": 64064, "sentence_idx": 996, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle~{}\\mathbb{E}_{\\bm{\\xi}}\\left[\\sum_{i=1}^{k}\\left(\\mathcal{T}^{i}%\n(\\mathbf{x})+\\bm{\\xi}^{T}\\nabla_{x}\\mathcal{T}^{i}(\\mathbf{x})-\\mathcal{S}^{i}%\n(\\mathbf{x})-\\bm{\\xi}^{T}\\nabla_{x}\\mathcal{S}^{i}(\\mathbf{x})\\right)^{2}\\right]$", "word_idx": 64078, "sentence_idx": 997, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle~{}\\mathbb{E}_{\\bm{\\xi}}\\left[\\sum_{i=1}^{k}\\left(\\mathcal{T}^{i}%\n(\\mathbf{x})+\\bm{\\xi}^{T}\\nabla_{x}\\mathcal{T}^{i}(\\mathbf{x})-\\mathcal{S}^{i}%\n(\\mathbf{x})-\\bm{\\xi}^{T}\\nabla_{x}\\mathcal{S}^{i}(\\mathbf{x})\\right)^{2}\\right]$$", "word_idx": 64320, "sentence_idx": 998, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle+$", "word_idx": 64560, "sentence_idx": 999, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle+$$", "word_idx": 64576, "sentence_idx": 1000, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\mathcal{O}(\\sigma^{4})$", "word_idx": 64590, "sentence_idx": 1001, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\mathcal{O}(\\sigma^{4})$$", "word_idx": 64628, "sentence_idx": 1002, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle=$", "word_idx": 64664, "sentence_idx": 1003, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle=$$", "word_idx": 64680, "sentence_idx": 1004, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\sum_{i=1}^{k}\\left(\\mathcal{T}^{i}(\\mathbf{x})-\\mathcal{S}^{i}(%\n\\mathbf{x})\\right)^{2}$", "word_idx": 64694, "sentence_idx": 1005, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\sum_{i=1}^{k}\\left(\\mathcal{T}^{i}(\\mathbf{x})-\\mathcal{S}^{i}(%\n\\mathbf{x})\\right)^{2}$$", "word_idx": 64797, "sentence_idx": 1006, "label": "unlabeled"}, {"type": "text", "expr": "+ (11)", "word_idx": 64898, "sentence_idx": 1007, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle+$", "word_idx": 64904, "sentence_idx": 1008, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle+$$", "word_idx": 64920, "sentence_idx": 1009, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\mathbb{E}_{\\bm{\\xi}}\\left[\\sum_{i=1}^{k}\\left[\\bm{\\xi}^{T}\\left(%\n\\nabla_{x}\\mathcal{T}^{i}(\\mathbf{x})-\\nabla_{x}\\mathcal{S}^{i}(\\mathbf{x})%\n\\right)\\right]^{2}\\right]+\\mathcal{O}(\\sigma^{4})$", "word_idx": 64934, "sentence_idx": 1010, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\mathbb{E}_{\\bm{\\xi}}\\left[\\sum_{i=1}^{k}\\left[\\bm{\\xi}^{T}\\left(%\n\\nabla_{x}\\mathcal{T}^{i}(\\mathbf{x})-\\nabla_{x}\\mathcal{S}^{i}(\\mathbf{x})%\n\\right)\\right]^{2}\\right]+\\mathcal{O}(\\sigma^{4})$$", "word_idx": 65142, "sentence_idx": 1011, "label": "unlabeled"}, {"type": "text", "expr": "To get equation  11 , we use the fact that mean of  $\\bm{\\xi}$  is zero", "word_idx": 65348, "sentence_idx": 1012, "label": "unlabeled"}, {"type": "text", "expr": " To complete the proof, we use the diagonal assumption on the covariance matrix of  $\\bm{\\xi}$ ", "word_idx": 65419, "sentence_idx": 1013, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{\\xi}$$", "word_idx": 65514, "sentence_idx": 1014, "label": "unlabeled"}, {"type": "math", "expr": "$$\\bm{\\xi}$$", "word_idx": 65522, "sentence_idx": 1015, "label": "unlabeled"}, {"type": "text", "expr": "Proofs of other statements are similar", "word_idx": 65530, "sentence_idx": 1016, "label": "unlabeled"}, {"type": "text", "expr": " For proof for cross-entropy loss of Proposition 2, use a second order Taylor series expansion of  $\\log(\\cdot)$  in the first step", "word_idx": 65568, "sentence_idx": 1017, "label": "unlabeled"}, {"type": "math", "expr": "$$\\log(\\cdot)$$", "word_idx": 65699, "sentence_idx": 1018, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0B  Proof for Proposition 3", "word_idx": 65710, "sentence_idx": 1019, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0B", "word_idx": 65745, "sentence_idx": 1020, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 2 ", "word_idx": 65755, "sentence_idx": 1021, "label": "unlabeled"}, {"type": "text", "expr": "Proposition 2", "word_idx": 65769, "sentence_idx": 1022, "label": "unlabeled"}, {"type": "text", "expr": "From the notations in the main text, we have", "word_idx": 65782, "sentence_idx": 1023, "label": "unlabeled"}, {"type": "text", "expr": "From the notations in the main text, we have", "word_idx": 65826, "sentence_idx": 1024, "label": "unlabeled"}, {"type": "text", "expr": "From the notations in the main text, we have", "word_idx": 65870, "sentence_idx": 1025, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\frac{1}{|\\mathcal{D}_{l}|}\\sum_{\\mathbf{x}\\sim\\mathcal{D}_{l}}%\n\\ell(f(\\mathbf{x}),g(\\mathbf{x}))$", "word_idx": 65914, "sentence_idx": 1026, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\frac{1}{|\\mathcal{D}_{l}|}\\sum_{\\mathbf{x}\\sim\\mathcal{D}_{l}}%\n\\ell(f(\\mathbf{x}),g(\\mathbf{x}))$$", "word_idx": 66027, "sentence_idx": 1027, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\leq\\max_{\\mathbf{x}\\sim\\mathcal{D}_{s}}\\ell(f(\\mathbf{x}),g(%\n\\mathbf{x}))$", "word_idx": 66138, "sentence_idx": 1028, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\leq\\max_{\\mathbf{x}\\sim\\mathcal{D}_{s}}\\ell(f(\\mathbf{x}),g(%\n\\mathbf{x}))$$", "word_idx": 66228, "sentence_idx": 1029, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle+\\mathrm{K}\\mathcal{H}_{a}(\\mathcal{D}_{l},\\mathcal{D}_{s})$", "word_idx": 66316, "sentence_idx": 1030, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle+\\mathrm{K}\\mathcal{H}_{a}(\\mathcal{D}_{l},\\mathcal{D}_{s})$$", "word_idx": 66390, "sentence_idx": 1031, "label": "unlabeled"}, {"type": "text", "expr": "Proof", "word_idx": 66462, "sentence_idx": 1032, "label": "unlabeled"}, {"type": "text", "expr": "Proof", "word_idx": 66467, "sentence_idx": 1033, "label": "unlabeled"}, {"type": "text", "expr": "Let us denote  $\\rho(\\mathbf{x})=\\ell(f(\\mathbf{x}),g(\\mathbf{x}))$  for convenience", "word_idx": 66472, "sentence_idx": 1034, "label": "unlabeled"}, {"type": "text", "expr": " Assume Lipschitz continuity for  $\\rho(\\mathbf{x})$  with Lipschitz constant  $\\mathrm{K}$ , and distance metric  $\\psi_{\\mathbf{x}}(\\cdot,\\cdot)$  in the input space -", "word_idx": 66556, "sentence_idx": 1035, "label": "unlabeled"}, {"type": "math", "expr": "$$\\rho(\\mathbf{x})=\\ell(f(\\mathbf{x}),g(\\mathbf{x}))$$", "word_idx": 66725, "sentence_idx": 1036, "label": "unlabeled"}, {"type": "math", "expr": "$$\\rho(\\mathbf{x})$$", "word_idx": 66775, "sentence_idx": 1037, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{K}$$", "word_idx": 66791, "sentence_idx": 1038, "label": "unlabeled"}, {"type": "math", "expr": "$$\\psi_{\\mathbf{x}}(\\cdot,\\cdot)$$", "word_idx": 66801, "sentence_idx": 1039, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\|\\rho(\\mathbf{x}_{1})-\\rho(\\mathbf{x}_{2})\\|_{1}\\leq\\mathrm{K}%\n\\psi_{\\mathbf{x}}(\\mathbf{x}_{1},\\mathbf{x}_{2})$", "word_idx": 66831, "sentence_idx": 1040, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\|\\rho(\\mathbf{x}_{1})-\\rho(\\mathbf{x}_{2})\\|_{1}\\leq\\mathrm{K}%\n\\psi_{\\mathbf{x}}(\\mathbf{x}_{1},\\mathbf{x}_{2})$$", "word_idx": 66959, "sentence_idx": 1041, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\implies\\rho(\\mathbf{x}_{1})\\leq\\rho(\\mathbf{x}_{2})+\\mathrm{K}%\n\\psi_{\\mathbf{x}}(\\mathbf{x}_{1},\\mathbf{x}_{2})$", "word_idx": 67085, "sentence_idx": 1042, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\implies\\rho(\\mathbf{x}_{1})\\leq\\rho(\\mathbf{x}_{2})+\\mathrm{K}%\n\\psi_{\\mathbf{x}}(\\mathbf{x}_{1},\\mathbf{x}_{2})$$", "word_idx": 67213, "sentence_idx": 1043, "label": "unlabeled"}, {"type": "text", "expr": "Assuming that  $\\rho(\\mathbf{x}_{1})\\geq\\rho(\\mathbf{x}_{2})$ ", "word_idx": 67339, "sentence_idx": 1044, "label": "unlabeled"}, {"type": "text", "expr": " Note that it holds even otherwise, but is trivial", "word_idx": 67401, "sentence_idx": 1045, "label": "unlabeled"}, {"type": "math", "expr": "$$\\rho(\\mathbf{x}_{1})\\geq\\rho(\\mathbf{x}_{2})$$", "word_idx": 67451, "sentence_idx": 1046, "label": "unlabeled"}, {"type": "text", "expr": "Now, for every datapoint  $\\mathbf{x}_{l}\\in\\mathcal{D}_{l}$ , there exists a point  $\\mathbf{x}_{s}\\in\\mathcal{D}_{s}$  such that  $\\psi_{\\mathbf{x}}(\\mathbf{x}_{s},\\mathbf{x}_{l})$  is the smallest among all points in  $\\mathcal{D}_{s}$ ", "word_idx": 67495, "sentence_idx": 1047, "label": "unlabeled"}, {"type": "text", "expr": " In other words, we look at the point in  $\\mathcal{D}_{s}$  closest to each point  $\\mathbf{x}_{l}$ ", "word_idx": 67734, "sentence_idx": 1048, "label": "unlabeled"}, {"type": "text", "expr": " Note that in this process only a subset of points  $\\mathrm{d}_{s}\\subseteq\\mathcal{D}_{s}$  are chosen, and individual points can be chosen multiple times", "word_idx": 67835, "sentence_idx": 1049, "label": "unlabeled"}, {"type": "text", "expr": " For these points, we can write", "word_idx": 67991, "sentence_idx": 1050, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}_{l}\\in\\mathcal{D}_{l}$$", "word_idx": 68022, "sentence_idx": 1051, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}_{s}\\in\\mathcal{D}_{s}$$", "word_idx": 68054, "sentence_idx": 1052, "label": "unlabeled"}, {"type": "math", "expr": "$$\\psi_{\\mathbf{x}}(\\mathbf{x}_{s},\\mathbf{x}_{l})$$", "word_idx": 68086, "sentence_idx": 1053, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{D}_{s}$$", "word_idx": 68134, "sentence_idx": 1054, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{D}_{s}$$", "word_idx": 68149, "sentence_idx": 1055, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}_{l}$$", "word_idx": 68164, "sentence_idx": 1056, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{d}_{s}\\subseteq\\mathcal{D}_{s}$$", "word_idx": 68178, "sentence_idx": 1057, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\rho(\\mathbf{x}_{l})\\leq\\rho(\\mathbf{x}_{s})+\\mathrm{K}\\psi_{%\n\\mathbf{x}}(\\mathbf{x}_{s},\\mathbf{x}_{l})$", "word_idx": 68216, "sentence_idx": 1058, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\rho(\\mathbf{x}_{l})\\leq\\rho(\\mathbf{x}_{s})+\\mathrm{K}\\psi_{%\n\\mathbf{x}}(\\mathbf{x}_{s},\\mathbf{x}_{l})$$", "word_idx": 68336, "sentence_idx": 1059, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\implies\\frac{1}{|\\mathcal{D}_{l}|}\\sum_{\\mathbf{x}_{l}\\sim%\n\\mathcal{D}_{l}}\\rho(\\mathbf{x}_{l})\\leq\\frac{1}{|\\mathcal{D}_{l}|}\\sum_{%\n\\mathbf{x}_{s}\\mathrm{~{}closest~{}to~{}}\\mathbf{x}_{l}}$", "word_idx": 68454, "sentence_idx": 1060, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\implies\\frac{1}{|\\mathcal{D}_{l}|}\\sum_{\\mathbf{x}_{l}\\sim%\n\\mathcal{D}_{l}}\\rho(\\mathbf{x}_{l})\\leq\\frac{1}{|\\mathcal{D}_{l}|}\\sum_{%\n\\mathbf{x}_{s}\\mathrm{~{}closest~{}to~{}}\\mathbf{x}_{l}}$$", "word_idx": 68661, "sentence_idx": 1061, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\rho(\\mathbf{x}_{s})$", "word_idx": 68866, "sentence_idx": 1062, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\rho(\\mathbf{x}_{s})$$", "word_idx": 68901, "sentence_idx": 1063, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle+\\frac{1}{|\\mathcal{D}_{l}|}\\sum_{\\mathbf{x}_{s}\\mathrm{~{}%\nclosest~{}to~{}}\\mathbf{x}_{l}}$", "word_idx": 68934, "sentence_idx": 1064, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle+\\frac{1}{|\\mathcal{D}_{l}|}\\sum_{\\mathbf{x}_{s}\\mathrm{~{}%\nclosest~{}to~{}}\\mathbf{x}_{l}}$$", "word_idx": 69041, "sentence_idx": 1065, "label": "unlabeled"}, {"type": "text", "expr": "$\\displaystyle\\mathrm{K}\\psi_{\\mathbf{x}}(\\mathbf{x}_{s},\\mathbf{x}_{l})$", "word_idx": 69146, "sentence_idx": 1066, "label": "unlabeled"}, {"type": "math", "expr": "$$\\displaystyle\\mathrm{K}\\psi_{\\mathbf{x}}(\\mathbf{x}_{s},\\mathbf{x}_{l})$$", "word_idx": 69219, "sentence_idx": 1067, "label": "unlabeled"}, {"type": "text", "expr": "We see that  $\\frac{1}{|\\mathcal{D}_{l}|}\\sum_{\\mathbf{x}_{s}}\\rho(\\mathbf{x}_{s})\\leq\\max_{%\n\\mathbf{x}\\sim\\mathrm{d}_{s}}\\rho(\\mathbf{x})\\leq\\max_{\\mathbf{x}\\sim\\mathcal{%\nD}_{s}}\\rho(\\mathbf{x})$ , which is a consequence of the fact that the max is greater than any convex combination of elements", "word_idx": 69290, "sentence_idx": 1068, "label": "unlabeled"}, {"type": "math", "expr": "$$\\frac{1}{|\\mathcal{D}_{l}|}\\sum_{\\mathbf{x}_{s}}\\rho(\\mathbf{x}_{s})\\leq\\max_{%\n\\mathbf{x}\\sim\\mathrm{d}_{s}}\\rho(\\mathbf{x})\\leq\\max_{\\mathbf{x}\\sim\\mathcal{%\nD}_{s}}\\rho(\\mathbf{x})$$", "word_idx": 69589, "sentence_idx": 1069, "label": "unlabeled"}, {"type": "text", "expr": "Also, we have  $\\psi_{\\mathbf{x}}(\\mathbf{x}_{l},\\mathbf{x}_{s})\\leq\\mathcal{H}_{a}(\\mathcal{D%\n}_{l},\\mathcal{D}_{s})$ , which is the maximum distance between any two \u2018closest\u2019 points from  $\\mathcal{D}_{l}$  to  $\\mathcal{D}_{s}$  (by definition)", "word_idx": 69772, "sentence_idx": 1070, "label": "unlabeled"}, {"type": "math", "expr": "$$\\psi_{\\mathbf{x}}(\\mathbf{x}_{l},\\mathbf{x}_{s})\\leq\\mathcal{H}_{a}(\\mathcal{D%\n}_{l},\\mathcal{D}_{s})$$", "word_idx": 70020, "sentence_idx": 1071, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{D}_{l}$$", "word_idx": 70122, "sentence_idx": 1072, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{D}_{s}$$", "word_idx": 70137, "sentence_idx": 1073, "label": "unlabeled"}, {"type": "text", "expr": "Applying these bounds, we have the final result", "word_idx": 70152, "sentence_idx": 1074, "label": "unlabeled"}, {"type": "text", "expr": "Applying these bounds, we have the final result", "word_idx": 70199, "sentence_idx": 1075, "label": "unlabeled"}, {"type": "text", "expr": "1  Proof for Corollary", "word_idx": 70246, "sentence_idx": 1076, "label": "unlabeled"}, {"type": "text", "expr": "Corollary ", "word_idx": 70268, "sentence_idx": 1077, "label": "unlabeled"}, {"type": "text", "expr": "Corollary", "word_idx": 70278, "sentence_idx": 1078, "label": "unlabeled"}, {"type": "text", "expr": "For any superset  $\\mathrm{D^{\\prime}}_{s}\\supseteq\\mathcal{D}_{s}$  of the target dataset,  $\\mathcal{H}_{a}(\\mathcal{D}_{l},\\mathcal{D^{\\prime}}_{s})\\leq\\mathcal{H}_{a}(%\n\\mathcal{D}_{l},\\mathcal{D}_{s})$", "word_idx": 70287, "sentence_idx": 1079, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{D^{\\prime}}_{s}\\supseteq\\mathcal{D}_{s}$$", "word_idx": 70493, "sentence_idx": 1080, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{H}_{a}(\\mathcal{D}_{l},\\mathcal{D^{\\prime}}_{s})\\leq\\mathcal{H}_{a}(%\n\\mathcal{D}_{l},\\mathcal{D}_{s})$$", "word_idx": 70540, "sentence_idx": 1081, "label": "unlabeled"}, {"type": "text", "expr": "Proof", "word_idx": 70651, "sentence_idx": 1082, "label": "unlabeled"}, {"type": "text", "expr": "Proof", "word_idx": 70656, "sentence_idx": 1083, "label": "unlabeled"}, {"type": "text", "expr": "From the previous proof, we have  $\\rho(\\mathbf{x}_{l})\\leq\\rho(\\mathbf{x}_{s})+\\mathrm{K}\\psi_{\\mathbf{x}}(%\n\\mathbf{x}_{s},\\mathbf{x}_{l})$  for an individual point  $\\mathbf{x}_{l}$ ", "word_idx": 70661, "sentence_idx": 1084, "label": "unlabeled"}, {"type": "text", "expr": " Now if we have  $\\mathrm{D^{\\prime}}_{s}\\supseteq\\mathcal{D}_{s}$ , then we have  $\\rho(\\mathbf{x}_{l})\\leq\\rho(\\mathbf{x}^{\\prime}_{s})+\\mathrm{K}\\psi_{\\mathbf{%\nx}}(\\mathbf{x}^{\\prime}_{s},\\mathbf{x}_{l})$ , where  $\\mathbf{x}^{\\prime}_{s}$  is the new point closest to  $\\mathbf{x}_{l}$ ", "word_idx": 70846, "sentence_idx": 1085, "label": "unlabeled"}, {"type": "text", "expr": " It is clear that  $\\psi_{\\mathbf{x}}(\\mathbf{x}^{\\prime}_{s},\\mathbf{x}_{l})\\leq\\psi_{\\mathbf{x}}%\n(\\mathbf{x}_{s},\\mathbf{x}_{l})$  for all  $\\mathbf{x}_{l}$ ", "word_idx": 71137, "sentence_idx": 1086, "label": "unlabeled"}, {"type": "text", "expr": " Hence it follows that  $\\mathcal{H}_{a}(\\mathcal{D}_{l},\\mathcal{D^{\\prime}}_{s})\\leq\\mathcal{H}_{a}(%\n\\mathcal{D}_{l},\\mathcal{D}_{s})$ ", "word_idx": 71297, "sentence_idx": 1087, "label": "unlabeled"}, {"type": "math", "expr": "$$\\rho(\\mathbf{x}_{l})\\leq\\rho(\\mathbf{x}_{s})+\\mathrm{K}\\psi_{\\mathbf{x}}(%\n\\mathbf{x}_{s},\\mathbf{x}_{l})$$", "word_idx": 71435, "sentence_idx": 1088, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}_{l}$$", "word_idx": 71540, "sentence_idx": 1089, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathrm{D^{\\prime}}_{s}\\supseteq\\mathcal{D}_{s}$$", "word_idx": 71554, "sentence_idx": 1090, "label": "unlabeled"}, {"type": "math", "expr": "$$\\rho(\\mathbf{x}_{l})\\leq\\rho(\\mathbf{x}^{\\prime}_{s})+\\mathrm{K}\\psi_{\\mathbf{%\nx}}(\\mathbf{x}^{\\prime}_{s},\\mathbf{x}_{l})$$", "word_idx": 71601, "sentence_idx": 1091, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}^{\\prime}_{s}$$", "word_idx": 71724, "sentence_idx": 1092, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}_{l}$$", "word_idx": 71747, "sentence_idx": 1093, "label": "unlabeled"}, {"type": "math", "expr": "$$\\psi_{\\mathbf{x}}(\\mathbf{x}^{\\prime}_{s},\\mathbf{x}_{l})\\leq\\psi_{\\mathbf{x}}%\n(\\mathbf{x}_{s},\\mathbf{x}_{l})$$", "word_idx": 71761, "sentence_idx": 1094, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathbf{x}_{l}$$", "word_idx": 71872, "sentence_idx": 1095, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{H}_{a}(\\mathcal{D}_{l},\\mathcal{D^{\\prime}}_{s})\\leq\\mathcal{H}_{a}(%\n\\mathcal{D}_{l},\\mathcal{D}_{s})$$", "word_idx": 71886, "sentence_idx": 1096, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0C  Experimental details", "word_idx": 71997, "sentence_idx": 1097, "label": "unlabeled"}, {"type": "text", "expr": "Appendix\u00a0C", "word_idx": 72029, "sentence_idx": 1098, "label": "unlabeled"}, {"type": "text", "expr": "1  VGG Network Architectures", "word_idx": 72039, "sentence_idx": 1099, "label": "unlabeled"}, {"type": "text", "expr": "The architecture for our networks follow the VGG design philosophy", "word_idx": 72067, "sentence_idx": 1100, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, we have blocks with the following elements:", "word_idx": 72133, "sentence_idx": 1101, "label": "unlabeled"}, {"type": "text", "expr": "The architecture for our networks follow the VGG design philosophy", "word_idx": 72191, "sentence_idx": 1102, "label": "unlabeled"}, {"type": "text", "expr": " Specifically, we have blocks with the following elements:", "word_idx": 72257, "sentence_idx": 1103, "label": "unlabeled"}, {"type": "text", "expr": "$3\\times 3$  conv kernels with  $c$  channels of stride 1", "word_idx": 72315, "sentence_idx": 1104, "label": "unlabeled"}, {"type": "math", "expr": "$$3\\times 3$$", "word_idx": 72372, "sentence_idx": 1105, "label": "unlabeled"}, {"type": "text", "expr": "Batch Normalization", "word_idx": 72381, "sentence_idx": 1106, "label": "unlabeled"}, {"type": "text", "expr": "Batch Normalization", "word_idx": 72400, "sentence_idx": 1107, "label": "unlabeled"}, {"type": "text", "expr": "Whenever we use Max-pooling (M), we use stride  $2$  and window size  $2$ ", "word_idx": 72419, "sentence_idx": 1108, "label": "unlabeled"}, {"type": "text", "expr": "The architecture for VGG-9 is -  $[64-M-128-M-256-256-M-512-512-M-512-512-M]$ ", "word_idx": 72493, "sentence_idx": 1109, "label": "unlabeled"}, {"type": "text", "expr": " Here, the number stands for the number of convolution channels, and  $M$  represents max-pooling", "word_idx": 72571, "sentence_idx": 1110, "label": "unlabeled"}, {"type": "text", "expr": " At the end of all the convolutional and max-pooling layers, we have a Global Average Pooling (GAP) layer, after which we have a fully connected layer leading up to the final classes", "word_idx": 72668, "sentence_idx": 1111, "label": "unlabeled"}, {"type": "text", "expr": " Similar architecture is used for the case of both CIFAR and MIT Scene experiments", "word_idx": 72850, "sentence_idx": 1112, "label": "unlabeled"}, {"type": "math", "expr": "$$[64-M-128-M-256-256-M-512-512-M-512-512-M]$$", "word_idx": 72932, "sentence_idx": 1113, "label": "unlabeled"}, {"type": "text", "expr": "The architecture for VGG-4 is -  $[64-M-128-M-512-M]$ ", "word_idx": 72974, "sentence_idx": 1114, "label": "unlabeled"}, {"type": "math", "expr": "$$[64-M-128-M-512-M]$$", "word_idx": 73028, "sentence_idx": 1115, "label": "unlabeled"}, {"type": "text", "expr": "2  Loss function", "word_idx": 73046, "sentence_idx": 1116, "label": "unlabeled"}, {"type": "text", "expr": "The loss function for distillation experiments use the following form", "word_idx": 73062, "sentence_idx": 1117, "label": "unlabeled"}, {"type": "text", "expr": "The loss function for distillation experiments use the following form", "word_idx": 73131, "sentence_idx": 1118, "label": "unlabeled"}, {"type": "text", "expr": "$\\ell(\\mathcal{S},\\mathcal{T})=\\alpha\\times\\mathrm{(CE)}+\\beta\\times\\mathrm{(%\nMatch~{}Activations)}+\\gamma\\times\\mathrm{(Match~{}Jacobians)}$", "word_idx": 73200, "sentence_idx": 1119, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ell(\\mathcal{S},\\mathcal{T})=\\alpha\\times\\mathrm{(CE)}+\\beta\\times\\mathrm{(%\nMatch~{}Activations)}+\\gamma\\times\\mathrm{(Match~{}Jacobians)}$$", "word_idx": 73342, "sentence_idx": 1120, "label": "unlabeled"}, {"type": "text", "expr": "In our experiments,  $\\alpha,\\beta,\\gamma$  are either set to  $1$  or  $0$ ", "word_idx": 73482, "sentence_idx": 1121, "label": "unlabeled"}, {"type": "text", "expr": " In other words, all regularization constants are  $1$ ", "word_idx": 73558, "sentence_idx": 1122, "label": "unlabeled"}, {"type": "math", "expr": "$$\\alpha,\\beta,\\gamma$$", "word_idx": 73613, "sentence_idx": 1123, "label": "unlabeled"}, {"type": "text", "expr": "Here, \u2018CE\u2019 refers to cross-entropy with ground truth labels", "word_idx": 73632, "sentence_idx": 1124, "label": "unlabeled"}, {"type": "text", "expr": " \u2018Match Activations\u2019 refers to squared error term over pre-softmax activations of the form  $(y_{s}-y_{t})^{2}$ ", "word_idx": 73691, "sentence_idx": 1125, "label": "unlabeled"}, {"type": "text", "expr": " \u2018Match Jacobians\u2019 refers to the same squared error term, but for Jacobians", "word_idx": 73803, "sentence_idx": 1126, "label": "unlabeled"}, {"type": "math", "expr": "$$(y_{s}-y_{t})^{2}$$", "word_idx": 73878, "sentence_idx": 1127, "label": "unlabeled"}, {"type": "text", "expr": "For the MIT Scene experiments,  $\\alpha,\\beta,\\gamma$  are either set to  $10$  or  $0$ , depending on the specific method", "word_idx": 73895, "sentence_idx": 1128, "label": "unlabeled"}, {"type": "text", "expr": " To compute the Jacobian, we use average pooling over a  $feature~{}size/5$  window with a stride of  $1$ ", "word_idx": 74017, "sentence_idx": 1129, "label": "unlabeled"}, {"type": "text", "expr": " We match the Jacobian after the first residual block for resnet, and after the second max-pool for VGG", "word_idx": 74123, "sentence_idx": 1130, "label": "unlabeled"}, {"type": "text", "expr": " This corresponds to feature level \u201c1\u201d in the ablation experiments", "word_idx": 74226, "sentence_idx": 1131, "label": "unlabeled"}, {"type": "math", "expr": "$$\\alpha,\\beta,\\gamma$$", "word_idx": 74292, "sentence_idx": 1132, "label": "unlabeled"}, {"type": "math", "expr": "$$feature~{}size/5$$", "word_idx": 74311, "sentence_idx": 1133, "label": "unlabeled"}, {"type": "text", "expr": "3  Optimization", "word_idx": 74327, "sentence_idx": 1134, "label": "unlabeled"}, {"type": "text", "expr": "For CIFAR100 experiments, we run optimization for  $500$  epochs", "word_idx": 74342, "sentence_idx": 1135, "label": "unlabeled"}, {"type": "text", "expr": " We use the Adam optimizer, with an initial learning rate of  $1e-3$ , and a single learning rate annealing (to  $1e-4$ ) at  $400$  epochs", "word_idx": 74406, "sentence_idx": 1136, "label": "unlabeled"}, {"type": "text", "expr": " We used a batch size of  $128$ ", "word_idx": 74545, "sentence_idx": 1137, "label": "unlabeled"}, {"type": "math", "expr": "$$500$$", "word_idx": 74577, "sentence_idx": 1138, "label": "unlabeled"}, {"type": "math", "expr": "$$1e-3$$", "word_idx": 74580, "sentence_idx": 1139, "label": "unlabeled"}, {"type": "math", "expr": "$$1e-4$$", "word_idx": 74584, "sentence_idx": 1140, "label": "unlabeled"}, {"type": "math", "expr": "$$400$$", "word_idx": 74588, "sentence_idx": 1141, "label": "unlabeled"}, {"type": "math", "expr": "$$128$$", "word_idx": 74591, "sentence_idx": 1142, "label": "unlabeled"}, {"type": "text", "expr": "For MIT Scenes, we used SGD with momentum of  $09$ , for  $75$  epochs", "word_idx": 74594, "sentence_idx": 1143, "label": "unlabeled"}, {"type": "text", "expr": " The initial learning rate is  $1e-3$ , and it is reduced  $10$  times after  $40$  and  $60$  epochs", "word_idx": 74664, "sentence_idx": 1144, "label": "unlabeled"}, {"type": "text", "expr": " We used batch size  $8$ ", "word_idx": 74765, "sentence_idx": 1145, "label": "unlabeled"}, {"type": "text", "expr": " This is because the Jacobian computation is very memory intensive", "word_idx": 74790, "sentence_idx": 1146, "label": "unlabeled"}, {"type": "math", "expr": "$$0.9$$", "word_idx": 74856, "sentence_idx": 1147, "label": "unlabeled"}, {"type": "math", "expr": "$$1e-3$$", "word_idx": 74859, "sentence_idx": 1148, "label": "unlabeled"}, {"type": "text", "expr": "Generated  on Tue Apr  3 11:32:54 2018 by", "word_idx": 74863, "sentence_idx": 1149, "label": "unlabeled"}, {"type": "text", "expr": "LaTeXML", "word_idx": 74904, "sentence_idx": 1150, "label": "unlabeled"}], "End_to_end_weakly_supervised_semantic_alignment": [{"type": "text", "expr": "End-to-end weakly-supervised semantic alignment", "word_idx": 0, "sentence_idx": 0, "label": "unlabeled"}, {"type": "text", "expr": "End-to-end weakly-supervised semantic alignment", "word_idx": 47, "sentence_idx": 1, "label": "unlabeled"}, {"type": "text", "expr": "Ignacio Rocco ${}^{1,2}$  \u2003\u2003Relja Arandjelovi\u0107\u2009 ${}^{3}$ \n\u2003\u2003Josef Sivic ${}^{1,2,4}$ ${}^{1}$ DI ENS \u2003\u2003\u2003\u2003 ${}^{2}$ INRIA \u2003\u2003\u2003\u2003 ${}^{3}$ DeepMind \u2003\u2003\u2003\u2003 ${}^{4}$ CIIRC", "word_idx": 94, "sentence_idx": 2, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{1,2}$$", "word_idx": 257, "sentence_idx": 3, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{3}$$", "word_idx": 265, "sentence_idx": 4, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{1,2,4}$$", "word_idx": 271, "sentence_idx": 5, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{1}$$", "word_idx": 281, "sentence_idx": 6, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{2}$$", "word_idx": 287, "sentence_idx": 7, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{3}$$", "word_idx": 293, "sentence_idx": 8, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{4}$$", "word_idx": 299, "sentence_idx": 9, "label": "unlabeled"}, {"type": "text", "expr": "${}^{1}$ D\u00e9partement d\u2019informatique de l\u2019ENS, \u00c9cole normale sup\u00e9rieure, CNRS, PSL Research University, 75005 Paris, France", "word_idx": 305, "sentence_idx": 10, "label": "unlabeled"}, {"type": "text", "expr": " ${}^{4}$ Czech Institute of Informatics, Robotics and Cybernetics at the Czech Technical University in Prague", "word_idx": 427, "sentence_idx": 11, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{1}$$", "word_idx": 537, "sentence_idx": 12, "label": "unlabeled"}, {"type": "math", "expr": "$${}^{4}$$", "word_idx": 543, "sentence_idx": 13, "label": "unlabeled"}, {"type": "text", "expr": "Abstract We tackle the task of semantic alignment where the goal is to compute dense semantic correspondence\naligning two images depicting objects of the same category", "word_idx": 549, "sentence_idx": 14, "label": "unlabeled"}, {"type": "text", "expr": " This is a challenging task due to large intra-class variation, changes in viewpoint and background clutter", "word_idx": 716, "sentence_idx": 15, "label": "unlabeled"}, {"type": "text", "expr": " We present the following three principal contributions", "word_idx": 823, "sentence_idx": 16, "label": "unlabeled"}, {"type": "text", "expr": "\nFirst, we develop a convolutional neural network architecture for semantic alignment that is trainable in an end-to-end manner from weak image-level supervision in the form of matching image pairs", "word_idx": 878, "sentence_idx": 17, "label": "unlabeled"}, {"type": "text", "expr": " The outcome is that parameters are learnt from rich appearance variation present in different but semantically related images without the need for tedious manual annotation of correspondences at training time", "word_idx": 1075, "sentence_idx": 18, "label": "unlabeled"}, {"type": "text", "expr": "\nSecond, the main component of this architecture is a differentiable soft inlier scoring module, inspired by the RANSAC inlier scoring procedure, that computes the quality of the alignment based on only geometrically consistent correspondences thereby reducing the effect of background clutter", "word_idx": 1284, "sentence_idx": 19, "label": "unlabeled"}, {"type": "text", "expr": "\nThird, we demonstrate that the proposed approach achieves state-of-the-art performance on multiple standard benchmarks for semantic alignment", "word_idx": 1577, "sentence_idx": 20, "label": "unlabeled"}, {"type": "text", "expr": "Abstract", "word_idx": 1719, "sentence_idx": 21, "label": "unlabeled"}, {"type": "text", "expr": "We tackle the task of semantic alignment where the goal is to compute dense semantic correspondence\naligning two images depicting objects of the same category", "word_idx": 1727, "sentence_idx": 22, "label": "unlabeled"}, {"type": "text", "expr": " This is a challenging task due to large intra-class variation, changes in viewpoint and background clutter", "word_idx": 1885, "sentence_idx": 23, "label": "unlabeled"}, {"type": "text", "expr": " We present the following three principal contributions", "word_idx": 1992, "sentence_idx": 24, "label": "unlabeled"}, {"type": "text", "expr": "\nFirst, we develop a convolutional neural network architecture for semantic alignment that is trainable in an end-to-end manner from weak image-level supervision in the form of matching image pairs", "word_idx": 2047, "sentence_idx": 25, "label": "unlabeled"}, {"type": "text", "expr": " The outcome is that parameters are learnt from rich appearance variation present in different but semantically related images without the need for tedious manual annotation of correspondences at training time", "word_idx": 2244, "sentence_idx": 26, "label": "unlabeled"}, {"type": "text", "expr": "\nSecond, the main component of this architecture is a differentiable soft inlier scoring module, inspired by the RANSAC inlier scoring procedure, that computes the quality of the alignment based on only geometrically consistent correspondences thereby reducing the effect of background clutter", "word_idx": 2453, "sentence_idx": 27, "label": "unlabeled"}, {"type": "text", "expr": "\nThird, we demonstrate that the proposed approach achieves state-of-the-art performance on multiple standard benchmarks for semantic alignment", "word_idx": 2746, "sentence_idx": 28, "label": "unlabeled"}, {"type": "text", "expr": "\\patchcmd", "word_idx": 2888, "sentence_idx": 29, "label": "unlabeled"}, {"type": "text", "expr": "\\Hy@backout", "word_idx": 2897, "sentence_idx": 30, "label": "unlabeled"}, {"type": "text", "expr": "Doc-Start \\@currentHref \\cvprfinalcopy", "word_idx": 2908, "sentence_idx": 31, "label": "unlabeled"}, {"type": "text", "expr": "\\@currentHref", "word_idx": 2946, "sentence_idx": 32, "label": "unlabeled"}, {"type": "text", "expr": "\\cvprfinalcopy", "word_idx": 2959, "sentence_idx": 33, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:  \nWe describe a CNN architecture that, given an input image pair (top),\noutputs dense semantic correspondence between the two images together with the aligning geometric transformation (middle) and discards geometrically inconsistent matches (bottom)", "word_idx": 2973, "sentence_idx": 34, "label": "unlabeled"}, {"type": "text", "expr": " The alignment model is learnt from weak supervision in the form of matching image pairs without correspondences", "word_idx": 3233, "sentence_idx": 35, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a01:", "word_idx": 3345, "sentence_idx": 36, "label": "unlabeled"}, {"type": "text", "expr": "1  Introduction", "word_idx": 3354, "sentence_idx": 37, "label": "unlabeled"}, {"type": "text", "expr": "Finding correspondence is one of the fundamental problems in computer vision", "word_idx": 3369, "sentence_idx": 38, "label": "unlabeled"}, {"type": "text", "expr": " Initial work has focused on finding correspondence between images depicting the same object or scene with applications in image stitching  , multi-view 3D reconstruction\u00a0 , motion estimation   or tracking  ", "word_idx": 3445, "sentence_idx": 39, "label": "unlabeled"}, {"type": "text", "expr": "\nIn this work we study the problem of finding category-level correspondence, or  semantic alignment \u00a0 , where the goal is to establish dense correspondence between different objects belonging to the same category, such as the two different motorcycles illustrated in Fig", "word_idx": 3652, "sentence_idx": 40, "label": "unlabeled"}, {"type": "text", "expr": " This is an important problem with applications in object recognition\u00a0 , image editing\u00a0 , or robotics\u00a0 ", "word_idx": 3922, "sentence_idx": 41, "label": "unlabeled"}, {"type": "text", "expr": "\nThis is also an extremely challenging task because of the large intra-class variation, changes in viewpoint and presence of background clutter", "word_idx": 4025, "sentence_idx": 42, "label": "unlabeled"}, {"type": "text", "expr": "semantic alignment", "word_idx": 4168, "sentence_idx": 43, "label": "unlabeled"}, {"type": "text", "expr": "The current best semantic alignment methods\u00a0  employ powerful image representations based on convolutional neural networks coupled with a geometric deformation model", "word_idx": 4186, "sentence_idx": 44, "label": "unlabeled"}, {"type": "text", "expr": " However, these methods suffer from one of the following two major limitations", "word_idx": 4351, "sentence_idx": 45, "label": "unlabeled"}, {"type": "text", "expr": "\nFirst, the image representation and the geometric alignment model are not trained together in an end-to-end manner", "word_idx": 4429, "sentence_idx": 46, "label": "unlabeled"}, {"type": "text", "expr": " Typically, the image representation is trained on some auxiliary task such as image classification and then employed in an often ad-hoc geometric alignment model", "word_idx": 4544, "sentence_idx": 47, "label": "unlabeled"}, {"type": "text", "expr": "\nSecond, while trainable geometric alignment models exist\u00a0 , they require strong supervision in the form of ground truth correspondences, which is hard to obtain for a diverse set of real images on a large scale", "word_idx": 4706, "sentence_idx": 48, "label": "unlabeled"}, {"type": "text", "expr": "In this paper, we address both these limitations and develop a semantic alignment model that is  trainable end-to-end  from  weakly supervised  data in the form of matching\nimage pairs\nwithout the need for ground truth correspondences", "word_idx": 4917, "sentence_idx": 49, "label": "unlabeled"}, {"type": "text", "expr": "\nTo achieve that we design a novel convolutional neural network architecture for semantic alignment with a differentiable soft inlier scoring module inspired by the RANSAC inlier scoring procedure", "word_idx": 5151, "sentence_idx": 50, "label": "unlabeled"}, {"type": "text", "expr": " The resulting architecture is end-to-end trainable with only image-level supervision", "word_idx": 5347, "sentence_idx": 51, "label": "unlabeled"}, {"type": "text", "expr": " The outcome is that the image representation can be trained from rich appearance variations present in different but semantically related image pairs, rather than synthetically deformed imagery\u00a0 ", "word_idx": 5432, "sentence_idx": 52, "label": "unlabeled"}, {"type": "text", "expr": " We show that our approach allows to significantly improve the performance of the baseline deep CNN alignment model, achieving state-of-the-art performance on multiple standard benchmarks for semantic alignment", "word_idx": 5628, "sentence_idx": 53, "label": "unlabeled"}, {"type": "text", "expr": " Our code and trained models are available online\u00a0 ", "word_idx": 5838, "sentence_idx": 54, "label": "unlabeled"}, {"type": "text", "expr": "trainable end-to-end", "word_idx": 5889, "sentence_idx": 55, "label": "unlabeled"}, {"type": "text", "expr": "weakly supervised", "word_idx": 5909, "sentence_idx": 56, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:  End-to-end weakly-supervised alignment", "word_idx": 5926, "sentence_idx": 57, "label": "unlabeled"}, {"type": "text", "expr": " \nSource and target images  $(I^{s},I^{t})$  are passed through an alignment network\nthat estimates the geometric transformation  $g$ ", "word_idx": 5975, "sentence_idx": 58, "label": "unlabeled"}, {"type": "text", "expr": "\nThen, the soft-inlier count is computed (in green) by first finding the inlier region  $m$ \nin agreement with  $g$ , and then summing up the pairwise matching scores  $s$  inside this\narea of the discretized matches space", "word_idx": 6109, "sentence_idx": 59, "label": "unlabeled"}, {"type": "text", "expr": " The soft-inlier count is differentiable, which allows the whole model to be trained using back-propagation", "word_idx": 6331, "sentence_idx": 60, "label": "unlabeled"}, {"type": "text", "expr": " Functions are represented in blue and tensors in pink", "word_idx": 6438, "sentence_idx": 61, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a02:", "word_idx": 6492, "sentence_idx": 62, "label": "unlabeled"}, {"type": "text", "expr": "End-to-end weakly-supervised alignment", "word_idx": 6501, "sentence_idx": 63, "label": "unlabeled"}, {"type": "math", "expr": "$$(I^{s},I^{t})$$", "word_idx": 6539, "sentence_idx": 64, "label": "unlabeled"}, {"type": "text", "expr": "2  Related work", "word_idx": 6552, "sentence_idx": 65, "label": "unlabeled"}, {"type": "text", "expr": "The problem of semantic alignment has received significant attention in the last few years with progress in both (i) image descriptors and (ii) geometric models", "word_idx": 6567, "sentence_idx": 66, "label": "unlabeled"}, {"type": "text", "expr": " The key innovation has been making the two components trainable from data", "word_idx": 6727, "sentence_idx": 67, "label": "unlabeled"}, {"type": "text", "expr": "\nWe summarize the recent progress in\u00a0Table\u00a0 1  where we indicate for each method whether the descriptor (D) or the alignment model (A) are trainable, whether the entire architecture is trainable end-to-end (E-E), and whether the required supervision is strong (s) or weak (w)", "word_idx": 6801, "sentence_idx": 68, "label": "unlabeled"}, {"type": "text", "expr": "Trainable", "word_idx": 7076, "sentence_idx": 69, "label": "unlabeled"}, {"type": "text", "expr": "Paper", "word_idx": 7085, "sentence_idx": 70, "label": "unlabeled"}, {"type": "text", "expr": "Paper", "word_idx": 7090, "sentence_idx": 71, "label": "unlabeled"}, {"type": "text", "expr": "Descriptor", "word_idx": 7095, "sentence_idx": 72, "label": "unlabeled"}, {"type": "text", "expr": "Descriptor", "word_idx": 7105, "sentence_idx": 73, "label": "unlabeled"}, {"type": "text", "expr": "Alignment", "word_idx": 7115, "sentence_idx": 74, "label": "unlabeled"}, {"type": "text", "expr": "Alignment", "word_idx": 7124, "sentence_idx": 75, "label": "unlabeled"}, {"type": "text", "expr": "method", "word_idx": 7133, "sentence_idx": 76, "label": "unlabeled"}, {"type": "text", "expr": "method", "word_idx": 7139, "sentence_idx": 77, "label": "unlabeled"}, {"type": "text", "expr": "Trainable", "word_idx": 7145, "sentence_idx": 78, "label": "unlabeled"}, {"type": "text", "expr": "Trainable", "word_idx": 7154, "sentence_idx": 79, "label": "unlabeled"}, {"type": "text", "expr": "D A E-E S", "word_idx": 7163, "sentence_idx": 80, "label": "unlabeled"}, {"type": "text", "expr": "SIFT SIFT Flow \u2737 \u2737 \u2737 -", "word_idx": 7172, "sentence_idx": 81, "label": "unlabeled"}, {"type": "text", "expr": "Liu  et al", "word_idx": 7194, "sentence_idx": 82, "label": "unlabeled"}, {"type": "text", "expr": "et al", "word_idx": 7204, "sentence_idx": 83, "label": "unlabeled"}, {"type": "text", "expr": "SIFT Flow", "word_idx": 7209, "sentence_idx": 84, "label": "unlabeled"}, {"type": "text", "expr": "SIFT Flow", "word_idx": 7218, "sentence_idx": 85, "label": "unlabeled"}, {"type": "text", "expr": "SIFT+PCA DSP \u2737 \u2737 \u2737 -", "word_idx": 7227, "sentence_idx": 86, "label": "unlabeled"}, {"type": "text", "expr": "Kim  et al", "word_idx": 7247, "sentence_idx": 87, "label": "unlabeled"}, {"type": "text", "expr": "et al", "word_idx": 7257, "sentence_idx": 88, "label": "unlabeled"}, {"type": "text", "expr": "SIFT+PCA", "word_idx": 7262, "sentence_idx": 89, "label": "unlabeled"}, {"type": "text", "expr": "SIFT+PCA", "word_idx": 7270, "sentence_idx": 90, "label": "unlabeled"}, {"type": "text", "expr": "HOG TSS \u2737 \u2737 \u2737 -", "word_idx": 7278, "sentence_idx": 91, "label": "unlabeled"}, {"type": "text", "expr": "Taniai  et al", "word_idx": 7293, "sentence_idx": 92, "label": "unlabeled"}, {"type": "text", "expr": "Taniai", "word_idx": 7306, "sentence_idx": 93, "label": "unlabeled"}, {"type": "text", "expr": "et al", "word_idx": 7312, "sentence_idx": 94, "label": "unlabeled"}, {"type": "text", "expr": "HOG PF-LOM \u2737 \u2737 \u2737 -", "word_idx": 7317, "sentence_idx": 95, "label": "unlabeled"}, {"type": "text", "expr": "Ham  et al", "word_idx": 7335, "sentence_idx": 96, "label": "unlabeled"}, {"type": "text", "expr": "et al", "word_idx": 7345, "sentence_idx": 97, "label": "unlabeled"}, {"type": "text", "expr": "PF-LOM", "word_idx": 7350, "sentence_idx": 98, "label": "unlabeled"}, {"type": "text", "expr": "PF-LOM", "word_idx": 7356, "sentence_idx": 99, "label": "unlabeled"}, {"type": "text", "expr": "HOG OADSC \u2737 \u2737 \u2737 -", "word_idx": 7362, "sentence_idx": 100, "label": "unlabeled"}, {"type": "text", "expr": "Yang  et al", "word_idx": 7379, "sentence_idx": 101, "label": "unlabeled"}, {"type": "text", "expr": "et al", "word_idx": 7390, "sentence_idx": 102, "label": "unlabeled"}, {"type": "text", "expr": "OADSC", "word_idx": 7395, "sentence_idx": 103, "label": "unlabeled"}, {"type": "text", "expr": "OADSC", "word_idx": 7400, "sentence_idx": 104, "label": "unlabeled"}, {"type": "text", "expr": "AlexNet DSFM \u2737 \u2737 \u2737 -", "word_idx": 7405, "sentence_idx": 105, "label": "unlabeled"}, {"type": "text", "expr": "Ufer  et al", "word_idx": 7425, "sentence_idx": 106, "label": "unlabeled"}, {"type": "text", "expr": "et al", "word_idx": 7436, "sentence_idx": 107, "label": "unlabeled"}, {"type": "text", "expr": "AlexNet", "word_idx": 7441, "sentence_idx": 108, "label": "unlabeled"}, {"type": "text", "expr": "AlexNet", "word_idx": 7448, "sentence_idx": 109, "label": "unlabeled"}, {"type": "text", "expr": "DSP \u2733 \u2737 \u2737 w", "word_idx": 7455, "sentence_idx": 110, "label": "unlabeled"}, {"type": "text", "expr": "Novotny  et al", "word_idx": 7466, "sentence_idx": 111, "label": "unlabeled"}, {"type": "text", "expr": "et al", "word_idx": 7480, "sentence_idx": 112, "label": "unlabeled"}, {"type": "text", "expr": "AnchorNet", "word_idx": 7485, "sentence_idx": 113, "label": "unlabeled"}, {"type": "text", "expr": "AnchorNet", "word_idx": 7494, "sentence_idx": 114, "label": "unlabeled"}, {"type": "text", "expr": "PF-LOM \u2733 \u2737 \u2737 w", "word_idx": 7503, "sentence_idx": 115, "label": "unlabeled"}, {"type": "text", "expr": "PF-LOM", "word_idx": 7517, "sentence_idx": 116, "label": "unlabeled"}, {"type": "text", "expr": "PF-LOM", "word_idx": 7523, "sentence_idx": 117, "label": "unlabeled"}, {"type": "text", "expr": "SIFT Flow \u2733 \u2737 \u2737 s", "word_idx": 7529, "sentence_idx": 118, "label": "unlabeled"}, {"type": "text", "expr": "Kim  et al", "word_idx": 7546, "sentence_idx": 119, "label": "unlabeled"}, {"type": "text", "expr": "et al", "word_idx": 7556, "sentence_idx": 120, "label": "unlabeled"}, {"type": "text", "expr": "SIFT Flow", "word_idx": 7561, "sentence_idx": 121, "label": "unlabeled"}, {"type": "text", "expr": "SIFT Flow", "word_idx": 7570, "sentence_idx": 122, "label": "unlabeled"}, {"type": "text", "expr": "PF-LOM \u2733 \u2737 \u2737 s", "word_idx": 7579, "sentence_idx": 123, "label": "unlabeled"}, {"type": "text", "expr": "PF-LOM", "word_idx": 7593, "sentence_idx": 124, "label": "unlabeled"}, {"type": "text", "expr": "PF-LOM", "word_idx": 7599, "sentence_idx": 125, "label": "unlabeled"}, {"type": "text", "expr": "FCSS DCTM \u2733 \u2737 \u2737 s", "word_idx": 7605, "sentence_idx": 126, "label": "unlabeled"}, {"type": "text", "expr": "Kim  et al", "word_idx": 7622, "sentence_idx": 127, "label": "unlabeled"}, {"type": "text", "expr": "et al", "word_idx": 7632, "sentence_idx": 128, "label": "unlabeled"}, {"type": "text", "expr": "SCNet-A \u2733 \u2733 \u2737 s", "word_idx": 7637, "sentence_idx": 129, "label": "unlabeled"}, {"type": "text", "expr": "Han  et al", "word_idx": 7652, "sentence_idx": 130, "label": "unlabeled"}, {"type": "text", "expr": "et al", "word_idx": 7662, "sentence_idx": 131, "label": "unlabeled"}, {"type": "text", "expr": "VGG-16", "word_idx": 7667, "sentence_idx": 132, "label": "unlabeled"}, {"type": "text", "expr": "VGG-16", "word_idx": 7673, "sentence_idx": 133, "label": "unlabeled"}, {"type": "text", "expr": "SCNet-A", "word_idx": 7679, "sentence_idx": 134, "label": "unlabeled"}, {"type": "text", "expr": "SCNet-A", "word_idx": 7686, "sentence_idx": 135, "label": "unlabeled"}, {"type": "text", "expr": "SCNet-AG \u2733 \u2733 \u2737 s", "word_idx": 7693, "sentence_idx": 136, "label": "unlabeled"}, {"type": "text", "expr": "SCNet-AG", "word_idx": 7709, "sentence_idx": 137, "label": "unlabeled"}, {"type": "text", "expr": "SCNet-AG", "word_idx": 7717, "sentence_idx": 138, "label": "unlabeled"}, {"type": "text", "expr": "SCNet-AG+ \u2733 \u2733 \u2737 s", "word_idx": 7725, "sentence_idx": 139, "label": "unlabeled"}, {"type": "text", "expr": "SCNet-AG+", "word_idx": 7742, "sentence_idx": 140, "label": "unlabeled"}, {"type": "text", "expr": "SCNet-AG+", "word_idx": 7751, "sentence_idx": 141, "label": "unlabeled"}, {"type": "text", "expr": "VGG-16 CNN Geo", "word_idx": 7760, "sentence_idx": 142, "label": "unlabeled"}, {"type": "text", "expr": " \u2733 \u2733 \u2733 s", "word_idx": 7774, "sentence_idx": 143, "label": "unlabeled"}, {"type": "text", "expr": "Rocco  et al", "word_idx": 7782, "sentence_idx": 144, "label": "unlabeled"}, {"type": "text", "expr": "et al", "word_idx": 7794, "sentence_idx": 145, "label": "unlabeled"}, {"type": "text", "expr": "VGG-16", "word_idx": 7799, "sentence_idx": 146, "label": "unlabeled"}, {"type": "text", "expr": "VGG-16", "word_idx": 7805, "sentence_idx": 147, "label": "unlabeled"}, {"type": "text", "expr": "CNN Geo", "word_idx": 7811, "sentence_idx": 148, "label": "unlabeled"}, {"type": "text", "expr": "CNN Geo", "word_idx": 7818, "sentence_idx": 149, "label": "unlabeled"}, {"type": "text", "expr": "ResNet-101 CNN Geo", "word_idx": 7825, "sentence_idx": 150, "label": "unlabeled"}, {"type": "text", "expr": " \u2733 \u2733 \u2733 s", "word_idx": 7843, "sentence_idx": 151, "label": "unlabeled"}, {"type": "text", "expr": "ResNet-101", "word_idx": 7851, "sentence_idx": 152, "label": "unlabeled"}, {"type": "text", "expr": "ResNet-101", "word_idx": 7861, "sentence_idx": 153, "label": "unlabeled"}, {"type": "text", "expr": "CNN Geo", "word_idx": 7871, "sentence_idx": 154, "label": "unlabeled"}, {"type": "text", "expr": "CNN Geo", "word_idx": 7878, "sentence_idx": 155, "label": "unlabeled"}, {"type": "text", "expr": "Proposed method ResNet-101 CNN Geo", "word_idx": 7885, "sentence_idx": 156, "label": "unlabeled"}, {"type": "text", "expr": " \u2733 \u2733 \u2733 w", "word_idx": 7919, "sentence_idx": 157, "label": "unlabeled"}, {"type": "text", "expr": "Proposed method", "word_idx": 7927, "sentence_idx": 158, "label": "unlabeled"}, {"type": "text", "expr": "Proposed method", "word_idx": 7942, "sentence_idx": 159, "label": "unlabeled"}, {"type": "text", "expr": "ResNet-101", "word_idx": 7957, "sentence_idx": 160, "label": "unlabeled"}, {"type": "text", "expr": "ResNet-101", "word_idx": 7967, "sentence_idx": 161, "label": "unlabeled"}, {"type": "text", "expr": "CNN Geo", "word_idx": 7977, "sentence_idx": 162, "label": "unlabeled"}, {"type": "text", "expr": "CNN Geo", "word_idx": 7984, "sentence_idx": 163, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:  Comparison of recent related work", "word_idx": 7991, "sentence_idx": 164, "label": "unlabeled"}, {"type": "text", "expr": "  The table indicates employed image descriptor and alignment method", "word_idx": 8034, "sentence_idx": 165, "label": "unlabeled"}, {"type": "text", "expr": " The last four columns show which components of the approach are trained for the semantic alignment task: descriptor (D), alignment (A) or both in end-to-end manner (E-E); and the level of supervision (S): strong (s) or weak (w)", "word_idx": 8102, "sentence_idx": 166, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a01:", "word_idx": 8330, "sentence_idx": 167, "label": "unlabeled"}, {"type": "text", "expr": "Comparison of recent related work", "word_idx": 8338, "sentence_idx": 168, "label": "unlabeled"}, {"type": "text", "expr": "Early methods, such as  , employed hand-engineered descriptors like SIFT or HOG together with hand-engineered alignment models based on minimizing a given matching energy", "word_idx": 8371, "sentence_idx": 169, "label": "unlabeled"}, {"type": "text", "expr": " This approach has been quite successful\u00a0  using in some cases\u00a0  pre-trained (but fixed) convolutional neural network (CNN) descriptors", "word_idx": 8541, "sentence_idx": 170, "label": "unlabeled"}, {"type": "text", "expr": " However, none of these methods train the image descriptor or the geometric model directly for semantic alignment", "word_idx": 8676, "sentence_idx": 171, "label": "unlabeled"}, {"type": "text", "expr": "Others\u00a0  have investigated trainable image descriptors for semantic matching\nbut have combined them with hand-engineered alignment models still rendering the alignment pipeline not trainable end-to-end", "word_idx": 8789, "sentence_idx": 172, "label": "unlabeled"}, {"type": "text", "expr": "Finally, recent work\u00a0  has employed trainable CNN descriptors together with trainable geometric alignment methods", "word_idx": 8990, "sentence_idx": 173, "label": "unlabeled"}, {"type": "text", "expr": " However, in\u00a0  the matching is learned at the object-proposal level and a non-trainable fusion step is necessary to output the final alignment making the method non end-to-end trainable", "word_idx": 9103, "sentence_idx": 174, "label": "unlabeled"}, {"type": "text", "expr": " On the contrary,   estimate a parametric geometric model, which can be converted into dense pixel correspondences in a differentiable way, making the method end-to-end trainable", "word_idx": 9288, "sentence_idx": 175, "label": "unlabeled"}, {"type": "text", "expr": " However, the method is trained with strong supervision in the form of ground truth correspondences obtained from synthetically warped images, which significantly limits the appearance variation in the training data", "word_idx": 9466, "sentence_idx": 176, "label": "unlabeled"}, {"type": "text", "expr": "Contributions", "word_idx": 9681, "sentence_idx": 177, "label": "unlabeled"}, {"type": "text", "expr": "We develop a network architecture where both the descriptor and the alignment model are trainable in an end-to-end manner from weakly supervised data", "word_idx": 9694, "sentence_idx": 178, "label": "unlabeled"}, {"type": "text", "expr": " This enables training from real images with rich appearance variation and without the need for manual ground-truth correspondence", "word_idx": 9843, "sentence_idx": 179, "label": "unlabeled"}, {"type": "text", "expr": " We demonstrate that the proposed approach significantly improves alignment results achieving state-of-the-art performance on several datasets for semantic alignment", "word_idx": 9973, "sentence_idx": 180, "label": "unlabeled"}, {"type": "text", "expr": "We develop a network architecture where both the descriptor and the alignment model are trainable in an end-to-end manner from weakly supervised data", "word_idx": 10138, "sentence_idx": 181, "label": "unlabeled"}, {"type": "text", "expr": " This enables training from real images with rich appearance variation and without the need for manual ground-truth correspondence", "word_idx": 10287, "sentence_idx": 182, "label": "unlabeled"}, {"type": "text", "expr": " We demonstrate that the proposed approach significantly improves alignment results achieving state-of-the-art performance on several datasets for semantic alignment", "word_idx": 10417, "sentence_idx": 183, "label": "unlabeled"}, {"type": "text", "expr": "3  Weakly-supervised semantic alignment", "word_idx": 10582, "sentence_idx": 184, "label": "unlabeled"}, {"type": "text", "expr": "This section presents a method for training a semantic alignment model in an\nend-to-end fashion using only weak supervision \u2013 the information that two\nimages should match \u2013 but without access to the underlying geometric transformation\nat training time", "word_idx": 10621, "sentence_idx": 185, "label": "unlabeled"}, {"type": "text", "expr": "\nThe approach is outlined in Fig", "word_idx": 10872, "sentence_idx": 186, "label": "unlabeled"}, {"type": "text", "expr": "\nNamely,\ngiven a pair of images, an alignment network estimates the geometric transformation\nthat aligns them", "word_idx": 10904, "sentence_idx": 187, "label": "unlabeled"}, {"type": "text", "expr": "\nThe quality of the estimated transformation is assessed using\nthe proposed  soft-inlier count  which aggregates the observed evidence\nin the form of feature matches", "word_idx": 11013, "sentence_idx": 188, "label": "unlabeled"}, {"type": "text", "expr": "\nThe training objective then is to maximize the alignment quality for\npairs of images which should match", "word_idx": 11178, "sentence_idx": 189, "label": "unlabeled"}, {"type": "text", "expr": "soft-inlier count", "word_idx": 11282, "sentence_idx": 190, "label": "unlabeled"}, {"type": "text", "expr": "The key idea is that, instead of requiring strongly supervised training\ndata in the form of known pairwise alignments and training the alignment\nnetwork with these,\nthe network is \u201cforced\u201d into learning to estimate good alignments in order\nto achieve high alignment scores (soft-inlier counts) for matching image pairs", "word_idx": 11299, "sentence_idx": 191, "label": "unlabeled"}, {"type": "text", "expr": "\nThe details of the alignment network and the soft-inlier count are presented next", "word_idx": 11617, "sentence_idx": 192, "label": "unlabeled"}, {"type": "text", "expr": "The key idea is that, instead of requiring strongly supervised training\ndata in the form of known pairwise alignments and training the alignment\nnetwork with these,\nthe network is \u201cforced\u201d into learning to estimate good alignments in order\nto achieve high alignment scores (soft-inlier counts) for matching image pairs", "word_idx": 11699, "sentence_idx": 193, "label": "unlabeled"}, {"type": "text", "expr": "\nThe details of the alignment network and the soft-inlier count are presented next", "word_idx": 12017, "sentence_idx": 194, "label": "unlabeled"}, {"type": "text", "expr": "1  Semantic alignment network", "word_idx": 12099, "sentence_idx": 195, "label": "unlabeled"}, {"type": "text", "expr": "In order to make use of the error signal coming from the soft-inlier count,\nour framework requires an alignment network which is trainable end-to-end", "word_idx": 12128, "sentence_idx": 196, "label": "unlabeled"}, {"type": "text", "expr": "\nWe build on the Siamese CNN architecture described in\u00a0 , illustrated in the left section of Fig", "word_idx": 12277, "sentence_idx": 197, "label": "unlabeled"}, {"type": "text", "expr": "\nThe architecture is composed of three main stages \u2013\nfeature extraction, followed by feature matching and\ngeometric transformation estimation \u2013\nwhich we review below", "word_idx": 12373, "sentence_idx": 198, "label": "unlabeled"}, {"type": "text", "expr": "Feature extraction", "word_idx": 12538, "sentence_idx": 199, "label": "unlabeled"}, {"type": "text", "expr": "The input source and target images,  $(I^{s},I^{t})$ , are passed through two fully-convolutional feature extraction CNN branches,  $F$ , with shared weights", "word_idx": 12556, "sentence_idx": 200, "label": "unlabeled"}, {"type": "text", "expr": "\nThe resulting feature maps  $(f^{s},f^{t})$  are  $h\\times w\\times d$  tensors which can be interpreted as dense  $h\\times w$  grids of  $d$ -dimensional local features  $f_{ij\\mathbf{:}}\\in\\mathbb{R}^{d}$ ", "word_idx": 12713, "sentence_idx": 201, "label": "unlabeled"}, {"type": "text", "expr": " These individual  $d$ -dimensional features are L2 normalized", "word_idx": 12920, "sentence_idx": 202, "label": "unlabeled"}, {"type": "math", "expr": "$$(I^{s},I^{t})$$", "word_idx": 12982, "sentence_idx": 203, "label": "unlabeled"}, {"type": "math", "expr": "$$(f^{s},f^{t})$$", "word_idx": 12995, "sentence_idx": 204, "label": "unlabeled"}, {"type": "math", "expr": "$$h\\times w\\times d$$", "word_idx": 13008, "sentence_idx": 205, "label": "unlabeled"}, {"type": "math", "expr": "$$h\\times w$$", "word_idx": 13025, "sentence_idx": 206, "label": "unlabeled"}, {"type": "math", "expr": "$$f_{ij\\mathbf{:}}\\in\\mathbb{R}^{d}$$", "word_idx": 13034, "sentence_idx": 207, "label": "unlabeled"}, {"type": "text", "expr": "Pairwise feature matching", "word_idx": 13067, "sentence_idx": 208, "label": "unlabeled"}, {"type": "text", "expr": "This stage computes all pairwise similarities, or match scores,\nbetween local features in the two images", "word_idx": 13092, "sentence_idx": 209, "label": "unlabeled"}, {"type": "text", "expr": " This is done with the normalized correlation function, defined as:", "word_idx": 13196, "sentence_idx": 210, "label": "unlabeled"}, {"type": "text", "expr": "This stage computes all pairwise similarities, or match scores,\nbetween local features in the two images", "word_idx": 13263, "sentence_idx": 211, "label": "unlabeled"}, {"type": "text", "expr": " This is done with the normalized correlation function, defined as:", "word_idx": 13367, "sentence_idx": 212, "label": "unlabeled"}, {"type": "text", "expr": "$\\begin{split}\\displaystyle S:\\mathbb{R}^{h\\times w\\times d}\\times\\mathbb{R}^{h%\n\\times w\\times d}&\\displaystyle\\to\\mathbb{R}^{h\\times w\\times h\\times w}\\\\\n\\end{split}$", "word_idx": 13434, "sentence_idx": 213, "label": "unlabeled"}, {"type": "math", "expr": "$$\\begin{split}\\displaystyle S:\\mathbb{R}^{h\\times w\\times d}\\times\\mathbb{R}^{h%\n\\times w\\times d}&\\displaystyle\\to\\mathbb{R}^{h\\times w\\times h\\times w}\\\\\n\\end{split}$$", "word_idx": 13602, "sentence_idx": 214, "label": "unlabeled"}, {"type": "text", "expr": "$s_{ijkl}=S(f^{s},f^{t})_{ijkl}=\\frac{\\langle f^{s}_{ij:},f^{t}_{kl:}\\rangle}{%\n\\sqrt{\\sum_{a,b}\\langle f^{s}_{ab:},f^{t}_{kl:}\\rangle^{2}}},$", "word_idx": 13768, "sentence_idx": 215, "label": "unlabeled"}, {"type": "math", "expr": "$$s_{ijkl}=S(f^{s},f^{t})_{ijkl}=\\frac{\\langle f^{s}_{ij:},f^{t}_{kl:}\\rangle}{%\n\\sqrt{\\sum_{a,b}\\langle f^{s}_{ab:},f^{t}_{kl:}\\rangle^{2}}},$$", "word_idx": 13910, "sentence_idx": 216, "label": "unlabeled"}, {"type": "text", "expr": "where the numerator in ( 2 ) computes the  raw  pairwise match scores by computing the dot product between features pairs", "word_idx": 14050, "sentence_idx": 217, "label": "unlabeled"}, {"type": "text", "expr": " The denominator performs a normalization operation with the effect of down-weighing ambiguous matches, by penalizing features from one image which have multiple highly-rated matches in the other image", "word_idx": 14171, "sentence_idx": 218, "label": "unlabeled"}, {"type": "text", "expr": "\nThis is in line with the classical second nearest neighbour test of Lowe  ", "word_idx": 14372, "sentence_idx": 219, "label": "unlabeled"}, {"type": "text", "expr": "\nThe resulting tensor  $s$  contains all normalized match scores between the source and target features", "word_idx": 14447, "sentence_idx": 220, "label": "unlabeled"}, {"type": "text", "expr": "{subfigure}", "word_idx": 14550, "sentence_idx": 221, "label": "unlabeled"}, {"type": "text", "expr": "{subfigure}", "word_idx": 14561, "sentence_idx": 222, "label": "unlabeled"}, {"type": "text", "expr": "3\n \n\n\n\n\u2003 {subfigure} [b]0", "word_idx": 14572, "sentence_idx": 223, "label": "unlabeled"}, {"type": "text", "expr": "3\n \n\n\n\n\u2003 {subfigure} [b]0", "word_idx": 14597, "sentence_idx": 224, "label": "unlabeled"}, {"type": "text", "expr": "{subfigure}", "word_idx": 14622, "sentence_idx": 225, "label": "unlabeled"}, {"type": "text", "expr": "{subfigure}", "word_idx": 14633, "sentence_idx": 226, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:  Inliers   and outliers", "word_idx": 14644, "sentence_idx": 227, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a03:", "word_idx": 14677, "sentence_idx": 228, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:  Inlier mask function", "word_idx": 14686, "sentence_idx": 229, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a04:", "word_idx": 14717, "sentence_idx": 230, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a05:  Discretized space", "word_idx": 14726, "sentence_idx": 231, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a05:", "word_idx": 14754, "sentence_idx": 232, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a06:  Line-fitting example", "word_idx": 14763, "sentence_idx": 233, "label": "unlabeled"}, {"type": "text", "expr": "  (a) The line hypothesis  $\\ell$  can be evaluated in terms of the number of inliers", "word_idx": 14794, "sentence_idx": 234, "label": "unlabeled"}, {"type": "text", "expr": " (b) The inlier mask  $m$  specifies the region where the inlier distance threshold is satisfied", "word_idx": 14879, "sentence_idx": 235, "label": "unlabeled"}, {"type": "text", "expr": "\n(c) In the discretized space setting, where the match score  $s_{ij}$  exists for every point  $(i,j)$ , the soft-inlier count is computed by summing up match scores masked by the inlier mask  $m$  from (b)", "word_idx": 14975, "sentence_idx": 236, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a06:", "word_idx": 15182, "sentence_idx": 237, "label": "unlabeled"}, {"type": "text", "expr": "Line-fitting example", "word_idx": 15191, "sentence_idx": 238, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ell$$", "word_idx": 15211, "sentence_idx": 239, "label": "unlabeled"}, {"type": "math", "expr": "$$s_{ij}$$", "word_idx": 15215, "sentence_idx": 240, "label": "unlabeled"}, {"type": "math", "expr": "$$(i,j)$$", "word_idx": 15221, "sentence_idx": 241, "label": "unlabeled"}, {"type": "text", "expr": "Geometric transformation estimation", "word_idx": 15226, "sentence_idx": 242, "label": "unlabeled"}, {"type": "text", "expr": "The final stage of the alignment network consists of estimating the parameters of a geometric transformation  $g$  given the match scores  $s$ ", "word_idx": 15261, "sentence_idx": 243, "label": "unlabeled"}, {"type": "text", "expr": "\nThis is done by a transformation regression CNN,\nrepresented by the function  $G$ :", "word_idx": 15404, "sentence_idx": 244, "label": "unlabeled"}, {"type": "text", "expr": "$\\begin{gathered}\\displaystyle G:\\mathbb{R}^{h\\times w\\times h\\times w}\\to%\n\\mathbb{R}^{K},\\quad g=G(s)\\end{gathered}$", "word_idx": 15488, "sentence_idx": 245, "label": "unlabeled"}, {"type": "math", "expr": "$$\\begin{gathered}\\displaystyle G:\\mathbb{R}^{h\\times w\\times h\\times w}\\to%\n\\mathbb{R}^{K},\\quad g=G(s)\\end{gathered}$$", "word_idx": 15606, "sentence_idx": 246, "label": "unlabeled"}, {"type": "text", "expr": "where  $K$  is the number of degrees of freedom, or parameters, of the geometric model;  \\eg $K=6$  for an affine model", "word_idx": 15722, "sentence_idx": 247, "label": "unlabeled"}, {"type": "text", "expr": " The estimated transformation parameters  $g$  are used to define the 2-D warping  $\\mathcal{T}_{g}$ :", "word_idx": 15841, "sentence_idx": 248, "label": "unlabeled"}, {"type": "math", "expr": "$$K=6$$", "word_idx": 15943, "sentence_idx": 249, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{T}_{g}$$", "word_idx": 15946, "sentence_idx": 250, "label": "unlabeled"}, {"type": "text", "expr": "$\\begin{gathered}\\displaystyle\\mathcal{T}_{g}:\\mathbb{R}^{2}\\to\\mathbb{R}^{2},%\n\\quad(u^{s},v^{s})=\\mathcal{T}_{g}(u^{t},v^{t})\\end{gathered}$", "word_idx": 15961, "sentence_idx": 251, "label": "unlabeled"}, {"type": "math", "expr": "$$\\begin{gathered}\\displaystyle\\mathcal{T}_{g}:\\mathbb{R}^{2}\\to\\mathbb{R}^{2},%\n\\quad(u^{s},v^{s})=\\mathcal{T}_{g}(u^{t},v^{t})\\end{gathered}$$", "word_idx": 16103, "sentence_idx": 252, "label": "unlabeled"}, {"type": "text", "expr": "where  $(u^{t},v^{t})$  are the spatial coordinates of the target image, and  $(u^{s},v^{s})$  the corresponding sampling coordinates in the source image", "word_idx": 16243, "sentence_idx": 253, "label": "unlabeled"}, {"type": "text", "expr": " Using  $\\mathcal{T}_{g}$ , it is possible to warp the source to the target image", "word_idx": 16396, "sentence_idx": 254, "label": "unlabeled"}, {"type": "math", "expr": "$$(u^{t},v^{t})$$", "word_idx": 16477, "sentence_idx": 255, "label": "unlabeled"}, {"type": "math", "expr": "$$(u^{s},v^{s})$$", "word_idx": 16490, "sentence_idx": 256, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{T}_{g}$$", "word_idx": 16503, "sentence_idx": 257, "label": "unlabeled"}, {"type": "text", "expr": "Note that all parts of the geometric alignment network are differentiable\nand therefore amenable to end-to-end training\u00a0 ,\nincluding the feature extractor  $F$  which can learn better features for the task\nof semantic alignment", "word_idx": 16518, "sentence_idx": 258, "label": "unlabeled"}, {"type": "text", "expr": "2  Soft-inlier count", "word_idx": 16745, "sentence_idx": 259, "label": "unlabeled"}, {"type": "text", "expr": "We propose the  soft-inlier count  used to automatically evaluate the\nestimated geometric transformation  $g$ ", "word_idx": 16765, "sentence_idx": 260, "label": "unlabeled"}, {"type": "text", "expr": "\nMaking an effort to maximize this count provides the weak-supervisory signal required\nto train the alignment network, avoiding the need for\nexpensive manual annotations for  $g$ ", "word_idx": 16875, "sentence_idx": 261, "label": "unlabeled"}, {"type": "text", "expr": "\nThe soft-inlier count is inspired by the inlier count used in the robust RANSAC method\u00a0 , which is reviewed first", "word_idx": 17054, "sentence_idx": 262, "label": "unlabeled"}, {"type": "text", "expr": "soft-inlier count", "word_idx": 17168, "sentence_idx": 263, "label": "unlabeled"}, {"type": "text", "expr": "RANSAC inlier count", "word_idx": 17185, "sentence_idx": 264, "label": "unlabeled"}, {"type": "text", "expr": "For simplicity, let us consider the problem of fitting a line to a set of observed points  $p_{i}$ , with  $i=1,\\dots N$ , as illustrated in Fig", "word_idx": 17204, "sentence_idx": 265, "label": "unlabeled"}, {"type": "text", "expr": "\nRANSAC proceeds by sampling random pairs of points used to propose line hypotheses,\neach of which is then scored using the inlier count, and the highest scoring line\nis chosen; here we only focus on the inlier count aspect of RANSAC used to score\na hypothesis", "word_idx": 17348, "sentence_idx": 266, "label": "unlabeled"}, {"type": "text", "expr": "\nGiven a hypothesized line  $\\ell$ , the RANSAC inlier scoring function counts the number of observed points which are in agreement with this hypothesis, called the  inliers ", "word_idx": 17608, "sentence_idx": 267, "label": "unlabeled"}, {"type": "text", "expr": "\nA point  $p$  is typically deemed to be an inlier iff its distance to the line is smaller\nthan a chosen distance threshold  $t$ ,  \\ie $\\text{d}(p,\\ell)<t$ ", "word_idx": 17782, "sentence_idx": 268, "label": "unlabeled"}, {"type": "math", "expr": "$$p_{i}$$", "word_idx": 17939, "sentence_idx": 269, "label": "unlabeled"}, {"type": "math", "expr": "$$i=1,\\dots N$$", "word_idx": 17944, "sentence_idx": 270, "label": "unlabeled"}, {"type": "math", "expr": "$$\\ell$$", "word_idx": 17955, "sentence_idx": 271, "label": "unlabeled"}, {"type": "text", "expr": "inliers", "word_idx": 17959, "sentence_idx": 272, "label": "unlabeled"}, {"type": "math", "expr": "$$\\text{d}(p,\\ell)<t$$", "word_idx": 17966, "sentence_idx": 273, "label": "unlabeled"}, {"type": "text", "expr": "The RANSAC inlier count,  $c_{R}$ , can be formulated by means of an auxiliary indicator function illustrated in Fig", "word_idx": 17984, "sentence_idx": 274, "label": "unlabeled"}, {"type": "text", "expr": "\u2009 6 , which we call the inlier mask function  $m$ :", "word_idx": 18100, "sentence_idx": 275, "label": "unlabeled"}, {"type": "math", "expr": "$$c_{R}$$", "word_idx": 18151, "sentence_idx": 276, "label": "unlabeled"}, {"type": "text", "expr": "$c_{R}=\\sum_{i}m(p_{i}),\\text{ where }m(p)=\\begin{cases}1,&\\text{if }\\text{d}(p%\n,\\ell)<t\\\\\n0,&\\text{otherwise}\\end{cases}$", "word_idx": 18156, "sentence_idx": 277, "label": "unlabeled"}, {"type": "math", "expr": "$$c_{R}=\\sum_{i}m(p_{i}),\\text{ where }m(p)=\\begin{cases}1,&\\text{if }\\text{d}(p%\n,\\ell)<t\\\\\n0,&\\text{otherwise}.\\end{cases}$$", "word_idx": 18279, "sentence_idx": 278, "label": "unlabeled"}, {"type": "text", "expr": "Soft-inlier count", "word_idx": 18401, "sentence_idx": 279, "label": "unlabeled"}, {"type": "text", "expr": "The RANSAC inlier count cannot be used directly in a neural network as it is not\ndifferentiable", "word_idx": 18418, "sentence_idx": 280, "label": "unlabeled"}, {"type": "text", "expr": "\nFurthermore, in our setting there is no sparse set of matching points,\nbut rather a match score for every match in a discretized match space", "word_idx": 18513, "sentence_idx": 281, "label": "unlabeled"}, {"type": "text", "expr": "\nTherefore, we propose a direct extension, the  soft-inlier count ,\nwhich, instead of counting over a sparse set of matches,\nsums the match scores over all possible matches", "word_idx": 18654, "sentence_idx": 282, "label": "unlabeled"}, {"type": "text", "expr": "soft-inlier count", "word_idx": 18826, "sentence_idx": 283, "label": "unlabeled"}, {"type": "text", "expr": "The running line-fitting example can now be revisited under the\ndiscrete-space conditions, as illustrated in Figure\u2009 6 ", "word_idx": 18843, "sentence_idx": 284, "label": "unlabeled"}, {"type": "text", "expr": "\nThe proposed soft-inlier count for this case is:", "word_idx": 18962, "sentence_idx": 285, "label": "unlabeled"}, {"type": "text", "expr": "$c=\\sum_{i,j}s_{ij}m_{ij},$", "word_idx": 19011, "sentence_idx": 286, "label": "unlabeled"}, {"type": "math", "expr": "$$c=\\sum_{i,j}s_{ij}m_{ij},$$", "word_idx": 19038, "sentence_idx": 287, "label": "unlabeled"}, {"type": "text", "expr": "where  $s_{ij}$  is the match score at each grid point (i,j), and  $m_{ij}$  is the discretized inlier mask:", "word_idx": 19063, "sentence_idx": 288, "label": "unlabeled"}, {"type": "math", "expr": "$$s_{ij}$$", "word_idx": 19171, "sentence_idx": 289, "label": "unlabeled"}, {"type": "math", "expr": "$$m_{ij}$$", "word_idx": 19177, "sentence_idx": 290, "label": "unlabeled"}, {"type": "text", "expr": "$m_{ij}=\\begin{cases}1&\\text{if }\\text{d}\\big{(}(i,j),\\ell\\big{)}<t\\\\\n0&\\text{otherwise}\\end{cases}$", "word_idx": 19183, "sentence_idx": 291, "label": "unlabeled"}, {"type": "math", "expr": "$$m_{ij}=\\begin{cases}1&\\text{if }\\text{d}\\big{(}(i,j),\\ell\\big{)}<t\\\\\n0&\\text{otherwise.}\\end{cases}$$", "word_idx": 19283, "sentence_idx": 292, "label": "unlabeled"}, {"type": "text", "expr": "Translating the discrete-space line-fitting example to our semantic alignment problem,\n $s$  is a 4-D tensor containing scores for all pairwise feature matches between\nthe two images (Section\u00a0 3", "word_idx": 19382, "sentence_idx": 293, "label": "unlabeled"}, {"type": "text", "expr": "1 ), and\nmatches are deemed to be inliers if they fit the estimated geometric transformation  $g$ ", "word_idx": 19576, "sentence_idx": 294, "label": "unlabeled"}, {"type": "text", "expr": "\nMore formally, the inlier mask  $m$  is now also a 4-D tensor, constructed by thresholding\nthe reprojection error:", "word_idx": 19674, "sentence_idx": 295, "label": "unlabeled"}, {"type": "text", "expr": "$m_{ijkl}=\\begin{cases}1&\\text{if }\\text{d}\\big{(}(i,j),\\mathcal{T}_{g}(k,l)%\n\\big{)}<t\\\\\n0&\\text{otherwise},\\end{cases}$", "word_idx": 19789, "sentence_idx": 296, "label": "unlabeled"}, {"type": "math", "expr": "$$m_{ijkl}=\\begin{cases}1&\\text{if }\\text{d}\\big{(}(i,j),\\mathcal{T}_{g}(k,l)%\n\\big{)}<t\\\\\n0&\\text{otherwise},\\end{cases}$$", "word_idx": 19910, "sentence_idx": 297, "label": "unlabeled"}, {"type": "text", "expr": "where  $\\mathcal{T}_{g}(k,l)$  are the estimated coordinates of\ntarget image\u2019s point  $(k,l)$  in the source image according to the\ngeometric transformation  $g$ ;\n $\\text{d}\\big{(}(i,j),\\mathcal{T}_{g}(k,l)\\big{)}$  is the reprojection error\nas it measures how aligned is the point  $(i,j)$  in the source image,\nwith the projection of the target image point  $(k,l)$  into the source image", "word_idx": 20029, "sentence_idx": 298, "label": "unlabeled"}, {"type": "text", "expr": "\nThe soft-inlier count  $c$  is then computed by summing the masked matching scores over the entire space of matches:", "word_idx": 20420, "sentence_idx": 299, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{T}_{g}(k,l)$$", "word_idx": 20537, "sentence_idx": 300, "label": "unlabeled"}, {"type": "math", "expr": "$$(k,l)$$", "word_idx": 20557, "sentence_idx": 301, "label": "unlabeled"}, {"type": "math", "expr": "$$\\text{d}\\big{(}(i,j),\\mathcal{T}_{g}(k,l)\\big{)}$$", "word_idx": 20562, "sentence_idx": 302, "label": "unlabeled"}, {"type": "math", "expr": "$$(i,j)$$", "word_idx": 20610, "sentence_idx": 303, "label": "unlabeled"}, {"type": "math", "expr": "$$(k,l)$$", "word_idx": 20615, "sentence_idx": 304, "label": "unlabeled"}, {"type": "text", "expr": "$c=\\sum_{i,j,k,l}s_{ijkl}m_{ijkl}$", "word_idx": 20620, "sentence_idx": 305, "label": "unlabeled"}, {"type": "math", "expr": "$$c=\\sum_{i,j,k,l}s_{ijkl}m_{ijkl}.$$", "word_idx": 20654, "sentence_idx": 306, "label": "unlabeled"}, {"type": "text", "expr": "Differentiability", "word_idx": 20687, "sentence_idx": 307, "label": "unlabeled"}, {"type": "text", "expr": "The proposed soft-inlier count  $c$  is differentiable with respect to the transformation parameters  $g$  as long as the geometric transformation  $\\mathcal{T}_{g}$  is differentiable\u00a0 , which is the case for a range of standard geometric transformations such as 2D affine, homography or thin-plate spline transformations", "word_idx": 20704, "sentence_idx": 308, "label": "unlabeled"}, {"type": "text", "expr": "\nFurthermore, it is also differentiable w", "word_idx": 21026, "sentence_idx": 309, "label": "unlabeled"}, {"type": "text", "expr": " the match scores, which facilitates\ntraining of the feature extractor", "word_idx": 21067, "sentence_idx": 310, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{T}_{g}$$", "word_idx": 21137, "sentence_idx": 311, "label": "unlabeled"}, {"type": "text", "expr": "Implementation as a CNN layer", "word_idx": 21152, "sentence_idx": 312, "label": "unlabeled"}, {"type": "text", "expr": "The inlier mask  $m$  can be computed by warping an identity mask  $m^{\\mathit{Id}}$  with the estimated transformation  $\\mathcal{T}_{g}$ , where  $m^{\\mathit{Id}}$  is constructed by thresholding the reprojection error of the identity transformation:", "word_idx": 21181, "sentence_idx": 313, "label": "unlabeled"}, {"type": "math", "expr": "$$m^{\\mathit{Id}}$$", "word_idx": 21433, "sentence_idx": 314, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{T}_{g}$$", "word_idx": 21448, "sentence_idx": 315, "label": "unlabeled"}, {"type": "math", "expr": "$$m^{\\mathit{Id}}$$", "word_idx": 21463, "sentence_idx": 316, "label": "unlabeled"}, {"type": "text", "expr": "$m^{\\mathit{Id}}_{ijkl}=\\begin{cases}1&\\text{d}\\big{(}(i,j),(k,l)\\big{)}<t\\\\\n0&\\text{otherwise}\\end{cases}$", "word_idx": 21478, "sentence_idx": 317, "label": "unlabeled"}, {"type": "math", "expr": "$$m^{\\mathit{Id}}_{ijkl}=\\begin{cases}1&\\text{d}\\big{(}(i,j),(k,l)\\big{)}<t\\\\\n0&\\text{otherwise}.\\end{cases}$$", "word_idx": 21585, "sentence_idx": 318, "label": "unlabeled"}, {"type": "text", "expr": "The warping is implemented using a spatial transformer layer\u00a0 , which consists of a grid generation layer and a bilinear sampling layer", "word_idx": 21691, "sentence_idx": 319, "label": "unlabeled"}, {"type": "text", "expr": " Both of these functions are readily available in most deep learning frameworks", "word_idx": 21826, "sentence_idx": 320, "label": "unlabeled"}, {"type": "text", "expr": "Optimization objective", "word_idx": 21905, "sentence_idx": 321, "label": "unlabeled"}, {"type": "text", "expr": "For a given training pair of images that should match, the goal is to maximize\nthe soft-inlier count  $c$ , or, equivalently, to minimize the loss  $\\mathcal{L}=-c$ ", "word_idx": 21927, "sentence_idx": 322, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{L}=-c$$", "word_idx": 22092, "sentence_idx": 323, "label": "unlabeled"}, {"type": "text", "expr": "4  Evaluation and results", "word_idx": 22106, "sentence_idx": 324, "label": "unlabeled"}, {"type": "text", "expr": "In this section we provide implementation details,\nbenchmarks used to evaluate our approach, and quantitative and qualitative results", "word_idx": 22131, "sentence_idx": 325, "label": "unlabeled"}, {"type": "text", "expr": "In this section we provide implementation details,\nbenchmarks used to evaluate our approach, and quantitative and qualitative results", "word_idx": 22264, "sentence_idx": 326, "label": "unlabeled"}, {"type": "text", "expr": "1  Implementation details", "word_idx": 22397, "sentence_idx": 327, "label": "unlabeled"}, {"type": "text", "expr": "Semantic alignment network", "word_idx": 22422, "sentence_idx": 328, "label": "unlabeled"}, {"type": "text", "expr": "For the underlying semantic alignment network,\nwe use the best-performing architecture from\u00a0  which employs a ResNet-101\u00a0 , cropped after  conv4-23 , as the feature extraction CNN  $F$ ", "word_idx": 22448, "sentence_idx": 329, "label": "unlabeled"}, {"type": "text", "expr": "\nNote that this is a better performing model than the one described in the original paper\u00a0 , mainly due to use of ResNet versus VGG-16\u00a0 ", "word_idx": 22633, "sentence_idx": 330, "label": "unlabeled"}, {"type": "text", "expr": "\nGiven an image pair, the model produces a thin-plate spline geometric transformation  $\\mathcal{T}_{g}$  which aligns the two images;\n $\\mathcal{T}_{g}$  has 18 degrees of freedom", "word_idx": 22769, "sentence_idx": 331, "label": "unlabeled"}, {"type": "text", "expr": "\nThe network is initialized with the pre-trained weights from\u00a0 ,\nand we finetune it with our weakly supervised method", "word_idx": 22949, "sentence_idx": 332, "label": "unlabeled"}, {"type": "text", "expr": "\nNote that the initial model has been trained from synthetic data without human\nsupervision  , therefore not affecting our claim of\nweakly supervised training", "word_idx": 23066, "sentence_idx": 333, "label": "unlabeled"}, {"type": "text", "expr": "conv4-23", "word_idx": 23224, "sentence_idx": 334, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{T}_{g}$$", "word_idx": 23232, "sentence_idx": 335, "label": "unlabeled"}, {"type": "math", "expr": "$$\\mathcal{T}_{g}$$", "word_idx": 23247, "sentence_idx": 336, "label": "unlabeled"}, {"type": "text", "expr": "Training details", "word_idx": 23262, "sentence_idx": 337, "label": "unlabeled"}, {"type": "text", "expr": "Training and validation image pairs are obtained from the training set of PF-PASCAL, described in Section\u00a0 4", "word_idx": 23278, "sentence_idx": 338, "label": "unlabeled"}, {"type": "text", "expr": "2 ;\nall images are resized to  $240\\times 240$ ", "word_idx": 23386, "sentence_idx": 339, "label": "unlabeled"}, {"type": "text", "expr": "\nThe whole model is trained end-to-end, including the affine parameters in the batch normalization layers", "word_idx": 23433, "sentence_idx": 340, "label": "unlabeled"}, {"type": "text", "expr": " However, the running averages of the batch normalization layers are kept fixed, in order to be less dependent on the particular statistics of the training dataset", "word_idx": 23538, "sentence_idx": 341, "label": "unlabeled"}, {"type": "text", "expr": "\nThe network is implemented in the PyTorch deep learning framework\u00a0 \nand trained using\nthe Adam optimizer\u00a0  with learning rate  $5\\cdot 10^{-8}$ , no weight decay and batch size of 16", "word_idx": 23701, "sentence_idx": 342, "label": "unlabeled"}, {"type": "text", "expr": "\nThe training dataset is augmented by horizontal flipping, swapping the source and target images, and random cropping", "word_idx": 23884, "sentence_idx": 343, "label": "unlabeled"}, {"type": "text", "expr": " Early stopping is used to avoid overfitting, which results in\n13 training epochs, taking about an hour on a modern GPU", "word_idx": 24001, "sentence_idx": 344, "label": "unlabeled"}, {"type": "math", "expr": "$$240\\times 240$$", "word_idx": 24120, "sentence_idx": 345, "label": "unlabeled"}, {"type": "math", "expr": "$$5\\cdot 10^{-8}$$", "word_idx": 24133, "sentence_idx": 346, "label": "unlabeled"}, {"type": "text", "expr": "2  Evaluation benchmarks", "word_idx": 24147, "sentence_idx": 347, "label": "unlabeled"}, {"type": "text", "expr": "Evaluation is performed on three standard image alignment benchmarks: PF-PASCAL, Caltech-101 and TSS", "word_idx": 24171, "sentence_idx": 348, "label": "unlabeled"}, {"type": "text", "expr": "Evaluation is performed on three standard image alignment benchmarks: PF-PASCAL, Caltech-101 and TSS", "word_idx": 24271, "sentence_idx": 349, "label": "unlabeled"}, {"type": "text", "expr": "PF-PASCAL\u00a0 ", "word_idx": 24371, "sentence_idx": 350, "label": "unlabeled"}, {"type": "text", "expr": "This dataset contains 1351 semantically related image pairs from 20 object categories, which present challenging appearance differences and background clutter", "word_idx": 24382, "sentence_idx": 351, "label": "unlabeled"}, {"type": "text", "expr": " We use the split proposed in\u00a0 , which divides the dataset into roughly 700 pairs for training, 300 pairs for validation, and 300 pairs for testing", "word_idx": 24540, "sentence_idx": 352, "label": "unlabeled"}, {"type": "text", "expr": " Keypoint annotations are provided for each image pair, which are used only for evaluation purposes", "word_idx": 24687, "sentence_idx": 353, "label": "unlabeled"}, {"type": "text", "expr": "\nAlignment quality is evaluated in terms of the percentage of correct keypoints (PCK) metric\u00a0 , which counts the number of keypoints which have a reprojection error below a given threshold", "word_idx": 24786, "sentence_idx": 354, "label": "unlabeled"}, {"type": "text", "expr": " This distance threshold is defined as  $\\alpha\\,\\text{max}(w^{s}_{b},h^{s}_{b})$  with  $(w^{s}_{b},h^{s}_{b})$  being the side of the annotated object bounding box on the source image, and  $\\alpha=01$ ", "word_idx": 24974, "sentence_idx": 355, "label": "unlabeled"}, {"type": "math", "expr": "$$\\alpha\\,\\text{max}(w^{s}_{b},h^{s}_{b})$$", "word_idx": 25178, "sentence_idx": 356, "label": "unlabeled"}, {"type": "math", "expr": "$$(w^{s}_{b},h^{s}_{b})$$", "word_idx": 25217, "sentence_idx": 357, "label": "unlabeled"}, {"type": "math", "expr": "$$\\alpha=0.1$$", "word_idx": 25238, "sentence_idx": 358, "label": "unlabeled"}, {"type": "text", "expr": "aero bike bird boat bottle bus car cat chair cow d", "word_idx": 25248, "sentence_idx": 359, "label": "unlabeled"}, {"type": "text", "expr": "table dog horse moto person plant sheep sofa train tv", "word_idx": 25298, "sentence_idx": 360, "label": "unlabeled"}, {"type": "text", "expr": "\\bigstrut [tb]\nMethod", "word_idx": 25351, "sentence_idx": 361, "label": "unlabeled"}, {"type": "text", "expr": "\\bigstrut", "word_idx": 25372, "sentence_idx": 362, "label": "unlabeled"}, {"type": "text", "expr": "[tb]\nMethod", "word_idx": 25381, "sentence_idx": 363, "label": "unlabeled"}, {"type": "text", "expr": "bottle", "word_idx": 25392, "sentence_idx": 364, "label": "unlabeled"}, {"type": "text", "expr": "bottle", "word_idx": 25398, "sentence_idx": 365, "label": "unlabeled"}, {"type": "text", "expr": "chair", "word_idx": 25404, "sentence_idx": 366, "label": "unlabeled"}, {"type": "text", "expr": "chair", "word_idx": 25409, "sentence_idx": 367, "label": "unlabeled"}, {"type": "text", "expr": "table", "word_idx": 25414, "sentence_idx": 368, "label": "unlabeled"}, {"type": "text", "expr": "table", "word_idx": 25419, "sentence_idx": 369, "label": "unlabeled"}, {"type": "text", "expr": "horse", "word_idx": 25424, "sentence_idx": 370, "label": "unlabeled"}, {"type": "text", "expr": "horse", "word_idx": 25429, "sentence_idx": 371, "label": "unlabeled"}, {"type": "text", "expr": "person", "word_idx": 25434, "sentence_idx": 372, "label": "unlabeled"}, {"type": "text", "expr": "person", "word_idx": 25440, "sentence_idx": 373, "label": "unlabeled"}, {"type": "text", "expr": "plant", "word_idx": 25446, "sentence_idx": 374, "label": "unlabeled"}, {"type": "text", "expr": "plant", "word_idx": 25451, "sentence_idx": 375, "label": "unlabeled"}, {"type": "text", "expr": "sheep", "word_idx": 25456, "sentence_idx": 376, "label": "unlabeled"}, {"type": "text", "expr": "sheep", "word_idx": 25461, "sentence_idx": 377, "label": "unlabeled"}, {"type": "text", "expr": "train", "word_idx": 25466, "sentence_idx": 378, "label": "unlabeled"}, {"type": "text", "expr": "train", "word_idx": 25471, "sentence_idx": 379, "label": "unlabeled"}, {"type": "text", "expr": "\\bigstrut [t]\nHOG+PF-LOM", "word_idx": 25476, "sentence_idx": 380, "label": "unlabeled"}, {"type": "text", "expr": "\\bigstrut", "word_idx": 25500, "sentence_idx": 381, "label": "unlabeled"}, {"type": "text", "expr": "[t]\nHOG+PF-LOM", "word_idx": 25509, "sentence_idx": 382, "label": "unlabeled"}, {"type": "text", "expr": "VGG-16+SCNet-A", "word_idx": 25523, "sentence_idx": 383, "label": "unlabeled"}, {"type": "text", "expr": "VGG-16+SCNet-A", "word_idx": 25537, "sentence_idx": 384, "label": "unlabeled"}, {"type": "text", "expr": "VGG-16+SCNet-AG", "word_idx": 25551, "sentence_idx": 385, "label": "unlabeled"}, {"type": "text", "expr": "VGG-16+SCNet-AG", "word_idx": 25566, "sentence_idx": 386, "label": "unlabeled"}, {"type": "text", "expr": "VGG-16+SCNet-AG+", "word_idx": 25581, "sentence_idx": 387, "label": "unlabeled"}, {"type": "text", "expr": "VGG-16+SCNet-AG+", "word_idx": 25597, "sentence_idx": 388, "label": "unlabeled"}, {"type": "text", "expr": "VGG-16+CNNGeo", "word_idx": 25613, "sentence_idx": 389, "label": "unlabeled"}, {"type": "text", "expr": "VGG-16+CNNGeo", "word_idx": 25626, "sentence_idx": 390, "label": "unlabeled"}, {"type": "text", "expr": "ResNet-101+CNNGeo", "word_idx": 25639, "sentence_idx": 391, "label": "unlabeled"}, {"type": "text", "expr": "ResNet-101+CNNGeo", "word_idx": 25656, "sentence_idx": 392, "label": "unlabeled"}, {"type": "text", "expr": "\\bigstrut [b]\nProposed", "word_idx": 25673, "sentence_idx": 393, "label": "unlabeled"}, {"type": "text", "expr": "\\bigstrut", "word_idx": 25695, "sentence_idx": 394, "label": "unlabeled"}, {"type": "text", "expr": "[b]\nProposed", "word_idx": 25704, "sentence_idx": 395, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:  Per-class PCK on the PF-PASCAL dataset", "word_idx": 25716, "sentence_idx": 396, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a02:", "word_idx": 25764, "sentence_idx": 397, "label": "unlabeled"}, {"type": "text", "expr": "Per-class PCK on the PF-PASCAL dataset", "word_idx": 25772, "sentence_idx": 398, "label": "unlabeled"}, {"type": "text", "expr": "Caltech-101\u00a0 ", "word_idx": 25810, "sentence_idx": 399, "label": "unlabeled"}, {"type": "text", "expr": "Although originally introduced for the image classification task, the dataset was adopted in\u00a0  for assessing semantic alignment, and has been then extensively used for this purpose\u00a0 ", "word_idx": 25823, "sentence_idx": 400, "label": "unlabeled"}, {"type": "text", "expr": " The evaluation is performed on 1515 semantically related image pairs, 15 pairs for each of the 101 object categories of the dataset", "word_idx": 26005, "sentence_idx": 401, "label": "unlabeled"}, {"type": "text", "expr": "\nThe semantic alignment is evaluated using three different metrics: (i) the label transfer accuracy (LT-ACC); (ii) the intersection-over-union (IoU), and; (iii) the object localization error (LOC-ERR)", "word_idx": 26137, "sentence_idx": 402, "label": "unlabeled"}, {"type": "text", "expr": " The label transfer accuracy and the intersection-over-union both measure the overlap between the annotated foreground object segmentation masks, with former putting more emphasis on the background class and the latter on the foreground object", "word_idx": 26337, "sentence_idx": 403, "label": "unlabeled"}, {"type": "text", "expr": " The localization error computes a dense displacement error", "word_idx": 26580, "sentence_idx": 404, "label": "unlabeled"}, {"type": "text", "expr": " However, given the lack of dense displacement annotations, the metric computes the ground-truth transformation\nfrom the source and target bounding boxes, thus assuming that the transformation\nis a simple translation with axis-aligned anisotropic scaling", "word_idx": 26639, "sentence_idx": 405, "label": "unlabeled"}, {"type": "text", "expr": "\nThis assumption is unrealistic as, amongst others, it does not cover rotations, affine or deformable transformations", "word_idx": 26893, "sentence_idx": 406, "label": "unlabeled"}, {"type": "text", "expr": "\nTherefore, we believe that LOC-ERR should not be reported any more, but report it here for completeness and in order to adhere to the currently adopted evaluation protocol", "word_idx": 27010, "sentence_idx": 407, "label": "unlabeled"}, {"type": "text", "expr": "TSS\u00a0 ", "word_idx": 27182, "sentence_idx": 408, "label": "unlabeled"}, {"type": "text", "expr": "The recently introduced TSS dataset contains 400 semantically related image pairs, which are split into three different subsets: FG3DCar, JODS and PASCAL, according to the origin of the images", "word_idx": 27187, "sentence_idx": 409, "label": "unlabeled"}, {"type": "text", "expr": " Ground-truth flow is provided for each pair, which was obtained by manual annotation of sparse keypoints, followed by automatic densification using an interpolation algorithm", "word_idx": 27379, "sentence_idx": 410, "label": "unlabeled"}, {"type": "text", "expr": " The evaluation metric is the PCK computed densely over the foreground object", "word_idx": 27554, "sentence_idx": 411, "label": "unlabeled"}, {"type": "text", "expr": " The distance threshold is defined as  $\\alpha\\,\\text{max}(w^{s},h^{s})$  with  $(w^{s},h^{s})$  being the dimensions of the source image, and  $\\alpha=005$ ", "word_idx": 27631, "sentence_idx": 412, "label": "unlabeled"}, {"type": "math", "expr": "$$\\alpha\\,\\text{max}(w^{s},h^{s})$$", "word_idx": 27788, "sentence_idx": 413, "label": "unlabeled"}, {"type": "math", "expr": "$$(w^{s},h^{s})$$", "word_idx": 27819, "sentence_idx": 414, "label": "unlabeled"}, {"type": "math", "expr": "$$\\alpha=0.05$$", "word_idx": 27832, "sentence_idx": 415, "label": "unlabeled"}, {"type": "text", "expr": "Assessing generalization", "word_idx": 27843, "sentence_idx": 416, "label": "unlabeled"}, {"type": "text", "expr": "We train a single semantic alignment network with the 700 training pairs from PF-PASCAL  without  using the keypoint annotations,\nand stress that our weakly-supervised training objective only uses\nthe information that the image pair should match", "word_idx": 27867, "sentence_idx": 417, "label": "unlabeled"}, {"type": "text", "expr": "\n The same  model is then used for all experiments \u2013 evaluation on the\ntest sets of PF-PASCAL, Caltech-101 and TSS datasets", "word_idx": 28112, "sentence_idx": 418, "label": "unlabeled"}, {"type": "text", "expr": "\nThis poses an additional difficulty as these datasets contain images of different object categories or of different nature", "word_idx": 28235, "sentence_idx": 419, "label": "unlabeled"}, {"type": "text", "expr": " While PF-PASCAL contains images of common objects such as car, bicycle, boat,  \\etc , Caltech-101 contains images of much less common categories such as accordion, buddha or octopus", "word_idx": 28358, "sentence_idx": 420, "label": "unlabeled"}, {"type": "text", "expr": " On the other hand, while the classes of TSS do appear in PF-PASCAL,\nthe pose differences in TSS are usually smaller than in PF-PASCAL, which modifies the challenge into obtaining a very precise alignment", "word_idx": 28540, "sentence_idx": 421, "label": "unlabeled"}, {"type": "text", "expr": "without", "word_idx": 28744, "sentence_idx": 422, "label": "unlabeled"}, {"type": "text", "expr": "The same", "word_idx": 28751, "sentence_idx": 423, "label": "unlabeled"}, {"type": "text", "expr": "3  Results", "word_idx": 28759, "sentence_idx": 424, "label": "unlabeled"}, {"type": "text", "expr": "In the following, our alignment network trained with  weak supervision \nis compared to the state-of-the-art alignment methods, many of which require\n manual annotations  or  strong supervision \n( \\cf Table\u00a0 1 )", "word_idx": 28769, "sentence_idx": 425, "label": "unlabeled"}, {"type": "text", "expr": "weak supervision", "word_idx": 28979, "sentence_idx": 426, "label": "unlabeled"}, {"type": "text", "expr": "manual annotations", "word_idx": 28995, "sentence_idx": 427, "label": "unlabeled"}, {"type": "text", "expr": "strong supervision", "word_idx": 29013, "sentence_idx": 428, "label": "unlabeled"}, {"type": "text", "expr": "PF-PASCAL", "word_idx": 29031, "sentence_idx": 429, "label": "unlabeled"}, {"type": "text", "expr": "From Table  2  it is clear\nthat our method sets the new state-of-the-art, achieving an overall PCK of 74", "word_idx": 29040, "sentence_idx": 430, "label": "unlabeled"}, {"type": "text", "expr": "8%,\nwhich is a 2", "word_idx": 29144, "sentence_idx": 431, "label": "unlabeled"}, {"type": "text", "expr": "6% improvement over the best competitor  ", "word_idx": 29160, "sentence_idx": 432, "label": "unlabeled"}, {"type": "text", "expr": "\nThis result is impressive as the two methods are trained on the same image pairs,\nwith ours being weakly supervised while   makes use of\nbounding box annotations", "word_idx": 29201, "sentence_idx": 433, "label": "unlabeled"}, {"type": "text", "expr": "The benefits of weakly supervised training can be seen by comparing our method\nwith ResNet-101+CNNGeo  ", "word_idx": 29363, "sentence_idx": 434, "label": "unlabeled"}, {"type": "text", "expr": "\nThe two use the same base alignment network ( \\cf Section\u00a0 4", "word_idx": 29466, "sentence_idx": 435, "label": "unlabeled"}, {"type": "text", "expr": "1 ),\nbut ResNet-101+CNNGeo was trained only on synthetically deformed image pairs,\nwhile ours employs the proposed weakly supervised fine-tuning", "word_idx": 29527, "sentence_idx": 436, "label": "unlabeled"}, {"type": "text", "expr": "\nThe 5", "word_idx": 29671, "sentence_idx": 437, "label": "unlabeled"}, {"type": "text", "expr": "3% boost clearly demonstrates the advantage obtained by training\non real image pairs and thus encountering rich appearance variations,\nas opposed to using synthetically transformed pairs in ResNet-101+CNNGeo  ", "word_idx": 29677, "sentence_idx": 438, "label": "unlabeled"}, {"type": "text", "expr": "Method LT-ACC IoU LOC-ERR", "word_idx": 29886, "sentence_idx": 439, "label": "unlabeled"}, {"type": "text", "expr": "Method", "word_idx": 29911, "sentence_idx": 440, "label": "unlabeled"}, {"type": "text", "expr": "LT-ACC", "word_idx": 29917, "sentence_idx": 441, "label": "unlabeled"}, {"type": "text", "expr": "LOC-ERR", "word_idx": 29923, "sentence_idx": 442, "label": "unlabeled"}, {"type": "text", "expr": "HOG+PF-LOM", "word_idx": 29930, "sentence_idx": 443, "label": "unlabeled"}, {"type": "text", "expr": "FCSS+SIFT Flow", "word_idx": 29940, "sentence_idx": 444, "label": "unlabeled"}, {"type": "text", "expr": "FCSS+PF-LOM", "word_idx": 29954, "sentence_idx": 445, "label": "unlabeled"}, {"type": "text", "expr": "VGG-16+SCNet-A", "word_idx": 29965, "sentence_idx": 446, "label": "unlabeled"}, {"type": "text", "expr": "VGG-16+SCNet-AG", "word_idx": 29979, "sentence_idx": 447, "label": "unlabeled"}, {"type": "text", "expr": "VGG-16+SCNet-AG+", "word_idx": 29994, "sentence_idx": 448, "label": "unlabeled"}, {"type": "text", "expr": "HOG+OADSC", "word_idx": 30010, "sentence_idx": 449, "label": "unlabeled"}, {"type": "text", "expr": "VGG-16+CNNGeo", "word_idx": 30019, "sentence_idx": 450, "label": "unlabeled"}, {"type": "text", "expr": "ResNet-101+CNNGeo", "word_idx": 30032, "sentence_idx": 451, "label": "unlabeled"}, {"type": "text", "expr": "Proposed 0", "word_idx": 30049, "sentence_idx": 452, "label": "unlabeled"}, {"type": "text", "expr": "Proposed", "word_idx": 30059, "sentence_idx": 453, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a03:  Results on the Caltech-101 dataset", "word_idx": 30067, "sentence_idx": 454, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a03:", "word_idx": 30111, "sentence_idx": 455, "label": "unlabeled"}, {"type": "text", "expr": "Results on the Caltech-101 dataset", "word_idx": 30119, "sentence_idx": 456, "label": "unlabeled"}, {"type": "text", "expr": "Caltech-101", "word_idx": 30153, "sentence_idx": 457, "label": "unlabeled"}, {"type": "text", "expr": "Table  3  presents the quantitative results for this dataset", "word_idx": 30164, "sentence_idx": 458, "label": "unlabeled"}, {"type": "text", "expr": "\nThe proposed method beats state-of-the-art results in terms of the label-transfer accuracy and intersection-over-union metrics", "word_idx": 30224, "sentence_idx": 459, "label": "unlabeled"}, {"type": "text", "expr": "\nWeakly supervised training again improves the results, by 2%, over the synthetically trained ResNet-101+CNNGeo", "word_idx": 30351, "sentence_idx": 460, "label": "unlabeled"}, {"type": "text", "expr": "\nIn terms of the localization-error metric, our model does not attain state-of-the-art performance,\nbut we argue that this metric is not a good indication of the alignment quality, as explained in section  4", "word_idx": 30462, "sentence_idx": 461, "label": "unlabeled"}, {"type": "text", "expr": "\nThis claim is further backed up by noticing that the relative ordering of various methods\nbased on this metric is in direct opposition with the other two metrics", "word_idx": 30669, "sentence_idx": 462, "label": "unlabeled"}, {"type": "text", "expr": "The quantitative results for the TSS dataset are presented in Table  4 ", "word_idx": 30831, "sentence_idx": 463, "label": "unlabeled"}, {"type": "text", "expr": "\nWe set the state-of-the-art for two of the three subsets of the TSS dataset: FG3DCar and JODS", "word_idx": 30902, "sentence_idx": 464, "label": "unlabeled"}, {"type": "text", "expr": "\nAlthough our weakly supervised training provides an improvement over the base alignment network, ResNet-101+CNNGeo, the gain is modest", "word_idx": 30996, "sentence_idx": 465, "label": "unlabeled"}, {"type": "text", "expr": "\nWe believe the reason is a very different balancing of classes in this dataset compared to our training", "word_idx": 31131, "sentence_idx": 466, "label": "unlabeled"}, {"type": "text", "expr": " Recall our model is trained  only once  on the TF-PASCAL dataset,\nand is then applied without any further training on TSS and Caltech-101", "word_idx": 31235, "sentence_idx": 467, "label": "unlabeled"}, {"type": "text", "expr": "only once", "word_idx": 31373, "sentence_idx": 468, "label": "unlabeled"}, {"type": "text", "expr": "Qualitative results", "word_idx": 31382, "sentence_idx": 469, "label": "unlabeled"}, {"type": "text", "expr": "Figures\u00a0 9  and\u00a0 11  show qualitative results on the Caltech-101 dataset, figures\u00a0 9  and\u00a0 12  on TSS, and figures\u00a0 10  and\u00a0 13  on PF-PASCAL", "word_idx": 31401, "sentence_idx": 470, "label": "unlabeled"}, {"type": "text", "expr": "\nOur method is able to align images across prominent viewpoint changes,\nin the presence of significant clutter,\nwhile simultaneously tolerating large intra-class variations", "word_idx": 31542, "sentence_idx": 471, "label": "unlabeled"}, {"type": "text", "expr": "{subfigure} {subfigure}", "word_idx": 31714, "sentence_idx": 472, "label": "unlabeled"}, {"type": "text", "expr": "{subfigure}", "word_idx": 31737, "sentence_idx": 473, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a07:  Caltech-101", "word_idx": 31748, "sentence_idx": 474, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a07:", "word_idx": 31770, "sentence_idx": 475, "label": "unlabeled"}, {"type": "text", "expr": "{subfigure}", "word_idx": 31779, "sentence_idx": 476, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a08:  TSS", "word_idx": 31790, "sentence_idx": 477, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a08:", "word_idx": 31804, "sentence_idx": 478, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a09:  Alignment examples on the Caltech-101 and TSS datasets", "word_idx": 31813, "sentence_idx": 479, "label": "unlabeled"}, {"type": "text", "expr": "  Each row shows the (left) source and (middle) target images, and (right) the automatic semantic alignment", "word_idx": 31878, "sentence_idx": 480, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a09:", "word_idx": 31985, "sentence_idx": 481, "label": "unlabeled"}, {"type": "text", "expr": "Alignment examples on the Caltech-101 and TSS datasets", "word_idx": 31994, "sentence_idx": 482, "label": "unlabeled"}, {"type": "text", "expr": "(a) Semantic alignment (b) Strongest inlier matches", "word_idx": 32048, "sentence_idx": 483, "label": "unlabeled"}, {"type": "text", "expr": "(b) Strongest inlier matches", "word_idx": 32099, "sentence_idx": 484, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a010:  Alignment examples on the PF-PASCAL dataset", "word_idx": 32127, "sentence_idx": 485, "label": "unlabeled"}, {"type": "text", "expr": "  Each row corresponds to one example", "word_idx": 32182, "sentence_idx": 486, "label": "unlabeled"}, {"type": "text", "expr": " (a) shows the (right) automatic semantic alignment of the (left) source and (middle) target images", "word_idx": 32219, "sentence_idx": 487, "label": "unlabeled"}, {"type": "text", "expr": " (b) shows the strongest inlier feature matches", "word_idx": 32318, "sentence_idx": 488, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a010:", "word_idx": 32365, "sentence_idx": 489, "label": "unlabeled"}, {"type": "text", "expr": "Alignment examples on the PF-PASCAL dataset", "word_idx": 32375, "sentence_idx": 490, "label": "unlabeled"}, {"type": "text", "expr": "Method FG3D", "word_idx": 32418, "sentence_idx": 491, "label": "unlabeled"}, {"type": "text", "expr": " JODS PASC", "word_idx": 32429, "sentence_idx": 492, "label": "unlabeled"}, {"type": "text", "expr": "Method", "word_idx": 32439, "sentence_idx": 493, "label": "unlabeled"}, {"type": "text", "expr": "786 0", "word_idx": 32445, "sentence_idx": 494, "label": "unlabeled"}, {"type": "text", "expr": "653 0", "word_idx": 32450, "sentence_idx": 495, "label": "unlabeled"}, {"type": "text", "expr": "531 0", "word_idx": 32455, "sentence_idx": 496, "label": "unlabeled"}, {"type": "text", "expr": "HOG+PF-LOM", "word_idx": 32460, "sentence_idx": 497, "label": "unlabeled"}, {"type": "text", "expr": "830 0", "word_idx": 32470, "sentence_idx": 498, "label": "unlabeled"}, {"type": "text", "expr": "595 0", "word_idx": 32475, "sentence_idx": 499, "label": "unlabeled"}, {"type": "text", "expr": "483 0", "word_idx": 32480, "sentence_idx": 500, "label": "unlabeled"}, {"type": "text", "expr": "HOG+TSS", "word_idx": 32485, "sentence_idx": 501, "label": "unlabeled"}, {"type": "text", "expr": "830 0", "word_idx": 32492, "sentence_idx": 502, "label": "unlabeled"}, {"type": "text", "expr": "656 0", "word_idx": 32497, "sentence_idx": 503, "label": "unlabeled"}, {"type": "text", "expr": "494 0", "word_idx": 32502, "sentence_idx": 504, "label": "unlabeled"}, {"type": "text", "expr": "FCSS+SIFT Flow", "word_idx": 32507, "sentence_idx": 505, "label": "unlabeled"}, {"type": "text", "expr": "839 0", "word_idx": 32521, "sentence_idx": 506, "label": "unlabeled"}, {"type": "text", "expr": "635 0", "word_idx": 32526, "sentence_idx": 507, "label": "unlabeled"}, {"type": "text", "expr": "582 0", "word_idx": 32531, "sentence_idx": 508, "label": "unlabeled"}, {"type": "text", "expr": "FCSS+PF-LOM", "word_idx": 32536, "sentence_idx": 509, "label": "unlabeled"}, {"type": "text", "expr": "875 0", "word_idx": 32547, "sentence_idx": 510, "label": "unlabeled"}, {"type": "text", "expr": "708 0", "word_idx": 32552, "sentence_idx": 511, "label": "unlabeled"}, {"type": "text", "expr": "HOG+OADSC", "word_idx": 32557, "sentence_idx": 512, "label": "unlabeled"}, {"type": "text", "expr": "891 0", "word_idx": 32566, "sentence_idx": 513, "label": "unlabeled"}, {"type": "text", "expr": "721 0", "word_idx": 32571, "sentence_idx": 514, "label": "unlabeled"}, {"type": "text", "expr": "610 0", "word_idx": 32576, "sentence_idx": 515, "label": "unlabeled"}, {"type": "text", "expr": "FCSS+DCTM", "word_idx": 32581, "sentence_idx": 516, "label": "unlabeled"}, {"type": "text", "expr": "835 0", "word_idx": 32590, "sentence_idx": 517, "label": "unlabeled"}, {"type": "text", "expr": "656 0", "word_idx": 32595, "sentence_idx": 518, "label": "unlabeled"}, {"type": "text", "expr": "527 0", "word_idx": 32600, "sentence_idx": 519, "label": "unlabeled"}, {"type": "text", "expr": "VGG-16+CNNGeo", "word_idx": 32605, "sentence_idx": 520, "label": "unlabeled"}, {"type": "text", "expr": "886 0", "word_idx": 32618, "sentence_idx": 521, "label": "unlabeled"}, {"type": "text", "expr": "560 0", "word_idx": 32623, "sentence_idx": 522, "label": "unlabeled"}, {"type": "text", "expr": "ResNet-101+CNNGeo", "word_idx": 32628, "sentence_idx": 523, "label": "unlabeled"}, {"type": "text", "expr": "Proposed 0", "word_idx": 32645, "sentence_idx": 524, "label": "unlabeled"}, {"type": "text", "expr": "562 0", "word_idx": 32655, "sentence_idx": 525, "label": "unlabeled"}, {"type": "text", "expr": "Proposed", "word_idx": 32660, "sentence_idx": 526, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a04:  Results on the TSS dataset", "word_idx": 32668, "sentence_idx": 527, "label": "unlabeled"}, {"type": "text", "expr": "Table\u00a04:", "word_idx": 32704, "sentence_idx": 528, "label": "unlabeled"}, {"type": "text", "expr": "Results on the TSS dataset", "word_idx": 32712, "sentence_idx": 529, "label": "unlabeled"}, {"type": "text", "expr": "5  Conclusions", "word_idx": 32738, "sentence_idx": 530, "label": "unlabeled"}, {"type": "text", "expr": "We have designed a network architecture and training procedure\nfor semantic image alignment inspired by the robust inlier scoring used in the widely successful RANSAC fitting algorithm\u00a0 ", "word_idx": 32752, "sentence_idx": 531, "label": "unlabeled"}, {"type": "text", "expr": " The architecture requires supervision only in the form of matching\nimage pairs\nand sets the new state-of-the-art on multiple standard semantic alignment benchmarks, even beating alignment methods that require geometric supervision at training time", "word_idx": 32938, "sentence_idx": 532, "label": "unlabeled"}, {"type": "text", "expr": "\nThe results open-up the possibility of learning powerful correspondence networks from large-scale datasets such as ImageNet", "word_idx": 33186, "sentence_idx": 533, "label": "unlabeled"}, {"type": "text", "expr": "Acknowledgements", "word_idx": 33310, "sentence_idx": 534, "label": "unlabeled"}, {"type": "text", "expr": "This work has been partly supported by ERC grant LEAP (no", "word_idx": 33326, "sentence_idx": 535, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0336845), the Inria CityLab IPL, CIFAR Learning in Machines  $\\&$  Brains program and ESIF, OP Research, development and education Project IMPACT No", "word_idx": 33383, "sentence_idx": 536, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0CZ $02101/00/00/15\\_003/0000468$ ", "word_idx": 33531, "sentence_idx": 537, "label": "unlabeled"}, {"type": "text", "expr": "This work has been partly supported by ERC grant LEAP (no", "word_idx": 33565, "sentence_idx": 538, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0336845), the Inria CityLab IPL, CIFAR Learning in Machines", "word_idx": 33622, "sentence_idx": 539, "label": "unlabeled"}, {"type": "text", "expr": "Brains program and ESIF, OP Research, development and education Project IMPACT No", "word_idx": 33681, "sentence_idx": 540, "label": "unlabeled"}, {"type": "math", "expr": "$$.02.1.01/0.0/0.0/15\\_003/0000468$$", "word_idx": 33762, "sentence_idx": 541, "label": "unlabeled"}, {"type": "text", "expr": "References", "word_idx": 33794, "sentence_idx": 542, "label": "unlabeled"}, {"type": "text", "expr": "PyTorch", "word_idx": 33804, "sentence_idx": 543, "label": "unlabeled"}, {"type": "text", "expr": "PyTorch", "word_idx": 33811, "sentence_idx": 544, "label": "unlabeled"}, {"type": "text", "expr": "http://pytorch", "word_idx": 33818, "sentence_idx": 545, "label": "unlabeled"}, {"type": "text", "expr": "http://pytorch", "word_idx": 33832, "sentence_idx": 546, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Berg, T", "word_idx": 33846, "sentence_idx": 547, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Berg, and J", "word_idx": 33854, "sentence_idx": 548, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Malik", "word_idx": 33866, "sentence_idx": 549, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Berg, T", "word_idx": 33872, "sentence_idx": 550, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Berg, and J", "word_idx": 33880, "sentence_idx": 551, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Malik", "word_idx": 33892, "sentence_idx": 552, "label": "unlabeled"}, {"type": "text", "expr": "Shape matching and object recognition using low distortion\ncorrespondence", "word_idx": 33898, "sentence_idx": 553, "label": "unlabeled"}, {"type": "text", "expr": "Shape matching and object recognition using low distortion\ncorrespondence", "word_idx": 33971, "sentence_idx": 554, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2005", "word_idx": 34044, "sentence_idx": 555, "label": "unlabeled"}, {"type": "text", "expr": ", 2005", "word_idx": 34059, "sentence_idx": 556, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Brachmann, A", "word_idx": 34065, "sentence_idx": 557, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Krull, S", "word_idx": 34078, "sentence_idx": 558, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Nowozin, J", "word_idx": 34087, "sentence_idx": 559, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Shotton, F", "word_idx": 34098, "sentence_idx": 560, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Michel, S", "word_idx": 34109, "sentence_idx": 561, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gumhold, and\nC", "word_idx": 34119, "sentence_idx": 562, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rother", "word_idx": 34134, "sentence_idx": 563, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Brachmann, A", "word_idx": 34141, "sentence_idx": 564, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Krull, S", "word_idx": 34154, "sentence_idx": 565, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Nowozin, J", "word_idx": 34163, "sentence_idx": 566, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Shotton, F", "word_idx": 34174, "sentence_idx": 567, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Michel, S", "word_idx": 34185, "sentence_idx": 568, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Gumhold, and\nC", "word_idx": 34195, "sentence_idx": 569, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rother", "word_idx": 34210, "sentence_idx": 570, "label": "unlabeled"}, {"type": "text", "expr": "DSAC - Differentiable RANSAC for camera localization", "word_idx": 34217, "sentence_idx": 571, "label": "unlabeled"}, {"type": "text", "expr": "DSAC - Differentiable RANSAC for camera localization", "word_idx": 34269, "sentence_idx": 572, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2017", "word_idx": 34321, "sentence_idx": 573, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 34336, "sentence_idx": 574, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dale, M", "word_idx": 34342, "sentence_idx": 575, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Johnson, K", "word_idx": 34350, "sentence_idx": 576, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sunkavalli, W", "word_idx": 34361, "sentence_idx": 577, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Matusik, and H", "word_idx": 34375, "sentence_idx": 578, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Pfister", "word_idx": 34390, "sentence_idx": 579, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dale, M", "word_idx": 34398, "sentence_idx": 580, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Johnson, K", "word_idx": 34406, "sentence_idx": 581, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sunkavalli, W", "word_idx": 34417, "sentence_idx": 582, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Matusik, and H", "word_idx": 34431, "sentence_idx": 583, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Pfister", "word_idx": 34446, "sentence_idx": 584, "label": "unlabeled"}, {"type": "text", "expr": "Image restoration using online photo collections", "word_idx": 34454, "sentence_idx": 585, "label": "unlabeled"}, {"type": "text", "expr": "Image restoration using online photo collections", "word_idx": 34502, "sentence_idx": 586, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2017", "word_idx": 34550, "sentence_idx": 587, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 34565, "sentence_idx": 588, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Engel, T", "word_idx": 34571, "sentence_idx": 589, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sch\u00f6ps, and D", "word_idx": 34580, "sentence_idx": 590, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Cremers", "word_idx": 34594, "sentence_idx": 591, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Engel, T", "word_idx": 34602, "sentence_idx": 592, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sch\u00f6ps, and D", "word_idx": 34611, "sentence_idx": 593, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Cremers", "word_idx": 34625, "sentence_idx": 594, "label": "unlabeled"}, {"type": "text", "expr": "LSD-SLAM: Large-scale direct monocular SLAM", "word_idx": 34633, "sentence_idx": 595, "label": "unlabeled"}, {"type": "text", "expr": "LSD-SLAM: Large-scale direct monocular SLAM", "word_idx": 34676, "sentence_idx": 596, "label": "unlabeled"}, {"type": "text", "expr": "In  ECCV , 2014", "word_idx": 34719, "sentence_idx": 597, "label": "unlabeled"}, {"type": "text", "expr": ", 2014", "word_idx": 34734, "sentence_idx": 598, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fei-Fei, R", "word_idx": 34740, "sentence_idx": 599, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fergus, and P", "word_idx": 34751, "sentence_idx": 600, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Perona", "word_idx": 34765, "sentence_idx": 601, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fei-Fei, R", "word_idx": 34772, "sentence_idx": 602, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fergus, and P", "word_idx": 34783, "sentence_idx": 603, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Perona", "word_idx": 34797, "sentence_idx": 604, "label": "unlabeled"}, {"type": "text", "expr": "One-shot learning of object categories", "word_idx": 34804, "sentence_idx": 605, "label": "unlabeled"}, {"type": "text", "expr": "One-shot learning of object categories", "word_idx": 34842, "sentence_idx": 606, "label": "unlabeled"}, {"type": "text", "expr": "IEEE PAMI , 28(4):594\u2013611, 2006", "word_idx": 34880, "sentence_idx": 607, "label": "unlabeled"}, {"type": "text", "expr": "IEEE PAMI", "word_idx": 34911, "sentence_idx": 608, "label": "unlabeled"}, {"type": "text", "expr": ", 28(4):594\u2013611, 2006", "word_idx": 34920, "sentence_idx": 609, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fischer, A", "word_idx": 34941, "sentence_idx": 610, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dosovitskiy, E", "word_idx": 34952, "sentence_idx": 611, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ilg, P", "word_idx": 34967, "sentence_idx": 612, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0H\u00e4usser, C", "word_idx": 34974, "sentence_idx": 613, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Haz\u0131rba\u015f,\nV", "word_idx": 34985, "sentence_idx": 614, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Golkov, P", "word_idx": 34997, "sentence_idx": 615, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0van\u00a0der Smagt, D", "word_idx": 35007, "sentence_idx": 616, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Cremers, and T", "word_idx": 35024, "sentence_idx": 617, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Brox", "word_idx": 35039, "sentence_idx": 618, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Fischer, A", "word_idx": 35044, "sentence_idx": 619, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Dosovitskiy, E", "word_idx": 35055, "sentence_idx": 620, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ilg, P", "word_idx": 35070, "sentence_idx": 621, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0H\u00e4usser, C", "word_idx": 35077, "sentence_idx": 622, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Haz\u0131rba\u015f,\nV", "word_idx": 35088, "sentence_idx": 623, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Golkov, P", "word_idx": 35100, "sentence_idx": 624, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0van\u00a0der Smagt, D", "word_idx": 35110, "sentence_idx": 625, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Cremers, and T", "word_idx": 35127, "sentence_idx": 626, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Brox", "word_idx": 35142, "sentence_idx": 627, "label": "unlabeled"}, {"type": "text", "expr": "FlowNet: Learning optical flow with convolutional networks", "word_idx": 35147, "sentence_idx": 628, "label": "unlabeled"}, {"type": "text", "expr": "FlowNet: Learning optical flow with convolutional networks", "word_idx": 35205, "sentence_idx": 629, "label": "unlabeled"}, {"type": "text", "expr": "In  ICCV , 2015", "word_idx": 35263, "sentence_idx": 630, "label": "unlabeled"}, {"type": "text", "expr": ", 2015", "word_idx": 35278, "sentence_idx": 631, "label": "unlabeled"}, {"type": "text", "expr": " Fischler and R", "word_idx": 35284, "sentence_idx": 632, "label": "unlabeled"}, {"type": "text", "expr": " Bolles", "word_idx": 35299, "sentence_idx": 633, "label": "unlabeled"}, {"type": "text", "expr": " Fischler and R", "word_idx": 35306, "sentence_idx": 634, "label": "unlabeled"}, {"type": "text", "expr": " Bolles", "word_idx": 35321, "sentence_idx": 635, "label": "unlabeled"}, {"type": "text", "expr": "Random sample consensus: a paradigm for model fitting with\napplications to image analysis and automated cartography", "word_idx": 35328, "sentence_idx": 636, "label": "unlabeled"}, {"type": "text", "expr": "Random sample consensus: a paradigm for model fitting with\napplications to image analysis and automated cartography", "word_idx": 35443, "sentence_idx": 637, "label": "unlabeled"}, {"type": "text", "expr": "Communications of the ACM , 24(6):381\u2013395, 1981", "word_idx": 35558, "sentence_idx": 638, "label": "unlabeled"}, {"type": "text", "expr": "Communications of the ACM", "word_idx": 35605, "sentence_idx": 639, "label": "unlabeled"}, {"type": "text", "expr": ", 24(6):381\u2013395, 1981", "word_idx": 35630, "sentence_idx": 640, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ham, M", "word_idx": 35651, "sentence_idx": 641, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Cho, C", "word_idx": 35658, "sentence_idx": 642, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Schmid, and J", "word_idx": 35665, "sentence_idx": 643, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ponce", "word_idx": 35679, "sentence_idx": 644, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ham, M", "word_idx": 35685, "sentence_idx": 645, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Cho, C", "word_idx": 35692, "sentence_idx": 646, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Schmid, and J", "word_idx": 35699, "sentence_idx": 647, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ponce", "word_idx": 35713, "sentence_idx": 648, "label": "unlabeled"}, {"type": "text", "expr": "Proposal flow: Semantic correspondences from object proposals", "word_idx": 35719, "sentence_idx": 649, "label": "unlabeled"}, {"type": "text", "expr": "Proposal flow: Semantic correspondences from object proposals", "word_idx": 35780, "sentence_idx": 650, "label": "unlabeled"}, {"type": "text", "expr": "IEEE PAMI , 2017", "word_idx": 35841, "sentence_idx": 651, "label": "unlabeled"}, {"type": "text", "expr": "IEEE PAMI", "word_idx": 35857, "sentence_idx": 652, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 35866, "sentence_idx": 653, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Han, R", "word_idx": 35872, "sentence_idx": 654, "label": "unlabeled"}, {"type": "text", "expr": " Rezende, B", "word_idx": 35879, "sentence_idx": 655, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ham, K", "word_idx": 35890, "sentence_idx": 656, "label": "unlabeled"}, {"type": "text", "expr": " Wong, M", "word_idx": 35897, "sentence_idx": 657, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Cho, C", "word_idx": 35905, "sentence_idx": 658, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Schmid, and J", "word_idx": 35912, "sentence_idx": 659, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ponce", "word_idx": 35926, "sentence_idx": 660, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Han, R", "word_idx": 35932, "sentence_idx": 661, "label": "unlabeled"}, {"type": "text", "expr": " Rezende, B", "word_idx": 35939, "sentence_idx": 662, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ham, K", "word_idx": 35950, "sentence_idx": 663, "label": "unlabeled"}, {"type": "text", "expr": " Wong, M", "word_idx": 35957, "sentence_idx": 664, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Cho, C", "word_idx": 35965, "sentence_idx": 665, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Schmid, and J", "word_idx": 35972, "sentence_idx": 666, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ponce", "word_idx": 35986, "sentence_idx": 667, "label": "unlabeled"}, {"type": "text", "expr": "SCNet: Learning semantic correspondence", "word_idx": 35992, "sentence_idx": 668, "label": "unlabeled"}, {"type": "text", "expr": "SCNet: Learning semantic correspondence", "word_idx": 36031, "sentence_idx": 669, "label": "unlabeled"}, {"type": "text", "expr": "In  ICCV , 2017", "word_idx": 36070, "sentence_idx": 670, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 36085, "sentence_idx": 671, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hartley and A", "word_idx": 36091, "sentence_idx": 672, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zisserman", "word_idx": 36105, "sentence_idx": 673, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Hartley and A", "word_idx": 36115, "sentence_idx": 674, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zisserman", "word_idx": 36129, "sentence_idx": 675, "label": "unlabeled"}, {"type": "text", "expr": "Multiple view geometry in computer vision ", "word_idx": 36139, "sentence_idx": 676, "label": "unlabeled"}, {"type": "text", "expr": "Multiple view geometry in computer vision", "word_idx": 36181, "sentence_idx": 677, "label": "unlabeled"}, {"type": "text", "expr": "Cambridge university press, 2003", "word_idx": 36222, "sentence_idx": 678, "label": "unlabeled"}, {"type": "text", "expr": "Cambridge university press, 2003", "word_idx": 36254, "sentence_idx": 679, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, X", "word_idx": 36286, "sentence_idx": 680, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang, S", "word_idx": 36292, "sentence_idx": 681, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ren, and J", "word_idx": 36301, "sentence_idx": 682, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0He, X", "word_idx": 36312, "sentence_idx": 683, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zhang, S", "word_idx": 36318, "sentence_idx": 684, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ren, and J", "word_idx": 36327, "sentence_idx": 685, "label": "unlabeled"}, {"type": "text", "expr": "Deep residual learning for image recognition", "word_idx": 36338, "sentence_idx": 686, "label": "unlabeled"}, {"type": "text", "expr": "Deep residual learning for image recognition", "word_idx": 36382, "sentence_idx": 687, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2016", "word_idx": 36426, "sentence_idx": 688, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 36441, "sentence_idx": 689, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Jaderberg, K", "word_idx": 36447, "sentence_idx": 690, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Simonyan, A", "word_idx": 36460, "sentence_idx": 691, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zisserman, and K", "word_idx": 36472, "sentence_idx": 692, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kavukcuoglu", "word_idx": 36489, "sentence_idx": 693, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Jaderberg, K", "word_idx": 36501, "sentence_idx": 694, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Simonyan, A", "word_idx": 36514, "sentence_idx": 695, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zisserman, and K", "word_idx": 36526, "sentence_idx": 696, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kavukcuoglu", "word_idx": 36543, "sentence_idx": 697, "label": "unlabeled"}, {"type": "text", "expr": "Spatial transformer networks", "word_idx": 36555, "sentence_idx": 698, "label": "unlabeled"}, {"type": "text", "expr": "Spatial transformer networks", "word_idx": 36583, "sentence_idx": 699, "label": "unlabeled"}, {"type": "text", "expr": "In  NIPS , 2015", "word_idx": 36611, "sentence_idx": 700, "label": "unlabeled"}, {"type": "text", "expr": ", 2015", "word_idx": 36626, "sentence_idx": 701, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kanazawa, D", "word_idx": 36632, "sentence_idx": 702, "label": "unlabeled"}, {"type": "text", "expr": " Jacobs, and M", "word_idx": 36644, "sentence_idx": 703, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chandraker", "word_idx": 36658, "sentence_idx": 704, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kanazawa, D", "word_idx": 36669, "sentence_idx": 705, "label": "unlabeled"}, {"type": "text", "expr": " Jacobs, and M", "word_idx": 36681, "sentence_idx": 706, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chandraker", "word_idx": 36695, "sentence_idx": 707, "label": "unlabeled"}, {"type": "text", "expr": "WarpNet: Weakly supervised matching for single-view reconstruction", "word_idx": 36706, "sentence_idx": 708, "label": "unlabeled"}, {"type": "text", "expr": "WarpNet: Weakly supervised matching for single-view reconstruction", "word_idx": 36772, "sentence_idx": 709, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2016", "word_idx": 36838, "sentence_idx": 710, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 36853, "sentence_idx": 711, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kim, C", "word_idx": 36859, "sentence_idx": 712, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Liu, F", "word_idx": 36866, "sentence_idx": 713, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sha, and K", "word_idx": 36873, "sentence_idx": 714, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Grauman", "word_idx": 36884, "sentence_idx": 715, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kim, C", "word_idx": 36892, "sentence_idx": 716, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Liu, F", "word_idx": 36899, "sentence_idx": 717, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sha, and K", "word_idx": 36906, "sentence_idx": 718, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Grauman", "word_idx": 36917, "sentence_idx": 719, "label": "unlabeled"}, {"type": "text", "expr": "Deformable spatial pyramid matching for fast dense correspondences", "word_idx": 36925, "sentence_idx": 720, "label": "unlabeled"}, {"type": "text", "expr": "Deformable spatial pyramid matching for fast dense correspondences", "word_idx": 36991, "sentence_idx": 721, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2013", "word_idx": 37057, "sentence_idx": 722, "label": "unlabeled"}, {"type": "text", "expr": ", 2013", "word_idx": 37072, "sentence_idx": 723, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kim, D", "word_idx": 37078, "sentence_idx": 724, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Min, B", "word_idx": 37085, "sentence_idx": 725, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ham, S", "word_idx": 37092, "sentence_idx": 726, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Jeon, S", "word_idx": 37099, "sentence_idx": 727, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lin, and K", "word_idx": 37107, "sentence_idx": 728, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sohn", "word_idx": 37118, "sentence_idx": 729, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kim, D", "word_idx": 37123, "sentence_idx": 730, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Min, B", "word_idx": 37130, "sentence_idx": 731, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ham, S", "word_idx": 37137, "sentence_idx": 732, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Jeon, S", "word_idx": 37144, "sentence_idx": 733, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lin, and K", "word_idx": 37152, "sentence_idx": 734, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sohn", "word_idx": 37163, "sentence_idx": 735, "label": "unlabeled"}, {"type": "text", "expr": "FCSS: Fully convolutional self-similarity for dense semantic\ncorrespondence", "word_idx": 37168, "sentence_idx": 736, "label": "unlabeled"}, {"type": "text", "expr": "FCSS: Fully convolutional self-similarity for dense semantic\ncorrespondence", "word_idx": 37243, "sentence_idx": 737, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2017", "word_idx": 37318, "sentence_idx": 738, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 37333, "sentence_idx": 739, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kim, D", "word_idx": 37339, "sentence_idx": 740, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Min, S", "word_idx": 37346, "sentence_idx": 741, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lin, and K", "word_idx": 37353, "sentence_idx": 742, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sohn", "word_idx": 37364, "sentence_idx": 743, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kim, D", "word_idx": 37369, "sentence_idx": 744, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Min, S", "word_idx": 37376, "sentence_idx": 745, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Lin, and K", "word_idx": 37383, "sentence_idx": 746, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sohn", "word_idx": 37394, "sentence_idx": 747, "label": "unlabeled"}, {"type": "text", "expr": "DCTM: Discrete-continuous transformation matching for semantic\nflow", "word_idx": 37399, "sentence_idx": 748, "label": "unlabeled"}, {"type": "text", "expr": "DCTM: Discrete-continuous transformation matching for semantic\nflow", "word_idx": 37466, "sentence_idx": 749, "label": "unlabeled"}, {"type": "text", "expr": "In  ICCV , 2017", "word_idx": 37533, "sentence_idx": 750, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 37548, "sentence_idx": 751, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kingma and J", "word_idx": 37554, "sentence_idx": 752, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kingma and J", "word_idx": 37567, "sentence_idx": 753, "label": "unlabeled"}, {"type": "text", "expr": "Adam: A method for stochastic optimization", "word_idx": 37580, "sentence_idx": 754, "label": "unlabeled"}, {"type": "text", "expr": "Adam: A method for stochastic optimization", "word_idx": 37622, "sentence_idx": 755, "label": "unlabeled"}, {"type": "text", "expr": "In  ICLR , 2015", "word_idx": 37664, "sentence_idx": 756, "label": "unlabeled"}, {"type": "text", "expr": ", 2015", "word_idx": 37679, "sentence_idx": 757, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Liu, J", "word_idx": 37685, "sentence_idx": 758, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yuen, and A", "word_idx": 37692, "sentence_idx": 759, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Torralba", "word_idx": 37704, "sentence_idx": 760, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Liu, J", "word_idx": 37713, "sentence_idx": 761, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yuen, and A", "word_idx": 37720, "sentence_idx": 762, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Torralba", "word_idx": 37732, "sentence_idx": 763, "label": "unlabeled"}, {"type": "text", "expr": "SIFT flow: Dense correspondence across scenes and its applications", "word_idx": 37741, "sentence_idx": 764, "label": "unlabeled"}, {"type": "text", "expr": "SIFT flow: Dense correspondence across scenes and its applications", "word_idx": 37807, "sentence_idx": 765, "label": "unlabeled"}, {"type": "text", "expr": "IEEE PAMI , 33(5):978\u2013994, 2011", "word_idx": 37873, "sentence_idx": 766, "label": "unlabeled"}, {"type": "text", "expr": "IEEE PAMI", "word_idx": 37904, "sentence_idx": 767, "label": "unlabeled"}, {"type": "text", "expr": ", 33(5):978\u2013994, 2011", "word_idx": 37913, "sentence_idx": 768, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Liu, J", "word_idx": 37934, "sentence_idx": 769, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yuen, A", "word_idx": 37941, "sentence_idx": 770, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Torralba, J", "word_idx": 37949, "sentence_idx": 771, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sivic, and W", "word_idx": 37961, "sentence_idx": 772, "label": "unlabeled"}, {"type": "text", "expr": " Freeman", "word_idx": 37974, "sentence_idx": 773, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Liu, J", "word_idx": 37982, "sentence_idx": 774, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yuen, A", "word_idx": 37989, "sentence_idx": 775, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Torralba, J", "word_idx": 37997, "sentence_idx": 776, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sivic, and W", "word_idx": 38009, "sentence_idx": 777, "label": "unlabeled"}, {"type": "text", "expr": " Freeman", "word_idx": 38022, "sentence_idx": 778, "label": "unlabeled"}, {"type": "text", "expr": "SIFT flow: Dense correspondence across different scenes", "word_idx": 38030, "sentence_idx": 779, "label": "unlabeled"}, {"type": "text", "expr": "SIFT flow: Dense correspondence across different scenes", "word_idx": 38085, "sentence_idx": 780, "label": "unlabeled"}, {"type": "text", "expr": "In  ECCV , 2008", "word_idx": 38140, "sentence_idx": 781, "label": "unlabeled"}, {"type": "text", "expr": ", 2008", "word_idx": 38155, "sentence_idx": 782, "label": "unlabeled"}, {"type": "text", "expr": " Lowe", "word_idx": 38161, "sentence_idx": 783, "label": "unlabeled"}, {"type": "text", "expr": " Lowe", "word_idx": 38166, "sentence_idx": 784, "label": "unlabeled"}, {"type": "text", "expr": "Object recognition from local scale-invariant features", "word_idx": 38171, "sentence_idx": 785, "label": "unlabeled"}, {"type": "text", "expr": "Object recognition from local scale-invariant features", "word_idx": 38225, "sentence_idx": 786, "label": "unlabeled"}, {"type": "text", "expr": "In  ICCV , 1999", "word_idx": 38279, "sentence_idx": 787, "label": "unlabeled"}, {"type": "text", "expr": ", 1999", "word_idx": 38294, "sentence_idx": 788, "label": "unlabeled"}, {"type": "text", "expr": " Newcombe, S", "word_idx": 38300, "sentence_idx": 789, "label": "unlabeled"}, {"type": "text", "expr": " Lovegrove, and A", "word_idx": 38312, "sentence_idx": 790, "label": "unlabeled"}, {"type": "text", "expr": " Davison", "word_idx": 38329, "sentence_idx": 791, "label": "unlabeled"}, {"type": "text", "expr": " Newcombe, S", "word_idx": 38337, "sentence_idx": 792, "label": "unlabeled"}, {"type": "text", "expr": " Lovegrove, and A", "word_idx": 38349, "sentence_idx": 793, "label": "unlabeled"}, {"type": "text", "expr": " Davison", "word_idx": 38366, "sentence_idx": 794, "label": "unlabeled"}, {"type": "text", "expr": "DTAM: Dense tracking and mapping in real-time", "word_idx": 38374, "sentence_idx": 795, "label": "unlabeled"}, {"type": "text", "expr": "DTAM: Dense tracking and mapping in real-time", "word_idx": 38419, "sentence_idx": 796, "label": "unlabeled"}, {"type": "text", "expr": "In  ICCV , 2011", "word_idx": 38464, "sentence_idx": 797, "label": "unlabeled"}, {"type": "text", "expr": ", 2011", "word_idx": 38479, "sentence_idx": 798, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Nikandrova and V", "word_idx": 38485, "sentence_idx": 799, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kyrki", "word_idx": 38502, "sentence_idx": 800, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Nikandrova and V", "word_idx": 38508, "sentence_idx": 801, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Kyrki", "word_idx": 38525, "sentence_idx": 802, "label": "unlabeled"}, {"type": "text", "expr": "Category-based task specific grasping", "word_idx": 38531, "sentence_idx": 803, "label": "unlabeled"}, {"type": "text", "expr": "Category-based task specific grasping", "word_idx": 38568, "sentence_idx": 804, "label": "unlabeled"}, {"type": "text", "expr": "Robotics and Autonomous Systems , 70(Supplement C):25 \u2013 35,\n2015", "word_idx": 38605, "sentence_idx": 805, "label": "unlabeled"}, {"type": "text", "expr": "Robotics and Autonomous Systems", "word_idx": 38669, "sentence_idx": 806, "label": "unlabeled"}, {"type": "text", "expr": ", 70(Supplement C):25 \u2013 35,\n2015", "word_idx": 38700, "sentence_idx": 807, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Novotny, D", "word_idx": 38732, "sentence_idx": 808, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Larlus, and A", "word_idx": 38743, "sentence_idx": 809, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Vedaldi", "word_idx": 38757, "sentence_idx": 810, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Novotny, D", "word_idx": 38765, "sentence_idx": 811, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Larlus, and A", "word_idx": 38776, "sentence_idx": 812, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Vedaldi", "word_idx": 38790, "sentence_idx": 813, "label": "unlabeled"}, {"type": "text", "expr": "AnchorNet: A weakly supervised network to learn geometry-sensitive\nfeatures for semantic matching", "word_idx": 38798, "sentence_idx": 814, "label": "unlabeled"}, {"type": "text", "expr": "AnchorNet: A weakly supervised network to learn geometry-sensitive\nfeatures for semantic matching", "word_idx": 38895, "sentence_idx": 815, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2017", "word_idx": 38992, "sentence_idx": 816, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 39007, "sentence_idx": 817, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rocco, R", "word_idx": 39013, "sentence_idx": 818, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Arandjelovi\u0107, and J", "word_idx": 39022, "sentence_idx": 819, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sivic", "word_idx": 39042, "sentence_idx": 820, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rocco, R", "word_idx": 39048, "sentence_idx": 821, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Arandjelovi\u0107, and J", "word_idx": 39057, "sentence_idx": 822, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sivic", "word_idx": 39077, "sentence_idx": 823, "label": "unlabeled"}, {"type": "text", "expr": "Webpage: Convolutional neural network architecture for geometric\nmatching", "word_idx": 39083, "sentence_idx": 824, "label": "unlabeled"}, {"type": "text", "expr": "Webpage: Convolutional neural network architecture for geometric\nmatching", "word_idx": 39156, "sentence_idx": 825, "label": "unlabeled"}, {"type": "text", "expr": "http://www", "word_idx": 39229, "sentence_idx": 826, "label": "unlabeled"}, {"type": "text", "expr": "fr/willow/research/cnngeometric/ ", "word_idx": 39239, "sentence_idx": 827, "label": "unlabeled"}, {"type": "text", "expr": "http://www", "word_idx": 39272, "sentence_idx": 828, "label": "unlabeled"}, {"type": "text", "expr": "fr/willow/research/cnngeometric/", "word_idx": 39282, "sentence_idx": 829, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rocco, R", "word_idx": 39314, "sentence_idx": 830, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Arandjelovi\u0107, and J", "word_idx": 39323, "sentence_idx": 831, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sivic", "word_idx": 39343, "sentence_idx": 832, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rocco, R", "word_idx": 39349, "sentence_idx": 833, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Arandjelovi\u0107, and J", "word_idx": 39358, "sentence_idx": 834, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sivic", "word_idx": 39378, "sentence_idx": 835, "label": "unlabeled"}, {"type": "text", "expr": "Webpage: End-to-end weakly-supervised semantic alignment", "word_idx": 39384, "sentence_idx": 836, "label": "unlabeled"}, {"type": "text", "expr": "Webpage: End-to-end weakly-supervised semantic alignment", "word_idx": 39440, "sentence_idx": 837, "label": "unlabeled"}, {"type": "text", "expr": "http://www", "word_idx": 39496, "sentence_idx": 838, "label": "unlabeled"}, {"type": "text", "expr": "fr/willow/research/weakalign/ ", "word_idx": 39506, "sentence_idx": 839, "label": "unlabeled"}, {"type": "text", "expr": "http://www", "word_idx": 39536, "sentence_idx": 840, "label": "unlabeled"}, {"type": "text", "expr": "fr/willow/research/weakalign/", "word_idx": 39546, "sentence_idx": 841, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rocco, R", "word_idx": 39575, "sentence_idx": 842, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Arandjelovi\u0107, and J", "word_idx": 39584, "sentence_idx": 843, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sivic", "word_idx": 39604, "sentence_idx": 844, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Rocco, R", "word_idx": 39610, "sentence_idx": 845, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Arandjelovi\u0107, and J", "word_idx": 39619, "sentence_idx": 846, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sivic", "word_idx": 39639, "sentence_idx": 847, "label": "unlabeled"}, {"type": "text", "expr": "Convolutional neural network architecture for geometric matching", "word_idx": 39645, "sentence_idx": 848, "label": "unlabeled"}, {"type": "text", "expr": "Convolutional neural network architecture for geometric matching", "word_idx": 39709, "sentence_idx": 849, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2017", "word_idx": 39773, "sentence_idx": 850, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 39788, "sentence_idx": 851, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Simonyan and A", "word_idx": 39794, "sentence_idx": 852, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zisserman", "word_idx": 39809, "sentence_idx": 853, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Simonyan and A", "word_idx": 39819, "sentence_idx": 854, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Zisserman", "word_idx": 39834, "sentence_idx": 855, "label": "unlabeled"}, {"type": "text", "expr": "Very deep convolutional networks for large-scale image recognition", "word_idx": 39844, "sentence_idx": 856, "label": "unlabeled"}, {"type": "text", "expr": "Very deep convolutional networks for large-scale image recognition", "word_idx": 39910, "sentence_idx": 857, "label": "unlabeled"}, {"type": "text", "expr": "In  ICLR , 2015", "word_idx": 39976, "sentence_idx": 858, "label": "unlabeled"}, {"type": "text", "expr": ", 2015", "word_idx": 39991, "sentence_idx": 859, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Szeliski", "word_idx": 39997, "sentence_idx": 860, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Szeliski", "word_idx": 40006, "sentence_idx": 861, "label": "unlabeled"}, {"type": "text", "expr": "Image alignment and stitching: A tutorial", "word_idx": 40015, "sentence_idx": 862, "label": "unlabeled"}, {"type": "text", "expr": "Image alignment and stitching: A tutorial", "word_idx": 40056, "sentence_idx": 863, "label": "unlabeled"}, {"type": "text", "expr": "Foundations and Trends\u00ae in Computer Graphics and\nVision , 2(1):1\u2013104, 2006", "word_idx": 40097, "sentence_idx": 864, "label": "unlabeled"}, {"type": "text", "expr": "Foundations and Trends\u00ae in Computer Graphics and\nVision", "word_idx": 40171, "sentence_idx": 865, "label": "unlabeled"}, {"type": "text", "expr": ", 2(1):1\u2013104, 2006", "word_idx": 40226, "sentence_idx": 866, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Taniai, S", "word_idx": 40244, "sentence_idx": 867, "label": "unlabeled"}, {"type": "text", "expr": " Sinha, and Y", "word_idx": 40254, "sentence_idx": 868, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sato", "word_idx": 40267, "sentence_idx": 869, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Taniai, S", "word_idx": 40272, "sentence_idx": 870, "label": "unlabeled"}, {"type": "text", "expr": " Sinha, and Y", "word_idx": 40282, "sentence_idx": 871, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Sato", "word_idx": 40295, "sentence_idx": 872, "label": "unlabeled"}, {"type": "text", "expr": "Joint recovery of dense correspondence and cosegmentation in two\nimages", "word_idx": 40300, "sentence_idx": 873, "label": "unlabeled"}, {"type": "text", "expr": "Joint recovery of dense correspondence and cosegmentation in two\nimages", "word_idx": 40371, "sentence_idx": 874, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2016", "word_idx": 40442, "sentence_idx": 875, "label": "unlabeled"}, {"type": "text", "expr": ", 2016", "word_idx": 40457, "sentence_idx": 876, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ufer and B", "word_idx": 40463, "sentence_idx": 877, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ommer", "word_idx": 40474, "sentence_idx": 878, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ufer and B", "word_idx": 40480, "sentence_idx": 879, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ommer", "word_idx": 40491, "sentence_idx": 880, "label": "unlabeled"}, {"type": "text", "expr": "Deep semantic feature matching", "word_idx": 40497, "sentence_idx": 881, "label": "unlabeled"}, {"type": "text", "expr": "Deep semantic feature matching", "word_idx": 40527, "sentence_idx": 882, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2017", "word_idx": 40557, "sentence_idx": 883, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 40572, "sentence_idx": 884, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Weinzaepfel, J", "word_idx": 40578, "sentence_idx": 885, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Revaud, Z", "word_idx": 40593, "sentence_idx": 886, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Harchaoui, and C", "word_idx": 40603, "sentence_idx": 887, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Schmid", "word_idx": 40620, "sentence_idx": 888, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Weinzaepfel, J", "word_idx": 40627, "sentence_idx": 889, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Revaud, Z", "word_idx": 40642, "sentence_idx": 890, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Harchaoui, and C", "word_idx": 40652, "sentence_idx": 891, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Schmid", "word_idx": 40669, "sentence_idx": 892, "label": "unlabeled"}, {"type": "text", "expr": "DeepFlow: Large displacement optical flow with deep matching", "word_idx": 40676, "sentence_idx": 893, "label": "unlabeled"}, {"type": "text", "expr": "DeepFlow: Large displacement optical flow with deep matching", "word_idx": 40736, "sentence_idx": 894, "label": "unlabeled"}, {"type": "text", "expr": "In  ICCV , 2013", "word_idx": 40796, "sentence_idx": 895, "label": "unlabeled"}, {"type": "text", "expr": ", 2013", "word_idx": 40811, "sentence_idx": 896, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yang, X", "word_idx": 40817, "sentence_idx": 897, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Li, H", "word_idx": 40825, "sentence_idx": 898, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Cheng, J", "word_idx": 40831, "sentence_idx": 899, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Li, and L", "word_idx": 40840, "sentence_idx": 900, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chen", "word_idx": 40850, "sentence_idx": 901, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yang, X", "word_idx": 40855, "sentence_idx": 902, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Li, H", "word_idx": 40863, "sentence_idx": 903, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Cheng, J", "word_idx": 40869, "sentence_idx": 904, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Li, and L", "word_idx": 40878, "sentence_idx": 905, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Chen", "word_idx": 40888, "sentence_idx": 906, "label": "unlabeled"}, {"type": "text", "expr": "Object-aware dense semantic correspondence", "word_idx": 40893, "sentence_idx": 907, "label": "unlabeled"}, {"type": "text", "expr": "Object-aware dense semantic correspondence", "word_idx": 40935, "sentence_idx": 908, "label": "unlabeled"}, {"type": "text", "expr": "In  CVPR , 2017", "word_idx": 40977, "sentence_idx": 909, "label": "unlabeled"}, {"type": "text", "expr": ", 2017", "word_idx": 40992, "sentence_idx": 910, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yang and D", "word_idx": 40998, "sentence_idx": 911, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ramanan", "word_idx": 41009, "sentence_idx": 912, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Yang and D", "word_idx": 41017, "sentence_idx": 913, "label": "unlabeled"}, {"type": "text", "expr": "\u00a0Ramanan", "word_idx": 41028, "sentence_idx": 914, "label": "unlabeled"}, {"type": "text", "expr": "Articulated human detection with flexible mixtures of parts", "word_idx": 41036, "sentence_idx": 915, "label": "unlabeled"}, {"type": "text", "expr": "Articulated human detection with flexible mixtures of parts", "word_idx": 41095, "sentence_idx": 916, "label": "unlabeled"}, {"type": "text", "expr": "IEEE PAMI , 2013", "word_idx": 41154, "sentence_idx": 917, "label": "unlabeled"}, {"type": "text", "expr": "IEEE PAMI", "word_idx": 41170, "sentence_idx": 918, "label": "unlabeled"}, {"type": "text", "expr": ", 2013", "word_idx": 41179, "sentence_idx": 919, "label": "unlabeled"}, {"type": "text", "expr": "(b) Strongest inlier matches", "word_idx": 41185, "sentence_idx": 920, "label": "unlabeled"}, {"type": "text", "expr": "(b) Strongest inlier matches", "word_idx": 41213, "sentence_idx": 921, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a011:  Additional examples on the Caltech-101 dataset", "word_idx": 41241, "sentence_idx": 922, "label": "unlabeled"}, {"type": "text", "expr": "  Each row corresponds to one example", "word_idx": 41299, "sentence_idx": 923, "label": "unlabeled"}, {"type": "text", "expr": " (a) shows the (right) automatic semantic alignment of the (left) source and (middle) target images", "word_idx": 41336, "sentence_idx": 924, "label": "unlabeled"}, {"type": "text", "expr": " (b) shows the strongest inlier feature matches", "word_idx": 41435, "sentence_idx": 925, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a011:", "word_idx": 41482, "sentence_idx": 926, "label": "unlabeled"}, {"type": "text", "expr": "Additional examples on the Caltech-101 dataset", "word_idx": 41492, "sentence_idx": 927, "label": "unlabeled"}, {"type": "text", "expr": "(b) Strongest inlier matches", "word_idx": 41538, "sentence_idx": 928, "label": "unlabeled"}, {"type": "text", "expr": "(b) Strongest inlier matches", "word_idx": 41566, "sentence_idx": 929, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a012:  Additional examples on the TSS dataset", "word_idx": 41594, "sentence_idx": 930, "label": "unlabeled"}, {"type": "text", "expr": "  Each row corresponds to one example", "word_idx": 41644, "sentence_idx": 931, "label": "unlabeled"}, {"type": "text", "expr": " (a) shows the (right) automatic semantic alignment of the (left) source and (middle) target images", "word_idx": 41681, "sentence_idx": 932, "label": "unlabeled"}, {"type": "text", "expr": " (b) shows the strongest inlier feature matches", "word_idx": 41780, "sentence_idx": 933, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a012:", "word_idx": 41827, "sentence_idx": 934, "label": "unlabeled"}, {"type": "text", "expr": "Additional examples on the TSS dataset", "word_idx": 41837, "sentence_idx": 935, "label": "unlabeled"}, {"type": "text", "expr": "(b) Strongest inlier matches", "word_idx": 41875, "sentence_idx": 936, "label": "unlabeled"}, {"type": "text", "expr": "(b) Strongest inlier matches", "word_idx": 41903, "sentence_idx": 937, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a013:  Additional examples on the PF-PASCAL dataset", "word_idx": 41931, "sentence_idx": 938, "label": "unlabeled"}, {"type": "text", "expr": "  Each row corresponds to one example", "word_idx": 41987, "sentence_idx": 939, "label": "unlabeled"}, {"type": "text", "expr": " (a) shows the (right) automatic semantic alignment of the (left) source and (middle) target images", "word_idx": 42024, "sentence_idx": 940, "label": "unlabeled"}, {"type": "text", "expr": " (b) shows the strongest inlier feature matches", "word_idx": 42123, "sentence_idx": 941, "label": "unlabeled"}, {"type": "text", "expr": "Figure\u00a013:", "word_idx": 42170, "sentence_idx": 942, "label": "unlabeled"}, {"type": "text", "expr": "Additional examples on the PF-PASCAL dataset", "word_idx": 42180, "sentence_idx": 943, "label": "unlabeled"}, {"type": "text", "expr": "Generated  on Tue Apr  3 11:33:32 2018 by", "word_idx": 42224, "sentence_idx": 944, "label": "unlabeled"}, {"type": "text", "expr": "LaTeXML", "word_idx": 42265, "sentence_idx": 945, "label": "unlabeled"}]}