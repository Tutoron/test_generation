{
	"doc_name": "Fast_Parametric_Learning_with_Activation_Memorization",
	"symbol_expr": "\\lambda",
	"sentences": [{
		"type": "text",
		"expr": " Traditional  $n$ -gram models take frequency-based estimates of these conditional probabilities with truncated contexts  $p_{n}=p(w_{i}\\mid w_{i-n},\\ldots,w_{i-1})$  and smooth between them to estimate the full conditional probability,  $p(w_{i}\\mid w_{1},\\ldots,w_{i-1})=\\sum_{j=1}^{n}\\lambda_{j}p_{j}$ ",
		"word_idx": 11731,
		"sentence_idx": 127,
		"label": "usecase"
	}, {
		"type": "math",
		"expr": "$$p(w_{i}\\mid w_{1},\\ldots,w_{i-1})=\\sum_{j=1}^{n}\\lambda_{j}p_{j}$$",
		"word_idx": 12531,
		"sentence_idx": 131,
		"label": "none"
	}, {
		"type": "text",
		"expr": " Here,  $\\mathbb{I}\\{y_{t}\\}$  is the one-hot target vector,  $V$  denotes the vocabulary of classes, and  $c_{t}$  is defined to be a counter of class occurrences during training \u2014 which is used to anneal  $\\lambda_{t}$  as described in ( 4 )",
		"word_idx": 16026,
		"sentence_idx": 160,
		"label": "usecase"
	}, {
		"type": "math",
		"expr": "$$\\lambda_{t}$$",
		"word_idx": 16349,
		"sentence_idx": 168,
		"label": "none"
	}, {
		"type": "text",
		"expr": "$\\displaystyle\\theta_{t+1}[i]\\leftarrow\\begin{cases}\\lambda_{t}\\,h_{t}+(1-%\n\\lambda_{t})\\,\\hat{\\theta}_{t+05}[i]&i=y_{t}\\\\\n\\hat{\\theta}_{t+05}[i]&i\\neq y_{t}\\;,\\end{cases}$",
		"word_idx": 17447,
		"sentence_idx": 182,
		"label": "none"
	}, {
		"type": "math",
		"expr": "$$\\displaystyle\\theta_{t+1}[i]\\leftarrow\\begin{cases}\\lambda_{t}\\,h_{t}+(1-%\n\\lambda_{t})\\,\\hat{\\theta}_{t+0.5}[i]&i=y_{t}\\\\\n\\hat{\\theta}_{t+0.5}[i]&i\\neq y_{t}\\;,\\end{cases}$$",
		"word_idx": 17619,
		"sentence_idx": 183,
		"label": "none"
	}, {
		"type": "text",
		"expr": " When  $\\lambda_{t}=1$  this corresponds to the rule  $\\theta_{t+1}\\leftarrow h_{t}\\cdot\\mathbb{I}\\{y_{t}\\}$  where  $\\mathbb{I}\\{y_{t}\\}\\in[0,1]^{m}$  is a one-hot target vector",
		"word_idx": 17819,
		"sentence_idx": 185,
		"label": "usecase"
	}, {
		"type": "text",
		"expr": " Naturally when  $\\lambda=0$  this is gradient descent, and so we see Hebbian Softmax is mixture of the two learning rules",
		"word_idx": 18146,
		"sentence_idx": 187,
		"label": "usecase"
	}, {
		"type": "math",
		"expr": "$$\\lambda_{t}=1$$",
		"word_idx": 18351,
		"sentence_idx": 189,
		"label": "none"
	}, {
		"type": "math",
		"expr": "$$\\lambda=0$$",
		"word_idx": 18510,
		"sentence_idx": 195,
		"label": "none"
	}, {
		"type": "text",
		"expr": " As such we do not want  $\\lambda_{t}$  to be constant, but instead something that is eventually annealed to zero",
		"word_idx": 18712,
		"sentence_idx": 197,
		"label": "usecase"
	}, {
		"type": "math",
		"expr": "$$\\lambda_{t}$$",
		"word_idx": 18961,
		"sentence_idx": 199,
		"label": "none"
	}, {
		"type": "text",
		"expr": "$\\lambda_{t}=\\max(1\\,/\\,\\mathbf{c}[y_{t}],\\;\\gamma)\\,\\cdot\\,\\mathbb{I}\\{\\mathbf%\n{c}[y_{t}]<T\\}$",
		"word_idx": 18999,
		"sentence_idx": 201,
		"label": "definition"
	}, {
		"type": "math",
		"expr": "$$\\lambda_{t}=\\max(1\\,/\\,\\mathbf{c}[y_{t}],\\;\\gamma)\\,\\cdot\\,\\mathbb{I}\\{\\mathbf%\n{c}[y_{t}]<T\\}$$",
		"word_idx": 19095,
		"sentence_idx": 202,
		"label": "definition"
	}, {
		"type": "text",
		"expr": " a constant  $\\lambda$  or pure annealing  $\\lambda_{t}=1/c[y_{t}]$ ",
		"word_idx": 19439,
		"sentence_idx": 206,
		"label": "usecase"
	}, {
		"type": "text",
		"expr": " classes\n \\STATE $B\\leftarrow$  batch size\n \\STATE $\\mathbf{c}_{0}[i]\\leftarrow 0;\\quad i=1,\\ldots,M$ \\STATE \u2014 At iteration  $t$ \\STATE $\\mathbf{h}_{t,1:B}\\leftarrow$  softmax inputs\n \\STATE $\\mathbf{p}_{t,1:B}\\leftarrow$  softmax outputs\n \\STATE $\\mathbf{y}_{t,1:B}\\leftarrow$  target labels\n \\STATE $\\hat{\\theta}_{t+05}\\leftarrow$ \\FOR $i=1,\\ldots,M$ \\STATE $n_{t,i}\\leftarrow\\sum_{j=1}^{B}\\,\\mathbb{I}\\{y_{t,j}=i\\}$ \\IF $n_{t,i}>0$ \\STATE $\\lambda_{t,i}\\leftarrow\\max(1/\\mathbf{c}_{t}[i],\\gamma)\\,\\mathbb{I}\\{\\mathbf{c%\n}_{t}[i]<T\\}$ \\STATE $\\bar{h}_{t,i}\\leftarrow\\frac{1}{n_{t,i}}\\sum_{j=1}^{B}h_{t,j}\\mathbb{I}\\{y_{t,%\nj}=i\\}$ \\STATE $\\theta_{t+1}\\leftarrow\\lambda_{t,i}\\bar{h}_{t,i}+(1-\\lambda_{t,i})\\hat{\\theta}%\n_{t+05}[i]$ \\ELSE \\STATE $\\theta_{t+1}\\leftarrow\\hat{\\theta}_{t+05}[i]$ \\ENDIF \\STATE $\\mathbf{c}_{t+1}[i]\\leftarrow\\mathbf{c}_{t}[i]+n_{t,i}$ \\ENDFOR",
		"word_idx": 20217,
		"sentence_idx": 212,
		"label": "none"
	}, {
		"type": "math",
		"expr": "$$\\lambda$$",
		"word_idx": 21104,
		"sentence_idx": 215,
		"label": "none"
	}, {
		"type": "math",
		"expr": "$$\\lambda_{t}=1/c[y_{t}]$$",
		"word_idx": 21111,
		"sentence_idx": 216,
		"label": "usecase"
	}, {
		"type": "math",
		"expr": "$$\\lambda_{t,i}\\leftarrow\\max(1/\\mathbf{c}_{t}[i],\\gamma)\\,\\mathbb{I}\\{\\mathbf{c%\n}_{t}[i]<T\\}$$",
		"word_idx": 21772,
		"sentence_idx": 251,
		"label": "none"
	}, {
		"type": "math",
		"expr": "$$\\theta_{t+1}\\leftarrow\\lambda_{t,i}\\bar{h}_{t,i}+(1-\\lambda_{t,i})\\hat{\\theta}%\n_{t+0.5}[i]$$",
		"word_idx": 21962,
		"sentence_idx": 255,
		"label": "none"
	}, {
		"type": "text",
		"expr": "  $g(j)=\\lambda\\,(1-\\lambda)^{j-1}$ , then the resulting conditional probability is,",
		"word_idx": 25278,
		"sentence_idx": 295,
		"label": "none"
	}, {
		"type": "math",
		"expr": "$$g(j)=\\lambda\\,(1-\\lambda)^{j-1}$$",
		"word_idx": 25362,
		"sentence_idx": 296,
		"label": "none"
	}, {
		"type": "text",
		"expr": "$\\tilde{p}_{c}(w\\mid h_{t})\\propto\\sum_{j=1}^{N_{w}}e^{\\lambda\\,(1-\\lambda)^{j-%\n1}\\,h_{t}^{T}h_{I_{w}(j)}}$",
		"word_idx": 25393,
		"sentence_idx": 297,
		"label": "none"
	}, {
		"type": "math",
		"expr": "$$\\tilde{p}_{c}(w\\mid h_{t})\\propto\\sum_{j=1}^{N_{w}}e^{\\lambda\\,(1-\\lambda)^{j-%\n1}\\,h_{t}^{T}h_{I_{w}(j)}}$$",
		"word_idx": 25501,
		"sentence_idx": 298,
		"label": "none"
	}, {
		"type": "text",
		"expr": " If  $\\theta$  has not received large gradients from the occurrence of nearby neighboring classes, and we fix  $\\lambda_{t}=\\lambda$  over time, then ( 3 ) gives",
		"word_idx": 25810,
		"sentence_idx": 301,
		"label": "usecase"
	}, {
		"type": "math",
		"expr": "$$\\lambda_{t}=\\lambda$$",
		"word_idx": 25982,
		"sentence_idx": 304,
		"label": "usecase"
	}, {
		"type": "text",
		"expr": "$\\theta_{i}\\approx\\sum_{j=1}^{N_{w}}\\lambda\\,(1-\\lambda)^{j-1}h_{I_{w}(j)}\\;,$",
		"word_idx": 26001,
		"sentence_idx": 305,
		"label": "none"
	}, {
		"type": "math",
		"expr": "$$\\theta_{i}\\approx\\sum_{j=1}^{N_{w}}\\lambda\\,(1-\\lambda)^{j-1}h_{I_{w}(j)}\\;,$$",
		"word_idx": 26079,
		"sentence_idx": 306,
		"label": "none"
	}, {
		"type": "text",
		"expr": "$\\displaystyle\\approx e^{h_{t}^{T}\\sum_{j=1}^{N_{w}}\\lambda\\,(1-\\lambda)^{j-1}h%\n_{I_{w}(j)}}$",
		"word_idx": 26348,
		"sentence_idx": 310,
		"label": "none"
	}, {
		"type": "math",
		"expr": "$$\\displaystyle\\approx e^{h_{t}^{T}\\sum_{j=1}^{N_{w}}\\lambda\\,(1-\\lambda)^{j-1}h%\n_{I_{w}(j)}}$$",
		"word_idx": 26442,
		"sentence_idx": 311,
		"label": "none"
	}, {
		"type": "text",
		"expr": "$\\displaystyle=\\prod_{j=1}^{N_{w}}e^{\\lambda\\,(1-\\lambda)^{j-1}h_{t}^{T}h_{I_{w%\n}(j)}}\\,$",
		"word_idx": 26534,
		"sentence_idx": 312,
		"label": "none"
	}, {
		"type": "math",
		"expr": "$$\\displaystyle=\\prod_{j=1}^{N_{w}}e^{\\lambda\\,(1-\\lambda)^{j-1}h_{t}^{T}h_{I_{w%\n}(j)}}\\,.$$",
		"word_idx": 26624,
		"sentence_idx": 313,
		"label": "none"
	}, {
		"type": "text",
		"expr": "$\\theta_{t+1}[i]\\leftarrow\\lambda_{t}\\,h_{t}+(1-\\lambda_{t})\\,\\hat{\\theta}_{t+0%\n5}[i]$",
		"word_idx": 27436,
		"sentence_idx": 323,
		"label": "none"
	}, {
		"type": "math",
		"expr": "$$\\theta_{t+1}[i]\\leftarrow\\lambda_{t}\\,h_{t}+(1-\\lambda_{t})\\,\\hat{\\theta}_{t+0%\n.5}[i].$$",
		"word_idx": 27523,
		"sentence_idx": 324,
		"label": "none"
	}, {
		"type": "text",
		"expr": "$\\displaystyle\\theta_{t+1}[i]\\leftarrow\\lambda w^{*}+(1-\\lambda)\\,\\hat{\\theta}_%\n{t+05}[i]$",
		"word_idx": 27775,
		"sentence_idx": 327,
		"label": "none"
	}, {
		"type": "math",
		"expr": "$$\\displaystyle\\theta_{t+1}[i]\\leftarrow\\lambda w^{*}+(1-\\lambda)\\,\\hat{\\theta}_%\n{t+0.5}[i]$$",
		"word_idx": 27866,
		"sentence_idx": 328,
		"label": "none"
	}, {
		"type": "text",
		"expr": "Cache output interpolation:  $\\lambda_{cache}\\in\\{$ 0",
		"word_idx": 69291,
		"sentence_idx": 1111,
		"label": "usecase"
	}, {
		"type": "math",
		"expr": "$$\\lambda_{cache}\\in\\{$$",
		"word_idx": 69366,
		"sentence_idx": 1116,
		"label": "none"
	}, {
		"type": "text",
		"expr": "and chose  $\\theta_{cache}=03,\\,\\lambda_{cache}=01,\\,n_{cache}=10000$  by sweeping over the validation set",
		"word_idx": 69463,
		"sentence_idx": 1119,
		"label": "usecase"
	}, {
		"type": "math",
		"expr": "$$\\theta_{cache}=0.3,\\,\\lambda_{cache}=0.1,\\,n_{cache}=10000$$",
		"word_idx": 69569,
		"sentence_idx": 1120,
		"label": "usecase"
	}, {
		"type": "text",
		"expr": "MbPA output interpolation:  $\\lambda_{mbpa}\\in\\{$ 0",
		"word_idx": 69811,
		"sentence_idx": 1123,
		"label": "usecase"
	}, {
		"type": "math",
		"expr": "$$\\lambda_{mbpa}\\in\\{$$",
		"word_idx": 69891,
		"sentence_idx": 1129,
		"label": "usecase"
	}, {
		"type": "text",
		"expr": "and selected  $\\lambda_{mbpa}=004,\\lambda_{cache}=01,\\theta_{cache}=03,K=1024,n_{mbpa}=1,n%\n_{cache}=10000$ ",
		"word_idx": 70052,
		"sentence_idx": 1134,
		"label": "usecase"
	}, {
		"type": "math",
		"expr": "$$\\lambda_{mbpa}=0.04,\\lambda_{cache}=0.1,\\theta_{cache}=0.3,K=1024,n_{mbpa}=1,n%\n_{cache}=10000$$",
		"word_idx": 70364,
		"sentence_idx": 1137,
		"label": "usecase"
	}]
}